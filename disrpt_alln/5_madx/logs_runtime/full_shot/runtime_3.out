-------------------------------------------------------------------
Lang1:  deu.rst.pcc    Lang2:  eng.rst.rstdt
Saving run to:  runs/full_shot/FullShot=v4_deu.rst.pcc_eng.rst.rstdt_lrfunc=Adafactor_lr=2e-05_plm=bert-base-multilingual-cased
Running with params: BERT_MODEL=bert-base-multilingual-cased lr=2e-05
read 2164 examples
read 241 examples
read 260 examples
read 16002 examples
read 1621 examples
read 2155 examples
Total prediction labels:  36
BertAdapterModel(
  (shared_parameters): ModuleDict()
  (bert): BertModel(
    (shared_parameters): ModuleDict()
    (invertible_adapters): ModuleDict(
      (de): NICECouplingBlock(
        (F): Sequential(
          (0): Linear(in_features=384, out_features=192, bias=True)
          (1): Activation_Function_Class(
            (f): NewGELUActivation()
          )
          (2): Linear(in_features=192, out_features=384, bias=True)
        )
        (G): Sequential(
          (0): Linear(in_features=384, out_features=192, bias=True)
          (1): Activation_Function_Class(
            (f): NewGELUActivation()
          )
          (2): Linear(in_features=192, out_features=384, bias=True)
        )
      )
      (en): NICECouplingBlock(
        (F): Sequential(
          (0): Linear(in_features=384, out_features=192, bias=True)
          (1): Activation_Function_Class(
            (f): ReLU()
          )
          (2): Linear(in_features=192, out_features=384, bias=True)
        )
        (G): Sequential(
          (0): Linear(in_features=384, out_features=192, bias=True)
          (1): Activation_Function_Class(
            (f): ReLU()
          )
          (2): Linear(in_features=192, out_features=384, bias=True)
        )
      )
    )
    (embeddings): BertEmbeddings(
      (word_embeddings): Embedding(119547, 768, padding_idx=0)
      (position_embeddings): Embedding(512, 768)
      (token_type_embeddings): Embedding(2, 768)
      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
      (dropout): Dropout(p=0.1, inplace=False)
    )
    (encoder): BertEncoder(
      (layer): ModuleList(
        (0): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (de): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (en): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (1): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (de): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (en): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (2): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (de): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (en): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (3): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (de): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (en): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (4): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (de): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (en): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (5): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (de): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (en): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (6): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (de): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (en): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (7): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (de): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (en): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (8): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (de): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (en): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (9): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (de): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (en): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (10): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (de): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (en): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (11): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (de): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (en): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
      )
    )
    (pooler): BertPooler(
      (dense): Linear(in_features=768, out_features=768, bias=True)
      (activation): Tanh()
    )
    (prefix_tuning): PrefixTuningPool(
      (prefix_tunings): ModuleDict()
    )
  )
  (heads): ModuleDict(
    (disrpt): ClassificationHead(
      (0): Dropout(p=0.1, inplace=False)
      (1): Linear(in_features=768, out_features=768, bias=True)
      (2): Activation_Function_Class(
        (f): Tanh()
      )
      (3): Dropout(p=0.1, inplace=False)
      (4): Linear(in_features=768, out_features=36, bias=True)
    )
  )
)
{'train@deu.rst.pcc_loss': 3.1647326946258545, 'train@deu.rst.pcc_accuracy@deu.rst.pcc': 0.1012014787430684, 'train@deu.rst.pcc_f1@deu.rst.pcc': 0.012907677097988516, 'train@deu.rst.pcc_precision@deu.rst.pcc': 0.010405154413964245, 'train@deu.rst.pcc_recall@deu.rst.pcc': 0.04038104960053624, 'train@deu.rst.pcc_loss@deu.rst.pcc': 3.1647326946258545, 'train@deu.rst.pcc_runtime': 27.1498, 'train@deu.rst.pcc_samples_per_second': 79.706, 'train@deu.rst.pcc_steps_per_second': 2.505, 'epoch': 1.0}
{'loss': 3.3818, 'learning_rate': 1.8333333333333333e-05, 'epoch': 1.0}
{'eval_loss': 3.1885180473327637, 'eval_accuracy@deu.rst.pcc': 0.12033195020746888, 'eval_f1@deu.rst.pcc': 0.013960345210345207, 'eval_precision@deu.rst.pcc': 0.010872960585604266, 'eval_recall@deu.rst.pcc': 0.04487179487179487, 'eval_loss@deu.rst.pcc': 3.1885182857513428, 'eval_runtime': 3.3257, 'eval_samples_per_second': 72.466, 'eval_steps_per_second': 2.406, 'epoch': 1.0}
{'train@deu.rst.pcc_loss': 2.954789876937866, 'train@deu.rst.pcc_accuracy@deu.rst.pcc': 0.11275415896487985, 'train@deu.rst.pcc_f1@deu.rst.pcc': 0.019225786129310442, 'train@deu.rst.pcc_precision@deu.rst.pcc': 0.02880445459341942, 'train@deu.rst.pcc_recall@deu.rst.pcc': 0.045702371682791884, 'train@deu.rst.pcc_loss@deu.rst.pcc': 2.954789876937866, 'train@deu.rst.pcc_runtime': 26.7744, 'train@deu.rst.pcc_samples_per_second': 80.823, 'train@deu.rst.pcc_steps_per_second': 2.54, 'epoch': 2.0}
{'loss': 3.0594, 'learning_rate': 1.6666666666666667e-05, 'epoch': 2.0}
{'eval_loss': 2.996591091156006, 'eval_accuracy@deu.rst.pcc': 0.12448132780082988, 'eval_f1@deu.rst.pcc': 0.015748565466875326, 'eval_precision@deu.rst.pcc': 0.011171043202293202, 'eval_recall@deu.rst.pcc': 0.04635989010989011, 'eval_loss@deu.rst.pcc': 2.9965906143188477, 'eval_runtime': 3.397, 'eval_samples_per_second': 70.945, 'eval_steps_per_second': 2.355, 'epoch': 2.0}
{'train@deu.rst.pcc_loss': 2.889632225036621, 'train@deu.rst.pcc_accuracy@deu.rst.pcc': 0.13909426987061, 'train@deu.rst.pcc_f1@deu.rst.pcc': 0.03830515866812659, 'train@deu.rst.pcc_precision@deu.rst.pcc': 0.10075885720167393, 'train@deu.rst.pcc_recall@deu.rst.pcc': 0.06032743241034512, 'train@deu.rst.pcc_loss@deu.rst.pcc': 2.889631748199463, 'train@deu.rst.pcc_runtime': 26.6705, 'train@deu.rst.pcc_samples_per_second': 81.138, 'train@deu.rst.pcc_steps_per_second': 2.55, 'epoch': 3.0}
{'loss': 2.9483, 'learning_rate': 1.5000000000000002e-05, 'epoch': 3.0}
{'eval_loss': 2.947911500930786, 'eval_accuracy@deu.rst.pcc': 0.14522821576763487, 'eval_f1@deu.rst.pcc': 0.027868010493773798, 'eval_precision@deu.rst.pcc': 0.02226719812504126, 'eval_recall@deu.rst.pcc': 0.061368030118030124, 'eval_loss@deu.rst.pcc': 2.9479117393493652, 'eval_runtime': 3.4204, 'eval_samples_per_second': 70.46, 'eval_steps_per_second': 2.339, 'epoch': 3.0}
{'train@deu.rst.pcc_loss': 2.8409383296966553, 'train@deu.rst.pcc_accuracy@deu.rst.pcc': 0.17652495378927913, 'train@deu.rst.pcc_f1@deu.rst.pcc': 0.07567829259969551, 'train@deu.rst.pcc_precision@deu.rst.pcc': 0.08144388942875061, 'train@deu.rst.pcc_recall@deu.rst.pcc': 0.09500608859471592, 'train@deu.rst.pcc_loss@deu.rst.pcc': 2.840938091278076, 'train@deu.rst.pcc_runtime': 26.7248, 'train@deu.rst.pcc_samples_per_second': 80.973, 'train@deu.rst.pcc_steps_per_second': 2.544, 'epoch': 4.0}
{'loss': 2.9046, 'learning_rate': 1.3333333333333333e-05, 'epoch': 4.0}
{'eval_loss': 2.909276008605957, 'eval_accuracy@deu.rst.pcc': 0.1825726141078838, 'eval_f1@deu.rst.pcc': 0.06889744265770258, 'eval_precision@deu.rst.pcc': 0.08207584934558619, 'eval_recall@deu.rst.pcc': 0.10051638176638178, 'eval_loss@deu.rst.pcc': 2.909276008605957, 'eval_runtime': 3.4006, 'eval_samples_per_second': 70.87, 'eval_steps_per_second': 2.353, 'epoch': 4.0}
{'train@deu.rst.pcc_loss': 2.800462245941162, 'train@deu.rst.pcc_accuracy@deu.rst.pcc': 0.20425138632162662, 'train@deu.rst.pcc_f1@deu.rst.pcc': 0.08637953768708802, 'train@deu.rst.pcc_precision@deu.rst.pcc': 0.0890803390793149, 'train@deu.rst.pcc_recall@deu.rst.pcc': 0.12190726081966605, 'train@deu.rst.pcc_loss@deu.rst.pcc': 2.800462007522583, 'train@deu.rst.pcc_runtime': 26.7463, 'train@deu.rst.pcc_samples_per_second': 80.909, 'train@deu.rst.pcc_steps_per_second': 2.542, 'epoch': 5.0}
{'loss': 2.8553, 'learning_rate': 1.1666666666666668e-05, 'epoch': 5.0}
{'eval_loss': 2.8750569820404053, 'eval_accuracy@deu.rst.pcc': 0.18672199170124482, 'eval_f1@deu.rst.pcc': 0.07946335454015756, 'eval_precision@deu.rst.pcc': 0.07689087228560913, 'eval_recall@deu.rst.pcc': 0.1155499592999593, 'eval_loss@deu.rst.pcc': 2.8750572204589844, 'eval_runtime': 3.3971, 'eval_samples_per_second': 70.944, 'eval_steps_per_second': 2.355, 'epoch': 5.0}
{'train@deu.rst.pcc_loss': 2.7644412517547607, 'train@deu.rst.pcc_accuracy@deu.rst.pcc': 0.21719038817005545, 'train@deu.rst.pcc_f1@deu.rst.pcc': 0.09060669595527389, 'train@deu.rst.pcc_precision@deu.rst.pcc': 0.0900265657284783, 'train@deu.rst.pcc_recall@deu.rst.pcc': 0.1334632987243414, 'train@deu.rst.pcc_loss@deu.rst.pcc': 2.7644412517547607, 'train@deu.rst.pcc_runtime': 26.6198, 'train@deu.rst.pcc_samples_per_second': 81.293, 'train@deu.rst.pcc_steps_per_second': 2.554, 'epoch': 6.0}
{'loss': 2.8148, 'learning_rate': 1e-05, 'epoch': 6.0}
{'eval_loss': 2.843897819519043, 'eval_accuracy@deu.rst.pcc': 0.2033195020746888, 'eval_f1@deu.rst.pcc': 0.08218504808084212, 'eval_precision@deu.rst.pcc': 0.08194210149148089, 'eval_recall@deu.rst.pcc': 0.14010353072853074, 'eval_loss@deu.rst.pcc': 2.843898057937622, 'eval_runtime': 3.2808, 'eval_samples_per_second': 73.457, 'eval_steps_per_second': 2.438, 'epoch': 6.0}
{'train@deu.rst.pcc_loss': 2.7368545532226562, 'train@deu.rst.pcc_accuracy@deu.rst.pcc': 0.22181146025878004, 'train@deu.rst.pcc_f1@deu.rst.pcc': 0.0933467070237444, 'train@deu.rst.pcc_precision@deu.rst.pcc': 0.0923168933790679, 'train@deu.rst.pcc_recall@deu.rst.pcc': 0.1364017033461539, 'train@deu.rst.pcc_loss@deu.rst.pcc': 2.736854314804077, 'train@deu.rst.pcc_runtime': 26.5952, 'train@deu.rst.pcc_samples_per_second': 81.368, 'train@deu.rst.pcc_steps_per_second': 2.557, 'epoch': 7.0}
{'loss': 2.7901, 'learning_rate': 8.333333333333334e-06, 'epoch': 7.0}
{'eval_loss': 2.8215408325195312, 'eval_accuracy@deu.rst.pcc': 0.1991701244813278, 'eval_f1@deu.rst.pcc': 0.08277222515280296, 'eval_precision@deu.rst.pcc': 0.08073578342407743, 'eval_recall@deu.rst.pcc': 0.13944215506715507, 'eval_loss@deu.rst.pcc': 2.821540594100952, 'eval_runtime': 3.3423, 'eval_samples_per_second': 72.107, 'eval_steps_per_second': 2.394, 'epoch': 7.0}
{'train@deu.rst.pcc_loss': 2.7149155139923096, 'train@deu.rst.pcc_accuracy@deu.rst.pcc': 0.2245841035120148, 'train@deu.rst.pcc_f1@deu.rst.pcc': 0.09459647118086753, 'train@deu.rst.pcc_precision@deu.rst.pcc': 0.09722165250862726, 'train@deu.rst.pcc_recall@deu.rst.pcc': 0.1383167218293589, 'train@deu.rst.pcc_loss@deu.rst.pcc': 2.7149152755737305, 'train@deu.rst.pcc_runtime': 26.6933, 'train@deu.rst.pcc_samples_per_second': 81.069, 'train@deu.rst.pcc_steps_per_second': 2.547, 'epoch': 8.0}
{'loss': 2.7598, 'learning_rate': 6.666666666666667e-06, 'epoch': 8.0}
{'eval_loss': 2.8052172660827637, 'eval_accuracy@deu.rst.pcc': 0.1950207468879668, 'eval_f1@deu.rst.pcc': 0.08289433818201271, 'eval_precision@deu.rst.pcc': 0.09553563629330138, 'eval_recall@deu.rst.pcc': 0.13712734025234025, 'eval_loss@deu.rst.pcc': 2.8052172660827637, 'eval_runtime': 3.3677, 'eval_samples_per_second': 71.563, 'eval_steps_per_second': 2.376, 'epoch': 8.0}
{'train@deu.rst.pcc_loss': 2.6977083683013916, 'train@deu.rst.pcc_accuracy@deu.rst.pcc': 0.22643253234750463, 'train@deu.rst.pcc_f1@deu.rst.pcc': 0.09613926562752374, 'train@deu.rst.pcc_precision@deu.rst.pcc': 0.09755835396167437, 'train@deu.rst.pcc_recall@deu.rst.pcc': 0.13955514598345467, 'train@deu.rst.pcc_loss@deu.rst.pcc': 2.6977086067199707, 'train@deu.rst.pcc_runtime': 26.6661, 'train@deu.rst.pcc_samples_per_second': 81.152, 'train@deu.rst.pcc_steps_per_second': 2.55, 'epoch': 9.0}
{'loss': 2.7503, 'learning_rate': 5e-06, 'epoch': 9.0}
{'eval_loss': 2.7940306663513184, 'eval_accuracy@deu.rst.pcc': 0.1908713692946058, 'eval_f1@deu.rst.pcc': 0.08204032066599272, 'eval_precision@deu.rst.pcc': 0.09266697573149185, 'eval_recall@deu.rst.pcc': 0.135639245014245, 'eval_loss@deu.rst.pcc': 2.7940304279327393, 'eval_runtime': 3.3385, 'eval_samples_per_second': 72.188, 'eval_steps_per_second': 2.396, 'epoch': 9.0}
{'train@deu.rst.pcc_loss': 2.685985803604126, 'train@deu.rst.pcc_accuracy@deu.rst.pcc': 0.22504621072088724, 'train@deu.rst.pcc_f1@deu.rst.pcc': 0.09575001096813315, 'train@deu.rst.pcc_precision@deu.rst.pcc': 0.0960294759313701, 'train@deu.rst.pcc_recall@deu.rst.pcc': 0.13940618661651796, 'train@deu.rst.pcc_loss@deu.rst.pcc': 2.6859853267669678, 'train@deu.rst.pcc_runtime': 26.6977, 'train@deu.rst.pcc_samples_per_second': 81.056, 'train@deu.rst.pcc_steps_per_second': 2.547, 'epoch': 10.0}
{'loss': 2.7245, 'learning_rate': 3.3333333333333333e-06, 'epoch': 10.0}
{'eval_loss': 2.782654047012329, 'eval_accuracy@deu.rst.pcc': 0.18672199170124482, 'eval_f1@deu.rst.pcc': 0.07909452197614641, 'eval_precision@deu.rst.pcc': 0.0892608910754072, 'eval_recall@deu.rst.pcc': 0.1333244301994302, 'eval_loss@deu.rst.pcc': 2.782654047012329, 'eval_runtime': 3.3525, 'eval_samples_per_second': 71.886, 'eval_steps_per_second': 2.386, 'epoch': 10.0}
{'train@deu.rst.pcc_loss': 2.6788268089294434, 'train@deu.rst.pcc_accuracy@deu.rst.pcc': 0.2268946395563771, 'train@deu.rst.pcc_f1@deu.rst.pcc': 0.09695863145884871, 'train@deu.rst.pcc_precision@deu.rst.pcc': 0.1329102197801633, 'train@deu.rst.pcc_recall@deu.rst.pcc': 0.14049992737787587, 'train@deu.rst.pcc_loss@deu.rst.pcc': 2.6788268089294434, 'train@deu.rst.pcc_runtime': 26.7476, 'train@deu.rst.pcc_samples_per_second': 80.904, 'train@deu.rst.pcc_steps_per_second': 2.542, 'epoch': 11.0}
{'loss': 2.7202, 'learning_rate': 1.6666666666666667e-06, 'epoch': 11.0}
{'eval_loss': 2.7771270275115967, 'eval_accuracy@deu.rst.pcc': 0.1950207468879668, 'eval_f1@deu.rst.pcc': 0.08292861061678265, 'eval_precision@deu.rst.pcc': 0.0933398561037318, 'eval_recall@deu.rst.pcc': 0.13712734025234025, 'eval_loss@deu.rst.pcc': 2.7771267890930176, 'eval_runtime': 3.3334, 'eval_samples_per_second': 72.299, 'eval_steps_per_second': 2.4, 'epoch': 11.0}
{'train@deu.rst.pcc_loss': 2.6764743328094482, 'train@deu.rst.pcc_accuracy@deu.rst.pcc': 0.22597042513863216, 'train@deu.rst.pcc_f1@deu.rst.pcc': 0.09655109409603718, 'train@deu.rst.pcc_precision@deu.rst.pcc': 0.13265792574903384, 'train@deu.rst.pcc_recall@deu.rst.pcc': 0.13994040520841527, 'train@deu.rst.pcc_loss@deu.rst.pcc': 2.6764748096466064, 'train@deu.rst.pcc_runtime': 26.6713, 'train@deu.rst.pcc_samples_per_second': 81.136, 'train@deu.rst.pcc_steps_per_second': 2.55, 'epoch': 12.0}
{'loss': 2.7064, 'learning_rate': 0.0, 'epoch': 12.0}
{'eval_loss': 2.7746312618255615, 'eval_accuracy@deu.rst.pcc': 0.1950207468879668, 'eval_f1@deu.rst.pcc': 0.08306566492050363, 'eval_precision@deu.rst.pcc': 0.09440430260668638, 'eval_recall@deu.rst.pcc': 0.13712734025234025, 'eval_loss@deu.rst.pcc': 2.7746312618255615, 'eval_runtime': 3.3151, 'eval_samples_per_second': 72.699, 'eval_steps_per_second': 2.413, 'epoch': 12.0}
{'train_runtime': 1040.0561, 'train_samples_per_second': 24.968, 'train_steps_per_second': 0.785, 'train_loss': 2.8679488312964345, 'epoch': 12.0}
***** train_log metrics *****
  epoch                    =       12.0
  train_loss               =     2.8679
  train_runtime            = 0:17:20.05
  train_samples_per_second =     24.968
  train_steps_per_second   =      0.785
{'train@eng.rst.rstdt_loss': 1.6046650409698486, 'train@eng.rst.rstdt_accuracy@eng.rst.rstdt': 0.5481189851268592, 'train@eng.rst.rstdt_f1@eng.rst.rstdt': 0.11659099867145643, 'train@eng.rst.rstdt_precision@eng.rst.rstdt': 0.16304393311949908, 'train@eng.rst.rstdt_recall@eng.rst.rstdt': 0.1350156967748151, 'train@eng.rst.rstdt_loss@eng.rst.rstdt': 1.6046650409698486, 'train@eng.rst.rstdt_runtime': 190.8775, 'train@eng.rst.rstdt_samples_per_second': 83.834, 'train@eng.rst.rstdt_steps_per_second': 2.625, 'epoch': 1.0}
{'loss': 2.0286, 'learning_rate': 1.8333333333333333e-05, 'epoch': 1.0}
{'eval_loss': 1.5945777893066406, 'eval_accuracy@eng.rst.rstdt': 0.5533621221468229, 'eval_f1@eng.rst.rstdt': 0.12258487062049095, 'eval_precision@eng.rst.rstdt': 0.16614703147837487, 'eval_recall@eng.rst.rstdt': 0.13737086755108663, 'eval_loss@eng.rst.rstdt': 1.5945775508880615, 'eval_runtime': 22.5422, 'eval_samples_per_second': 71.909, 'eval_steps_per_second': 2.262, 'epoch': 1.0}
{'train@eng.rst.rstdt_loss': 1.3903677463531494, 'train@eng.rst.rstdt_accuracy@eng.rst.rstdt': 0.614360704911886, 'train@eng.rst.rstdt_f1@eng.rst.rstdt': 0.21529003331859262, 'train@eng.rst.rstdt_precision@eng.rst.rstdt': 0.35262314612457885, 'train@eng.rst.rstdt_recall@eng.rst.rstdt': 0.22643201721826256, 'train@eng.rst.rstdt_loss@eng.rst.rstdt': 1.390367865562439, 'train@eng.rst.rstdt_runtime': 191.2771, 'train@eng.rst.rstdt_samples_per_second': 83.659, 'train@eng.rst.rstdt_steps_per_second': 2.619, 'epoch': 2.0}
{'loss': 1.5322, 'learning_rate': 1.6666666666666667e-05, 'epoch': 2.0}
{'eval_loss': 1.4020085334777832, 'eval_accuracy@eng.rst.rstdt': 0.6199876619370759, 'eval_f1@eng.rst.rstdt': 0.21769025875991294, 'eval_precision@eng.rst.rstdt': 0.29651880153517285, 'eval_recall@eng.rst.rstdt': 0.22786895856479594, 'eval_loss@eng.rst.rstdt': 1.4020086526870728, 'eval_runtime': 19.7478, 'eval_samples_per_second': 82.085, 'eval_steps_per_second': 2.583, 'epoch': 2.0}
{'train@eng.rst.rstdt_loss': 1.292060375213623, 'train@eng.rst.rstdt_accuracy@eng.rst.rstdt': 0.6381077365329334, 'train@eng.rst.rstdt_f1@eng.rst.rstdt': 0.30649519023515576, 'train@eng.rst.rstdt_precision@eng.rst.rstdt': 0.45165052599727823, 'train@eng.rst.rstdt_recall@eng.rst.rstdt': 0.2889558852951965, 'train@eng.rst.rstdt_loss@eng.rst.rstdt': 1.292060375213623, 'train@eng.rst.rstdt_runtime': 190.6672, 'train@eng.rst.rstdt_samples_per_second': 83.926, 'train@eng.rst.rstdt_steps_per_second': 2.628, 'epoch': 3.0}
{'loss': 1.4007, 'learning_rate': 1.5000000000000002e-05, 'epoch': 3.0}
{'eval_loss': 1.328244924545288, 'eval_accuracy@eng.rst.rstdt': 0.6366440468846392, 'eval_f1@eng.rst.rstdt': 0.3009589221466392, 'eval_precision@eng.rst.rstdt': 0.4187222525598232, 'eval_recall@eng.rst.rstdt': 0.2876462112878876, 'eval_loss@eng.rst.rstdt': 1.328244924545288, 'eval_runtime': 19.8351, 'eval_samples_per_second': 81.724, 'eval_steps_per_second': 2.571, 'epoch': 3.0}
{'train@eng.rst.rstdt_loss': 1.229559302330017, 'train@eng.rst.rstdt_accuracy@eng.rst.rstdt': 0.6498562679665042, 'train@eng.rst.rstdt_f1@eng.rst.rstdt': 0.34275855556925405, 'train@eng.rst.rstdt_precision@eng.rst.rstdt': 0.4398142269067755, 'train@eng.rst.rstdt_recall@eng.rst.rstdt': 0.32847694502785635, 'train@eng.rst.rstdt_loss@eng.rst.rstdt': 1.2295591831207275, 'train@eng.rst.rstdt_runtime': 191.0927, 'train@eng.rst.rstdt_samples_per_second': 83.739, 'train@eng.rst.rstdt_steps_per_second': 2.622, 'epoch': 4.0}
{'loss': 1.3091, 'learning_rate': 1.3333333333333333e-05, 'epoch': 4.0}
{'eval_loss': 1.29438054561615, 'eval_accuracy@eng.rst.rstdt': 0.634793337446021, 'eval_f1@eng.rst.rstdt': 0.3255914072275778, 'eval_precision@eng.rst.rstdt': 0.39846291540912626, 'eval_recall@eng.rst.rstdt': 0.32425999576634984, 'eval_loss@eng.rst.rstdt': 1.29438054561615, 'eval_runtime': 19.8489, 'eval_samples_per_second': 81.667, 'eval_steps_per_second': 2.569, 'epoch': 4.0}
{'train@eng.rst.rstdt_loss': 1.1834887266159058, 'train@eng.rst.rstdt_accuracy@eng.rst.rstdt': 0.6591676040494938, 'train@eng.rst.rstdt_f1@eng.rst.rstdt': 0.36158753213330413, 'train@eng.rst.rstdt_precision@eng.rst.rstdt': 0.5275814785027149, 'train@eng.rst.rstdt_recall@eng.rst.rstdt': 0.3388464760490919, 'train@eng.rst.rstdt_loss@eng.rst.rstdt': 1.1834888458251953, 'train@eng.rst.rstdt_runtime': 190.9044, 'train@eng.rst.rstdt_samples_per_second': 83.822, 'train@eng.rst.rstdt_steps_per_second': 2.624, 'epoch': 5.0}
{'loss': 1.2611, 'learning_rate': 1.1666666666666668e-05, 'epoch': 5.0}
{'eval_loss': 1.2541570663452148, 'eval_accuracy@eng.rst.rstdt': 0.6378778531770513, 'eval_f1@eng.rst.rstdt': 0.32322743828268724, 'eval_precision@eng.rst.rstdt': 0.3950912836957039, 'eval_recall@eng.rst.rstdt': 0.32466210922597905, 'eval_loss@eng.rst.rstdt': 1.2541571855545044, 'eval_runtime': 19.7927, 'eval_samples_per_second': 81.899, 'eval_steps_per_second': 2.577, 'epoch': 5.0}
{'train@eng.rst.rstdt_loss': 1.1532658338546753, 'train@eng.rst.rstdt_accuracy@eng.rst.rstdt': 0.664729408823897, 'train@eng.rst.rstdt_f1@eng.rst.rstdt': 0.3797810850592056, 'train@eng.rst.rstdt_precision@eng.rst.rstdt': 0.5845596763552261, 'train@eng.rst.rstdt_recall@eng.rst.rstdt': 0.349541704651729, 'train@eng.rst.rstdt_loss@eng.rst.rstdt': 1.1532657146453857, 'train@eng.rst.rstdt_runtime': 190.923, 'train@eng.rst.rstdt_samples_per_second': 83.814, 'train@eng.rst.rstdt_steps_per_second': 2.624, 'epoch': 6.0}
{'loss': 1.2199, 'learning_rate': 1e-05, 'epoch': 6.0}
{'eval_loss': 1.2294950485229492, 'eval_accuracy@eng.rst.rstdt': 0.6440468846391116, 'eval_f1@eng.rst.rstdt': 0.33740712686863816, 'eval_precision@eng.rst.rstdt': 0.45271727636494874, 'eval_recall@eng.rst.rstdt': 0.3307478072960849, 'eval_loss@eng.rst.rstdt': 1.2294951677322388, 'eval_runtime': 19.9383, 'eval_samples_per_second': 81.301, 'eval_steps_per_second': 2.558, 'epoch': 6.0}
{'train@eng.rst.rstdt_loss': 1.1359503269195557, 'train@eng.rst.rstdt_accuracy@eng.rst.rstdt': 0.6671041119860017, 'train@eng.rst.rstdt_f1@eng.rst.rstdt': 0.3893113322908412, 'train@eng.rst.rstdt_precision@eng.rst.rstdt': 0.6822180020255616, 'train@eng.rst.rstdt_recall@eng.rst.rstdt': 0.3558741594385093, 'train@eng.rst.rstdt_loss@eng.rst.rstdt': 1.1359503269195557, 'train@eng.rst.rstdt_runtime': 191.6277, 'train@eng.rst.rstdt_samples_per_second': 83.506, 'train@eng.rst.rstdt_steps_per_second': 2.614, 'epoch': 7.0}
{'loss': 1.1981, 'learning_rate': 8.333333333333334e-06, 'epoch': 7.0}
{'eval_loss': 1.2137713432312012, 'eval_accuracy@eng.rst.rstdt': 0.6434299814929056, 'eval_f1@eng.rst.rstdt': 0.3473548033981998, 'eval_precision@eng.rst.rstdt': 0.511169554228335, 'eval_recall@eng.rst.rstdt': 0.3356693455546328, 'eval_loss@eng.rst.rstdt': 1.2137713432312012, 'eval_runtime': 19.9058, 'eval_samples_per_second': 81.434, 'eval_steps_per_second': 2.562, 'epoch': 7.0}
{'train@eng.rst.rstdt_loss': 1.1187492609024048, 'train@eng.rst.rstdt_accuracy@eng.rst.rstdt': 0.6704161979752531, 'train@eng.rst.rstdt_f1@eng.rst.rstdt': 0.4033182599501968, 'train@eng.rst.rstdt_precision@eng.rst.rstdt': 0.604102144807336, 'train@eng.rst.rstdt_recall@eng.rst.rstdt': 0.3702485004308227, 'train@eng.rst.rstdt_loss@eng.rst.rstdt': 1.1187492609024048, 'train@eng.rst.rstdt_runtime': 190.7491, 'train@eng.rst.rstdt_samples_per_second': 83.89, 'train@eng.rst.rstdt_steps_per_second': 2.626, 'epoch': 8.0}
{'loss': 1.1779, 'learning_rate': 6.666666666666667e-06, 'epoch': 8.0}
{'eval_loss': 1.2109609842300415, 'eval_accuracy@eng.rst.rstdt': 0.6421961752004935, 'eval_f1@eng.rst.rstdt': 0.35556267213251136, 'eval_precision@eng.rst.rstdt': 0.5097521101869148, 'eval_recall@eng.rst.rstdt': 0.3448225026921189, 'eval_loss@eng.rst.rstdt': 1.210960865020752, 'eval_runtime': 19.7735, 'eval_samples_per_second': 81.978, 'eval_steps_per_second': 2.579, 'epoch': 8.0}
{'train@eng.rst.rstdt_loss': 1.1077892780303955, 'train@eng.rst.rstdt_accuracy@eng.rst.rstdt': 0.671666041744782, 'train@eng.rst.rstdt_f1@eng.rst.rstdt': 0.4058274430903536, 'train@eng.rst.rstdt_precision@eng.rst.rstdt': 0.6095692550376883, 'train@eng.rst.rstdt_recall@eng.rst.rstdt': 0.3702741651839286, 'train@eng.rst.rstdt_loss@eng.rst.rstdt': 1.1077892780303955, 'train@eng.rst.rstdt_runtime': 191.2671, 'train@eng.rst.rstdt_samples_per_second': 83.663, 'train@eng.rst.rstdt_steps_per_second': 2.619, 'epoch': 9.0}
{'loss': 1.167, 'learning_rate': 5e-06, 'epoch': 9.0}
{'eval_loss': 1.199431300163269, 'eval_accuracy@eng.rst.rstdt': 0.6440468846391116, 'eval_f1@eng.rst.rstdt': 0.35700503440318254, 'eval_precision@eng.rst.rstdt': 0.5193513058316405, 'eval_recall@eng.rst.rstdt': 0.3429904859518172, 'eval_loss@eng.rst.rstdt': 1.1994311809539795, 'eval_runtime': 19.9246, 'eval_samples_per_second': 81.357, 'eval_steps_per_second': 2.56, 'epoch': 9.0}
{'train@eng.rst.rstdt_loss': 1.1040716171264648, 'train@eng.rst.rstdt_accuracy@eng.rst.rstdt': 0.6727909011373578, 'train@eng.rst.rstdt_f1@eng.rst.rstdt': 0.4214234671904161, 'train@eng.rst.rstdt_precision@eng.rst.rstdt': 0.6064138627740361, 'train@eng.rst.rstdt_recall@eng.rst.rstdt': 0.38547787822307467, 'train@eng.rst.rstdt_loss@eng.rst.rstdt': 1.1040717363357544, 'train@eng.rst.rstdt_runtime': 191.3674, 'train@eng.rst.rstdt_samples_per_second': 83.619, 'train@eng.rst.rstdt_steps_per_second': 2.618, 'epoch': 10.0}
{'loss': 1.1545, 'learning_rate': 3.3333333333333333e-06, 'epoch': 10.0}
{'eval_loss': 1.2063634395599365, 'eval_accuracy@eng.rst.rstdt': 0.6354102405922271, 'eval_f1@eng.rst.rstdt': 0.35941852409918534, 'eval_precision@eng.rst.rstdt': 0.47534528172456614, 'eval_recall@eng.rst.rstdt': 0.3513875628955821, 'eval_loss@eng.rst.rstdt': 1.206363558769226, 'eval_runtime': 19.7875, 'eval_samples_per_second': 81.92, 'eval_steps_per_second': 2.577, 'epoch': 10.0}
{'train@eng.rst.rstdt_loss': 1.096543312072754, 'train@eng.rst.rstdt_accuracy@eng.rst.rstdt': 0.6738532683414573, 'train@eng.rst.rstdt_f1@eng.rst.rstdt': 0.4169707736707416, 'train@eng.rst.rstdt_precision@eng.rst.rstdt': 0.6100026128211633, 'train@eng.rst.rstdt_recall@eng.rst.rstdt': 0.3794322371086018, 'train@eng.rst.rstdt_loss@eng.rst.rstdt': 1.096543312072754, 'train@eng.rst.rstdt_runtime': 190.9016, 'train@eng.rst.rstdt_samples_per_second': 83.823, 'train@eng.rst.rstdt_steps_per_second': 2.624, 'epoch': 11.0}
{'loss': 1.1506, 'learning_rate': 1.6666666666666667e-06, 'epoch': 11.0}
{'eval_loss': 1.1968544721603394, 'eval_accuracy@eng.rst.rstdt': 0.6415792720542874, 'eval_f1@eng.rst.rstdt': 0.3582305222449108, 'eval_precision@eng.rst.rstdt': 0.45168207430666557, 'eval_recall@eng.rst.rstdt': 0.34732221406726843, 'eval_loss@eng.rst.rstdt': 1.1968543529510498, 'eval_runtime': 19.7981, 'eval_samples_per_second': 81.876, 'eval_steps_per_second': 2.576, 'epoch': 11.0}
{'train@eng.rst.rstdt_loss': 1.095375418663025, 'train@eng.rst.rstdt_accuracy@eng.rst.rstdt': 0.6739157605299337, 'train@eng.rst.rstdt_f1@eng.rst.rstdt': 0.41697635878392775, 'train@eng.rst.rstdt_precision@eng.rst.rstdt': 0.6126809896434365, 'train@eng.rst.rstdt_recall@eng.rst.rstdt': 0.3792773018306277, 'train@eng.rst.rstdt_loss@eng.rst.rstdt': 1.0953752994537354, 'train@eng.rst.rstdt_runtime': 190.8749, 'train@eng.rst.rstdt_samples_per_second': 83.835, 'train@eng.rst.rstdt_steps_per_second': 2.625, 'epoch': 12.0}
{'loss': 1.1457, 'learning_rate': 0.0, 'epoch': 12.0}
{'eval_loss': 1.1955556869506836, 'eval_accuracy@eng.rst.rstdt': 0.6434299814929056, 'eval_f1@eng.rst.rstdt': 0.35874288533683324, 'eval_precision@eng.rst.rstdt': 0.45129795679575846, 'eval_recall@eng.rst.rstdt': 0.3477318168439885, 'eval_loss@eng.rst.rstdt': 1.1955559253692627, 'eval_runtime': 19.7811, 'eval_samples_per_second': 81.947, 'eval_steps_per_second': 2.578, 'epoch': 12.0}
{'train_runtime': 7410.4105, 'train_samples_per_second': 25.913, 'train_steps_per_second': 0.811, 'train_loss': 1.3121121546782737, 'epoch': 12.0}
***** train_logger metrics *****
  epoch                    =       12.0
  train_loss               =     2.8679
  train_runtime            = 0:17:20.05
  train_samples_per_second =     24.968
  train_steps_per_second   =      0.785
-------------------------------------------------------------------
Lang1:  eng.pdtb.pdtb    Lang2:  eng.rst.rstdt
Saving run to:  runs/full_shot/FullShot=v4_eng.pdtb.pdtb_eng.rst.rstdt_lrfunc=Adafactor_lr=2e-05_plm=bert-base-multilingual-cased
Running with params: BERT_MODEL=bert-base-multilingual-cased lr=2e-05
read 43920 examples
read 1674 examples
read 2257 examples
read 16002 examples
read 1621 examples
read 2155 examples
Total prediction labels:  40
BertAdapterModel(
  (shared_parameters): ModuleDict()
  (bert): BertModel(
    (shared_parameters): ModuleDict()
    (invertible_adapters): ModuleDict(
      (en): NICECouplingBlock(
        (F): Sequential(
          (0): Linear(in_features=384, out_features=192, bias=True)
          (1): Activation_Function_Class(
            (f): ReLU()
          )
          (2): Linear(in_features=192, out_features=384, bias=True)
        )
        (G): Sequential(
          (0): Linear(in_features=384, out_features=192, bias=True)
          (1): Activation_Function_Class(
            (f): ReLU()
          )
          (2): Linear(in_features=192, out_features=384, bias=True)
        )
      )
    )
    (embeddings): BertEmbeddings(
      (word_embeddings): Embedding(119547, 768, padding_idx=0)
      (position_embeddings): Embedding(512, 768)
      (token_type_embeddings): Embedding(2, 768)
      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
      (dropout): Dropout(p=0.1, inplace=False)
    )
    (encoder): BertEncoder(
      (layer): ModuleList(
        (0): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (en): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (1): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (en): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (2): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (en): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (3): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (en): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (4): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (en): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (5): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (en): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (6): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (en): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (7): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (en): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (8): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (en): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (9): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (en): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (10): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (en): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (11): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (en): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
      )
    )
    (pooler): BertPooler(
      (dense): Linear(in_features=768, out_features=768, bias=True)
      (activation): Tanh()
    )
    (prefix_tuning): PrefixTuningPool(
      (prefix_tunings): ModuleDict()
    )
  )
  (heads): ModuleDict(
    (disrpt): ClassificationHead(
      (0): Dropout(p=0.1, inplace=False)
      (1): Linear(in_features=768, out_features=768, bias=True)
      (2): Activation_Function_Class(
        (f): Tanh()
      )
      (3): Dropout(p=0.1, inplace=False)
      (4): Linear(in_features=768, out_features=40, bias=True)
    )
  )
)
{'train@eng.pdtb.pdtb_loss': 1.2803131341934204, 'train@eng.pdtb.pdtb_accuracy@eng.pdtb.pdtb': 0.5941256830601093, 'train@eng.pdtb.pdtb_f1@eng.pdtb.pdtb': 0.2551887581526308, 'train@eng.pdtb.pdtb_precision@eng.pdtb.pdtb': 0.3256225292063163, 'train@eng.pdtb.pdtb_recall@eng.pdtb.pdtb': 0.2507620278245817, 'train@eng.pdtb.pdtb_loss@eng.pdtb.pdtb': 1.2803131341934204, 'train@eng.pdtb.pdtb_runtime': 524.4838, 'train@eng.pdtb.pdtb_samples_per_second': 83.739, 'train@eng.pdtb.pdtb_steps_per_second': 2.618, 'epoch': 1.0}
{'loss': 1.8719, 'learning_rate': 1.8333333333333333e-05, 'epoch': 1.0}
{'eval_loss': 1.1936198472976685, 'eval_accuracy@eng.pdtb.pdtb': 0.6272401433691757, 'eval_f1@eng.pdtb.pdtb': 0.29875529309395743, 'eval_precision@eng.pdtb.pdtb': 0.33720003922775355, 'eval_recall@eng.pdtb.pdtb': 0.29733481856902433, 'eval_loss@eng.pdtb.pdtb': 1.1936198472976685, 'eval_runtime': 20.5586, 'eval_samples_per_second': 81.426, 'eval_steps_per_second': 2.578, 'epoch': 1.0}
{'train@eng.pdtb.pdtb_loss': 1.1074235439300537, 'train@eng.pdtb.pdtb_accuracy@eng.pdtb.pdtb': 0.6358151183970856, 'train@eng.pdtb.pdtb_f1@eng.pdtb.pdtb': 0.361371973541717, 'train@eng.pdtb.pdtb_precision@eng.pdtb.pdtb': 0.4812102844960274, 'train@eng.pdtb.pdtb_recall@eng.pdtb.pdtb': 0.34686463191497535, 'train@eng.pdtb.pdtb_loss@eng.pdtb.pdtb': 1.1074236631393433, 'train@eng.pdtb.pdtb_runtime': 524.923, 'train@eng.pdtb.pdtb_samples_per_second': 83.669, 'train@eng.pdtb.pdtb_steps_per_second': 2.616, 'epoch': 2.0}
{'loss': 1.2365, 'learning_rate': 1.6666666666666667e-05, 'epoch': 2.0}
{'eval_loss': 1.0355963706970215, 'eval_accuracy@eng.pdtb.pdtb': 0.6642771804062126, 'eval_f1@eng.pdtb.pdtb': 0.38768957163058204, 'eval_precision@eng.pdtb.pdtb': 0.4280406108653835, 'eval_recall@eng.pdtb.pdtb': 0.3811575564102784, 'eval_loss@eng.pdtb.pdtb': 1.0355963706970215, 'eval_runtime': 23.2142, 'eval_samples_per_second': 72.111, 'eval_steps_per_second': 2.283, 'epoch': 2.0}
{'train@eng.pdtb.pdtb_loss': 1.056363582611084, 'train@eng.pdtb.pdtb_accuracy@eng.pdtb.pdtb': 0.6510701275045537, 'train@eng.pdtb.pdtb_f1@eng.pdtb.pdtb': 0.42802018429630906, 'train@eng.pdtb.pdtb_precision@eng.pdtb.pdtb': 0.4680155218841738, 'train@eng.pdtb.pdtb_recall@eng.pdtb.pdtb': 0.411236348554233, 'train@eng.pdtb.pdtb_loss@eng.pdtb.pdtb': 1.0563637018203735, 'train@eng.pdtb.pdtb_runtime': 524.9414, 'train@eng.pdtb.pdtb_samples_per_second': 83.666, 'train@eng.pdtb.pdtb_steps_per_second': 2.616, 'epoch': 3.0}
{'loss': 1.1309, 'learning_rate': 1.5000000000000002e-05, 'epoch': 3.0}
{'eval_loss': 0.9952160716056824, 'eval_accuracy@eng.pdtb.pdtb': 0.6810035842293907, 'eval_f1@eng.pdtb.pdtb': 0.47156272111818287, 'eval_precision@eng.pdtb.pdtb': 0.5352531404255803, 'eval_recall@eng.pdtb.pdtb': 0.44926118575363433, 'eval_loss@eng.pdtb.pdtb': 0.9952160716056824, 'eval_runtime': 20.4933, 'eval_samples_per_second': 81.685, 'eval_steps_per_second': 2.586, 'epoch': 3.0}
{'train@eng.pdtb.pdtb_loss': 1.007939100265503, 'train@eng.pdtb.pdtb_accuracy@eng.pdtb.pdtb': 0.6638433515482696, 'train@eng.pdtb.pdtb_f1@eng.pdtb.pdtb': 0.44355587422160114, 'train@eng.pdtb.pdtb_precision@eng.pdtb.pdtb': 0.5166229012696077, 'train@eng.pdtb.pdtb_recall@eng.pdtb.pdtb': 0.43477577281302354, 'train@eng.pdtb.pdtb_loss@eng.pdtb.pdtb': 1.007939100265503, 'train@eng.pdtb.pdtb_runtime': 524.7747, 'train@eng.pdtb.pdtb_samples_per_second': 83.693, 'train@eng.pdtb.pdtb_steps_per_second': 2.616, 'epoch': 4.0}
{'loss': 1.0826, 'learning_rate': 1.3333333333333333e-05, 'epoch': 4.0}
{'eval_loss': 0.9554979205131531, 'eval_accuracy@eng.pdtb.pdtb': 0.6804062126642771, 'eval_f1@eng.pdtb.pdtb': 0.48409082165727585, 'eval_precision@eng.pdtb.pdtb': 0.5374049778520338, 'eval_recall@eng.pdtb.pdtb': 0.46418656958466353, 'eval_loss@eng.pdtb.pdtb': 0.9554978013038635, 'eval_runtime': 20.4762, 'eval_samples_per_second': 81.753, 'eval_steps_per_second': 2.588, 'epoch': 4.0}
{'train@eng.pdtb.pdtb_loss': 0.9852407574653625, 'train@eng.pdtb.pdtb_accuracy@eng.pdtb.pdtb': 0.6704690346083789, 'train@eng.pdtb.pdtb_f1@eng.pdtb.pdtb': 0.4524597805388526, 'train@eng.pdtb.pdtb_precision@eng.pdtb.pdtb': 0.5210117140114946, 'train@eng.pdtb.pdtb_recall@eng.pdtb.pdtb': 0.4462122556097142, 'train@eng.pdtb.pdtb_loss@eng.pdtb.pdtb': 0.985240638256073, 'train@eng.pdtb.pdtb_runtime': 524.4183, 'train@eng.pdtb.pdtb_samples_per_second': 83.75, 'train@eng.pdtb.pdtb_steps_per_second': 2.618, 'epoch': 5.0}
{'loss': 1.052, 'learning_rate': 1.1666666666666668e-05, 'epoch': 5.0}
{'eval_loss': 0.9367193579673767, 'eval_accuracy@eng.pdtb.pdtb': 0.6893667861409797, 'eval_f1@eng.pdtb.pdtb': 0.5202254361828725, 'eval_precision@eng.pdtb.pdtb': 0.5614643026403273, 'eval_recall@eng.pdtb.pdtb': 0.5058092103520555, 'eval_loss@eng.pdtb.pdtb': 0.9367193579673767, 'eval_runtime': 20.4691, 'eval_samples_per_second': 81.782, 'eval_steps_per_second': 2.589, 'epoch': 5.0}
{'train@eng.pdtb.pdtb_loss': 0.964794933795929, 'train@eng.pdtb.pdtb_accuracy@eng.pdtb.pdtb': 0.676183970856102, 'train@eng.pdtb.pdtb_f1@eng.pdtb.pdtb': 0.46025358291176155, 'train@eng.pdtb.pdtb_precision@eng.pdtb.pdtb': 0.524868735836793, 'train@eng.pdtb.pdtb_recall@eng.pdtb.pdtb': 0.4542594931429949, 'train@eng.pdtb.pdtb_loss@eng.pdtb.pdtb': 0.9647949934005737, 'train@eng.pdtb.pdtb_runtime': 518.9923, 'train@eng.pdtb.pdtb_samples_per_second': 84.626, 'train@eng.pdtb.pdtb_steps_per_second': 2.646, 'epoch': 6.0}
{'loss': 1.0295, 'learning_rate': 1e-05, 'epoch': 6.0}
{'eval_loss': 0.9273382425308228, 'eval_accuracy@eng.pdtb.pdtb': 0.6851851851851852, 'eval_f1@eng.pdtb.pdtb': 0.5173283032671092, 'eval_precision@eng.pdtb.pdtb': 0.5526182976869825, 'eval_recall@eng.pdtb.pdtb': 0.5078909987717714, 'eval_loss@eng.pdtb.pdtb': 0.9273382425308228, 'eval_runtime': 20.1545, 'eval_samples_per_second': 83.058, 'eval_steps_per_second': 2.63, 'epoch': 6.0}
{'train@eng.pdtb.pdtb_loss': 0.9542576670646667, 'train@eng.pdtb.pdtb_accuracy@eng.pdtb.pdtb': 0.6781420765027323, 'train@eng.pdtb.pdtb_f1@eng.pdtb.pdtb': 0.46231196755025983, 'train@eng.pdtb.pdtb_precision@eng.pdtb.pdtb': 0.5330459321388986, 'train@eng.pdtb.pdtb_recall@eng.pdtb.pdtb': 0.45234197874821175, 'train@eng.pdtb.pdtb_loss@eng.pdtb.pdtb': 0.9542577266693115, 'train@eng.pdtb.pdtb_runtime': 518.9037, 'train@eng.pdtb.pdtb_samples_per_second': 84.64, 'train@eng.pdtb.pdtb_steps_per_second': 2.646, 'epoch': 7.0}
{'loss': 1.0122, 'learning_rate': 8.333333333333334e-06, 'epoch': 7.0}
{'eval_loss': 0.913476824760437, 'eval_accuracy@eng.pdtb.pdtb': 0.6917562724014337, 'eval_f1@eng.pdtb.pdtb': 0.5262035843469097, 'eval_precision@eng.pdtb.pdtb': 0.5780430555623955, 'eval_recall@eng.pdtb.pdtb': 0.5109761482539326, 'eval_loss@eng.pdtb.pdtb': 0.913476824760437, 'eval_runtime': 20.1495, 'eval_samples_per_second': 83.079, 'eval_steps_per_second': 2.63, 'epoch': 7.0}
{'train@eng.pdtb.pdtb_loss': 0.9416165947914124, 'train@eng.pdtb.pdtb_accuracy@eng.pdtb.pdtb': 0.6810792349726776, 'train@eng.pdtb.pdtb_f1@eng.pdtb.pdtb': 0.4671884762293947, 'train@eng.pdtb.pdtb_precision@eng.pdtb.pdtb': 0.5267995979658775, 'train@eng.pdtb.pdtb_recall@eng.pdtb.pdtb': 0.4629851306473314, 'train@eng.pdtb.pdtb_loss@eng.pdtb.pdtb': 0.9416165947914124, 'train@eng.pdtb.pdtb_runtime': 518.4754, 'train@eng.pdtb.pdtb_samples_per_second': 84.71, 'train@eng.pdtb.pdtb_steps_per_second': 2.648, 'epoch': 8.0}
{'loss': 1.0005, 'learning_rate': 6.666666666666667e-06, 'epoch': 8.0}
{'eval_loss': 0.9100891351699829, 'eval_accuracy@eng.pdtb.pdtb': 0.6953405017921147, 'eval_f1@eng.pdtb.pdtb': 0.5428321566787606, 'eval_precision@eng.pdtb.pdtb': 0.6230523087277954, 'eval_recall@eng.pdtb.pdtb': 0.5261153202132608, 'eval_loss@eng.pdtb.pdtb': 0.9100892543792725, 'eval_runtime': 20.0923, 'eval_samples_per_second': 83.315, 'eval_steps_per_second': 2.638, 'epoch': 8.0}
{'train@eng.pdtb.pdtb_loss': 0.9332154989242554, 'train@eng.pdtb.pdtb_accuracy@eng.pdtb.pdtb': 0.6846766848816029, 'train@eng.pdtb.pdtb_f1@eng.pdtb.pdtb': 0.47115556146814774, 'train@eng.pdtb.pdtb_precision@eng.pdtb.pdtb': 0.5326966723295307, 'train@eng.pdtb.pdtb_recall@eng.pdtb.pdtb': 0.4649333981890154, 'train@eng.pdtb.pdtb_loss@eng.pdtb.pdtb': 0.9332154989242554, 'train@eng.pdtb.pdtb_runtime': 517.9635, 'train@eng.pdtb.pdtb_samples_per_second': 84.794, 'train@eng.pdtb.pdtb_steps_per_second': 2.651, 'epoch': 9.0}
{'loss': 0.9891, 'learning_rate': 5e-06, 'epoch': 9.0}
{'eval_loss': 0.907021164894104, 'eval_accuracy@eng.pdtb.pdtb': 0.7007168458781362, 'eval_f1@eng.pdtb.pdtb': 0.5524660455107583, 'eval_precision@eng.pdtb.pdtb': 0.6260747824060071, 'eval_recall@eng.pdtb.pdtb': 0.5365715705186915, 'eval_loss@eng.pdtb.pdtb': 0.907021164894104, 'eval_runtime': 20.1913, 'eval_samples_per_second': 82.907, 'eval_steps_per_second': 2.625, 'epoch': 9.0}
{'train@eng.pdtb.pdtb_loss': 0.9294810891151428, 'train@eng.pdtb.pdtb_accuracy@eng.pdtb.pdtb': 0.6859517304189435, 'train@eng.pdtb.pdtb_f1@eng.pdtb.pdtb': 0.47133177236836826, 'train@eng.pdtb.pdtb_precision@eng.pdtb.pdtb': 0.5344702008496585, 'train@eng.pdtb.pdtb_recall@eng.pdtb.pdtb': 0.46547102561084863, 'train@eng.pdtb.pdtb_loss@eng.pdtb.pdtb': 0.9294809699058533, 'train@eng.pdtb.pdtb_runtime': 517.9738, 'train@eng.pdtb.pdtb_samples_per_second': 84.792, 'train@eng.pdtb.pdtb_steps_per_second': 2.651, 'epoch': 10.0}
{'loss': 0.9856, 'learning_rate': 3.3333333333333333e-06, 'epoch': 10.0}
{'eval_loss': 0.9025053977966309, 'eval_accuracy@eng.pdtb.pdtb': 0.6941457586618877, 'eval_f1@eng.pdtb.pdtb': 0.5482835052831506, 'eval_precision@eng.pdtb.pdtb': 0.6310452261036377, 'eval_recall@eng.pdtb.pdtb': 0.5306871387875542, 'eval_loss@eng.pdtb.pdtb': 0.9025053977966309, 'eval_runtime': 20.0965, 'eval_samples_per_second': 83.298, 'eval_steps_per_second': 2.637, 'epoch': 10.0}
{'train@eng.pdtb.pdtb_loss': 0.9253310561180115, 'train@eng.pdtb.pdtb_accuracy@eng.pdtb.pdtb': 0.6865437158469946, 'train@eng.pdtb.pdtb_f1@eng.pdtb.pdtb': 0.47440030750398027, 'train@eng.pdtb.pdtb_precision@eng.pdtb.pdtb': 0.526588165505424, 'train@eng.pdtb.pdtb_recall@eng.pdtb.pdtb': 0.46937538486620983, 'train@eng.pdtb.pdtb_loss@eng.pdtb.pdtb': 0.9253309369087219, 'train@eng.pdtb.pdtb_runtime': 518.066, 'train@eng.pdtb.pdtb_samples_per_second': 84.777, 'train@eng.pdtb.pdtb_steps_per_second': 2.65, 'epoch': 11.0}
{'loss': 0.978, 'learning_rate': 1.6666666666666667e-06, 'epoch': 11.0}
{'eval_loss': 0.9015107154846191, 'eval_accuracy@eng.pdtb.pdtb': 0.6923536439665472, 'eval_f1@eng.pdtb.pdtb': 0.5485216591719377, 'eval_precision@eng.pdtb.pdtb': 0.6167973119367642, 'eval_recall@eng.pdtb.pdtb': 0.5337142725974202, 'eval_loss@eng.pdtb.pdtb': 0.9015107154846191, 'eval_runtime': 20.1174, 'eval_samples_per_second': 83.212, 'eval_steps_per_second': 2.635, 'epoch': 11.0}
{'train@eng.pdtb.pdtb_loss': 0.9241754412651062, 'train@eng.pdtb.pdtb_accuracy@eng.pdtb.pdtb': 0.6873406193078324, 'train@eng.pdtb.pdtb_f1@eng.pdtb.pdtb': 0.4741879305998176, 'train@eng.pdtb.pdtb_precision@eng.pdtb.pdtb': 0.535444320127122, 'train@eng.pdtb.pdtb_recall@eng.pdtb.pdtb': 0.46749238899308965, 'train@eng.pdtb.pdtb_loss@eng.pdtb.pdtb': 0.9241754412651062, 'train@eng.pdtb.pdtb_runtime': 518.9824, 'train@eng.pdtb.pdtb_samples_per_second': 84.627, 'train@eng.pdtb.pdtb_steps_per_second': 2.646, 'epoch': 12.0}
{'loss': 0.9739, 'learning_rate': 0.0, 'epoch': 12.0}
{'eval_loss': 0.8996577858924866, 'eval_accuracy@eng.pdtb.pdtb': 0.6935483870967742, 'eval_f1@eng.pdtb.pdtb': 0.5486694160393053, 'eval_precision@eng.pdtb.pdtb': 0.6191574391499735, 'eval_recall@eng.pdtb.pdtb': 0.5326591950905017, 'eval_loss@eng.pdtb.pdtb': 0.8996579051017761, 'eval_runtime': 20.1766, 'eval_samples_per_second': 82.968, 'eval_steps_per_second': 2.627, 'epoch': 12.0}
{'train_runtime': 19689.3609, 'train_samples_per_second': 26.768, 'train_steps_per_second': 0.837, 'train_loss': 1.1118906960669117, 'epoch': 12.0}
***** train_log metrics *****
  epoch                    =       12.0
  train_loss               =     1.1119
  train_runtime            = 5:28:09.36
  train_samples_per_second =     26.768
  train_steps_per_second   =      0.837
{'train@eng.rst.rstdt_loss': 1.4319136142730713, 'train@eng.rst.rstdt_accuracy@eng.rst.rstdt': 0.6020497437820272, 'train@eng.rst.rstdt_f1@eng.rst.rstdt': 0.2215185600558842, 'train@eng.rst.rstdt_precision@eng.rst.rstdt': 0.22904262102873607, 'train@eng.rst.rstdt_recall@eng.rst.rstdt': 0.2344973273849521, 'train@eng.rst.rstdt_loss@eng.rst.rstdt': 1.4319134950637817, 'train@eng.rst.rstdt_runtime': 188.4777, 'train@eng.rst.rstdt_samples_per_second': 84.901, 'train@eng.rst.rstdt_steps_per_second': 2.658, 'epoch': 1.0}
{'loss': 1.9681, 'learning_rate': 1.8333333333333333e-05, 'epoch': 1.0}
{'eval_loss': 1.4448022842407227, 'eval_accuracy@eng.rst.rstdt': 0.5940777297964219, 'eval_f1@eng.rst.rstdt': 0.2154104046175282, 'eval_precision@eng.rst.rstdt': 0.2267240329941883, 'eval_recall@eng.rst.rstdt': 0.22584084271246, 'eval_loss@eng.rst.rstdt': 1.4448022842407227, 'eval_runtime': 19.4691, 'eval_samples_per_second': 83.26, 'eval_steps_per_second': 2.62, 'epoch': 1.0}
{'train@eng.rst.rstdt_loss': 1.2788997888565063, 'train@eng.rst.rstdt_accuracy@eng.rst.rstdt': 0.6370453693288339, 'train@eng.rst.rstdt_f1@eng.rst.rstdt': 0.28435097546163923, 'train@eng.rst.rstdt_precision@eng.rst.rstdt': 0.36242529485520175, 'train@eng.rst.rstdt_recall@eng.rst.rstdt': 0.28881004656572307, 'train@eng.rst.rstdt_loss@eng.rst.rstdt': 1.278899908065796, 'train@eng.rst.rstdt_runtime': 188.9479, 'train@eng.rst.rstdt_samples_per_second': 84.69, 'train@eng.rst.rstdt_steps_per_second': 2.652, 'epoch': 2.0}
{'loss': 1.3957, 'learning_rate': 1.6666666666666667e-05, 'epoch': 2.0}
{'eval_loss': 1.3106247186660767, 'eval_accuracy@eng.rst.rstdt': 0.6255397902529303, 'eval_f1@eng.rst.rstdt': 0.26814915053866195, 'eval_precision@eng.rst.rstdt': 0.30949838571436294, 'eval_recall@eng.rst.rstdt': 0.2758172445276901, 'eval_loss@eng.rst.rstdt': 1.3106248378753662, 'eval_runtime': 19.4702, 'eval_samples_per_second': 83.256, 'eval_steps_per_second': 2.619, 'epoch': 2.0}
{'train@eng.rst.rstdt_loss': 1.2111588716506958, 'train@eng.rst.rstdt_accuracy@eng.rst.rstdt': 0.6537932758405199, 'train@eng.rst.rstdt_f1@eng.rst.rstdt': 0.3498358873877225, 'train@eng.rst.rstdt_precision@eng.rst.rstdt': 0.4275165689528275, 'train@eng.rst.rstdt_recall@eng.rst.rstdt': 0.3349063306300612, 'train@eng.rst.rstdt_loss@eng.rst.rstdt': 1.2111589908599854, 'train@eng.rst.rstdt_runtime': 189.0329, 'train@eng.rst.rstdt_samples_per_second': 84.652, 'train@eng.rst.rstdt_steps_per_second': 2.65, 'epoch': 3.0}
{'loss': 1.2995, 'learning_rate': 1.5000000000000002e-05, 'epoch': 3.0}
{'eval_loss': 1.266506552696228, 'eval_accuracy@eng.rst.rstdt': 0.6335595311536089, 'eval_f1@eng.rst.rstdt': 0.3188036275291517, 'eval_precision@eng.rst.rstdt': 0.4011458263254776, 'eval_recall@eng.rst.rstdt': 0.31413749844618266, 'eval_loss@eng.rst.rstdt': 1.2665066719055176, 'eval_runtime': 19.5216, 'eval_samples_per_second': 83.036, 'eval_steps_per_second': 2.612, 'epoch': 3.0}
{'train@eng.rst.rstdt_loss': 1.170920491218567, 'train@eng.rst.rstdt_accuracy@eng.rst.rstdt': 0.6614798150231221, 'train@eng.rst.rstdt_f1@eng.rst.rstdt': 0.38614350290858707, 'train@eng.rst.rstdt_precision@eng.rst.rstdt': 0.5558807376315418, 'train@eng.rst.rstdt_recall@eng.rst.rstdt': 0.36278121253380524, 'train@eng.rst.rstdt_loss@eng.rst.rstdt': 1.1709203720092773, 'train@eng.rst.rstdt_runtime': 188.5051, 'train@eng.rst.rstdt_samples_per_second': 84.889, 'train@eng.rst.rstdt_steps_per_second': 2.658, 'epoch': 4.0}
{'loss': 1.24, 'learning_rate': 1.3333333333333333e-05, 'epoch': 4.0}
{'eval_loss': 1.2452259063720703, 'eval_accuracy@eng.rst.rstdt': 0.6335595311536089, 'eval_f1@eng.rst.rstdt': 0.35002111581145695, 'eval_precision@eng.rst.rstdt': 0.47456529230360806, 'eval_recall@eng.rst.rstdt': 0.33768882879682854, 'eval_loss@eng.rst.rstdt': 1.2452260255813599, 'eval_runtime': 19.476, 'eval_samples_per_second': 83.231, 'eval_steps_per_second': 2.619, 'epoch': 4.0}
{'train@eng.rst.rstdt_loss': 1.1365078687667847, 'train@eng.rst.rstdt_accuracy@eng.rst.rstdt': 0.6707286589176353, 'train@eng.rst.rstdt_f1@eng.rst.rstdt': 0.4041051567861048, 'train@eng.rst.rstdt_precision@eng.rst.rstdt': 0.5583658424761546, 'train@eng.rst.rstdt_recall@eng.rst.rstdt': 0.3747299592287033, 'train@eng.rst.rstdt_loss@eng.rst.rstdt': 1.1365078687667847, 'train@eng.rst.rstdt_runtime': 189.0089, 'train@eng.rst.rstdt_samples_per_second': 84.663, 'train@eng.rst.rstdt_steps_per_second': 2.651, 'epoch': 5.0}
{'loss': 1.1993, 'learning_rate': 1.1666666666666668e-05, 'epoch': 5.0}
{'eval_loss': 1.2214090824127197, 'eval_accuracy@eng.rst.rstdt': 0.6458975940777298, 'eval_f1@eng.rst.rstdt': 0.3726915686899998, 'eval_precision@eng.rst.rstdt': 0.49681345195577237, 'eval_recall@eng.rst.rstdt': 0.351953663325114, 'eval_loss@eng.rst.rstdt': 1.2214089632034302, 'eval_runtime': 19.4992, 'eval_samples_per_second': 83.132, 'eval_steps_per_second': 2.615, 'epoch': 5.0}
{'train@eng.rst.rstdt_loss': 1.1146706342697144, 'train@eng.rst.rstdt_accuracy@eng.rst.rstdt': 0.6760404949381328, 'train@eng.rst.rstdt_f1@eng.rst.rstdt': 0.4125601897856652, 'train@eng.rst.rstdt_precision@eng.rst.rstdt': 0.5105033459259524, 'train@eng.rst.rstdt_recall@eng.rst.rstdt': 0.3783319771573156, 'train@eng.rst.rstdt_loss@eng.rst.rstdt': 1.1146706342697144, 'train@eng.rst.rstdt_runtime': 188.8839, 'train@eng.rst.rstdt_samples_per_second': 84.719, 'train@eng.rst.rstdt_steps_per_second': 2.652, 'epoch': 6.0}
{'loss': 1.1735, 'learning_rate': 1e-05, 'epoch': 6.0}
{'eval_loss': 1.1987802982330322, 'eval_accuracy@eng.rst.rstdt': 0.6508328192473781, 'eval_f1@eng.rst.rstdt': 0.38239133400489633, 'eval_precision@eng.rst.rstdt': 0.502351634023913, 'eval_recall@eng.rst.rstdt': 0.3548454706575612, 'eval_loss@eng.rst.rstdt': 1.1987802982330322, 'eval_runtime': 19.4617, 'eval_samples_per_second': 83.292, 'eval_steps_per_second': 2.621, 'epoch': 6.0}
{'train@eng.rst.rstdt_loss': 1.102272391319275, 'train@eng.rst.rstdt_accuracy@eng.rst.rstdt': 0.6762904636920385, 'train@eng.rst.rstdt_f1@eng.rst.rstdt': 0.41508626844955665, 'train@eng.rst.rstdt_precision@eng.rst.rstdt': 0.5070859420457319, 'train@eng.rst.rstdt_recall@eng.rst.rstdt': 0.3813948162085935, 'train@eng.rst.rstdt_loss@eng.rst.rstdt': 1.1022725105285645, 'train@eng.rst.rstdt_runtime': 188.6374, 'train@eng.rst.rstdt_samples_per_second': 84.829, 'train@eng.rst.rstdt_steps_per_second': 2.656, 'epoch': 7.0}
{'loss': 1.156, 'learning_rate': 8.333333333333334e-06, 'epoch': 7.0}
{'eval_loss': 1.189989447593689, 'eval_accuracy@eng.rst.rstdt': 0.6458975940777298, 'eval_f1@eng.rst.rstdt': 0.37409331876767893, 'eval_precision@eng.rst.rstdt': 0.4809409680703689, 'eval_recall@eng.rst.rstdt': 0.3503091322898882, 'eval_loss@eng.rst.rstdt': 1.1899893283843994, 'eval_runtime': 19.5036, 'eval_samples_per_second': 83.113, 'eval_steps_per_second': 2.615, 'epoch': 7.0}
{'train@eng.rst.rstdt_loss': 1.088411569595337, 'train@eng.rst.rstdt_accuracy@eng.rst.rstdt': 0.6776652918385202, 'train@eng.rst.rstdt_f1@eng.rst.rstdt': 0.4219347158000425, 'train@eng.rst.rstdt_precision@eng.rst.rstdt': 0.6023073175709008, 'train@eng.rst.rstdt_recall@eng.rst.rstdt': 0.3887604745897817, 'train@eng.rst.rstdt_loss@eng.rst.rstdt': 1.088411569595337, 'train@eng.rst.rstdt_runtime': 188.9639, 'train@eng.rst.rstdt_samples_per_second': 84.683, 'train@eng.rst.rstdt_steps_per_second': 2.651, 'epoch': 8.0}
{'loss': 1.1438, 'learning_rate': 6.666666666666667e-06, 'epoch': 8.0}
{'eval_loss': 1.1877366304397583, 'eval_accuracy@eng.rst.rstdt': 0.6557680444170265, 'eval_f1@eng.rst.rstdt': 0.38697661046482507, 'eval_precision@eng.rst.rstdt': 0.4961066951661786, 'eval_recall@eng.rst.rstdt': 0.36409345451511904, 'eval_loss@eng.rst.rstdt': 1.1877367496490479, 'eval_runtime': 19.5182, 'eval_samples_per_second': 83.051, 'eval_steps_per_second': 2.613, 'epoch': 8.0}
{'train@eng.rst.rstdt_loss': 1.0788275003433228, 'train@eng.rst.rstdt_accuracy@eng.rst.rstdt': 0.6802899637545307, 'train@eng.rst.rstdt_f1@eng.rst.rstdt': 0.42699677073599707, 'train@eng.rst.rstdt_precision@eng.rst.rstdt': 0.6065360371481207, 'train@eng.rst.rstdt_recall@eng.rst.rstdt': 0.3912113860210042, 'train@eng.rst.rstdt_loss@eng.rst.rstdt': 1.0788277387619019, 'train@eng.rst.rstdt_runtime': 188.4544, 'train@eng.rst.rstdt_samples_per_second': 84.912, 'train@eng.rst.rstdt_steps_per_second': 2.658, 'epoch': 9.0}
{'loss': 1.1346, 'learning_rate': 5e-06, 'epoch': 9.0}
{'eval_loss': 1.1755956411361694, 'eval_accuracy@eng.rst.rstdt': 0.6514497223935842, 'eval_f1@eng.rst.rstdt': 0.37841030629833416, 'eval_precision@eng.rst.rstdt': 0.4899960383708881, 'eval_recall@eng.rst.rstdt': 0.3564595469885802, 'eval_loss@eng.rst.rstdt': 1.175595760345459, 'eval_runtime': 19.4375, 'eval_samples_per_second': 83.396, 'eval_steps_per_second': 2.624, 'epoch': 9.0}
{'train@eng.rst.rstdt_loss': 1.0762284994125366, 'train@eng.rst.rstdt_accuracy@eng.rst.rstdt': 0.679477565304337, 'train@eng.rst.rstdt_f1@eng.rst.rstdt': 0.4356194587139818, 'train@eng.rst.rstdt_precision@eng.rst.rstdt': 0.5856915653563698, 'train@eng.rst.rstdt_recall@eng.rst.rstdt': 0.4014940269706092, 'train@eng.rst.rstdt_loss@eng.rst.rstdt': 1.0762284994125366, 'train@eng.rst.rstdt_runtime': 188.797, 'train@eng.rst.rstdt_samples_per_second': 84.758, 'train@eng.rst.rstdt_steps_per_second': 2.654, 'epoch': 10.0}
{'loss': 1.1239, 'learning_rate': 3.3333333333333333e-06, 'epoch': 10.0}
{'eval_loss': 1.1833255290985107, 'eval_accuracy@eng.rst.rstdt': 0.6440468846391116, 'eval_f1@eng.rst.rstdt': 0.3848660047208511, 'eval_precision@eng.rst.rstdt': 0.5292193723452518, 'eval_recall@eng.rst.rstdt': 0.3656334744774469, 'eval_loss@eng.rst.rstdt': 1.1833255290985107, 'eval_runtime': 19.5136, 'eval_samples_per_second': 83.07, 'eval_steps_per_second': 2.614, 'epoch': 10.0}
{'train@eng.rst.rstdt_loss': 1.0697771310806274, 'train@eng.rst.rstdt_accuracy@eng.rst.rstdt': 0.6809773778277716, 'train@eng.rst.rstdt_f1@eng.rst.rstdt': 0.43377190984450203, 'train@eng.rst.rstdt_precision@eng.rst.rstdt': 0.589151343723961, 'train@eng.rst.rstdt_recall@eng.rst.rstdt': 0.39676462665287643, 'train@eng.rst.rstdt_loss@eng.rst.rstdt': 1.069777250289917, 'train@eng.rst.rstdt_runtime': 188.9054, 'train@eng.rst.rstdt_samples_per_second': 84.709, 'train@eng.rst.rstdt_steps_per_second': 2.652, 'epoch': 11.0}
{'loss': 1.119, 'learning_rate': 1.6666666666666667e-06, 'epoch': 11.0}
{'eval_loss': 1.1753251552581787, 'eval_accuracy@eng.rst.rstdt': 0.6502159161011721, 'eval_f1@eng.rst.rstdt': 0.38505758434453885, 'eval_precision@eng.rst.rstdt': 0.5426112772218121, 'eval_recall@eng.rst.rstdt': 0.3620482812478292, 'eval_loss@eng.rst.rstdt': 1.1753252744674683, 'eval_runtime': 19.5084, 'eval_samples_per_second': 83.092, 'eval_steps_per_second': 2.614, 'epoch': 11.0}
{'train@eng.rst.rstdt_loss': 1.0684912204742432, 'train@eng.rst.rstdt_accuracy@eng.rst.rstdt': 0.6815398075240595, 'train@eng.rst.rstdt_f1@eng.rst.rstdt': 0.43485340730196037, 'train@eng.rst.rstdt_precision@eng.rst.rstdt': 0.5997762454961153, 'train@eng.rst.rstdt_recall@eng.rst.rstdt': 0.3975873040055494, 'train@eng.rst.rstdt_loss@eng.rst.rstdt': 1.0684912204742432, 'train@eng.rst.rstdt_runtime': 188.505, 'train@eng.rst.rstdt_samples_per_second': 84.889, 'train@eng.rst.rstdt_steps_per_second': 2.658, 'epoch': 12.0}
{'loss': 1.1134, 'learning_rate': 0.0, 'epoch': 12.0}
{'eval_loss': 1.1740590333938599, 'eval_accuracy@eng.rst.rstdt': 0.6520666255397902, 'eval_f1@eng.rst.rstdt': 0.38870264916626196, 'eval_precision@eng.rst.rstdt': 0.5471728432889957, 'eval_recall@eng.rst.rstdt': 0.3644951069536137, 'eval_loss@eng.rst.rstdt': 1.1740590333938599, 'eval_runtime': 19.5467, 'eval_samples_per_second': 82.93, 'eval_steps_per_second': 2.609, 'epoch': 12.0}
{'train_runtime': 7271.0131, 'train_samples_per_second': 26.41, 'train_steps_per_second': 0.827, 'train_loss': 1.2555755797974364, 'epoch': 12.0}
***** train_logger metrics *****
  epoch                    =       12.0
  train_loss               =     1.1119
  train_runtime            = 5:28:09.36
  train_samples_per_second =     26.768
  train_steps_per_second   =      0.837
-------------------------------------------------------------------
Lang1:  eng.rst.gum    Lang2:  eng.rst.rstdt
Saving run to:  runs/full_shot/FullShot=v4_eng.rst.gum_eng.rst.rstdt_lrfunc=Adafactor_lr=2e-05_plm=bert-base-multilingual-cased
Running with params: BERT_MODEL=bert-base-multilingual-cased lr=2e-05
read 13897 examples
read 2149 examples
read 2091 examples
read 16002 examples
read 1621 examples
read 2155 examples
Total prediction labels:  32
BertAdapterModel(
  (shared_parameters): ModuleDict()
  (bert): BertModel(
    (shared_parameters): ModuleDict()
    (invertible_adapters): ModuleDict(
      (en): NICECouplingBlock(
        (F): Sequential(
          (0): Linear(in_features=384, out_features=192, bias=True)
          (1): Activation_Function_Class(
            (f): ReLU()
          )
          (2): Linear(in_features=192, out_features=384, bias=True)
        )
        (G): Sequential(
          (0): Linear(in_features=384, out_features=192, bias=True)
          (1): Activation_Function_Class(
            (f): ReLU()
          )
          (2): Linear(in_features=192, out_features=384, bias=True)
        )
      )
    )
    (embeddings): BertEmbeddings(
      (word_embeddings): Embedding(119547, 768, padding_idx=0)
      (position_embeddings): Embedding(512, 768)
      (token_type_embeddings): Embedding(2, 768)
      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
      (dropout): Dropout(p=0.1, inplace=False)
    )
    (encoder): BertEncoder(
      (layer): ModuleList(
        (0): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (en): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (1): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (en): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (2): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (en): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (3): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (en): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (4): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (en): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (5): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (en): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (6): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (en): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (7): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (en): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (8): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (en): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (9): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (en): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (10): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (en): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (11): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (en): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
      )
    )
    (pooler): BertPooler(
      (dense): Linear(in_features=768, out_features=768, bias=True)
      (activation): Tanh()
    )
    (prefix_tuning): PrefixTuningPool(
      (prefix_tunings): ModuleDict()
    )
  )
  (heads): ModuleDict(
    (disrpt): ClassificationHead(
      (0): Dropout(p=0.1, inplace=False)
      (1): Linear(in_features=768, out_features=768, bias=True)
      (2): Activation_Function_Class(
        (f): Tanh()
      )
      (3): Dropout(p=0.1, inplace=False)
      (4): Linear(in_features=768, out_features=32, bias=True)
    )
  )
)
{'train@eng.rst.gum_loss': 2.5156805515289307, 'train@eng.rst.gum_accuracy@eng.rst.gum': 0.2457364898899043, 'train@eng.rst.gum_f1@eng.rst.gum': 0.03993869093544944, 'train@eng.rst.gum_precision@eng.rst.gum': 0.06462888796915721, 'train@eng.rst.gum_recall@eng.rst.gum': 0.05736647192854045, 'train@eng.rst.gum_loss@eng.rst.gum': 2.5156805515289307, 'train@eng.rst.gum_runtime': 164.1155, 'train@eng.rst.gum_samples_per_second': 84.678, 'train@eng.rst.gum_steps_per_second': 2.651, 'epoch': 1.0}
{'loss': 2.7684, 'learning_rate': 1.8333333333333333e-05, 'epoch': 1.0}
{'eval_loss': 2.599254846572876, 'eval_accuracy@eng.rst.gum': 0.24011167985109352, 'eval_f1@eng.rst.gum': 0.04435425775220489, 'eval_precision@eng.rst.gum': 0.0634983747287315, 'eval_recall@eng.rst.gum': 0.06140821192334586, 'eval_loss@eng.rst.gum': 2.5992543697357178, 'eval_runtime': 25.6809, 'eval_samples_per_second': 83.681, 'eval_steps_per_second': 2.648, 'epoch': 1.0}
{'train@eng.rst.gum_loss': 2.0856447219848633, 'train@eng.rst.gum_accuracy@eng.rst.gum': 0.3882852414190113, 'train@eng.rst.gum_f1@eng.rst.gum': 0.14370423495570617, 'train@eng.rst.gum_precision@eng.rst.gum': 0.19429645215609942, 'train@eng.rst.gum_recall@eng.rst.gum': 0.1620764333094725, 'train@eng.rst.gum_loss@eng.rst.gum': 2.0856447219848633, 'train@eng.rst.gum_runtime': 164.1208, 'train@eng.rst.gum_samples_per_second': 84.675, 'train@eng.rst.gum_steps_per_second': 2.65, 'epoch': 2.0}
{'loss': 2.3572, 'learning_rate': 1.6666666666666667e-05, 'epoch': 2.0}
{'eval_loss': 2.1921615600585938, 'eval_accuracy@eng.rst.gum': 0.3592368543508609, 'eval_f1@eng.rst.gum': 0.13445386732762538, 'eval_precision@eng.rst.gum': 0.16825274952231908, 'eval_recall@eng.rst.gum': 0.15902718092456464, 'eval_loss@eng.rst.gum': 2.192161798477173, 'eval_runtime': 25.6774, 'eval_samples_per_second': 83.692, 'eval_steps_per_second': 2.648, 'epoch': 2.0}
{'train@eng.rst.gum_loss': 1.8068183660507202, 'train@eng.rst.gum_accuracy@eng.rst.gum': 0.47952795567388645, 'train@eng.rst.gum_f1@eng.rst.gum': 0.26422700361084633, 'train@eng.rst.gum_precision@eng.rst.gum': 0.34077365097989015, 'train@eng.rst.gum_recall@eng.rst.gum': 0.2790559696663823, 'train@eng.rst.gum_loss@eng.rst.gum': 1.8068187236785889, 'train@eng.rst.gum_runtime': 163.9011, 'train@eng.rst.gum_samples_per_second': 84.789, 'train@eng.rst.gum_steps_per_second': 2.654, 'epoch': 3.0}
{'loss': 2.0112, 'learning_rate': 1.5000000000000002e-05, 'epoch': 3.0}
{'eval_loss': 1.942409873008728, 'eval_accuracy@eng.rst.gum': 0.43973941368078173, 'eval_f1@eng.rst.gum': 0.2528426384965244, 'eval_precision@eng.rst.gum': 0.28847562302646673, 'eval_recall@eng.rst.gum': 0.26930248342907726, 'eval_loss@eng.rst.gum': 1.9424101114273071, 'eval_runtime': 25.6132, 'eval_samples_per_second': 83.902, 'eval_steps_per_second': 2.655, 'epoch': 3.0}
{'train@eng.rst.gum_loss': 1.6797924041748047, 'train@eng.rst.gum_accuracy@eng.rst.gum': 0.5044973735338563, 'train@eng.rst.gum_f1@eng.rst.gum': 0.29377584109195504, 'train@eng.rst.gum_precision@eng.rst.gum': 0.4175065020241856, 'train@eng.rst.gum_recall@eng.rst.gum': 0.3056452665424631, 'train@eng.rst.gum_loss@eng.rst.gum': 1.6797921657562256, 'train@eng.rst.gum_runtime': 164.2564, 'train@eng.rst.gum_samples_per_second': 84.606, 'train@eng.rst.gum_steps_per_second': 2.648, 'epoch': 4.0}
{'loss': 1.8177, 'learning_rate': 1.3333333333333333e-05, 'epoch': 4.0}
{'eval_loss': 1.8457858562469482, 'eval_accuracy@eng.rst.gum': 0.4667287110283853, 'eval_f1@eng.rst.gum': 0.2786991545855337, 'eval_precision@eng.rst.gum': 0.32277955992754004, 'eval_recall@eng.rst.gum': 0.29562295792118987, 'eval_loss@eng.rst.gum': 1.8457857370376587, 'eval_runtime': 25.7255, 'eval_samples_per_second': 83.536, 'eval_steps_per_second': 2.643, 'epoch': 4.0}
{'train@eng.rst.gum_loss': 1.6051889657974243, 'train@eng.rst.gum_accuracy@eng.rst.gum': 0.5183852630064043, 'train@eng.rst.gum_f1@eng.rst.gum': 0.3308590981236577, 'train@eng.rst.gum_precision@eng.rst.gum': 0.4313014335129276, 'train@eng.rst.gum_recall@eng.rst.gum': 0.3416036739432304, 'train@eng.rst.gum_loss@eng.rst.gum': 1.6051889657974243, 'train@eng.rst.gum_runtime': 164.2627, 'train@eng.rst.gum_samples_per_second': 84.602, 'train@eng.rst.gum_steps_per_second': 2.648, 'epoch': 5.0}
{'loss': 1.7165, 'learning_rate': 1.1666666666666668e-05, 'epoch': 5.0}
{'eval_loss': 1.7864638566970825, 'eval_accuracy@eng.rst.gum': 0.4811540251279665, 'eval_f1@eng.rst.gum': 0.3120826331227241, 'eval_precision@eng.rst.gum': 0.3660604643359088, 'eval_recall@eng.rst.gum': 0.3312304065739203, 'eval_loss@eng.rst.gum': 1.7864636182785034, 'eval_runtime': 25.6608, 'eval_samples_per_second': 83.746, 'eval_steps_per_second': 2.65, 'epoch': 5.0}
{'train@eng.rst.gum_loss': 1.5536572933197021, 'train@eng.rst.gum_accuracy@eng.rst.gum': 0.5345758077282867, 'train@eng.rst.gum_f1@eng.rst.gum': 0.3670622037261343, 'train@eng.rst.gum_precision@eng.rst.gum': 0.4945520012153721, 'train@eng.rst.gum_recall@eng.rst.gum': 0.3699286774331997, 'train@eng.rst.gum_loss@eng.rst.gum': 1.5536572933197021, 'train@eng.rst.gum_runtime': 163.8237, 'train@eng.rst.gum_samples_per_second': 84.829, 'train@eng.rst.gum_steps_per_second': 2.655, 'epoch': 6.0}
{'loss': 1.6539, 'learning_rate': 1e-05, 'epoch': 6.0}
{'eval_loss': 1.749671459197998, 'eval_accuracy@eng.rst.gum': 0.49185667752442996, 'eval_f1@eng.rst.gum': 0.34354783043163, 'eval_precision@eng.rst.gum': 0.4187607311663157, 'eval_recall@eng.rst.gum': 0.3551314967323657, 'eval_loss@eng.rst.gum': 1.7496713399887085, 'eval_runtime': 25.6546, 'eval_samples_per_second': 83.767, 'eval_steps_per_second': 2.651, 'epoch': 6.0}
{'train@eng.rst.gum_loss': 1.5169984102249146, 'train@eng.rst.gum_accuracy@eng.rst.gum': 0.5430668489602073, 'train@eng.rst.gum_f1@eng.rst.gum': 0.3799204288746093, 'train@eng.rst.gum_precision@eng.rst.gum': 0.5375698969589249, 'train@eng.rst.gum_recall@eng.rst.gum': 0.38482284047177295, 'train@eng.rst.gum_loss@eng.rst.gum': 1.5169984102249146, 'train@eng.rst.gum_runtime': 164.3738, 'train@eng.rst.gum_samples_per_second': 84.545, 'train@eng.rst.gum_steps_per_second': 2.646, 'epoch': 7.0}
{'loss': 1.6104, 'learning_rate': 8.333333333333334e-06, 'epoch': 7.0}
{'eval_loss': 1.727789282798767, 'eval_accuracy@eng.rst.gum': 0.49371800837598884, 'eval_f1@eng.rst.gum': 0.34686050416278447, 'eval_precision@eng.rst.gum': 0.4243593629729063, 'eval_recall@eng.rst.gum': 0.36056157572981995, 'eval_loss@eng.rst.gum': 1.727789282798767, 'eval_runtime': 25.6915, 'eval_samples_per_second': 83.646, 'eval_steps_per_second': 2.647, 'epoch': 7.0}
{'train@eng.rst.gum_loss': 1.4929934740066528, 'train@eng.rst.gum_accuracy@eng.rst.gum': 0.54911131898971, 'train@eng.rst.gum_f1@eng.rst.gum': 0.3907625800610583, 'train@eng.rst.gum_precision@eng.rst.gum': 0.532530227627821, 'train@eng.rst.gum_recall@eng.rst.gum': 0.3942492183788119, 'train@eng.rst.gum_loss@eng.rst.gum': 1.4929934740066528, 'train@eng.rst.gum_runtime': 164.3392, 'train@eng.rst.gum_samples_per_second': 84.563, 'train@eng.rst.gum_steps_per_second': 2.647, 'epoch': 8.0}
{'loss': 1.5804, 'learning_rate': 6.666666666666667e-06, 'epoch': 8.0}
{'eval_loss': 1.7123044729232788, 'eval_accuracy@eng.rst.gum': 0.5011633317822243, 'eval_f1@eng.rst.gum': 0.35422791024966394, 'eval_precision@eng.rst.gum': 0.41451349405553456, 'eval_recall@eng.rst.gum': 0.3687004889795402, 'eval_loss@eng.rst.gum': 1.7123045921325684, 'eval_runtime': 25.7919, 'eval_samples_per_second': 83.321, 'eval_steps_per_second': 2.636, 'epoch': 8.0}
{'train@eng.rst.gum_loss': 1.475401759147644, 'train@eng.rst.gum_accuracy@eng.rst.gum': 0.5575304022450889, 'train@eng.rst.gum_f1@eng.rst.gum': 0.4053157739405461, 'train@eng.rst.gum_precision@eng.rst.gum': 0.5267345953154371, 'train@eng.rst.gum_recall@eng.rst.gum': 0.41008391925658005, 'train@eng.rst.gum_loss@eng.rst.gum': 1.475401759147644, 'train@eng.rst.gum_runtime': 163.7302, 'train@eng.rst.gum_samples_per_second': 84.877, 'train@eng.rst.gum_steps_per_second': 2.657, 'epoch': 9.0}
{'loss': 1.5561, 'learning_rate': 5e-06, 'epoch': 9.0}
{'eval_loss': 1.6971747875213623, 'eval_accuracy@eng.rst.gum': 0.504885993485342, 'eval_f1@eng.rst.gum': 0.37164574680505796, 'eval_precision@eng.rst.gum': 0.49040240461176016, 'eval_recall@eng.rst.gum': 0.3906099675748222, 'eval_loss@eng.rst.gum': 1.6971749067306519, 'eval_runtime': 27.3222, 'eval_samples_per_second': 78.654, 'eval_steps_per_second': 2.489, 'epoch': 9.0}
{'train@eng.rst.gum_loss': 1.4585589170455933, 'train@eng.rst.gum_accuracy@eng.rst.gum': 0.5589695617759228, 'train@eng.rst.gum_f1@eng.rst.gum': 0.40819520504721335, 'train@eng.rst.gum_precision@eng.rst.gum': 0.5269868733509456, 'train@eng.rst.gum_recall@eng.rst.gum': 0.40914842492165915, 'train@eng.rst.gum_loss@eng.rst.gum': 1.4585589170455933, 'train@eng.rst.gum_runtime': 164.3108, 'train@eng.rst.gum_samples_per_second': 84.577, 'train@eng.rst.gum_steps_per_second': 2.647, 'epoch': 10.0}
{'loss': 1.5428, 'learning_rate': 3.3333333333333333e-06, 'epoch': 10.0}
{'eval_loss': 1.687707781791687, 'eval_accuracy@eng.rst.gum': 0.5109353187529083, 'eval_f1@eng.rst.gum': 0.37633191266287547, 'eval_precision@eng.rst.gum': 0.5110417132315122, 'eval_recall@eng.rst.gum': 0.389877914430601, 'eval_loss@eng.rst.gum': 1.687707781791687, 'eval_runtime': 25.7304, 'eval_samples_per_second': 83.52, 'eval_steps_per_second': 2.643, 'epoch': 10.0}
{'train@eng.rst.gum_loss': 1.45151948928833, 'train@eng.rst.gum_accuracy@eng.rst.gum': 0.5638627041807585, 'train@eng.rst.gum_f1@eng.rst.gum': 0.4141951821121127, 'train@eng.rst.gum_precision@eng.rst.gum': 0.5248609627973089, 'train@eng.rst.gum_recall@eng.rst.gum': 0.41825862375122436, 'train@eng.rst.gum_loss@eng.rst.gum': 1.451519250869751, 'train@eng.rst.gum_runtime': 164.3202, 'train@eng.rst.gum_samples_per_second': 84.573, 'train@eng.rst.gum_steps_per_second': 2.647, 'epoch': 11.0}
{'loss': 1.5302, 'learning_rate': 1.6666666666666667e-06, 'epoch': 11.0}
{'eval_loss': 1.6812833547592163, 'eval_accuracy@eng.rst.gum': 0.5132619823173569, 'eval_f1@eng.rst.gum': 0.3824249482026334, 'eval_precision@eng.rst.gum': 0.4978797383771213, 'eval_recall@eng.rst.gum': 0.3966584421114335, 'eval_loss@eng.rst.gum': 1.6812831163406372, 'eval_runtime': 25.7553, 'eval_samples_per_second': 83.439, 'eval_steps_per_second': 2.64, 'epoch': 11.0}
{'train@eng.rst.gum_loss': 1.448217749595642, 'train@eng.rst.gum_accuracy@eng.rst.gum': 0.5635748722745917, 'train@eng.rst.gum_f1@eng.rst.gum': 0.413271529937188, 'train@eng.rst.gum_precision@eng.rst.gum': 0.5286312798357333, 'train@eng.rst.gum_recall@eng.rst.gum': 0.41681189985701517, 'train@eng.rst.gum_loss@eng.rst.gum': 1.4482179880142212, 'train@eng.rst.gum_runtime': 163.7398, 'train@eng.rst.gum_samples_per_second': 84.872, 'train@eng.rst.gum_steps_per_second': 2.657, 'epoch': 12.0}
{'loss': 1.5221, 'learning_rate': 0.0, 'epoch': 12.0}
{'eval_loss': 1.6795427799224854, 'eval_accuracy@eng.rst.gum': 0.5146579804560261, 'eval_f1@eng.rst.gum': 0.3839784453668238, 'eval_precision@eng.rst.gum': 0.5101861905645599, 'eval_recall@eng.rst.gum': 0.3962737336572515, 'eval_loss@eng.rst.gum': 1.6795426607131958, 'eval_runtime': 25.6256, 'eval_samples_per_second': 83.861, 'eval_steps_per_second': 2.654, 'epoch': 12.0}
{'train_runtime': 6420.8417, 'train_samples_per_second': 25.972, 'train_steps_per_second': 0.813, 'train_loss': 1.8055678912049509, 'epoch': 12.0}
***** train_log metrics *****
  epoch                    =       12.0
  train_loss               =     1.8056
  train_runtime            = 1:47:00.84
  train_samples_per_second =     25.972
  train_steps_per_second   =      0.813
{'train@eng.rst.rstdt_loss': 1.4097537994384766, 'train@eng.rst.rstdt_accuracy@eng.rst.rstdt': 0.6073615798025247, 'train@eng.rst.rstdt_f1@eng.rst.rstdt': 0.24274893742972364, 'train@eng.rst.rstdt_precision@eng.rst.rstdt': 0.303894794043675, 'train@eng.rst.rstdt_recall@eng.rst.rstdt': 0.25064569084394583, 'train@eng.rst.rstdt_loss@eng.rst.rstdt': 1.4097539186477661, 'train@eng.rst.rstdt_runtime': 189.0427, 'train@eng.rst.rstdt_samples_per_second': 84.648, 'train@eng.rst.rstdt_steps_per_second': 2.65, 'epoch': 1.0}
{'loss': 1.8839, 'learning_rate': 1.8333333333333333e-05, 'epoch': 1.0}
{'eval_loss': 1.42936110496521, 'eval_accuracy@eng.rst.rstdt': 0.6088834053053671, 'eval_f1@eng.rst.rstdt': 0.23455645452335577, 'eval_precision@eng.rst.rstdt': 0.2527082639279407, 'eval_recall@eng.rst.rstdt': 0.24932588498574978, 'eval_loss@eng.rst.rstdt': 1.42936110496521, 'eval_runtime': 19.4634, 'eval_samples_per_second': 83.285, 'eval_steps_per_second': 2.62, 'epoch': 1.0}
{'train@eng.rst.rstdt_loss': 1.271460771560669, 'train@eng.rst.rstdt_accuracy@eng.rst.rstdt': 0.6401074865641795, 'train@eng.rst.rstdt_f1@eng.rst.rstdt': 0.31756425632868457, 'train@eng.rst.rstdt_precision@eng.rst.rstdt': 0.4408609906527878, 'train@eng.rst.rstdt_recall@eng.rst.rstdt': 0.3087768857683267, 'train@eng.rst.rstdt_loss@eng.rst.rstdt': 1.271460771560669, 'train@eng.rst.rstdt_runtime': 188.8401, 'train@eng.rst.rstdt_samples_per_second': 84.738, 'train@eng.rst.rstdt_steps_per_second': 2.653, 'epoch': 2.0}
{'loss': 1.3844, 'learning_rate': 1.6666666666666667e-05, 'epoch': 2.0}
{'eval_loss': 1.3101462125778198, 'eval_accuracy@eng.rst.rstdt': 0.6298581122763726, 'eval_f1@eng.rst.rstdt': 0.2966031353551082, 'eval_precision@eng.rst.rstdt': 0.4071009421535186, 'eval_recall@eng.rst.rstdt': 0.2959355567716133, 'eval_loss@eng.rst.rstdt': 1.3101462125778198, 'eval_runtime': 19.4431, 'eval_samples_per_second': 83.371, 'eval_steps_per_second': 2.623, 'epoch': 2.0}
{'train@eng.rst.rstdt_loss': 1.204429030418396, 'train@eng.rst.rstdt_accuracy@eng.rst.rstdt': 0.6548556430446194, 'train@eng.rst.rstdt_f1@eng.rst.rstdt': 0.35771313467326005, 'train@eng.rst.rstdt_precision@eng.rst.rstdt': 0.43833523613957165, 'train@eng.rst.rstdt_recall@eng.rst.rstdt': 0.33886900687832844, 'train@eng.rst.rstdt_loss@eng.rst.rstdt': 1.2044291496276855, 'train@eng.rst.rstdt_runtime': 188.5733, 'train@eng.rst.rstdt_samples_per_second': 84.858, 'train@eng.rst.rstdt_steps_per_second': 2.657, 'epoch': 3.0}
{'loss': 1.294, 'learning_rate': 1.5000000000000002e-05, 'epoch': 3.0}
{'eval_loss': 1.2616997957229614, 'eval_accuracy@eng.rst.rstdt': 0.6391116594694632, 'eval_f1@eng.rst.rstdt': 0.33588064315483346, 'eval_precision@eng.rst.rstdt': 0.4022550396315764, 'eval_recall@eng.rst.rstdt': 0.32996292252478576, 'eval_loss@eng.rst.rstdt': 1.2616996765136719, 'eval_runtime': 19.4126, 'eval_samples_per_second': 83.502, 'eval_steps_per_second': 2.627, 'epoch': 3.0}
{'train@eng.rst.rstdt_loss': 1.1665763854980469, 'train@eng.rst.rstdt_accuracy@eng.rst.rstdt': 0.6629171353580803, 'train@eng.rst.rstdt_f1@eng.rst.rstdt': 0.39370478161594147, 'train@eng.rst.rstdt_precision@eng.rst.rstdt': 0.5117664872616287, 'train@eng.rst.rstdt_recall@eng.rst.rstdt': 0.3681165824495918, 'train@eng.rst.rstdt_loss@eng.rst.rstdt': 1.1665763854980469, 'train@eng.rst.rstdt_runtime': 189.2256, 'train@eng.rst.rstdt_samples_per_second': 84.566, 'train@eng.rst.rstdt_steps_per_second': 2.648, 'epoch': 4.0}
{'loss': 1.2371, 'learning_rate': 1.3333333333333333e-05, 'epoch': 4.0}
{'eval_loss': 1.239984154701233, 'eval_accuracy@eng.rst.rstdt': 0.634793337446021, 'eval_f1@eng.rst.rstdt': 0.37127041059186855, 'eval_precision@eng.rst.rstdt': 0.46075091701911514, 'eval_recall@eng.rst.rstdt': 0.3593448809215184, 'eval_loss@eng.rst.rstdt': 1.2399840354919434, 'eval_runtime': 19.524, 'eval_samples_per_second': 83.026, 'eval_steps_per_second': 2.612, 'epoch': 4.0}
{'train@eng.rst.rstdt_loss': 1.1357247829437256, 'train@eng.rst.rstdt_accuracy@eng.rst.rstdt': 0.6684789401324834, 'train@eng.rst.rstdt_f1@eng.rst.rstdt': 0.39980263192774707, 'train@eng.rst.rstdt_precision@eng.rst.rstdt': 0.5067562063085731, 'train@eng.rst.rstdt_recall@eng.rst.rstdt': 0.3701522800075352, 'train@eng.rst.rstdt_loss@eng.rst.rstdt': 1.1357247829437256, 'train@eng.rst.rstdt_runtime': 188.6562, 'train@eng.rst.rstdt_samples_per_second': 84.821, 'train@eng.rst.rstdt_steps_per_second': 2.656, 'epoch': 5.0}
{'loss': 1.2008, 'learning_rate': 1.1666666666666668e-05, 'epoch': 5.0}
{'eval_loss': 1.2212107181549072, 'eval_accuracy@eng.rst.rstdt': 0.6421961752004935, 'eval_f1@eng.rst.rstdt': 0.3688740495091221, 'eval_precision@eng.rst.rstdt': 0.468427047600482, 'eval_recall@eng.rst.rstdt': 0.3539764945063557, 'eval_loss@eng.rst.rstdt': 1.2212107181549072, 'eval_runtime': 19.4037, 'eval_samples_per_second': 83.541, 'eval_steps_per_second': 2.628, 'epoch': 5.0}
{'train@eng.rst.rstdt_loss': 1.1130270957946777, 'train@eng.rst.rstdt_accuracy@eng.rst.rstdt': 0.6724784401949756, 'train@eng.rst.rstdt_f1@eng.rst.rstdt': 0.4093300452989289, 'train@eng.rst.rstdt_precision@eng.rst.rstdt': 0.5692666689685739, 'train@eng.rst.rstdt_recall@eng.rst.rstdt': 0.3755945219739405, 'train@eng.rst.rstdt_loss@eng.rst.rstdt': 1.1130269765853882, 'train@eng.rst.rstdt_runtime': 189.2577, 'train@eng.rst.rstdt_samples_per_second': 84.551, 'train@eng.rst.rstdt_steps_per_second': 2.647, 'epoch': 6.0}
{'loss': 1.1752, 'learning_rate': 1e-05, 'epoch': 6.0}
{'eval_loss': 1.1983789205551147, 'eval_accuracy@eng.rst.rstdt': 0.6471314003701419, 'eval_f1@eng.rst.rstdt': 0.3784125276991307, 'eval_precision@eng.rst.rstdt': 0.534101822984355, 'eval_recall@eng.rst.rstdt': 0.3585774691592252, 'eval_loss@eng.rst.rstdt': 1.1983788013458252, 'eval_runtime': 19.4792, 'eval_samples_per_second': 83.217, 'eval_steps_per_second': 2.618, 'epoch': 6.0}
{'train@eng.rst.rstdt_loss': 1.1011027097702026, 'train@eng.rst.rstdt_accuracy@eng.rst.rstdt': 0.6766654168228972, 'train@eng.rst.rstdt_f1@eng.rst.rstdt': 0.4249109477274239, 'train@eng.rst.rstdt_precision@eng.rst.rstdt': 0.6370541001882929, 'train@eng.rst.rstdt_recall@eng.rst.rstdt': 0.38477513977910244, 'train@eng.rst.rstdt_loss@eng.rst.rstdt': 1.1011027097702026, 'train@eng.rst.rstdt_runtime': 189.1053, 'train@eng.rst.rstdt_samples_per_second': 84.62, 'train@eng.rst.rstdt_steps_per_second': 2.649, 'epoch': 7.0}
{'loss': 1.1578, 'learning_rate': 8.333333333333334e-06, 'epoch': 7.0}
{'eval_loss': 1.189144492149353, 'eval_accuracy@eng.rst.rstdt': 0.6434299814929056, 'eval_f1@eng.rst.rstdt': 0.37384190912303683, 'eval_precision@eng.rst.rstdt': 0.48025059432746636, 'eval_recall@eng.rst.rstdt': 0.35430566728687185, 'eval_loss@eng.rst.rstdt': 1.1891446113586426, 'eval_runtime': 19.4727, 'eval_samples_per_second': 83.245, 'eval_steps_per_second': 2.619, 'epoch': 7.0}
{'train@eng.rst.rstdt_loss': 1.0880205631256104, 'train@eng.rst.rstdt_accuracy@eng.rst.rstdt': 0.677790276215473, 'train@eng.rst.rstdt_f1@eng.rst.rstdt': 0.43228735161171633, 'train@eng.rst.rstdt_precision@eng.rst.rstdt': 0.6280547178867747, 'train@eng.rst.rstdt_recall@eng.rst.rstdt': 0.3942523636957253, 'train@eng.rst.rstdt_loss@eng.rst.rstdt': 1.0880205631256104, 'train@eng.rst.rstdt_runtime': 188.6523, 'train@eng.rst.rstdt_samples_per_second': 84.823, 'train@eng.rst.rstdt_steps_per_second': 2.656, 'epoch': 8.0}
{'loss': 1.146, 'learning_rate': 6.666666666666667e-06, 'epoch': 8.0}
{'eval_loss': 1.1864501237869263, 'eval_accuracy@eng.rst.rstdt': 0.6477483035163479, 'eval_f1@eng.rst.rstdt': 0.3795110839690941, 'eval_precision@eng.rst.rstdt': 0.4837121826237539, 'eval_recall@eng.rst.rstdt': 0.3633787885446607, 'eval_loss@eng.rst.rstdt': 1.1864500045776367, 'eval_runtime': 19.3988, 'eval_samples_per_second': 83.562, 'eval_steps_per_second': 2.629, 'epoch': 8.0}
{'train@eng.rst.rstdt_loss': 1.0788887739181519, 'train@eng.rst.rstdt_accuracy@eng.rst.rstdt': 0.679915010623672, 'train@eng.rst.rstdt_f1@eng.rst.rstdt': 0.4362773953363015, 'train@eng.rst.rstdt_precision@eng.rst.rstdt': 0.6242471007745661, 'train@eng.rst.rstdt_recall@eng.rst.rstdt': 0.3952438932684235, 'train@eng.rst.rstdt_loss@eng.rst.rstdt': 1.0788887739181519, 'train@eng.rst.rstdt_runtime': 189.0041, 'train@eng.rst.rstdt_samples_per_second': 84.665, 'train@eng.rst.rstdt_steps_per_second': 2.651, 'epoch': 9.0}
{'loss': 1.1329, 'learning_rate': 5e-06, 'epoch': 9.0}
{'eval_loss': 1.1771998405456543, 'eval_accuracy@eng.rst.rstdt': 0.6502159161011721, 'eval_f1@eng.rst.rstdt': 0.37815444025577766, 'eval_precision@eng.rst.rstdt': 0.4842100342936117, 'eval_recall@eng.rst.rstdt': 0.36009162944304157, 'eval_loss@eng.rst.rstdt': 1.1771997213363647, 'eval_runtime': 19.441, 'eval_samples_per_second': 83.38, 'eval_steps_per_second': 2.623, 'epoch': 9.0}
{'train@eng.rst.rstdt_loss': 1.076666235923767, 'train@eng.rst.rstdt_accuracy@eng.rst.rstdt': 0.6792275965504312, 'train@eng.rst.rstdt_f1@eng.rst.rstdt': 0.45263195297819864, 'train@eng.rst.rstdt_precision@eng.rst.rstdt': 0.638691731741727, 'train@eng.rst.rstdt_recall@eng.rst.rstdt': 0.4126916003555523, 'train@eng.rst.rstdt_loss@eng.rst.rstdt': 1.076666235923767, 'train@eng.rst.rstdt_runtime': 188.9708, 'train@eng.rst.rstdt_samples_per_second': 84.68, 'train@eng.rst.rstdt_steps_per_second': 2.651, 'epoch': 10.0}
{'loss': 1.1254, 'learning_rate': 3.3333333333333333e-06, 'epoch': 10.0}
{'eval_loss': 1.1828327178955078, 'eval_accuracy@eng.rst.rstdt': 0.6428130783466995, 'eval_f1@eng.rst.rstdt': 0.3905349626197629, 'eval_precision@eng.rst.rstdt': 0.48712541582411906, 'eval_recall@eng.rst.rstdt': 0.37421535061309175, 'eval_loss@eng.rst.rstdt': 1.1828327178955078, 'eval_runtime': 19.4183, 'eval_samples_per_second': 83.478, 'eval_steps_per_second': 2.626, 'epoch': 10.0}
{'train@eng.rst.rstdt_loss': 1.0696730613708496, 'train@eng.rst.rstdt_accuracy@eng.rst.rstdt': 0.6808523934508186, 'train@eng.rst.rstdt_f1@eng.rst.rstdt': 0.4474516906013668, 'train@eng.rst.rstdt_precision@eng.rst.rstdt': 0.6769296194701362, 'train@eng.rst.rstdt_recall@eng.rst.rstdt': 0.4061279074122304, 'train@eng.rst.rstdt_loss@eng.rst.rstdt': 1.0696730613708496, 'train@eng.rst.rstdt_runtime': 188.7935, 'train@eng.rst.rstdt_samples_per_second': 84.759, 'train@eng.rst.rstdt_steps_per_second': 2.654, 'epoch': 11.0}
{'loss': 1.1252, 'learning_rate': 1.6666666666666667e-06, 'epoch': 11.0}
{'eval_loss': 1.1750377416610718, 'eval_accuracy@eng.rst.rstdt': 0.6471314003701419, 'eval_f1@eng.rst.rstdt': 0.3910195021979202, 'eval_precision@eng.rst.rstdt': 0.4935455389828122, 'eval_recall@eng.rst.rstdt': 0.37305759339740524, 'eval_loss@eng.rst.rstdt': 1.1750377416610718, 'eval_runtime': 19.438, 'eval_samples_per_second': 83.393, 'eval_steps_per_second': 2.624, 'epoch': 11.0}
{'train@eng.rst.rstdt_loss': 1.0684922933578491, 'train@eng.rst.rstdt_accuracy@eng.rst.rstdt': 0.6813523309586301, 'train@eng.rst.rstdt_f1@eng.rst.rstdt': 0.44938048921956425, 'train@eng.rst.rstdt_precision@eng.rst.rstdt': 0.6767636270678159, 'train@eng.rst.rstdt_recall@eng.rst.rstdt': 0.40673486745103815, 'train@eng.rst.rstdt_loss@eng.rst.rstdt': 1.0684924125671387, 'train@eng.rst.rstdt_runtime': 189.0956, 'train@eng.rst.rstdt_samples_per_second': 84.624, 'train@eng.rst.rstdt_steps_per_second': 2.649, 'epoch': 12.0}
{'loss': 1.1191, 'learning_rate': 0.0, 'epoch': 12.0}
{'eval_loss': 1.1731966733932495, 'eval_accuracy@eng.rst.rstdt': 0.64898210980876, 'eval_f1@eng.rst.rstdt': 0.3917645795463388, 'eval_precision@eng.rst.rstdt': 0.4985318645071801, 'eval_recall@eng.rst.rstdt': 0.3708476782034072, 'eval_loss@eng.rst.rstdt': 1.17319655418396, 'eval_runtime': 19.4674, 'eval_samples_per_second': 83.268, 'eval_steps_per_second': 2.62, 'epoch': 12.0}
{'train_runtime': 7272.6509, 'train_samples_per_second': 26.404, 'train_steps_per_second': 0.827, 'train_loss': 1.248480923082221, 'epoch': 12.0}
***** train_logger metrics *****
  epoch                    =       12.0
  train_loss               =     1.8056
  train_runtime            = 1:47:00.84
  train_samples_per_second =     25.972
  train_steps_per_second   =      0.813
-------------------------------------------------------------------
Lang1:  eng.sdrt.stac    Lang2:  eng.rst.rstdt
Saving run to:  runs/full_shot/FullShot=v4_eng.sdrt.stac_eng.rst.rstdt_lrfunc=Adafactor_lr=2e-05_plm=bert-base-multilingual-cased
Running with params: BERT_MODEL=bert-base-multilingual-cased lr=2e-05
read 9580 examples
read 1145 examples
read 1510 examples
read 16002 examples
read 1621 examples
read 2155 examples
Total prediction labels:  33
BertAdapterModel(
  (shared_parameters): ModuleDict()
  (bert): BertModel(
    (shared_parameters): ModuleDict()
    (invertible_adapters): ModuleDict(
      (en): NICECouplingBlock(
        (F): Sequential(
          (0): Linear(in_features=384, out_features=192, bias=True)
          (1): Activation_Function_Class(
            (f): ReLU()
          )
          (2): Linear(in_features=192, out_features=384, bias=True)
        )
        (G): Sequential(
          (0): Linear(in_features=384, out_features=192, bias=True)
          (1): Activation_Function_Class(
            (f): ReLU()
          )
          (2): Linear(in_features=192, out_features=384, bias=True)
        )
      )
    )
    (embeddings): BertEmbeddings(
      (word_embeddings): Embedding(119547, 768, padding_idx=0)
      (position_embeddings): Embedding(512, 768)
      (token_type_embeddings): Embedding(2, 768)
      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
      (dropout): Dropout(p=0.1, inplace=False)
    )
    (encoder): BertEncoder(
      (layer): ModuleList(
        (0): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (en): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (1): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (en): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (2): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (en): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (3): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (en): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (4): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (en): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (5): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (en): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (6): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (en): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (7): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (en): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (8): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (en): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (9): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (en): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (10): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (en): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (11): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (en): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
      )
    )
    (pooler): BertPooler(
      (dense): Linear(in_features=768, out_features=768, bias=True)
      (activation): Tanh()
    )
    (prefix_tuning): PrefixTuningPool(
      (prefix_tunings): ModuleDict()
    )
  )
  (heads): ModuleDict(
    (disrpt): ClassificationHead(
      (0): Dropout(p=0.1, inplace=False)
      (1): Linear(in_features=768, out_features=768, bias=True)
      (2): Activation_Function_Class(
        (f): Tanh()
      )
      (3): Dropout(p=0.1, inplace=False)
      (4): Linear(in_features=768, out_features=33, bias=True)
    )
  )
)
{'train@eng.sdrt.stac_loss': 2.091207504272461, 'train@eng.sdrt.stac_accuracy@eng.sdrt.stac': 0.35323590814196243, 'train@eng.sdrt.stac_f1@eng.sdrt.stac': 0.06934599593459041, 'train@eng.sdrt.stac_precision@eng.sdrt.stac': 0.08725249129780767, 'train@eng.sdrt.stac_recall@eng.sdrt.stac': 0.11112865787333244, 'train@eng.sdrt.stac_loss@eng.sdrt.stac': 2.09120774269104, 'train@eng.sdrt.stac_runtime': 113.0006, 'train@eng.sdrt.stac_samples_per_second': 84.778, 'train@eng.sdrt.stac_steps_per_second': 2.655, 'epoch': 1.0}
{'loss': 2.5182, 'learning_rate': 1.8333333333333333e-05, 'epoch': 1.0}
{'eval_loss': 2.0447652339935303, 'eval_accuracy@eng.sdrt.stac': 0.36943231441048036, 'eval_f1@eng.sdrt.stac': 0.074579855149463, 'eval_precision@eng.sdrt.stac': 0.07065220535018502, 'eval_recall@eng.sdrt.stac': 0.11640036587126733, 'eval_loss@eng.sdrt.stac': 2.0447652339935303, 'eval_runtime': 13.8558, 'eval_samples_per_second': 82.637, 'eval_steps_per_second': 2.598, 'epoch': 1.0}
{'train@eng.sdrt.stac_loss': 1.875585913658142, 'train@eng.sdrt.stac_accuracy@eng.sdrt.stac': 0.4282881002087683, 'train@eng.sdrt.stac_f1@eng.sdrt.stac': 0.12793688476020915, 'train@eng.sdrt.stac_precision@eng.sdrt.stac': 0.13858686866835412, 'train@eng.sdrt.stac_recall@eng.sdrt.stac': 0.1746359569120603, 'train@eng.sdrt.stac_loss@eng.sdrt.stac': 1.8755861520767212, 'train@eng.sdrt.stac_runtime': 113.0249, 'train@eng.sdrt.stac_samples_per_second': 84.76, 'train@eng.sdrt.stac_steps_per_second': 2.654, 'epoch': 2.0}
{'loss': 2.0178, 'learning_rate': 1.6666666666666667e-05, 'epoch': 2.0}
{'eval_loss': 1.829505205154419, 'eval_accuracy@eng.sdrt.stac': 0.42358078602620086, 'eval_f1@eng.sdrt.stac': 0.1222175603642668, 'eval_precision@eng.sdrt.stac': 0.12102591294242745, 'eval_recall@eng.sdrt.stac': 0.16983523928500666, 'eval_loss@eng.sdrt.stac': 1.829505205154419, 'eval_runtime': 13.8211, 'eval_samples_per_second': 82.845, 'eval_steps_per_second': 2.605, 'epoch': 2.0}
{'train@eng.sdrt.stac_loss': 1.7742644548416138, 'train@eng.sdrt.stac_accuracy@eng.sdrt.stac': 0.45167014613778705, 'train@eng.sdrt.stac_f1@eng.sdrt.stac': 0.1495530998653718, 'train@eng.sdrt.stac_precision@eng.sdrt.stac': 0.19800846446208958, 'train@eng.sdrt.stac_recall@eng.sdrt.stac': 0.1920798820929854, 'train@eng.sdrt.stac_loss@eng.sdrt.stac': 1.7742644548416138, 'train@eng.sdrt.stac_runtime': 113.0657, 'train@eng.sdrt.stac_samples_per_second': 84.729, 'train@eng.sdrt.stac_steps_per_second': 2.653, 'epoch': 3.0}
{'loss': 1.8583, 'learning_rate': 1.5000000000000002e-05, 'epoch': 3.0}
{'eval_loss': 1.7255102396011353, 'eval_accuracy@eng.sdrt.stac': 0.4550218340611354, 'eval_f1@eng.sdrt.stac': 0.14397156987637663, 'eval_precision@eng.sdrt.stac': 0.13231752749708667, 'eval_recall@eng.sdrt.stac': 0.18817244409453399, 'eval_loss@eng.sdrt.stac': 1.7255103588104248, 'eval_runtime': 13.8012, 'eval_samples_per_second': 82.964, 'eval_steps_per_second': 2.608, 'epoch': 3.0}
{'train@eng.sdrt.stac_loss': 1.7040889263153076, 'train@eng.sdrt.stac_accuracy@eng.sdrt.stac': 0.469937369519833, 'train@eng.sdrt.stac_f1@eng.sdrt.stac': 0.18419558308191514, 'train@eng.sdrt.stac_precision@eng.sdrt.stac': 0.23530104584685696, 'train@eng.sdrt.stac_recall@eng.sdrt.stac': 0.21348009804155285, 'train@eng.sdrt.stac_loss@eng.sdrt.stac': 1.7040889263153076, 'train@eng.sdrt.stac_runtime': 112.9993, 'train@eng.sdrt.stac_samples_per_second': 84.779, 'train@eng.sdrt.stac_steps_per_second': 2.655, 'epoch': 4.0}
{'loss': 1.7764, 'learning_rate': 1.3333333333333333e-05, 'epoch': 4.0}
{'eval_loss': 1.6514291763305664, 'eval_accuracy@eng.sdrt.stac': 0.474235807860262, 'eval_f1@eng.sdrt.stac': 0.17772572104251028, 'eval_precision@eng.sdrt.stac': 0.24915046615202827, 'eval_recall@eng.sdrt.stac': 0.20785185960220312, 'eval_loss@eng.sdrt.stac': 1.6514291763305664, 'eval_runtime': 13.7996, 'eval_samples_per_second': 82.974, 'eval_steps_per_second': 2.609, 'epoch': 4.0}
{'train@eng.sdrt.stac_loss': 1.6531509160995483, 'train@eng.sdrt.stac_accuracy@eng.sdrt.stac': 0.4872651356993737, 'train@eng.sdrt.stac_f1@eng.sdrt.stac': 0.20936844939386842, 'train@eng.sdrt.stac_precision@eng.sdrt.stac': 0.21150815122714428, 'train@eng.sdrt.stac_recall@eng.sdrt.stac': 0.23469119884178452, 'train@eng.sdrt.stac_loss@eng.sdrt.stac': 1.653151035308838, 'train@eng.sdrt.stac_runtime': 112.8788, 'train@eng.sdrt.stac_samples_per_second': 84.87, 'train@eng.sdrt.stac_steps_per_second': 2.658, 'epoch': 5.0}
{'loss': 1.7198, 'learning_rate': 1.1666666666666668e-05, 'epoch': 5.0}
{'eval_loss': 1.6021089553833008, 'eval_accuracy@eng.sdrt.stac': 0.48646288209606986, 'eval_f1@eng.sdrt.stac': 0.19855252963412126, 'eval_precision@eng.sdrt.stac': 0.22104884142226647, 'eval_recall@eng.sdrt.stac': 0.22218610476112854, 'eval_loss@eng.sdrt.stac': 1.6021090745925903, 'eval_runtime': 13.8056, 'eval_samples_per_second': 82.938, 'eval_steps_per_second': 2.608, 'epoch': 5.0}
{'train@eng.sdrt.stac_loss': 1.608595848083496, 'train@eng.sdrt.stac_accuracy@eng.sdrt.stac': 0.4961377870563674, 'train@eng.sdrt.stac_f1@eng.sdrt.stac': 0.21933160044121147, 'train@eng.sdrt.stac_precision@eng.sdrt.stac': 0.23423603834011236, 'train@eng.sdrt.stac_recall@eng.sdrt.stac': 0.24115291060449798, 'train@eng.sdrt.stac_loss@eng.sdrt.stac': 1.608595848083496, 'train@eng.sdrt.stac_runtime': 113.1744, 'train@eng.sdrt.stac_samples_per_second': 84.648, 'train@eng.sdrt.stac_steps_per_second': 2.651, 'epoch': 6.0}
{'loss': 1.6651, 'learning_rate': 1e-05, 'epoch': 6.0}
{'eval_loss': 1.573167085647583, 'eval_accuracy@eng.sdrt.stac': 0.48384279475982533, 'eval_f1@eng.sdrt.stac': 0.20087470192695855, 'eval_precision@eng.sdrt.stac': 0.22942805807671635, 'eval_recall@eng.sdrt.stac': 0.22298007171110457, 'eval_loss@eng.sdrt.stac': 1.573167324066162, 'eval_runtime': 13.8477, 'eval_samples_per_second': 82.685, 'eval_steps_per_second': 2.6, 'epoch': 6.0}
{'train@eng.sdrt.stac_loss': 1.5890321731567383, 'train@eng.sdrt.stac_accuracy@eng.sdrt.stac': 0.5040709812108559, 'train@eng.sdrt.stac_f1@eng.sdrt.stac': 0.23267237083254655, 'train@eng.sdrt.stac_precision@eng.sdrt.stac': 0.24108103795144842, 'train@eng.sdrt.stac_recall@eng.sdrt.stac': 0.2548582327793521, 'train@eng.sdrt.stac_loss@eng.sdrt.stac': 1.5890320539474487, 'train@eng.sdrt.stac_runtime': 112.8934, 'train@eng.sdrt.stac_samples_per_second': 84.859, 'train@eng.sdrt.stac_steps_per_second': 2.657, 'epoch': 7.0}
{'loss': 1.6357, 'learning_rate': 8.333333333333334e-06, 'epoch': 7.0}
{'eval_loss': 1.5553531646728516, 'eval_accuracy@eng.sdrt.stac': 0.503056768558952, 'eval_f1@eng.sdrt.stac': 0.21782198361300353, 'eval_precision@eng.sdrt.stac': 0.22238679504339587, 'eval_recall@eng.sdrt.stac': 0.23826926679915106, 'eval_loss@eng.sdrt.stac': 1.5553531646728516, 'eval_runtime': 13.8066, 'eval_samples_per_second': 82.931, 'eval_steps_per_second': 2.607, 'epoch': 7.0}
{'train@eng.sdrt.stac_loss': 1.5538997650146484, 'train@eng.sdrt.stac_accuracy@eng.sdrt.stac': 0.511482254697286, 'train@eng.sdrt.stac_f1@eng.sdrt.stac': 0.24521951152114985, 'train@eng.sdrt.stac_precision@eng.sdrt.stac': 0.3273072633597704, 'train@eng.sdrt.stac_recall@eng.sdrt.stac': 0.2663042499080904, 'train@eng.sdrt.stac_loss@eng.sdrt.stac': 1.5538997650146484, 'train@eng.sdrt.stac_runtime': 113.0401, 'train@eng.sdrt.stac_samples_per_second': 84.749, 'train@eng.sdrt.stac_steps_per_second': 2.654, 'epoch': 8.0}
{'loss': 1.6047, 'learning_rate': 6.666666666666667e-06, 'epoch': 8.0}
{'eval_loss': 1.5182139873504639, 'eval_accuracy@eng.sdrt.stac': 0.5100436681222708, 'eval_f1@eng.sdrt.stac': 0.2326729110503477, 'eval_precision@eng.sdrt.stac': 0.2522729664941151, 'eval_recall@eng.sdrt.stac': 0.25154100100248344, 'eval_loss@eng.sdrt.stac': 1.5182138681411743, 'eval_runtime': 13.8224, 'eval_samples_per_second': 82.837, 'eval_steps_per_second': 2.604, 'epoch': 8.0}
{'train@eng.sdrt.stac_loss': 1.5361011028289795, 'train@eng.sdrt.stac_accuracy@eng.sdrt.stac': 0.5181628392484342, 'train@eng.sdrt.stac_f1@eng.sdrt.stac': 0.26554210366171727, 'train@eng.sdrt.stac_precision@eng.sdrt.stac': 0.38398394612921516, 'train@eng.sdrt.stac_recall@eng.sdrt.stac': 0.2801293601515121, 'train@eng.sdrt.stac_loss@eng.sdrt.stac': 1.536101222038269, 'train@eng.sdrt.stac_runtime': 113.218, 'train@eng.sdrt.stac_samples_per_second': 84.616, 'train@eng.sdrt.stac_steps_per_second': 2.65, 'epoch': 9.0}
{'loss': 1.5821, 'learning_rate': 5e-06, 'epoch': 9.0}
{'eval_loss': 1.5056885480880737, 'eval_accuracy@eng.sdrt.stac': 0.5117903930131005, 'eval_f1@eng.sdrt.stac': 0.23663655283688856, 'eval_precision@eng.sdrt.stac': 0.29801003702480755, 'eval_recall@eng.sdrt.stac': 0.2539975495797451, 'eval_loss@eng.sdrt.stac': 1.5056884288787842, 'eval_runtime': 13.8285, 'eval_samples_per_second': 82.8, 'eval_steps_per_second': 2.603, 'epoch': 9.0}
{'train@eng.sdrt.stac_loss': 1.5187232494354248, 'train@eng.sdrt.stac_accuracy@eng.sdrt.stac': 0.5233820459290188, 'train@eng.sdrt.stac_f1@eng.sdrt.stac': 0.28999103913927293, 'train@eng.sdrt.stac_precision@eng.sdrt.stac': 0.44293914495239517, 'train@eng.sdrt.stac_recall@eng.sdrt.stac': 0.300741091392043, 'train@eng.sdrt.stac_loss@eng.sdrt.stac': 1.5187232494354248, 'train@eng.sdrt.stac_runtime': 112.8563, 'train@eng.sdrt.stac_samples_per_second': 84.887, 'train@eng.sdrt.stac_steps_per_second': 2.658, 'epoch': 10.0}
{'loss': 1.5671, 'learning_rate': 3.3333333333333333e-06, 'epoch': 10.0}
{'eval_loss': 1.4914196729660034, 'eval_accuracy@eng.sdrt.stac': 0.5187772925764192, 'eval_f1@eng.sdrt.stac': 0.2579894307823776, 'eval_precision@eng.sdrt.stac': 0.31732202406866244, 'eval_recall@eng.sdrt.stac': 0.27439002044059185, 'eval_loss@eng.sdrt.stac': 1.4914196729660034, 'eval_runtime': 13.8115, 'eval_samples_per_second': 82.902, 'eval_steps_per_second': 2.607, 'epoch': 10.0}
{'train@eng.sdrt.stac_loss': 1.5078996419906616, 'train@eng.sdrt.stac_accuracy@eng.sdrt.stac': 0.5245302713987474, 'train@eng.sdrt.stac_f1@eng.sdrt.stac': 0.2962367360977002, 'train@eng.sdrt.stac_precision@eng.sdrt.stac': 0.4148859463964877, 'train@eng.sdrt.stac_recall@eng.sdrt.stac': 0.3064632632285258, 'train@eng.sdrt.stac_loss@eng.sdrt.stac': 1.5078996419906616, 'train@eng.sdrt.stac_runtime': 113.1988, 'train@eng.sdrt.stac_samples_per_second': 84.63, 'train@eng.sdrt.stac_steps_per_second': 2.65, 'epoch': 11.0}
{'loss': 1.5582, 'learning_rate': 1.6666666666666667e-06, 'epoch': 11.0}
{'eval_loss': 1.4835360050201416, 'eval_accuracy@eng.sdrt.stac': 0.5213973799126638, 'eval_f1@eng.sdrt.stac': 0.2663123538492345, 'eval_precision@eng.sdrt.stac': 0.3817067235339858, 'eval_recall@eng.sdrt.stac': 0.27938454492891096, 'eval_loss@eng.sdrt.stac': 1.4835360050201416, 'eval_runtime': 13.8136, 'eval_samples_per_second': 82.889, 'eval_steps_per_second': 2.606, 'epoch': 11.0}
{'train@eng.sdrt.stac_loss': 1.507116675376892, 'train@eng.sdrt.stac_accuracy@eng.sdrt.stac': 0.5260960334029228, 'train@eng.sdrt.stac_f1@eng.sdrt.stac': 0.2994487820888655, 'train@eng.sdrt.stac_precision@eng.sdrt.stac': 0.41759204820467266, 'train@eng.sdrt.stac_recall@eng.sdrt.stac': 0.30954805269333824, 'train@eng.sdrt.stac_loss@eng.sdrt.stac': 1.507116675376892, 'train@eng.sdrt.stac_runtime': 112.9705, 'train@eng.sdrt.stac_samples_per_second': 84.801, 'train@eng.sdrt.stac_steps_per_second': 2.656, 'epoch': 12.0}
{'loss': 1.5517, 'learning_rate': 0.0, 'epoch': 12.0}
{'eval_loss': 1.483130693435669, 'eval_accuracy@eng.sdrt.stac': 0.5231441048034935, 'eval_f1@eng.sdrt.stac': 0.26606239780560403, 'eval_precision@eng.sdrt.stac': 0.3784723434127848, 'eval_recall@eng.sdrt.stac': 0.2804390051745477, 'eval_loss@eng.sdrt.stac': 1.483130693435669, 'eval_runtime': 13.8475, 'eval_samples_per_second': 82.686, 'eval_steps_per_second': 2.6, 'epoch': 12.0}
{'train_runtime': 4376.7155, 'train_samples_per_second': 26.266, 'train_steps_per_second': 0.823, 'train_loss': 1.754583240085178, 'epoch': 12.0}
***** train_log metrics *****
  epoch                    =       12.0
  train_loss               =     1.7546
  train_runtime            = 1:12:56.71
  train_samples_per_second =     26.266
  train_steps_per_second   =      0.823
{'train@eng.rst.rstdt_loss': 1.5771832466125488, 'train@eng.rst.rstdt_accuracy@eng.rst.rstdt': 0.5695538057742782, 'train@eng.rst.rstdt_f1@eng.rst.rstdt': 0.15598322999341985, 'train@eng.rst.rstdt_precision@eng.rst.rstdt': 0.1832866221865729, 'train@eng.rst.rstdt_recall@eng.rst.rstdt': 0.1677535530635206, 'train@eng.rst.rstdt_loss@eng.rst.rstdt': 1.5771833658218384, 'train@eng.rst.rstdt_runtime': 188.5167, 'train@eng.rst.rstdt_samples_per_second': 84.884, 'train@eng.rst.rstdt_steps_per_second': 2.658, 'epoch': 1.0}
{'loss': 1.9793, 'learning_rate': 1.8333333333333333e-05, 'epoch': 1.0}
{'eval_loss': 1.5953322649002075, 'eval_accuracy@eng.rst.rstdt': 0.5582973473164713, 'eval_f1@eng.rst.rstdt': 0.14952179792746176, 'eval_precision@eng.rst.rstdt': 0.15990978157123498, 'eval_recall@eng.rst.rstdt': 0.15895716462963724, 'eval_loss@eng.rst.rstdt': 1.5953326225280762, 'eval_runtime': 19.4256, 'eval_samples_per_second': 83.447, 'eval_steps_per_second': 2.625, 'epoch': 1.0}
{'train@eng.rst.rstdt_loss': 1.3661248683929443, 'train@eng.rst.rstdt_accuracy@eng.rst.rstdt': 0.6134858142732158, 'train@eng.rst.rstdt_f1@eng.rst.rstdt': 0.24863524496014958, 'train@eng.rst.rstdt_precision@eng.rst.rstdt': 0.35573225719420304, 'train@eng.rst.rstdt_recall@eng.rst.rstdt': 0.24844616199179667, 'train@eng.rst.rstdt_loss@eng.rst.rstdt': 1.3661247491836548, 'train@eng.rst.rstdt_runtime': 189.007, 'train@eng.rst.rstdt_samples_per_second': 84.664, 'train@eng.rst.rstdt_steps_per_second': 2.651, 'epoch': 2.0}
{'loss': 1.5195, 'learning_rate': 1.6666666666666667e-05, 'epoch': 2.0}
{'eval_loss': 1.3965072631835938, 'eval_accuracy@eng.rst.rstdt': 0.6138186304750154, 'eval_f1@eng.rst.rstdt': 0.24865017480764334, 'eval_precision@eng.rst.rstdt': 0.31711103072172653, 'eval_recall@eng.rst.rstdt': 0.24860693947228807, 'eval_loss@eng.rst.rstdt': 1.3965072631835938, 'eval_runtime': 19.452, 'eval_samples_per_second': 83.333, 'eval_steps_per_second': 2.622, 'epoch': 2.0}
{'train@eng.rst.rstdt_loss': 1.2696799039840698, 'train@eng.rst.rstdt_accuracy@eng.rst.rstdt': 0.6392325959255093, 'train@eng.rst.rstdt_f1@eng.rst.rstdt': 0.3344007734972929, 'train@eng.rst.rstdt_precision@eng.rst.rstdt': 0.4401594073434798, 'train@eng.rst.rstdt_recall@eng.rst.rstdt': 0.3150087235766348, 'train@eng.rst.rstdt_loss@eng.rst.rstdt': 1.2696800231933594, 'train@eng.rst.rstdt_runtime': 188.6966, 'train@eng.rst.rstdt_samples_per_second': 84.803, 'train@eng.rst.rstdt_steps_per_second': 2.655, 'epoch': 3.0}
{'loss': 1.3856, 'learning_rate': 1.5000000000000002e-05, 'epoch': 3.0}
{'eval_loss': 1.3139030933380127, 'eval_accuracy@eng.rst.rstdt': 0.6366440468846392, 'eval_f1@eng.rst.rstdt': 0.32559624711830476, 'eval_precision@eng.rst.rstdt': 0.41751031210296796, 'eval_recall@eng.rst.rstdt': 0.3174477840265681, 'eval_loss@eng.rst.rstdt': 1.3139029741287231, 'eval_runtime': 19.4176, 'eval_samples_per_second': 83.481, 'eval_steps_per_second': 2.626, 'epoch': 3.0}
{'train@eng.rst.rstdt_loss': 1.2125526666641235, 'train@eng.rst.rstdt_accuracy@eng.rst.rstdt': 0.6467316585426822, 'train@eng.rst.rstdt_f1@eng.rst.rstdt': 0.354818650218926, 'train@eng.rst.rstdt_precision@eng.rst.rstdt': 0.4617522907596139, 'train@eng.rst.rstdt_recall@eng.rst.rstdt': 0.3359953910156606, 'train@eng.rst.rstdt_loss@eng.rst.rstdt': 1.2125526666641235, 'train@eng.rst.rstdt_runtime': 189.2796, 'train@eng.rst.rstdt_samples_per_second': 84.542, 'train@eng.rst.rstdt_steps_per_second': 2.647, 'epoch': 4.0}
{'loss': 1.3008, 'learning_rate': 1.3333333333333333e-05, 'epoch': 4.0}
{'eval_loss': 1.2751668691635132, 'eval_accuracy@eng.rst.rstdt': 0.6366440468846392, 'eval_f1@eng.rst.rstdt': 0.3341333807012643, 'eval_precision@eng.rst.rstdt': 0.443669792135982, 'eval_recall@eng.rst.rstdt': 0.33081409576071646, 'eval_loss@eng.rst.rstdt': 1.2751668691635132, 'eval_runtime': 19.5008, 'eval_samples_per_second': 83.125, 'eval_steps_per_second': 2.615, 'epoch': 4.0}
{'train@eng.rst.rstdt_loss': 1.1723605394363403, 'train@eng.rst.rstdt_accuracy@eng.rst.rstdt': 0.6582927134108236, 'train@eng.rst.rstdt_f1@eng.rst.rstdt': 0.3777138122914189, 'train@eng.rst.rstdt_precision@eng.rst.rstdt': 0.49697438422699336, 'train@eng.rst.rstdt_recall@eng.rst.rstdt': 0.3477721666833722, 'train@eng.rst.rstdt_loss@eng.rst.rstdt': 1.1723605394363403, 'train@eng.rst.rstdt_runtime': 189.1337, 'train@eng.rst.rstdt_samples_per_second': 84.607, 'train@eng.rst.rstdt_steps_per_second': 2.649, 'epoch': 5.0}
{'loss': 1.2525, 'learning_rate': 1.1666666666666668e-05, 'epoch': 5.0}
{'eval_loss': 1.245082974433899, 'eval_accuracy@eng.rst.rstdt': 0.6471314003701419, 'eval_f1@eng.rst.rstdt': 0.3581579539535544, 'eval_precision@eng.rst.rstdt': 0.49586936011173155, 'eval_recall@eng.rst.rstdt': 0.34305835538529444, 'eval_loss@eng.rst.rstdt': 1.2450828552246094, 'eval_runtime': 19.4926, 'eval_samples_per_second': 83.16, 'eval_steps_per_second': 2.616, 'epoch': 5.0}
{'train@eng.rst.rstdt_loss': 1.145960807800293, 'train@eng.rst.rstdt_accuracy@eng.rst.rstdt': 0.6632920884889388, 'train@eng.rst.rstdt_f1@eng.rst.rstdt': 0.38921819674143565, 'train@eng.rst.rstdt_precision@eng.rst.rstdt': 0.546216187921372, 'train@eng.rst.rstdt_recall@eng.rst.rstdt': 0.35496305537851486, 'train@eng.rst.rstdt_loss@eng.rst.rstdt': 1.145960807800293, 'train@eng.rst.rstdt_runtime': 188.4724, 'train@eng.rst.rstdt_samples_per_second': 84.904, 'train@eng.rst.rstdt_steps_per_second': 2.658, 'epoch': 6.0}
{'loss': 1.2152, 'learning_rate': 1e-05, 'epoch': 6.0}
{'eval_loss': 1.2214630842208862, 'eval_accuracy@eng.rst.rstdt': 0.6514497223935842, 'eval_f1@eng.rst.rstdt': 0.36839835777018104, 'eval_precision@eng.rst.rstdt': 0.4869071014330739, 'eval_recall@eng.rst.rstdt': 0.3489288922991395, 'eval_loss@eng.rst.rstdt': 1.2214630842208862, 'eval_runtime': 19.3964, 'eval_samples_per_second': 83.572, 'eval_steps_per_second': 2.629, 'epoch': 6.0}
{'train@eng.rst.rstdt_loss': 1.131893277168274, 'train@eng.rst.rstdt_accuracy@eng.rst.rstdt': 0.6679790026246719, 'train@eng.rst.rstdt_f1@eng.rst.rstdt': 0.3994423633644812, 'train@eng.rst.rstdt_precision@eng.rst.rstdt': 0.5983032832997535, 'train@eng.rst.rstdt_recall@eng.rst.rstdt': 0.3635514257259392, 'train@eng.rst.rstdt_loss@eng.rst.rstdt': 1.131893277168274, 'train@eng.rst.rstdt_runtime': 189.1257, 'train@eng.rst.rstdt_samples_per_second': 84.61, 'train@eng.rst.rstdt_steps_per_second': 2.649, 'epoch': 7.0}
{'loss': 1.1996, 'learning_rate': 8.333333333333334e-06, 'epoch': 7.0}
{'eval_loss': 1.211578369140625, 'eval_accuracy@eng.rst.rstdt': 0.6545342381246144, 'eval_f1@eng.rst.rstdt': 0.37575639198520405, 'eval_precision@eng.rst.rstdt': 0.5020441364676042, 'eval_recall@eng.rst.rstdt': 0.35590555199037166, 'eval_loss@eng.rst.rstdt': 1.2115782499313354, 'eval_runtime': 19.4999, 'eval_samples_per_second': 83.128, 'eval_steps_per_second': 2.615, 'epoch': 7.0}
{'train@eng.rst.rstdt_loss': 1.1155661344528198, 'train@eng.rst.rstdt_accuracy@eng.rst.rstdt': 0.6701037370328708, 'train@eng.rst.rstdt_f1@eng.rst.rstdt': 0.40643219330156627, 'train@eng.rst.rstdt_precision@eng.rst.rstdt': 0.5976396310352906, 'train@eng.rst.rstdt_recall@eng.rst.rstdt': 0.3730476389624855, 'train@eng.rst.rstdt_loss@eng.rst.rstdt': 1.1155662536621094, 'train@eng.rst.rstdt_runtime': 189.1567, 'train@eng.rst.rstdt_samples_per_second': 84.597, 'train@eng.rst.rstdt_steps_per_second': 2.649, 'epoch': 8.0}
{'loss': 1.1757, 'learning_rate': 6.666666666666667e-06, 'epoch': 8.0}
{'eval_loss': 1.2087275981903076, 'eval_accuracy@eng.rst.rstdt': 0.64898210980876, 'eval_f1@eng.rst.rstdt': 0.37131947214339023, 'eval_precision@eng.rst.rstdt': 0.49252200229572574, 'eval_recall@eng.rst.rstdt': 0.35186023083235446, 'eval_loss@eng.rst.rstdt': 1.2087275981903076, 'eval_runtime': 19.4355, 'eval_samples_per_second': 83.404, 'eval_steps_per_second': 2.624, 'epoch': 8.0}
{'train@eng.rst.rstdt_loss': 1.1067107915878296, 'train@eng.rst.rstdt_accuracy@eng.rst.rstdt': 0.6719160104986877, 'train@eng.rst.rstdt_f1@eng.rst.rstdt': 0.4123300375008574, 'train@eng.rst.rstdt_precision@eng.rst.rstdt': 0.6458390565113815, 'train@eng.rst.rstdt_recall@eng.rst.rstdt': 0.37650271545659375, 'train@eng.rst.rstdt_loss@eng.rst.rstdt': 1.1067109107971191, 'train@eng.rst.rstdt_runtime': 188.684, 'train@eng.rst.rstdt_samples_per_second': 84.808, 'train@eng.rst.rstdt_steps_per_second': 2.655, 'epoch': 9.0}
{'loss': 1.1669, 'learning_rate': 5e-06, 'epoch': 9.0}
{'eval_loss': 1.2012747526168823, 'eval_accuracy@eng.rst.rstdt': 0.6520666255397902, 'eval_f1@eng.rst.rstdt': 0.37150494659241867, 'eval_precision@eng.rst.rstdt': 0.48965159266959085, 'eval_recall@eng.rst.rstdt': 0.35274568548124996, 'eval_loss@eng.rst.rstdt': 1.2012748718261719, 'eval_runtime': 19.4757, 'eval_samples_per_second': 83.232, 'eval_steps_per_second': 2.619, 'epoch': 9.0}
{'train@eng.rst.rstdt_loss': 1.103286623954773, 'train@eng.rst.rstdt_accuracy@eng.rst.rstdt': 0.6709161354830646, 'train@eng.rst.rstdt_f1@eng.rst.rstdt': 0.42147449839005136, 'train@eng.rst.rstdt_precision@eng.rst.rstdt': 0.6561949726902991, 'train@eng.rst.rstdt_recall@eng.rst.rstdt': 0.3879298601612488, 'train@eng.rst.rstdt_loss@eng.rst.rstdt': 1.103286623954773, 'train@eng.rst.rstdt_runtime': 188.9617, 'train@eng.rst.rstdt_samples_per_second': 84.684, 'train@eng.rst.rstdt_steps_per_second': 2.651, 'epoch': 10.0}
{'loss': 1.1602, 'learning_rate': 3.3333333333333333e-06, 'epoch': 10.0}
{'eval_loss': 1.2083606719970703, 'eval_accuracy@eng.rst.rstdt': 0.6465144972239358, 'eval_f1@eng.rst.rstdt': 0.3754321363289472, 'eval_precision@eng.rst.rstdt': 0.4905817958968428, 'eval_recall@eng.rst.rstdt': 0.35873054502075763, 'eval_loss@eng.rst.rstdt': 1.2083606719970703, 'eval_runtime': 19.4188, 'eval_samples_per_second': 83.476, 'eval_steps_per_second': 2.626, 'epoch': 10.0}
{'train@eng.rst.rstdt_loss': 1.0970269441604614, 'train@eng.rst.rstdt_accuracy@eng.rst.rstdt': 0.6741032370953631, 'train@eng.rst.rstdt_f1@eng.rst.rstdt': 0.4200322886699594, 'train@eng.rst.rstdt_precision@eng.rst.rstdt': 0.6582663179182168, 'train@eng.rst.rstdt_recall@eng.rst.rstdt': 0.3831551046940443, 'train@eng.rst.rstdt_loss@eng.rst.rstdt': 1.0970269441604614, 'train@eng.rst.rstdt_runtime': 188.8266, 'train@eng.rst.rstdt_samples_per_second': 84.744, 'train@eng.rst.rstdt_steps_per_second': 2.653, 'epoch': 11.0}
{'loss': 1.1476, 'learning_rate': 1.6666666666666667e-06, 'epoch': 11.0}
{'eval_loss': 1.2001667022705078, 'eval_accuracy@eng.rst.rstdt': 0.6508328192473781, 'eval_f1@eng.rst.rstdt': 0.37492999631844404, 'eval_precision@eng.rst.rstdt': 0.5165707486595784, 'eval_recall@eng.rst.rstdt': 0.35626375459049625, 'eval_loss@eng.rst.rstdt': 1.2001667022705078, 'eval_runtime': 19.4147, 'eval_samples_per_second': 83.494, 'eval_steps_per_second': 2.627, 'epoch': 11.0}
{'train@eng.rst.rstdt_loss': 1.0956106185913086, 'train@eng.rst.rstdt_accuracy@eng.rst.rstdt': 0.6743532058492688, 'train@eng.rst.rstdt_f1@eng.rst.rstdt': 0.4199515798047853, 'train@eng.rst.rstdt_precision@eng.rst.rstdt': 0.6592709921403561, 'train@eng.rst.rstdt_recall@eng.rst.rstdt': 0.38336482714998654, 'train@eng.rst.rstdt_loss@eng.rst.rstdt': 1.0956108570098877, 'train@eng.rst.rstdt_runtime': 188.5953, 'train@eng.rst.rstdt_samples_per_second': 84.848, 'train@eng.rst.rstdt_steps_per_second': 2.656, 'epoch': 12.0}
{'loss': 1.1478, 'learning_rate': 0.0, 'epoch': 12.0}
{'eval_loss': 1.1990553140640259, 'eval_accuracy@eng.rst.rstdt': 0.6520666255397902, 'eval_f1@eng.rst.rstdt': 0.37557538925228884, 'eval_precision@eng.rst.rstdt': 0.5137017054677233, 'eval_recall@eng.rst.rstdt': 0.35685144439867156, 'eval_loss@eng.rst.rstdt': 1.1990553140640259, 'eval_runtime': 19.4353, 'eval_samples_per_second': 83.405, 'eval_steps_per_second': 2.624, 'epoch': 12.0}
{'train_runtime': 7275.614, 'train_samples_per_second': 26.393, 'train_steps_per_second': 0.826, 'train_loss': 1.3042250764584113, 'epoch': 12.0}
***** train_logger metrics *****
  epoch                    =       12.0
  train_loss               =     1.7546
  train_runtime            = 1:12:56.71
  train_samples_per_second =     26.266
  train_steps_per_second   =      0.823
-------------------------------------------------------------------
Lang1:  fas.rst.prstc    Lang2:  eng.rst.rstdt
Saving run to:  runs/full_shot/FullShot=v4_fas.rst.prstc_eng.rst.rstdt_lrfunc=Adafactor_lr=2e-05_plm=bert-base-multilingual-cased
Running with params: BERT_MODEL=bert-base-multilingual-cased lr=2e-05
read 4100 examples
read 499 examples
read 592 examples
read 16002 examples
read 1621 examples
read 2155 examples
Total prediction labels:  20
BertAdapterModel(
  (shared_parameters): ModuleDict()
  (bert): BertModel(
    (shared_parameters): ModuleDict()
    (invertible_adapters): ModuleDict(
      (ar): NICECouplingBlock(
        (F): Sequential(
          (0): Linear(in_features=384, out_features=192, bias=True)
          (1): Activation_Function_Class(
            (f): ReLU()
          )
          (2): Linear(in_features=192, out_features=384, bias=True)
        )
        (G): Sequential(
          (0): Linear(in_features=384, out_features=192, bias=True)
          (1): Activation_Function_Class(
            (f): ReLU()
          )
          (2): Linear(in_features=192, out_features=384, bias=True)
        )
      )
      (en): NICECouplingBlock(
        (F): Sequential(
          (0): Linear(in_features=384, out_features=192, bias=True)
          (1): Activation_Function_Class(
            (f): ReLU()
          )
          (2): Linear(in_features=192, out_features=384, bias=True)
        )
        (G): Sequential(
          (0): Linear(in_features=384, out_features=192, bias=True)
          (1): Activation_Function_Class(
            (f): ReLU()
          )
          (2): Linear(in_features=192, out_features=384, bias=True)
        )
      )
    )
    (embeddings): BertEmbeddings(
      (word_embeddings): Embedding(119547, 768, padding_idx=0)
      (position_embeddings): Embedding(512, 768)
      (token_type_embeddings): Embedding(2, 768)
      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
      (dropout): Dropout(p=0.1, inplace=False)
    )
    (encoder): BertEncoder(
      (layer): ModuleList(
        (0): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (ar): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (en): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (1): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (ar): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (en): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (2): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (ar): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (en): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (3): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (ar): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (en): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (4): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (ar): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (en): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (5): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (ar): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (en): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (6): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (ar): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (en): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (7): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (ar): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (en): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (8): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (ar): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (en): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (9): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (ar): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (en): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (10): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (ar): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (en): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (11): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (ar): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (en): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
      )
    )
    (pooler): BertPooler(
      (dense): Linear(in_features=768, out_features=768, bias=True)
      (activation): Tanh()
    )
    (prefix_tuning): PrefixTuningPool(
      (prefix_tunings): ModuleDict()
    )
  )
  (heads): ModuleDict(
    (disrpt): ClassificationHead(
      (0): Dropout(p=0.1, inplace=False)
      (1): Linear(in_features=768, out_features=768, bias=True)
      (2): Activation_Function_Class(
        (f): Tanh()
      )
      (3): Dropout(p=0.1, inplace=False)
      (4): Linear(in_features=768, out_features=20, bias=True)
    )
  )
)
{'train@fas.rst.prstc_loss': 2.361032724380493, 'train@fas.rst.prstc_accuracy@fas.rst.prstc': 0.23780487804878048, 'train@fas.rst.prstc_f1@fas.rst.prstc': 0.02260214430599826, 'train@fas.rst.prstc_precision@fas.rst.prstc': 0.013988522238163558, 'train@fas.rst.prstc_recall@fas.rst.prstc': 0.058823529411764705, 'train@fas.rst.prstc_loss@fas.rst.prstc': 2.361032485961914, 'train@fas.rst.prstc_runtime': 48.4151, 'train@fas.rst.prstc_samples_per_second': 84.684, 'train@fas.rst.prstc_steps_per_second': 2.664, 'epoch': 1.0}
{'loss': 2.5576, 'learning_rate': 1.8333333333333333e-05, 'epoch': 1.0}
{'eval_loss': 2.288093328475952, 'eval_accuracy@fas.rst.prstc': 0.24448897795591182, 'eval_f1@fas.rst.prstc': 0.026194310252281264, 'eval_precision@fas.rst.prstc': 0.016299265197060788, 'eval_recall@fas.rst.prstc': 0.06666666666666667, 'eval_loss@fas.rst.prstc': 2.2880940437316895, 'eval_runtime': 6.1506, 'eval_samples_per_second': 81.13, 'eval_steps_per_second': 2.601, 'epoch': 1.0}
{'train@fas.rst.prstc_loss': 2.339236259460449, 'train@fas.rst.prstc_accuracy@fas.rst.prstc': 0.27195121951219514, 'train@fas.rst.prstc_f1@fas.rst.prstc': 0.044390856589162225, 'train@fas.rst.prstc_precision@fas.rst.prstc': 0.03449642550940594, 'train@fas.rst.prstc_recall@fas.rst.prstc': 0.07372805750778216, 'train@fas.rst.prstc_loss@fas.rst.prstc': 2.3392364978790283, 'train@fas.rst.prstc_runtime': 48.5651, 'train@fas.rst.prstc_samples_per_second': 84.423, 'train@fas.rst.prstc_steps_per_second': 2.656, 'epoch': 2.0}
{'loss': 2.3611, 'learning_rate': 1.6666666666666667e-05, 'epoch': 2.0}
{'eval_loss': 2.2549893856048584, 'eval_accuracy@fas.rst.prstc': 0.2725450901803607, 'eval_f1@fas.rst.prstc': 0.048484848484848485, 'eval_precision@fas.rst.prstc': 0.04105964446970414, 'eval_recall@fas.rst.prstc': 0.07751009741031124, 'eval_loss@fas.rst.prstc': 2.2549893856048584, 'eval_runtime': 6.1301, 'eval_samples_per_second': 81.402, 'eval_steps_per_second': 2.61, 'epoch': 2.0}
{'train@fas.rst.prstc_loss': 2.325533628463745, 'train@fas.rst.prstc_accuracy@fas.rst.prstc': 0.25585365853658537, 'train@fas.rst.prstc_f1@fas.rst.prstc': 0.03308517359910191, 'train@fas.rst.prstc_precision@fas.rst.prstc': 0.03835106676222504, 'train@fas.rst.prstc_recall@fas.rst.prstc': 0.06410684295540366, 'train@fas.rst.prstc_loss@fas.rst.prstc': 2.325533866882324, 'train@fas.rst.prstc_runtime': 48.5527, 'train@fas.rst.prstc_samples_per_second': 84.444, 'train@fas.rst.prstc_steps_per_second': 2.657, 'epoch': 3.0}
{'loss': 2.3404, 'learning_rate': 1.5000000000000002e-05, 'epoch': 3.0}
{'eval_loss': 2.2422120571136475, 'eval_accuracy@fas.rst.prstc': 0.2685370741482966, 'eval_f1@fas.rst.prstc': 0.04064025516852403, 'eval_precision@fas.rst.prstc': 0.050248756218905476, 'eval_recall@fas.rst.prstc': 0.07372297457828463, 'eval_loss@fas.rst.prstc': 2.2422118186950684, 'eval_runtime': 6.1584, 'eval_samples_per_second': 81.028, 'eval_steps_per_second': 2.598, 'epoch': 3.0}
{'train@fas.rst.prstc_loss': 2.3083012104034424, 'train@fas.rst.prstc_accuracy@fas.rst.prstc': 0.24, 'train@fas.rst.prstc_f1@fas.rst.prstc': 0.023901961224953763, 'train@fas.rst.prstc_precision@fas.rst.prstc': 0.047131560753586446, 'train@fas.rst.prstc_recall@fas.rst.prstc': 0.05944931163954944, 'train@fas.rst.prstc_loss@fas.rst.prstc': 2.3083012104034424, 'train@fas.rst.prstc_runtime': 48.5252, 'train@fas.rst.prstc_samples_per_second': 84.492, 'train@fas.rst.prstc_steps_per_second': 2.658, 'epoch': 4.0}
{'loss': 2.3312, 'learning_rate': 1.3333333333333333e-05, 'epoch': 4.0}
{'eval_loss': 2.229504108428955, 'eval_accuracy@fas.rst.prstc': 0.24448897795591182, 'eval_f1@fas.rst.prstc': 0.027268476272194454, 'eval_precision@fas.rst.prstc': 0.032962962962962965, 'eval_recall@fas.rst.prstc': 0.06669992872416251, 'eval_loss@fas.rst.prstc': 2.229503870010376, 'eval_runtime': 6.1459, 'eval_samples_per_second': 81.192, 'eval_steps_per_second': 2.603, 'epoch': 4.0}
{'train@fas.rst.prstc_loss': 2.282528877258301, 'train@fas.rst.prstc_accuracy@fas.rst.prstc': 0.3070731707317073, 'train@fas.rst.prstc_f1@fas.rst.prstc': 0.04903699371340948, 'train@fas.rst.prstc_precision@fas.rst.prstc': 0.03617327365728901, 'train@fas.rst.prstc_recall@fas.rst.prstc': 0.07987676903822084, 'train@fas.rst.prstc_loss@fas.rst.prstc': 2.28252911567688, 'train@fas.rst.prstc_runtime': 48.4411, 'train@fas.rst.prstc_samples_per_second': 84.639, 'train@fas.rst.prstc_steps_per_second': 2.663, 'epoch': 5.0}
{'loss': 2.317, 'learning_rate': 1.1666666666666668e-05, 'epoch': 5.0}
{'eval_loss': 2.2028095722198486, 'eval_accuracy@fas.rst.prstc': 0.3186372745490982, 'eval_f1@fas.rst.prstc': 0.0575187969924812, 'eval_precision@fas.rst.prstc': 0.044002903284340415, 'eval_recall@fas.rst.prstc': 0.08888096935138988, 'eval_loss@fas.rst.prstc': 2.2028095722198486, 'eval_runtime': 6.1164, 'eval_samples_per_second': 81.583, 'eval_steps_per_second': 2.616, 'epoch': 5.0}
{'train@fas.rst.prstc_loss': 2.216585874557495, 'train@fas.rst.prstc_accuracy@fas.rst.prstc': 0.30121951219512194, 'train@fas.rst.prstc_f1@fas.rst.prstc': 0.04822302348850967, 'train@fas.rst.prstc_precision@fas.rst.prstc': 0.03489948679887404, 'train@fas.rst.prstc_recall@fas.rst.prstc': 0.07869559171186205, 'train@fas.rst.prstc_loss@fas.rst.prstc': 2.216585874557495, 'train@fas.rst.prstc_runtime': 48.3625, 'train@fas.rst.prstc_samples_per_second': 84.777, 'train@fas.rst.prstc_steps_per_second': 2.667, 'epoch': 6.0}
{'loss': 2.275, 'learning_rate': 1e-05, 'epoch': 6.0}
{'eval_loss': 2.1404218673706055, 'eval_accuracy@fas.rst.prstc': 0.3106212424849699, 'eval_f1@fas.rst.prstc': 0.05568414707655214, 'eval_precision@fas.rst.prstc': 0.04136921655703585, 'eval_recall@fas.rst.prstc': 0.08676170111665478, 'eval_loss@fas.rst.prstc': 2.1404216289520264, 'eval_runtime': 6.1388, 'eval_samples_per_second': 81.286, 'eval_steps_per_second': 2.606, 'epoch': 6.0}
{'train@fas.rst.prstc_loss': 2.1631460189819336, 'train@fas.rst.prstc_accuracy@fas.rst.prstc': 0.32170731707317074, 'train@fas.rst.prstc_f1@fas.rst.prstc': 0.05314265646356365, 'train@fas.rst.prstc_precision@fas.rst.prstc': 0.0392465324736377, 'train@fas.rst.prstc_recall@fas.rst.prstc': 0.08523539039183596, 'train@fas.rst.prstc_loss@fas.rst.prstc': 2.1631457805633545, 'train@fas.rst.prstc_runtime': 48.3884, 'train@fas.rst.prstc_samples_per_second': 84.731, 'train@fas.rst.prstc_steps_per_second': 2.666, 'epoch': 7.0}
{'loss': 2.2163, 'learning_rate': 8.333333333333334e-06, 'epoch': 7.0}
{'eval_loss': 2.0742721557617188, 'eval_accuracy@fas.rst.prstc': 0.3406813627254509, 'eval_f1@fas.rst.prstc': 0.062224538157094826, 'eval_precision@fas.rst.prstc': 0.04657736629567615, 'eval_recall@fas.rst.prstc': 0.09565692563554289, 'eval_loss@fas.rst.prstc': 2.074272394180298, 'eval_runtime': 6.1124, 'eval_samples_per_second': 81.637, 'eval_steps_per_second': 2.618, 'epoch': 7.0}
{'train@fas.rst.prstc_loss': 2.1209444999694824, 'train@fas.rst.prstc_accuracy@fas.rst.prstc': 0.3492682926829268, 'train@fas.rst.prstc_f1@fas.rst.prstc': 0.06746814332789516, 'train@fas.rst.prstc_precision@fas.rst.prstc': 0.08822676726263105, 'train@fas.rst.prstc_recall@fas.rst.prstc': 0.09665994756750465, 'train@fas.rst.prstc_loss@fas.rst.prstc': 2.1209447383880615, 'train@fas.rst.prstc_runtime': 48.4204, 'train@fas.rst.prstc_samples_per_second': 84.675, 'train@fas.rst.prstc_steps_per_second': 2.664, 'epoch': 8.0}
{'loss': 2.1728, 'learning_rate': 6.666666666666667e-06, 'epoch': 8.0}
{'eval_loss': 2.0342068672180176, 'eval_accuracy@fas.rst.prstc': 0.3727454909819639, 'eval_f1@fas.rst.prstc': 0.07790646052199775, 'eval_precision@fas.rst.prstc': 0.11933006275496047, 'eval_recall@fas.rst.prstc': 0.10848957747183355, 'eval_loss@fas.rst.prstc': 2.0342068672180176, 'eval_runtime': 6.1346, 'eval_samples_per_second': 81.342, 'eval_steps_per_second': 2.608, 'epoch': 8.0}
{'train@fas.rst.prstc_loss': 2.0889501571655273, 'train@fas.rst.prstc_accuracy@fas.rst.prstc': 0.37365853658536585, 'train@fas.rst.prstc_f1@fas.rst.prstc': 0.08590662098517052, 'train@fas.rst.prstc_precision@fas.rst.prstc': 0.08835652282364875, 'train@fas.rst.prstc_recall@fas.rst.prstc': 0.11152717990560874, 'train@fas.rst.prstc_loss@fas.rst.prstc': 2.0889503955841064, 'train@fas.rst.prstc_runtime': 48.3676, 'train@fas.rst.prstc_samples_per_second': 84.768, 'train@fas.rst.prstc_steps_per_second': 2.667, 'epoch': 9.0}
{'loss': 2.137, 'learning_rate': 5e-06, 'epoch': 9.0}
{'eval_loss': 2.0068447589874268, 'eval_accuracy@fas.rst.prstc': 0.3907815631262525, 'eval_f1@fas.rst.prstc': 0.10143221869356746, 'eval_precision@fas.rst.prstc': 0.1137396134924558, 'eval_recall@fas.rst.prstc': 0.12532067874604544, 'eval_loss@fas.rst.prstc': 2.0068445205688477, 'eval_runtime': 6.1354, 'eval_samples_per_second': 81.331, 'eval_steps_per_second': 2.608, 'epoch': 9.0}
{'train@fas.rst.prstc_loss': 2.0642573833465576, 'train@fas.rst.prstc_accuracy@fas.rst.prstc': 0.3824390243902439, 'train@fas.rst.prstc_f1@fas.rst.prstc': 0.09033101367539373, 'train@fas.rst.prstc_precision@fas.rst.prstc': 0.10032496343798698, 'train@fas.rst.prstc_recall@fas.rst.prstc': 0.11705065417122616, 'train@fas.rst.prstc_loss@fas.rst.prstc': 2.0642569065093994, 'train@fas.rst.prstc_runtime': 49.7107, 'train@fas.rst.prstc_samples_per_second': 82.477, 'train@fas.rst.prstc_steps_per_second': 2.595, 'epoch': 10.0}
{'loss': 2.1096, 'learning_rate': 3.3333333333333333e-06, 'epoch': 10.0}
{'eval_loss': 1.9863213300704956, 'eval_accuracy@fas.rst.prstc': 0.3927855711422846, 'eval_f1@fas.rst.prstc': 0.10238447471799424, 'eval_precision@fas.rst.prstc': 0.10711641299876593, 'eval_recall@fas.rst.prstc': 0.12707506471095772, 'eval_loss@fas.rst.prstc': 1.9863214492797852, 'eval_runtime': 6.1362, 'eval_samples_per_second': 81.321, 'eval_steps_per_second': 2.607, 'epoch': 10.0}
{'train@fas.rst.prstc_loss': 2.0530781745910645, 'train@fas.rst.prstc_accuracy@fas.rst.prstc': 0.38414634146341464, 'train@fas.rst.prstc_f1@fas.rst.prstc': 0.09183566778255506, 'train@fas.rst.prstc_precision@fas.rst.prstc': 0.10532100683471646, 'train@fas.rst.prstc_recall@fas.rst.prstc': 0.1182706278979936, 'train@fas.rst.prstc_loss@fas.rst.prstc': 2.0530784130096436, 'train@fas.rst.prstc_runtime': 48.556, 'train@fas.rst.prstc_samples_per_second': 84.439, 'train@fas.rst.prstc_steps_per_second': 2.657, 'epoch': 11.0}
{'loss': 2.1028, 'learning_rate': 1.6666666666666667e-06, 'epoch': 11.0}
{'eval_loss': 1.977152705192566, 'eval_accuracy@fas.rst.prstc': 0.3927855711422846, 'eval_f1@fas.rst.prstc': 0.10417901036945804, 'eval_precision@fas.rst.prstc': 0.13435121908985284, 'eval_recall@fas.rst.prstc': 0.12820475627543187, 'eval_loss@fas.rst.prstc': 1.9771524667739868, 'eval_runtime': 6.1622, 'eval_samples_per_second': 80.977, 'eval_steps_per_second': 2.596, 'epoch': 11.0}
{'train@fas.rst.prstc_loss': 2.0508785247802734, 'train@fas.rst.prstc_accuracy@fas.rst.prstc': 0.38439024390243903, 'train@fas.rst.prstc_f1@fas.rst.prstc': 0.09215138755490596, 'train@fas.rst.prstc_precision@fas.rst.prstc': 0.11039587677642783, 'train@fas.rst.prstc_recall@fas.rst.prstc': 0.11855413741804878, 'train@fas.rst.prstc_loss@fas.rst.prstc': 2.0508782863616943, 'train@fas.rst.prstc_runtime': 48.5755, 'train@fas.rst.prstc_samples_per_second': 84.405, 'train@fas.rst.prstc_steps_per_second': 2.656, 'epoch': 12.0}
{'loss': 2.0901, 'learning_rate': 0.0, 'epoch': 12.0}
{'eval_loss': 1.976565957069397, 'eval_accuracy@fas.rst.prstc': 0.3867735470941884, 'eval_f1@fas.rst.prstc': 0.1029046271694867, 'eval_precision@fas.rst.prstc': 0.1331509813940906, 'eval_recall@fas.rst.prstc': 0.12646562584064927, 'eval_loss@fas.rst.prstc': 1.9765660762786865, 'eval_runtime': 6.1523, 'eval_samples_per_second': 81.108, 'eval_steps_per_second': 2.601, 'epoch': 12.0}
{'train_runtime': 1882.436, 'train_samples_per_second': 26.136, 'train_steps_per_second': 0.822, 'train_loss': 2.2509215401740654, 'epoch': 12.0}
***** train_log metrics *****
  epoch                    =       12.0
  train_loss               =     2.2509
  train_runtime            = 0:31:22.43
  train_samples_per_second =     26.136
  train_steps_per_second   =      0.822
{'train@eng.rst.rstdt_loss': 1.6298903226852417, 'train@eng.rst.rstdt_accuracy@eng.rst.rstdt': 0.5484939382577178, 'train@eng.rst.rstdt_f1@eng.rst.rstdt': 0.13239626411626815, 'train@eng.rst.rstdt_precision@eng.rst.rstdt': 0.1298707644000192, 'train@eng.rst.rstdt_recall@eng.rst.rstdt': 0.14808389515409304, 'train@eng.rst.rstdt_loss@eng.rst.rstdt': 1.6298903226852417, 'train@eng.rst.rstdt_runtime': 188.8387, 'train@eng.rst.rstdt_samples_per_second': 84.739, 'train@eng.rst.rstdt_steps_per_second': 2.653, 'epoch': 1.0}
{'loss': 1.985, 'learning_rate': 1.8333333333333333e-05, 'epoch': 1.0}
{'eval_loss': 1.6261600255966187, 'eval_accuracy@eng.rst.rstdt': 0.5502776064157927, 'eval_f1@eng.rst.rstdt': 0.13084940779780188, 'eval_precision@eng.rst.rstdt': 0.12618648222178971, 'eval_recall@eng.rst.rstdt': 0.1461283860741711, 'eval_loss@eng.rst.rstdt': 1.6261600255966187, 'eval_runtime': 19.3728, 'eval_samples_per_second': 83.674, 'eval_steps_per_second': 2.633, 'epoch': 1.0}
{'train@eng.rst.rstdt_loss': 1.3618749380111694, 'train@eng.rst.rstdt_accuracy@eng.rst.rstdt': 0.6184851893513311, 'train@eng.rst.rstdt_f1@eng.rst.rstdt': 0.2641345370601575, 'train@eng.rst.rstdt_precision@eng.rst.rstdt': 0.40672549630750376, 'train@eng.rst.rstdt_recall@eng.rst.rstdt': 0.2602858068546625, 'train@eng.rst.rstdt_loss@eng.rst.rstdt': 1.361875057220459, 'train@eng.rst.rstdt_runtime': 188.671, 'train@eng.rst.rstdt_samples_per_second': 84.814, 'train@eng.rst.rstdt_steps_per_second': 2.655, 'epoch': 2.0}
{'loss': 1.5268, 'learning_rate': 1.6666666666666667e-05, 'epoch': 2.0}
{'eval_loss': 1.3782470226287842, 'eval_accuracy@eng.rst.rstdt': 0.6156693399136336, 'eval_f1@eng.rst.rstdt': 0.24581164215055462, 'eval_precision@eng.rst.rstdt': 0.289233203712961, 'eval_recall@eng.rst.rstdt': 0.2475343335606467, 'eval_loss@eng.rst.rstdt': 1.3782471418380737, 'eval_runtime': 19.3582, 'eval_samples_per_second': 83.737, 'eval_steps_per_second': 2.635, 'epoch': 2.0}
{'train@eng.rst.rstdt_loss': 1.260286569595337, 'train@eng.rst.rstdt_accuracy@eng.rst.rstdt': 0.6434820647419073, 'train@eng.rst.rstdt_f1@eng.rst.rstdt': 0.3377785615823936, 'train@eng.rst.rstdt_precision@eng.rst.rstdt': 0.5106922242140455, 'train@eng.rst.rstdt_recall@eng.rst.rstdt': 0.31679742345203943, 'train@eng.rst.rstdt_loss@eng.rst.rstdt': 1.2602866888046265, 'train@eng.rst.rstdt_runtime': 189.2039, 'train@eng.rst.rstdt_samples_per_second': 84.575, 'train@eng.rst.rstdt_steps_per_second': 2.648, 'epoch': 3.0}
{'loss': 1.3685, 'learning_rate': 1.5000000000000002e-05, 'epoch': 3.0}
{'eval_loss': 1.2977004051208496, 'eval_accuracy@eng.rst.rstdt': 0.6304750154225787, 'eval_f1@eng.rst.rstdt': 0.3217605649185362, 'eval_precision@eng.rst.rstdt': 0.4475106309275154, 'eval_recall@eng.rst.rstdt': 0.31064886635062994, 'eval_loss@eng.rst.rstdt': 1.2977004051208496, 'eval_runtime': 19.4111, 'eval_samples_per_second': 83.509, 'eval_steps_per_second': 2.627, 'epoch': 3.0}
{'train@eng.rst.rstdt_loss': 1.2056586742401123, 'train@eng.rst.rstdt_accuracy@eng.rst.rstdt': 0.6517935258092739, 'train@eng.rst.rstdt_f1@eng.rst.rstdt': 0.36064248423850664, 'train@eng.rst.rstdt_precision@eng.rst.rstdt': 0.4721714058359568, 'train@eng.rst.rstdt_recall@eng.rst.rstdt': 0.3419470091664453, 'train@eng.rst.rstdt_loss@eng.rst.rstdt': 1.2056587934494019, 'train@eng.rst.rstdt_runtime': 188.536, 'train@eng.rst.rstdt_samples_per_second': 84.875, 'train@eng.rst.rstdt_steps_per_second': 2.657, 'epoch': 4.0}
{'loss': 1.2846, 'learning_rate': 1.3333333333333333e-05, 'epoch': 4.0}
{'eval_loss': 1.265089750289917, 'eval_accuracy@eng.rst.rstdt': 0.6261566933991364, 'eval_f1@eng.rst.rstdt': 0.3302241852808243, 'eval_precision@eng.rst.rstdt': 0.42123718005822197, 'eval_recall@eng.rst.rstdt': 0.3265475984779436, 'eval_loss@eng.rst.rstdt': 1.265089750289917, 'eval_runtime': 19.3609, 'eval_samples_per_second': 83.725, 'eval_steps_per_second': 2.634, 'epoch': 4.0}
{'train@eng.rst.rstdt_loss': 1.1651908159255981, 'train@eng.rst.rstdt_accuracy@eng.rst.rstdt': 0.6601674790651169, 'train@eng.rst.rstdt_f1@eng.rst.rstdt': 0.36860763972980665, 'train@eng.rst.rstdt_precision@eng.rst.rstdt': 0.5178852977247524, 'train@eng.rst.rstdt_recall@eng.rst.rstdt': 0.3465085835431312, 'train@eng.rst.rstdt_loss@eng.rst.rstdt': 1.1651909351348877, 'train@eng.rst.rstdt_runtime': 188.7241, 'train@eng.rst.rstdt_samples_per_second': 84.79, 'train@eng.rst.rstdt_steps_per_second': 2.655, 'epoch': 5.0}
{'loss': 1.2348, 'learning_rate': 1.1666666666666668e-05, 'epoch': 5.0}
{'eval_loss': 1.2314343452453613, 'eval_accuracy@eng.rst.rstdt': 0.6384947563232573, 'eval_f1@eng.rst.rstdt': 0.33475398913959875, 'eval_precision@eng.rst.rstdt': 0.4281039817976448, 'eval_recall@eng.rst.rstdt': 0.32588017595279634, 'eval_loss@eng.rst.rstdt': 1.2314342260360718, 'eval_runtime': 19.3911, 'eval_samples_per_second': 83.595, 'eval_steps_per_second': 2.63, 'epoch': 5.0}
{'train@eng.rst.rstdt_loss': 1.1371357440948486, 'train@eng.rst.rstdt_accuracy@eng.rst.rstdt': 0.6659167604049494, 'train@eng.rst.rstdt_f1@eng.rst.rstdt': 0.38926417507361455, 'train@eng.rst.rstdt_precision@eng.rst.rstdt': 0.5770950496836543, 'train@eng.rst.rstdt_recall@eng.rst.rstdt': 0.36052744989093877, 'train@eng.rst.rstdt_loss@eng.rst.rstdt': 1.1371357440948486, 'train@eng.rst.rstdt_runtime': 188.7078, 'train@eng.rst.rstdt_samples_per_second': 84.798, 'train@eng.rst.rstdt_steps_per_second': 2.655, 'epoch': 6.0}
{'loss': 1.1987, 'learning_rate': 1e-05, 'epoch': 6.0}
{'eval_loss': 1.2105587720870972, 'eval_accuracy@eng.rst.rstdt': 0.6378778531770513, 'eval_f1@eng.rst.rstdt': 0.3415761042629133, 'eval_precision@eng.rst.rstdt': 0.40820146170412946, 'eval_recall@eng.rst.rstdt': 0.33368544826402013, 'eval_loss@eng.rst.rstdt': 1.2105586528778076, 'eval_runtime': 19.4008, 'eval_samples_per_second': 83.553, 'eval_steps_per_second': 2.629, 'epoch': 6.0}
{'train@eng.rst.rstdt_loss': 1.120069146156311, 'train@eng.rst.rstdt_accuracy@eng.rst.rstdt': 0.6691663542057242, 'train@eng.rst.rstdt_f1@eng.rst.rstdt': 0.40268836888550474, 'train@eng.rst.rstdt_precision@eng.rst.rstdt': 0.6368394533453821, 'train@eng.rst.rstdt_recall@eng.rst.rstdt': 0.36806681511457534, 'train@eng.rst.rstdt_loss@eng.rst.rstdt': 1.120069146156311, 'train@eng.rst.rstdt_runtime': 188.6172, 'train@eng.rst.rstdt_samples_per_second': 84.839, 'train@eng.rst.rstdt_steps_per_second': 2.656, 'epoch': 7.0}
{'loss': 1.1814, 'learning_rate': 8.333333333333334e-06, 'epoch': 7.0}
{'eval_loss': 1.1967939138412476, 'eval_accuracy@eng.rst.rstdt': 0.6434299814929056, 'eval_f1@eng.rst.rstdt': 0.35236131621111055, 'eval_precision@eng.rst.rstdt': 0.46068896128782627, 'eval_recall@eng.rst.rstdt': 0.3418150654646863, 'eval_loss@eng.rst.rstdt': 1.196794033050537, 'eval_runtime': 19.3651, 'eval_samples_per_second': 83.707, 'eval_steps_per_second': 2.634, 'epoch': 7.0}
{'train@eng.rst.rstdt_loss': 1.1082929372787476, 'train@eng.rst.rstdt_accuracy@eng.rst.rstdt': 0.6712285964254469, 'train@eng.rst.rstdt_f1@eng.rst.rstdt': 0.4153449068264199, 'train@eng.rst.rstdt_precision@eng.rst.rstdt': 0.6295489335328646, 'train@eng.rst.rstdt_recall@eng.rst.rstdt': 0.3800645166126579, 'train@eng.rst.rstdt_loss@eng.rst.rstdt': 1.108292818069458, 'train@eng.rst.rstdt_runtime': 189.0958, 'train@eng.rst.rstdt_samples_per_second': 84.624, 'train@eng.rst.rstdt_steps_per_second': 2.649, 'epoch': 8.0}
{'loss': 1.1626, 'learning_rate': 6.666666666666667e-06, 'epoch': 8.0}
{'eval_loss': 1.1945781707763672, 'eval_accuracy@eng.rst.rstdt': 0.6440468846391116, 'eval_f1@eng.rst.rstdt': 0.3572366221303939, 'eval_precision@eng.rst.rstdt': 0.44935319810820157, 'eval_recall@eng.rst.rstdt': 0.34879725607081397, 'eval_loss@eng.rst.rstdt': 1.1945781707763672, 'eval_runtime': 19.4083, 'eval_samples_per_second': 83.521, 'eval_steps_per_second': 2.628, 'epoch': 8.0}
{'train@eng.rst.rstdt_loss': 1.0962870121002197, 'train@eng.rst.rstdt_accuracy@eng.rst.rstdt': 0.6751031121109862, 'train@eng.rst.rstdt_f1@eng.rst.rstdt': 0.4234246782920592, 'train@eng.rst.rstdt_precision@eng.rst.rstdt': 0.6244524816509751, 'train@eng.rst.rstdt_recall@eng.rst.rstdt': 0.38426018441693693, 'train@eng.rst.rstdt_loss@eng.rst.rstdt': 1.0962870121002197, 'train@eng.rst.rstdt_runtime': 188.8026, 'train@eng.rst.rstdt_samples_per_second': 84.755, 'train@eng.rst.rstdt_steps_per_second': 2.654, 'epoch': 9.0}
{'loss': 1.1507, 'learning_rate': 5e-06, 'epoch': 9.0}
{'eval_loss': 1.184565782546997, 'eval_accuracy@eng.rst.rstdt': 0.6446637877853177, 'eval_f1@eng.rst.rstdt': 0.3544769558469408, 'eval_precision@eng.rst.rstdt': 0.4548493131458305, 'eval_recall@eng.rst.rstdt': 0.3444424289761406, 'eval_loss@eng.rst.rstdt': 1.1845659017562866, 'eval_runtime': 19.351, 'eval_samples_per_second': 83.768, 'eval_steps_per_second': 2.636, 'epoch': 9.0}
{'train@eng.rst.rstdt_loss': 1.0922833681106567, 'train@eng.rst.rstdt_accuracy@eng.rst.rstdt': 0.673665791776028, 'train@eng.rst.rstdt_f1@eng.rst.rstdt': 0.43529284836209553, 'train@eng.rst.rstdt_precision@eng.rst.rstdt': 0.6059710216068839, 'train@eng.rst.rstdt_recall@eng.rst.rstdt': 0.397592074314845, 'train@eng.rst.rstdt_loss@eng.rst.rstdt': 1.0922834873199463, 'train@eng.rst.rstdt_runtime': 188.5407, 'train@eng.rst.rstdt_samples_per_second': 84.873, 'train@eng.rst.rstdt_steps_per_second': 2.657, 'epoch': 10.0}
{'loss': 1.1435, 'learning_rate': 3.3333333333333333e-06, 'epoch': 10.0}
{'eval_loss': 1.1893051862716675, 'eval_accuracy@eng.rst.rstdt': 0.6409623689080814, 'eval_f1@eng.rst.rstdt': 0.3684246706757282, 'eval_precision@eng.rst.rstdt': 0.4463577064278669, 'eval_recall@eng.rst.rstdt': 0.3599036613756672, 'eval_loss@eng.rst.rstdt': 1.189305305480957, 'eval_runtime': 19.3408, 'eval_samples_per_second': 83.813, 'eval_steps_per_second': 2.637, 'epoch': 10.0}
{'train@eng.rst.rstdt_loss': 1.085647463798523, 'train@eng.rst.rstdt_accuracy@eng.rst.rstdt': 0.6757280339957505, 'train@eng.rst.rstdt_f1@eng.rst.rstdt': 0.43322581872872595, 'train@eng.rst.rstdt_precision@eng.rst.rstdt': 0.6143912458638285, 'train@eng.rst.rstdt_recall@eng.rst.rstdt': 0.393383216797211, 'train@eng.rst.rstdt_loss@eng.rst.rstdt': 1.085647463798523, 'train@eng.rst.rstdt_runtime': 189.1581, 'train@eng.rst.rstdt_samples_per_second': 84.596, 'train@eng.rst.rstdt_steps_per_second': 2.649, 'epoch': 11.0}
{'loss': 1.1353, 'learning_rate': 1.6666666666666667e-06, 'epoch': 11.0}
{'eval_loss': 1.1806087493896484, 'eval_accuracy@eng.rst.rstdt': 0.6465144972239358, 'eval_f1@eng.rst.rstdt': 0.3618954068223067, 'eval_precision@eng.rst.rstdt': 0.4557493244615564, 'eval_recall@eng.rst.rstdt': 0.34997237342274373, 'eval_loss@eng.rst.rstdt': 1.1806087493896484, 'eval_runtime': 19.4045, 'eval_samples_per_second': 83.537, 'eval_steps_per_second': 2.628, 'epoch': 11.0}
{'train@eng.rst.rstdt_loss': 1.0844731330871582, 'train@eng.rst.rstdt_accuracy@eng.rst.rstdt': 0.6764779402574678, 'train@eng.rst.rstdt_f1@eng.rst.rstdt': 0.4342817825494218, 'train@eng.rst.rstdt_precision@eng.rst.rstdt': 0.6174126615085587, 'train@eng.rst.rstdt_recall@eng.rst.rstdt': 0.3937787184331399, 'train@eng.rst.rstdt_loss@eng.rst.rstdt': 1.0844732522964478, 'train@eng.rst.rstdt_runtime': 188.5013, 'train@eng.rst.rstdt_samples_per_second': 84.891, 'train@eng.rst.rstdt_steps_per_second': 2.658, 'epoch': 12.0}
{'loss': 1.1318, 'learning_rate': 0.0, 'epoch': 12.0}
{'eval_loss': 1.1794354915618896, 'eval_accuracy@eng.rst.rstdt': 0.6465144972239358, 'eval_f1@eng.rst.rstdt': 0.36476011603495784, 'eval_precision@eng.rst.rstdt': 0.4519442178155457, 'eval_recall@eng.rst.rstdt': 0.3517277862123675, 'eval_loss@eng.rst.rstdt': 1.1794356107711792, 'eval_runtime': 19.3519, 'eval_samples_per_second': 83.764, 'eval_steps_per_second': 2.635, 'epoch': 12.0}
{'train_runtime': 7279.4537, 'train_samples_per_second': 26.379, 'train_steps_per_second': 0.826, 'train_loss': 1.2919825428894816, 'epoch': 12.0}
***** train_logger metrics *****
  epoch                    =       12.0
  train_loss               =     2.2509
  train_runtime            = 0:31:22.43
  train_samples_per_second =     26.136
  train_steps_per_second   =      0.822
-------------------------------------------------------------------
Lang1:  fra.sdrt.annodis    Lang2:  eng.rst.rstdt
Saving run to:  runs/full_shot/FullShot=v4_fra.sdrt.annodis_eng.rst.rstdt_lrfunc=Adafactor_lr=2e-05_plm=bert-base-multilingual-cased
Running with params: BERT_MODEL=bert-base-multilingual-cased lr=2e-05
read 2185 examples
read 528 examples
read 625 examples
read 16002 examples
read 1621 examples
read 2155 examples
Total prediction labels:  30
BertAdapterModel(
  (shared_parameters): ModuleDict()
  (bert): BertModel(
    (shared_parameters): ModuleDict()
    (invertible_adapters): ModuleDict(
      (fr): NICECouplingBlock(
        (F): Sequential(
          (0): Linear(in_features=384, out_features=192, bias=True)
          (1): Activation_Function_Class(
            (f): NewGELUActivation()
          )
          (2): Linear(in_features=192, out_features=384, bias=True)
        )
        (G): Sequential(
          (0): Linear(in_features=384, out_features=192, bias=True)
          (1): Activation_Function_Class(
            (f): NewGELUActivation()
          )
          (2): Linear(in_features=192, out_features=384, bias=True)
        )
      )
      (en): NICECouplingBlock(
        (F): Sequential(
          (0): Linear(in_features=384, out_features=192, bias=True)
          (1): Activation_Function_Class(
            (f): ReLU()
          )
          (2): Linear(in_features=192, out_features=384, bias=True)
        )
        (G): Sequential(
          (0): Linear(in_features=384, out_features=192, bias=True)
          (1): Activation_Function_Class(
            (f): ReLU()
          )
          (2): Linear(in_features=192, out_features=384, bias=True)
        )
      )
    )
    (embeddings): BertEmbeddings(
      (word_embeddings): Embedding(119547, 768, padding_idx=0)
      (position_embeddings): Embedding(512, 768)
      (token_type_embeddings): Embedding(2, 768)
      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
      (dropout): Dropout(p=0.1, inplace=False)
    )
    (encoder): BertEncoder(
      (layer): ModuleList(
        (0): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (fr): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (en): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (1): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (fr): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (en): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (2): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (fr): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (en): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (3): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (fr): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (en): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (4): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (fr): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (en): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (5): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (fr): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (en): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (6): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (fr): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (en): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (7): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (fr): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (en): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (8): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (fr): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (en): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (9): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (fr): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (en): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (10): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (fr): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (en): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (11): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (fr): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (en): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
      )
    )
    (pooler): BertPooler(
      (dense): Linear(in_features=768, out_features=768, bias=True)
      (activation): Tanh()
    )
    (prefix_tuning): PrefixTuningPool(
      (prefix_tunings): ModuleDict()
    )
  )
  (heads): ModuleDict(
    (disrpt): ClassificationHead(
      (0): Dropout(p=0.1, inplace=False)
      (1): Linear(in_features=768, out_features=768, bias=True)
      (2): Activation_Function_Class(
        (f): Tanh()
      )
      (3): Dropout(p=0.1, inplace=False)
      (4): Linear(in_features=768, out_features=30, bias=True)
    )
  )
)
{'train@fra.sdrt.annodis_loss': 2.509974718093872, 'train@fra.sdrt.annodis_accuracy@fra.sdrt.annodis': 0.2334096109839817, 'train@fra.sdrt.annodis_f1@fra.sdrt.annodis': 0.04279342824160351, 'train@fra.sdrt.annodis_precision@fra.sdrt.annodis': 0.04578463376421903, 'train@fra.sdrt.annodis_recall@fra.sdrt.annodis': 0.06613380472910328, 'train@fra.sdrt.annodis_loss@fra.sdrt.annodis': 2.509974718093872, 'train@fra.sdrt.annodis_runtime': 29.1352, 'train@fra.sdrt.annodis_samples_per_second': 74.995, 'train@fra.sdrt.annodis_steps_per_second': 2.368, 'epoch': 1.0}
{'loss': 2.8842, 'learning_rate': 1.8333333333333333e-05, 'epoch': 1.0}
{'eval_loss': 2.5215446949005127, 'eval_accuracy@fra.sdrt.annodis': 0.20833333333333334, 'eval_f1@fra.sdrt.annodis': 0.03470236449302472, 'eval_precision@fra.sdrt.annodis': 0.03851979718398521, 'eval_recall@fra.sdrt.annodis': 0.05859268753003609, 'eval_loss@fra.sdrt.annodis': 2.5215444564819336, 'eval_runtime': 6.5892, 'eval_samples_per_second': 80.131, 'eval_steps_per_second': 2.58, 'epoch': 1.0}
{'train@fra.sdrt.annodis_loss': 2.34584379196167, 'train@fra.sdrt.annodis_accuracy@fra.sdrt.annodis': 0.2732265446224256, 'train@fra.sdrt.annodis_f1@fra.sdrt.annodis': 0.05884083977673233, 'train@fra.sdrt.annodis_precision@fra.sdrt.annodis': 0.07362509730488119, 'train@fra.sdrt.annodis_recall@fra.sdrt.annodis': 0.08183729884666044, 'train@fra.sdrt.annodis_loss@fra.sdrt.annodis': 2.345843553543091, 'train@fra.sdrt.annodis_runtime': 26.4079, 'train@fra.sdrt.annodis_samples_per_second': 82.74, 'train@fra.sdrt.annodis_steps_per_second': 2.613, 'epoch': 2.0}
{'loss': 2.4414, 'learning_rate': 1.6666666666666667e-05, 'epoch': 2.0}
{'eval_loss': 2.362936496734619, 'eval_accuracy@fra.sdrt.annodis': 0.2518939393939394, 'eval_f1@fra.sdrt.annodis': 0.053611239804453086, 'eval_precision@fra.sdrt.annodis': 0.04305908643961741, 'eval_recall@fra.sdrt.annodis': 0.07337954084233073, 'eval_loss@fra.sdrt.annodis': 2.362936496734619, 'eval_runtime': 6.6009, 'eval_samples_per_second': 79.989, 'eval_steps_per_second': 2.575, 'epoch': 2.0}
{'train@fra.sdrt.annodis_loss': 2.2766005992889404, 'train@fra.sdrt.annodis_accuracy@fra.sdrt.annodis': 0.29016018306636154, 'train@fra.sdrt.annodis_f1@fra.sdrt.annodis': 0.06455379064122499, 'train@fra.sdrt.annodis_precision@fra.sdrt.annodis': 0.09118471952011503, 'train@fra.sdrt.annodis_recall@fra.sdrt.annodis': 0.08563969045986072, 'train@fra.sdrt.annodis_loss@fra.sdrt.annodis': 2.2766005992889404, 'train@fra.sdrt.annodis_runtime': 26.4255, 'train@fra.sdrt.annodis_samples_per_second': 82.685, 'train@fra.sdrt.annodis_steps_per_second': 2.611, 'epoch': 3.0}
{'loss': 2.3368, 'learning_rate': 1.5000000000000002e-05, 'epoch': 3.0}
{'eval_loss': 2.303363800048828, 'eval_accuracy@fra.sdrt.annodis': 0.25946969696969696, 'eval_f1@fra.sdrt.annodis': 0.05484571354655465, 'eval_precision@fra.sdrt.annodis': 0.04787836744856377, 'eval_recall@fra.sdrt.annodis': 0.0747540548509742, 'eval_loss@fra.sdrt.annodis': 2.303363561630249, 'eval_runtime': 6.6298, 'eval_samples_per_second': 79.641, 'eval_steps_per_second': 2.564, 'epoch': 3.0}
{'train@fra.sdrt.annodis_loss': 2.2196407318115234, 'train@fra.sdrt.annodis_accuracy@fra.sdrt.annodis': 0.3148741418764302, 'train@fra.sdrt.annodis_f1@fra.sdrt.annodis': 0.07712542304990633, 'train@fra.sdrt.annodis_precision@fra.sdrt.annodis': 0.09083472070562021, 'train@fra.sdrt.annodis_recall@fra.sdrt.annodis': 0.09555379987590748, 'train@fra.sdrt.annodis_loss@fra.sdrt.annodis': 2.2196407318115234, 'train@fra.sdrt.annodis_runtime': 26.4374, 'train@fra.sdrt.annodis_samples_per_second': 82.648, 'train@fra.sdrt.annodis_steps_per_second': 2.61, 'epoch': 4.0}
{'loss': 2.2875, 'learning_rate': 1.3333333333333333e-05, 'epoch': 4.0}
{'eval_loss': 2.2535557746887207, 'eval_accuracy@fra.sdrt.annodis': 0.2727272727272727, 'eval_f1@fra.sdrt.annodis': 0.059161020294060576, 'eval_precision@fra.sdrt.annodis': 0.05065482144251134, 'eval_recall@fra.sdrt.annodis': 0.07884935818476836, 'eval_loss@fra.sdrt.annodis': 2.2535557746887207, 'eval_runtime': 6.6223, 'eval_samples_per_second': 79.731, 'eval_steps_per_second': 2.567, 'epoch': 4.0}
{'train@fra.sdrt.annodis_loss': 2.1690895557403564, 'train@fra.sdrt.annodis_accuracy@fra.sdrt.annodis': 0.3528604118993135, 'train@fra.sdrt.annodis_f1@fra.sdrt.annodis': 0.09068045365303078, 'train@fra.sdrt.annodis_precision@fra.sdrt.annodis': 0.12599820933172617, 'train@fra.sdrt.annodis_recall@fra.sdrt.annodis': 0.11171174789176935, 'train@fra.sdrt.annodis_loss@fra.sdrt.annodis': 2.1690895557403564, 'train@fra.sdrt.annodis_runtime': 26.4861, 'train@fra.sdrt.annodis_samples_per_second': 82.496, 'train@fra.sdrt.annodis_steps_per_second': 2.605, 'epoch': 5.0}
{'loss': 2.2345, 'learning_rate': 1.1666666666666668e-05, 'epoch': 5.0}
{'eval_loss': 2.2131686210632324, 'eval_accuracy@fra.sdrt.annodis': 0.29545454545454547, 'eval_f1@fra.sdrt.annodis': 0.06327955403068525, 'eval_precision@fra.sdrt.annodis': 0.05018979825376746, 'eval_recall@fra.sdrt.annodis': 0.0885066444083405, 'eval_loss@fra.sdrt.annodis': 2.213168144226074, 'eval_runtime': 6.6335, 'eval_samples_per_second': 79.596, 'eval_steps_per_second': 2.563, 'epoch': 5.0}
{'train@fra.sdrt.annodis_loss': 2.1218338012695312, 'train@fra.sdrt.annodis_accuracy@fra.sdrt.annodis': 0.3871853546910755, 'train@fra.sdrt.annodis_f1@fra.sdrt.annodis': 0.1183713747515588, 'train@fra.sdrt.annodis_precision@fra.sdrt.annodis': 0.12676774137201086, 'train@fra.sdrt.annodis_recall@fra.sdrt.annodis': 0.1358108988186368, 'train@fra.sdrt.annodis_loss@fra.sdrt.annodis': 2.121833562850952, 'train@fra.sdrt.annodis_runtime': 26.4644, 'train@fra.sdrt.annodis_samples_per_second': 82.564, 'train@fra.sdrt.annodis_steps_per_second': 2.607, 'epoch': 6.0}
{'loss': 2.1813, 'learning_rate': 1e-05, 'epoch': 6.0}
{'eval_loss': 2.1739046573638916, 'eval_accuracy@fra.sdrt.annodis': 0.3143939393939394, 'eval_f1@fra.sdrt.annodis': 0.08014999536277309, 'eval_precision@fra.sdrt.annodis': 0.08004492783792481, 'eval_recall@fra.sdrt.annodis': 0.10136866041616312, 'eval_loss@fra.sdrt.annodis': 2.1739048957824707, 'eval_runtime': 6.6294, 'eval_samples_per_second': 79.645, 'eval_steps_per_second': 2.564, 'epoch': 6.0}
{'train@fra.sdrt.annodis_loss': 2.077733039855957, 'train@fra.sdrt.annodis_accuracy@fra.sdrt.annodis': 0.40183066361556063, 'train@fra.sdrt.annodis_f1@fra.sdrt.annodis': 0.12954136693073554, 'train@fra.sdrt.annodis_precision@fra.sdrt.annodis': 0.12633034812112603, 'train@fra.sdrt.annodis_recall@fra.sdrt.annodis': 0.14915962429966712, 'train@fra.sdrt.annodis_loss@fra.sdrt.annodis': 2.077733039855957, 'train@fra.sdrt.annodis_runtime': 26.4377, 'train@fra.sdrt.annodis_samples_per_second': 82.647, 'train@fra.sdrt.annodis_steps_per_second': 2.61, 'epoch': 7.0}
{'loss': 2.1426, 'learning_rate': 8.333333333333334e-06, 'epoch': 7.0}
{'eval_loss': 2.1397457122802734, 'eval_accuracy@fra.sdrt.annodis': 0.32954545454545453, 'eval_f1@fra.sdrt.annodis': 0.08988353221797461, 'eval_precision@fra.sdrt.annodis': 0.09445915487479824, 'eval_recall@fra.sdrt.annodis': 0.1090353849124882, 'eval_loss@fra.sdrt.annodis': 2.1397459506988525, 'eval_runtime': 6.6463, 'eval_samples_per_second': 79.443, 'eval_steps_per_second': 2.558, 'epoch': 7.0}
{'train@fra.sdrt.annodis_loss': 2.0397098064422607, 'train@fra.sdrt.annodis_accuracy@fra.sdrt.annodis': 0.4183066361556064, 'train@fra.sdrt.annodis_f1@fra.sdrt.annodis': 0.13619180815095266, 'train@fra.sdrt.annodis_precision@fra.sdrt.annodis': 0.12732739461952194, 'train@fra.sdrt.annodis_recall@fra.sdrt.annodis': 0.1596999044538487, 'train@fra.sdrt.annodis_loss@fra.sdrt.annodis': 2.0397098064422607, 'train@fra.sdrt.annodis_runtime': 26.4545, 'train@fra.sdrt.annodis_samples_per_second': 82.595, 'train@fra.sdrt.annodis_steps_per_second': 2.608, 'epoch': 8.0}
{'loss': 2.1058, 'learning_rate': 6.666666666666667e-06, 'epoch': 8.0}
{'eval_loss': 2.1108570098876953, 'eval_accuracy@fra.sdrt.annodis': 0.32575757575757575, 'eval_f1@fra.sdrt.annodis': 0.09340835672392418, 'eval_precision@fra.sdrt.annodis': 0.09152235962502585, 'eval_recall@fra.sdrt.annodis': 0.11214207216239795, 'eval_loss@fra.sdrt.annodis': 2.1108572483062744, 'eval_runtime': 6.6418, 'eval_samples_per_second': 79.496, 'eval_steps_per_second': 2.56, 'epoch': 8.0}
{'train@fra.sdrt.annodis_loss': 2.0102877616882324, 'train@fra.sdrt.annodis_accuracy@fra.sdrt.annodis': 0.4274599542334096, 'train@fra.sdrt.annodis_f1@fra.sdrt.annodis': 0.13920427181398576, 'train@fra.sdrt.annodis_precision@fra.sdrt.annodis': 0.12905867824590578, 'train@fra.sdrt.annodis_recall@fra.sdrt.annodis': 0.1668253963032661, 'train@fra.sdrt.annodis_loss@fra.sdrt.annodis': 2.0102875232696533, 'train@fra.sdrt.annodis_runtime': 26.4887, 'train@fra.sdrt.annodis_samples_per_second': 82.488, 'train@fra.sdrt.annodis_steps_per_second': 2.605, 'epoch': 9.0}
{'loss': 2.0695, 'learning_rate': 5e-06, 'epoch': 9.0}
{'eval_loss': 2.0875492095947266, 'eval_accuracy@fra.sdrt.annodis': 0.3371212121212121, 'eval_f1@fra.sdrt.annodis': 0.09759411691910237, 'eval_precision@fra.sdrt.annodis': 0.09394177477100975, 'eval_recall@fra.sdrt.annodis': 0.11798686998691602, 'eval_loss@fra.sdrt.annodis': 2.0875489711761475, 'eval_runtime': 6.6631, 'eval_samples_per_second': 79.242, 'eval_steps_per_second': 2.551, 'epoch': 9.0}
{'train@fra.sdrt.annodis_loss': 1.9892488718032837, 'train@fra.sdrt.annodis_accuracy@fra.sdrt.annodis': 0.4260869565217391, 'train@fra.sdrt.annodis_f1@fra.sdrt.annodis': 0.1385316380406003, 'train@fra.sdrt.annodis_precision@fra.sdrt.annodis': 0.1275178113700061, 'train@fra.sdrt.annodis_recall@fra.sdrt.annodis': 0.16676584365794678, 'train@fra.sdrt.annodis_loss@fra.sdrt.annodis': 1.9892488718032837, 'train@fra.sdrt.annodis_runtime': 26.487, 'train@fra.sdrt.annodis_samples_per_second': 82.493, 'train@fra.sdrt.annodis_steps_per_second': 2.605, 'epoch': 10.0}
{'loss': 2.0497, 'learning_rate': 3.3333333333333333e-06, 'epoch': 10.0}
{'eval_loss': 2.07169246673584, 'eval_accuracy@fra.sdrt.annodis': 0.3371212121212121, 'eval_f1@fra.sdrt.annodis': 0.09749512275978373, 'eval_precision@fra.sdrt.annodis': 0.09172676282051283, 'eval_recall@fra.sdrt.annodis': 0.11796815968783189, 'eval_loss@fra.sdrt.annodis': 2.071692705154419, 'eval_runtime': 6.6377, 'eval_samples_per_second': 79.545, 'eval_steps_per_second': 2.561, 'epoch': 10.0}
{'train@fra.sdrt.annodis_loss': 1.9767093658447266, 'train@fra.sdrt.annodis_accuracy@fra.sdrt.annodis': 0.4260869565217391, 'train@fra.sdrt.annodis_f1@fra.sdrt.annodis': 0.13796804323951645, 'train@fra.sdrt.annodis_precision@fra.sdrt.annodis': 0.12651137054408168, 'train@fra.sdrt.annodis_recall@fra.sdrt.annodis': 0.16682486886941106, 'train@fra.sdrt.annodis_loss@fra.sdrt.annodis': 1.9767093658447266, 'train@fra.sdrt.annodis_runtime': 26.5071, 'train@fra.sdrt.annodis_samples_per_second': 82.431, 'train@fra.sdrt.annodis_steps_per_second': 2.603, 'epoch': 11.0}
{'loss': 2.0243, 'learning_rate': 1.6666666666666667e-06, 'epoch': 11.0}
{'eval_loss': 2.06134033203125, 'eval_accuracy@fra.sdrt.annodis': 0.3371212121212121, 'eval_f1@fra.sdrt.annodis': 0.0960751874838322, 'eval_precision@fra.sdrt.annodis': 0.08859316422898993, 'eval_recall@fra.sdrt.annodis': 0.11798686998691602, 'eval_loss@fra.sdrt.annodis': 2.06134033203125, 'eval_runtime': 6.6411, 'eval_samples_per_second': 79.505, 'eval_steps_per_second': 2.56, 'epoch': 11.0}
{'train@fra.sdrt.annodis_loss': 1.9722826480865479, 'train@fra.sdrt.annodis_accuracy@fra.sdrt.annodis': 0.4297482837528604, 'train@fra.sdrt.annodis_f1@fra.sdrt.annodis': 0.13930960326387776, 'train@fra.sdrt.annodis_precision@fra.sdrt.annodis': 0.12687033013560145, 'train@fra.sdrt.annodis_recall@fra.sdrt.annodis': 0.16842405114661751, 'train@fra.sdrt.annodis_loss@fra.sdrt.annodis': 1.972282886505127, 'train@fra.sdrt.annodis_runtime': 26.5364, 'train@fra.sdrt.annodis_samples_per_second': 82.34, 'train@fra.sdrt.annodis_steps_per_second': 2.6, 'epoch': 12.0}
{'loss': 2.0201, 'learning_rate': 0.0, 'epoch': 12.0}
{'eval_loss': 2.058246612548828, 'eval_accuracy@fra.sdrt.annodis': 0.3333333333333333, 'eval_f1@fra.sdrt.annodis': 0.09529495006568482, 'eval_precision@fra.sdrt.annodis': 0.0871639125681209, 'eval_recall@fra.sdrt.annodis': 0.11694844838774676, 'eval_loss@fra.sdrt.annodis': 2.058246612548828, 'eval_runtime': 6.658, 'eval_samples_per_second': 79.303, 'eval_steps_per_second': 2.553, 'epoch': 12.0}
{'train_runtime': 1064.8938, 'train_samples_per_second': 24.622, 'train_steps_per_second': 0.778, 'train_loss': 2.231481764051649, 'epoch': 12.0}
***** train_log metrics *****
  epoch                    =       12.0
  train_loss               =     2.2315
  train_runtime            = 0:17:44.89
  train_samples_per_second =     24.622
  train_steps_per_second   =      0.778
{'train@eng.rst.rstdt_loss': 1.5608090162277222, 'train@eng.rst.rstdt_accuracy@eng.rst.rstdt': 0.5563054618172728, 'train@eng.rst.rstdt_f1@eng.rst.rstdt': 0.13491804843673338, 'train@eng.rst.rstdt_precision@eng.rst.rstdt': 0.1610561523660049, 'train@eng.rst.rstdt_recall@eng.rst.rstdt': 0.15004125978858493, 'train@eng.rst.rstdt_loss@eng.rst.rstdt': 1.5608091354370117, 'train@eng.rst.rstdt_runtime': 188.6301, 'train@eng.rst.rstdt_samples_per_second': 84.833, 'train@eng.rst.rstdt_steps_per_second': 2.656, 'epoch': 1.0}
{'loss': 1.9476, 'learning_rate': 1.8333333333333333e-05, 'epoch': 1.0}
{'eval_loss': 1.557215929031372, 'eval_accuracy@eng.rst.rstdt': 0.5533621221468229, 'eval_f1@eng.rst.rstdt': 0.13333652203862598, 'eval_precision@eng.rst.rstdt': 0.15888785727311552, 'eval_recall@eng.rst.rstdt': 0.14544155702967138, 'eval_loss@eng.rst.rstdt': 1.5572161674499512, 'eval_runtime': 19.402, 'eval_samples_per_second': 83.548, 'eval_steps_per_second': 2.629, 'epoch': 1.0}
{'train@eng.rst.rstdt_loss': 1.364608645439148, 'train@eng.rst.rstdt_accuracy@eng.rst.rstdt': 0.6156730408698913, 'train@eng.rst.rstdt_f1@eng.rst.rstdt': 0.23170990608073674, 'train@eng.rst.rstdt_precision@eng.rst.rstdt': 0.41391222739794475, 'train@eng.rst.rstdt_recall@eng.rst.rstdt': 0.2364849677537851, 'train@eng.rst.rstdt_loss@eng.rst.rstdt': 1.3646085262298584, 'train@eng.rst.rstdt_runtime': 189.2165, 'train@eng.rst.rstdt_samples_per_second': 84.57, 'train@eng.rst.rstdt_steps_per_second': 2.648, 'epoch': 2.0}
{'loss': 1.4995, 'learning_rate': 1.6666666666666667e-05, 'epoch': 2.0}
{'eval_loss': 1.3772263526916504, 'eval_accuracy@eng.rst.rstdt': 0.6230721776681061, 'eval_f1@eng.rst.rstdt': 0.2501825874446583, 'eval_precision@eng.rst.rstdt': 0.3435197678986843, 'eval_recall@eng.rst.rstdt': 0.2507965179966014, 'eval_loss@eng.rst.rstdt': 1.37722647190094, 'eval_runtime': 19.4772, 'eval_samples_per_second': 83.225, 'eval_steps_per_second': 2.618, 'epoch': 2.0}
{'train@eng.rst.rstdt_loss': 1.270052194595337, 'train@eng.rst.rstdt_accuracy@eng.rst.rstdt': 0.6432945881764779, 'train@eng.rst.rstdt_f1@eng.rst.rstdt': 0.3254271568568279, 'train@eng.rst.rstdt_precision@eng.rst.rstdt': 0.44722298014088646, 'train@eng.rst.rstdt_recall@eng.rst.rstdt': 0.3045735263721179, 'train@eng.rst.rstdt_loss@eng.rst.rstdt': 1.270052194595337, 'train@eng.rst.rstdt_runtime': 189.1322, 'train@eng.rst.rstdt_samples_per_second': 84.607, 'train@eng.rst.rstdt_steps_per_second': 2.649, 'epoch': 3.0}
{'loss': 1.3755, 'learning_rate': 1.5000000000000002e-05, 'epoch': 3.0}
{'eval_loss': 1.303420066833496, 'eval_accuracy@eng.rst.rstdt': 0.6372609500308452, 'eval_f1@eng.rst.rstdt': 0.3175510284801373, 'eval_precision@eng.rst.rstdt': 0.3943793156321964, 'eval_recall@eng.rst.rstdt': 0.308660363738616, 'eval_loss@eng.rst.rstdt': 1.303420066833496, 'eval_runtime': 19.433, 'eval_samples_per_second': 83.415, 'eval_steps_per_second': 2.624, 'epoch': 3.0}
{'train@eng.rst.rstdt_loss': 1.2095457315444946, 'train@eng.rst.rstdt_accuracy@eng.rst.rstdt': 0.6504811898512686, 'train@eng.rst.rstdt_f1@eng.rst.rstdt': 0.3555923086974971, 'train@eng.rst.rstdt_precision@eng.rst.rstdt': 0.49873272993406154, 'train@eng.rst.rstdt_recall@eng.rst.rstdt': 0.3338111683117251, 'train@eng.rst.rstdt_loss@eng.rst.rstdt': 1.2095458507537842, 'train@eng.rst.rstdt_runtime': 188.8513, 'train@eng.rst.rstdt_samples_per_second': 84.733, 'train@eng.rst.rstdt_steps_per_second': 2.653, 'epoch': 4.0}
{'loss': 1.2915, 'learning_rate': 1.3333333333333333e-05, 'epoch': 4.0}
{'eval_loss': 1.2714747190475464, 'eval_accuracy@eng.rst.rstdt': 0.6378778531770513, 'eval_f1@eng.rst.rstdt': 0.3298980228320493, 'eval_precision@eng.rst.rstdt': 0.3944646811637952, 'eval_recall@eng.rst.rstdt': 0.32556401017661113, 'eval_loss@eng.rst.rstdt': 1.2714747190475464, 'eval_runtime': 19.4602, 'eval_samples_per_second': 83.298, 'eval_steps_per_second': 2.621, 'epoch': 4.0}
{'train@eng.rst.rstdt_loss': 1.1671234369277954, 'train@eng.rst.rstdt_accuracy@eng.rst.rstdt': 0.6598550181227346, 'train@eng.rst.rstdt_f1@eng.rst.rstdt': 0.3753652745407815, 'train@eng.rst.rstdt_precision@eng.rst.rstdt': 0.5584451909542216, 'train@eng.rst.rstdt_recall@eng.rst.rstdt': 0.34768874953213275, 'train@eng.rst.rstdt_loss@eng.rst.rstdt': 1.1671233177185059, 'train@eng.rst.rstdt_runtime': 188.827, 'train@eng.rst.rstdt_samples_per_second': 84.744, 'train@eng.rst.rstdt_steps_per_second': 2.653, 'epoch': 5.0}
{'loss': 1.2404, 'learning_rate': 1.1666666666666668e-05, 'epoch': 5.0}
{'eval_loss': 1.2367630004882812, 'eval_accuracy@eng.rst.rstdt': 0.6428130783466995, 'eval_f1@eng.rst.rstdt': 0.353259945407258, 'eval_precision@eng.rst.rstdt': 0.5365180584812863, 'eval_recall@eng.rst.rstdt': 0.3388896449654724, 'eval_loss@eng.rst.rstdt': 1.2367630004882812, 'eval_runtime': 19.4228, 'eval_samples_per_second': 83.459, 'eval_steps_per_second': 2.626, 'epoch': 5.0}
{'train@eng.rst.rstdt_loss': 1.1390912532806396, 'train@eng.rst.rstdt_accuracy@eng.rst.rstdt': 0.6647919010123734, 'train@eng.rst.rstdt_f1@eng.rst.rstdt': 0.3902092547254701, 'train@eng.rst.rstdt_precision@eng.rst.rstdt': 0.5440216563086685, 'train@eng.rst.rstdt_recall@eng.rst.rstdt': 0.3550188184041281, 'train@eng.rst.rstdt_loss@eng.rst.rstdt': 1.1390912532806396, 'train@eng.rst.rstdt_runtime': 188.8244, 'train@eng.rst.rstdt_samples_per_second': 84.745, 'train@eng.rst.rstdt_steps_per_second': 2.653, 'epoch': 6.0}
{'loss': 1.2026, 'learning_rate': 1e-05, 'epoch': 6.0}
{'eval_loss': 1.2141634225845337, 'eval_accuracy@eng.rst.rstdt': 0.6434299814929056, 'eval_f1@eng.rst.rstdt': 0.3597573651706136, 'eval_precision@eng.rst.rstdt': 0.528535844850608, 'eval_recall@eng.rst.rstdt': 0.34304037058804737, 'eval_loss@eng.rst.rstdt': 1.2141634225845337, 'eval_runtime': 19.4338, 'eval_samples_per_second': 83.411, 'eval_steps_per_second': 2.624, 'epoch': 6.0}
{'train@eng.rst.rstdt_loss': 1.1215823888778687, 'train@eng.rst.rstdt_accuracy@eng.rst.rstdt': 0.6677915260592426, 'train@eng.rst.rstdt_f1@eng.rst.rstdt': 0.40251077314509986, 'train@eng.rst.rstdt_precision@eng.rst.rstdt': 0.5687922840196533, 'train@eng.rst.rstdt_recall@eng.rst.rstdt': 0.3639341123558197, 'train@eng.rst.rstdt_loss@eng.rst.rstdt': 1.121582269668579, 'train@eng.rst.rstdt_runtime': 188.8683, 'train@eng.rst.rstdt_samples_per_second': 84.726, 'train@eng.rst.rstdt_steps_per_second': 2.653, 'epoch': 7.0}
{'loss': 1.1844, 'learning_rate': 8.333333333333334e-06, 'epoch': 7.0}
{'eval_loss': 1.2009918689727783, 'eval_accuracy@eng.rst.rstdt': 0.6458975940777298, 'eval_f1@eng.rst.rstdt': 0.3617430597575433, 'eval_precision@eng.rst.rstdt': 0.5246985570177013, 'eval_recall@eng.rst.rstdt': 0.350680003532859, 'eval_loss@eng.rst.rstdt': 1.2009918689727783, 'eval_runtime': 19.4136, 'eval_samples_per_second': 83.498, 'eval_steps_per_second': 2.627, 'epoch': 7.0}
{'train@eng.rst.rstdt_loss': 1.107340693473816, 'train@eng.rst.rstdt_accuracy@eng.rst.rstdt': 0.671541057367829, 'train@eng.rst.rstdt_f1@eng.rst.rstdt': 0.4155953139921531, 'train@eng.rst.rstdt_precision@eng.rst.rstdt': 0.5781029420820539, 'train@eng.rst.rstdt_recall@eng.rst.rstdt': 0.37820171405931047, 'train@eng.rst.rstdt_loss@eng.rst.rstdt': 1.1073408126831055, 'train@eng.rst.rstdt_runtime': 189.1438, 'train@eng.rst.rstdt_samples_per_second': 84.602, 'train@eng.rst.rstdt_steps_per_second': 2.649, 'epoch': 8.0}
{'loss': 1.1631, 'learning_rate': 6.666666666666667e-06, 'epoch': 8.0}
{'eval_loss': 1.198889970779419, 'eval_accuracy@eng.rst.rstdt': 0.6440468846391116, 'eval_f1@eng.rst.rstdt': 0.3704242694538228, 'eval_precision@eng.rst.rstdt': 0.48935145535301044, 'eval_recall@eng.rst.rstdt': 0.3585797672405466, 'eval_loss@eng.rst.rstdt': 1.1988898515701294, 'eval_runtime': 19.4553, 'eval_samples_per_second': 83.319, 'eval_steps_per_second': 2.621, 'epoch': 8.0}
{'train@eng.rst.rstdt_loss': 1.0960382223129272, 'train@eng.rst.rstdt_accuracy@eng.rst.rstdt': 0.6726034245719285, 'train@eng.rst.rstdt_f1@eng.rst.rstdt': 0.4202250719051115, 'train@eng.rst.rstdt_precision@eng.rst.rstdt': 0.5734988032132393, 'train@eng.rst.rstdt_recall@eng.rst.rstdt': 0.3814146968087857, 'train@eng.rst.rstdt_loss@eng.rst.rstdt': 1.0960382223129272, 'train@eng.rst.rstdt_runtime': 188.7178, 'train@eng.rst.rstdt_samples_per_second': 84.793, 'train@eng.rst.rstdt_steps_per_second': 2.655, 'epoch': 9.0}
{'loss': 1.1511, 'learning_rate': 5e-06, 'epoch': 9.0}
{'eval_loss': 1.1891659498214722, 'eval_accuracy@eng.rst.rstdt': 0.6458975940777298, 'eval_f1@eng.rst.rstdt': 0.36988568624804635, 'eval_precision@eng.rst.rstdt': 0.4859365950016329, 'eval_recall@eng.rst.rstdt': 0.3579182528360934, 'eval_loss@eng.rst.rstdt': 1.189165711402893, 'eval_runtime': 19.4198, 'eval_samples_per_second': 83.471, 'eval_steps_per_second': 2.626, 'epoch': 9.0}
{'train@eng.rst.rstdt_loss': 1.0919948816299438, 'train@eng.rst.rstdt_accuracy@eng.rst.rstdt': 0.6731033620797401, 'train@eng.rst.rstdt_f1@eng.rst.rstdt': 0.42740133370255723, 'train@eng.rst.rstdt_precision@eng.rst.rstdt': 0.6087919942812633, 'train@eng.rst.rstdt_recall@eng.rst.rstdt': 0.39072863191557194, 'train@eng.rst.rstdt_loss@eng.rst.rstdt': 1.0919947624206543, 'train@eng.rst.rstdt_runtime': 188.8801, 'train@eng.rst.rstdt_samples_per_second': 84.72, 'train@eng.rst.rstdt_steps_per_second': 2.652, 'epoch': 10.0}
{'loss': 1.1448, 'learning_rate': 3.3333333333333333e-06, 'epoch': 10.0}
{'eval_loss': 1.1954962015151978, 'eval_accuracy@eng.rst.rstdt': 0.6458975940777298, 'eval_f1@eng.rst.rstdt': 0.37489581164446123, 'eval_precision@eng.rst.rstdt': 0.4833921312620021, 'eval_recall@eng.rst.rstdt': 0.3646331342221148, 'eval_loss@eng.rst.rstdt': 1.1954962015151978, 'eval_runtime': 19.4018, 'eval_samples_per_second': 83.549, 'eval_steps_per_second': 2.629, 'epoch': 10.0}
{'train@eng.rst.rstdt_loss': 1.0854681730270386, 'train@eng.rst.rstdt_accuracy@eng.rst.rstdt': 0.6746031746031746, 'train@eng.rst.rstdt_f1@eng.rst.rstdt': 0.42494481238723814, 'train@eng.rst.rstdt_precision@eng.rst.rstdt': 0.5578810213867388, 'train@eng.rst.rstdt_recall@eng.rst.rstdt': 0.38660630599591983, 'train@eng.rst.rstdt_loss@eng.rst.rstdt': 1.0854681730270386, 'train@eng.rst.rstdt_runtime': 188.9939, 'train@eng.rst.rstdt_samples_per_second': 84.669, 'train@eng.rst.rstdt_steps_per_second': 2.651, 'epoch': 11.0}
{'loss': 1.1387, 'learning_rate': 1.6666666666666667e-06, 'epoch': 11.0}
{'eval_loss': 1.1855056285858154, 'eval_accuracy@eng.rst.rstdt': 0.6458975940777298, 'eval_f1@eng.rst.rstdt': 0.37323149456587595, 'eval_precision@eng.rst.rstdt': 0.4872158167225081, 'eval_recall@eng.rst.rstdt': 0.3609424449258516, 'eval_loss@eng.rst.rstdt': 1.1855056285858154, 'eval_runtime': 19.4101, 'eval_samples_per_second': 83.513, 'eval_steps_per_second': 2.627, 'epoch': 11.0}
{'train@eng.rst.rstdt_loss': 1.0841425657272339, 'train@eng.rst.rstdt_accuracy@eng.rst.rstdt': 0.6748531433570804, 'train@eng.rst.rstdt_f1@eng.rst.rstdt': 0.426398788748034, 'train@eng.rst.rstdt_precision@eng.rst.rstdt': 0.6177131167754437, 'train@eng.rst.rstdt_recall@eng.rst.rstdt': 0.3874333816613935, 'train@eng.rst.rstdt_loss@eng.rst.rstdt': 1.0841424465179443, 'train@eng.rst.rstdt_runtime': 189.143, 'train@eng.rst.rstdt_samples_per_second': 84.603, 'train@eng.rst.rstdt_steps_per_second': 2.649, 'epoch': 12.0}
{'loss': 1.1334, 'learning_rate': 0.0, 'epoch': 12.0}
{'eval_loss': 1.1845351457595825, 'eval_accuracy@eng.rst.rstdt': 0.6465144972239358, 'eval_f1@eng.rst.rstdt': 0.37320274147868615, 'eval_precision@eng.rst.rstdt': 0.4864106601089972, 'eval_recall@eng.rst.rstdt': 0.36117133025430204, 'eval_loss@eng.rst.rstdt': 1.1845349073410034, 'eval_runtime': 19.4587, 'eval_samples_per_second': 83.305, 'eval_steps_per_second': 2.621, 'epoch': 12.0}
{'train_runtime': 7279.4505, 'train_samples_per_second': 26.379, 'train_steps_per_second': 0.826, 'train_loss': 1.2893798279905033, 'epoch': 12.0}
***** train_logger metrics *****
  epoch                    =       12.0
  train_loss               =     2.2315
  train_runtime            = 0:17:44.89
  train_samples_per_second =     24.622
  train_steps_per_second   =      0.778
-------------------------------------------------------------------
Lang1:  nld.rst.nldt    Lang2:  eng.rst.rstdt
Saving run to:  runs/full_shot/FullShot=v4_nld.rst.nldt_eng.rst.rstdt_lrfunc=Adafactor_lr=2e-05_plm=bert-base-multilingual-cased
Running with params: BERT_MODEL=bert-base-multilingual-cased lr=2e-05
read 1608 examples
read 331 examples
read 326 examples
read 16002 examples
read 1621 examples
read 2155 examples
Total prediction labels:  42
BertAdapterModel(
  (shared_parameters): ModuleDict()
  (bert): BertModel(
    (shared_parameters): ModuleDict()
    (invertible_adapters): ModuleDict(
      (de): NICECouplingBlock(
        (F): Sequential(
          (0): Linear(in_features=384, out_features=192, bias=True)
          (1): Activation_Function_Class(
            (f): NewGELUActivation()
          )
          (2): Linear(in_features=192, out_features=384, bias=True)
        )
        (G): Sequential(
          (0): Linear(in_features=384, out_features=192, bias=True)
          (1): Activation_Function_Class(
            (f): NewGELUActivation()
          )
          (2): Linear(in_features=192, out_features=384, bias=True)
        )
      )
      (en): NICECouplingBlock(
        (F): Sequential(
          (0): Linear(in_features=384, out_features=192, bias=True)
          (1): Activation_Function_Class(
            (f): ReLU()
          )
          (2): Linear(in_features=192, out_features=384, bias=True)
        )
        (G): Sequential(
          (0): Linear(in_features=384, out_features=192, bias=True)
          (1): Activation_Function_Class(
            (f): ReLU()
          )
          (2): Linear(in_features=192, out_features=384, bias=True)
        )
      )
    )
    (embeddings): BertEmbeddings(
      (word_embeddings): Embedding(119547, 768, padding_idx=0)
      (position_embeddings): Embedding(512, 768)
      (token_type_embeddings): Embedding(2, 768)
      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
      (dropout): Dropout(p=0.1, inplace=False)
    )
    (encoder): BertEncoder(
      (layer): ModuleList(
        (0): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (de): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (en): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (1): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (de): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (en): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (2): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (de): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (en): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (3): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (de): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (en): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (4): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (de): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (en): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (5): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (de): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (en): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (6): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (de): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (en): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (7): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (de): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (en): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (8): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (de): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (en): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (9): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (de): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (en): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (10): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (de): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (en): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (11): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (de): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (en): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
      )
    )
    (pooler): BertPooler(
      (dense): Linear(in_features=768, out_features=768, bias=True)
      (activation): Tanh()
    )
    (prefix_tuning): PrefixTuningPool(
      (prefix_tunings): ModuleDict()
    )
  )
  (heads): ModuleDict(
    (disrpt): ClassificationHead(
      (0): Dropout(p=0.1, inplace=False)
      (1): Linear(in_features=768, out_features=768, bias=True)
      (2): Activation_Function_Class(
        (f): Tanh()
      )
      (3): Dropout(p=0.1, inplace=False)
      (4): Linear(in_features=768, out_features=42, bias=True)
    )
  )
)
{'train@nld.rst.nldt_loss': 3.2215168476104736, 'train@nld.rst.nldt_accuracy@nld.rst.nldt': 0.26430348258706465, 'train@nld.rst.nldt_f1@nld.rst.nldt': 0.01306566650270536, 'train@nld.rst.nldt_precision@nld.rst.nldt': 0.00825948383084577, 'train@nld.rst.nldt_recall@nld.rst.nldt': 0.03125, 'train@nld.rst.nldt_loss@nld.rst.nldt': 3.2215168476104736, 'train@nld.rst.nldt_runtime': 19.5486, 'train@nld.rst.nldt_samples_per_second': 82.257, 'train@nld.rst.nldt_steps_per_second': 2.609, 'epoch': 1.0}
{'loss': 3.5093, 'learning_rate': 1.8333333333333333e-05, 'epoch': 1.0}
{'eval_loss': 3.1905417442321777, 'eval_accuracy@nld.rst.nldt': 0.27794561933534745, 'eval_f1@nld.rst.nldt': 0.016110673321075214, 'eval_precision@nld.rst.nldt': 0.010294282197605462, 'eval_recall@nld.rst.nldt': 0.037037037037037035, 'eval_loss@nld.rst.nldt': 3.1905417442321777, 'eval_runtime': 4.355, 'eval_samples_per_second': 76.005, 'eval_steps_per_second': 2.526, 'epoch': 1.0}
{'train@nld.rst.nldt_loss': 2.9012582302093506, 'train@nld.rst.nldt_accuracy@nld.rst.nldt': 0.26430348258706465, 'train@nld.rst.nldt_f1@nld.rst.nldt': 0.01306566650270536, 'train@nld.rst.nldt_precision@nld.rst.nldt': 0.00825948383084577, 'train@nld.rst.nldt_recall@nld.rst.nldt': 0.03125, 'train@nld.rst.nldt_loss@nld.rst.nldt': 2.9012577533721924, 'train@nld.rst.nldt_runtime': 19.6023, 'train@nld.rst.nldt_samples_per_second': 82.031, 'train@nld.rst.nldt_steps_per_second': 2.602, 'epoch': 2.0}
{'loss': 3.0548, 'learning_rate': 1.6666666666666667e-05, 'epoch': 2.0}
{'eval_loss': 2.8436570167541504, 'eval_accuracy@nld.rst.nldt': 0.27794561933534745, 'eval_f1@nld.rst.nldt': 0.016110673321075214, 'eval_precision@nld.rst.nldt': 0.010294282197605462, 'eval_recall@nld.rst.nldt': 0.037037037037037035, 'eval_loss@nld.rst.nldt': 2.8436567783355713, 'eval_runtime': 4.3559, 'eval_samples_per_second': 75.989, 'eval_steps_per_second': 2.525, 'epoch': 2.0}
{'train@nld.rst.nldt_loss': 2.8013453483581543, 'train@nld.rst.nldt_accuracy@nld.rst.nldt': 0.27425373134328357, 'train@nld.rst.nldt_f1@nld.rst.nldt': 0.02151904975727159, 'train@nld.rst.nldt_precision@nld.rst.nldt': 0.020731392606942976, 'train@nld.rst.nldt_recall@nld.rst.nldt': 0.037606792717086834, 'train@nld.rst.nldt_loss@nld.rst.nldt': 2.8013453483581543, 'train@nld.rst.nldt_runtime': 19.6668, 'train@nld.rst.nldt_samples_per_second': 81.762, 'train@nld.rst.nldt_steps_per_second': 2.593, 'epoch': 3.0}
{'loss': 2.8835, 'learning_rate': 1.5000000000000002e-05, 'epoch': 3.0}
{'eval_loss': 2.746333360671997, 'eval_accuracy@nld.rst.nldt': 0.2990936555891239, 'eval_f1@nld.rst.nldt': 0.031557940895801675, 'eval_precision@nld.rst.nldt': 0.036373868351675546, 'eval_recall@nld.rst.nldt': 0.05044091710758377, 'eval_loss@nld.rst.nldt': 2.7463338375091553, 'eval_runtime': 4.3622, 'eval_samples_per_second': 75.879, 'eval_steps_per_second': 2.522, 'epoch': 3.0}
{'train@nld.rst.nldt_loss': 2.751967668533325, 'train@nld.rst.nldt_accuracy@nld.rst.nldt': 0.28171641791044777, 'train@nld.rst.nldt_f1@nld.rst.nldt': 0.02600460631459945, 'train@nld.rst.nldt_precision@nld.rst.nldt': 0.02402442150621762, 'train@nld.rst.nldt_recall@nld.rst.nldt': 0.042621381886087766, 'train@nld.rst.nldt_loss@nld.rst.nldt': 2.751967430114746, 'train@nld.rst.nldt_runtime': 19.6374, 'train@nld.rst.nldt_samples_per_second': 81.885, 'train@nld.rst.nldt_steps_per_second': 2.597, 'epoch': 4.0}
{'loss': 2.7843, 'learning_rate': 1.3333333333333333e-05, 'epoch': 4.0}
{'eval_loss': 2.702577829360962, 'eval_accuracy@nld.rst.nldt': 0.31722054380664655, 'eval_f1@nld.rst.nldt': 0.03811895995131863, 'eval_precision@nld.rst.nldt': 0.03889256319811875, 'eval_recall@nld.rst.nldt': 0.06244280857807428, 'eval_loss@nld.rst.nldt': 2.702577590942383, 'eval_runtime': 4.3442, 'eval_samples_per_second': 76.193, 'eval_steps_per_second': 2.532, 'epoch': 4.0}
{'train@nld.rst.nldt_loss': 2.7183797359466553, 'train@nld.rst.nldt_accuracy@nld.rst.nldt': 0.292910447761194, 'train@nld.rst.nldt_f1@nld.rst.nldt': 0.03118525712171661, 'train@nld.rst.nldt_precision@nld.rst.nldt': 0.027233115468409584, 'train@nld.rst.nldt_recall@nld.rst.nldt': 0.04778419701213819, 'train@nld.rst.nldt_loss@nld.rst.nldt': 2.718379497528076, 'train@nld.rst.nldt_runtime': 19.685, 'train@nld.rst.nldt_samples_per_second': 81.687, 'train@nld.rst.nldt_steps_per_second': 2.591, 'epoch': 5.0}
{'loss': 2.7573, 'learning_rate': 1.1666666666666668e-05, 'epoch': 5.0}
{'eval_loss': 2.669832706451416, 'eval_accuracy@nld.rst.nldt': 0.32326283987915405, 'eval_f1@nld.rst.nldt': 0.040386584741423454, 'eval_precision@nld.rst.nldt': 0.03700329884540411, 'eval_recall@nld.rst.nldt': 0.0645592106944764, 'eval_loss@nld.rst.nldt': 2.669832706451416, 'eval_runtime': 4.3348, 'eval_samples_per_second': 76.359, 'eval_steps_per_second': 2.538, 'epoch': 5.0}
{'train@nld.rst.nldt_loss': 2.6825897693634033, 'train@nld.rst.nldt_accuracy@nld.rst.nldt': 0.2997512437810945, 'train@nld.rst.nldt_f1@nld.rst.nldt': 0.033418484314309466, 'train@nld.rst.nldt_precision@nld.rst.nldt': 0.026740045933357347, 'train@nld.rst.nldt_recall@nld.rst.nldt': 0.05260854341736695, 'train@nld.rst.nldt_loss@nld.rst.nldt': 2.682589530944824, 'train@nld.rst.nldt_runtime': 19.6586, 'train@nld.rst.nldt_samples_per_second': 81.796, 'train@nld.rst.nldt_steps_per_second': 2.594, 'epoch': 6.0}
{'loss': 2.7245, 'learning_rate': 1e-05, 'epoch': 6.0}
{'eval_loss': 2.641988754272461, 'eval_accuracy@nld.rst.nldt': 0.32326283987915405, 'eval_f1@nld.rst.nldt': 0.040410956253193415, 'eval_precision@nld.rst.nldt': 0.03551226700235374, 'eval_recall@nld.rst.nldt': 0.06786928405285894, 'eval_loss@nld.rst.nldt': 2.641988754272461, 'eval_runtime': 4.3508, 'eval_samples_per_second': 76.078, 'eval_steps_per_second': 2.528, 'epoch': 6.0}
{'train@nld.rst.nldt_loss': 2.6563780307769775, 'train@nld.rst.nldt_accuracy@nld.rst.nldt': 0.3034825870646766, 'train@nld.rst.nldt_f1@nld.rst.nldt': 0.03505413238463398, 'train@nld.rst.nldt_precision@nld.rst.nldt': 0.026815705466202586, 'train@nld.rst.nldt_recall@nld.rst.nldt': 0.05619222689075631, 'train@nld.rst.nldt_loss@nld.rst.nldt': 2.6563780307769775, 'train@nld.rst.nldt_runtime': 19.6514, 'train@nld.rst.nldt_samples_per_second': 81.826, 'train@nld.rst.nldt_steps_per_second': 2.595, 'epoch': 7.0}
{'loss': 2.6966, 'learning_rate': 8.333333333333334e-06, 'epoch': 7.0}
{'eval_loss': 2.619968891143799, 'eval_accuracy@nld.rst.nldt': 0.32628398791540786, 'eval_f1@nld.rst.nldt': 0.042289518937814505, 'eval_precision@nld.rst.nldt': 0.03708397878233113, 'eval_recall@nld.rst.nldt': 0.06958310967972804, 'eval_loss@nld.rst.nldt': 2.6199686527252197, 'eval_runtime': 4.3413, 'eval_samples_per_second': 76.244, 'eval_steps_per_second': 2.534, 'epoch': 7.0}
{'train@nld.rst.nldt_loss': 2.6346309185028076, 'train@nld.rst.nldt_accuracy@nld.rst.nldt': 0.30783582089552236, 'train@nld.rst.nldt_f1@nld.rst.nldt': 0.036604272583259656, 'train@nld.rst.nldt_precision@nld.rst.nldt': 0.027297430179162904, 'train@nld.rst.nldt_recall@nld.rst.nldt': 0.05973272642390289, 'train@nld.rst.nldt_loss@nld.rst.nldt': 2.6346304416656494, 'train@nld.rst.nldt_runtime': 19.6289, 'train@nld.rst.nldt_samples_per_second': 81.92, 'train@nld.rst.nldt_steps_per_second': 2.598, 'epoch': 8.0}
{'loss': 2.6788, 'learning_rate': 6.666666666666667e-06, 'epoch': 8.0}
{'eval_loss': 2.6033449172973633, 'eval_accuracy@nld.rst.nldt': 0.32628398791540786, 'eval_f1@nld.rst.nldt': 0.04226283914882066, 'eval_precision@nld.rst.nldt': 0.036200381098781445, 'eval_recall@nld.rst.nldt': 0.07123814635891931, 'eval_loss@nld.rst.nldt': 2.6033449172973633, 'eval_runtime': 4.3618, 'eval_samples_per_second': 75.886, 'eval_steps_per_second': 2.522, 'epoch': 8.0}
{'train@nld.rst.nldt_loss': 2.621184825897217, 'train@nld.rst.nldt_accuracy@nld.rst.nldt': 0.31032338308457713, 'train@nld.rst.nldt_f1@nld.rst.nldt': 0.037078383641958576, 'train@nld.rst.nldt_precision@nld.rst.nldt': 0.02746762908882066, 'train@nld.rst.nldt_recall@nld.rst.nldt': 0.06031746031746032, 'train@nld.rst.nldt_loss@nld.rst.nldt': 2.6211845874786377, 'train@nld.rst.nldt_runtime': 19.6117, 'train@nld.rst.nldt_samples_per_second': 81.992, 'train@nld.rst.nldt_steps_per_second': 2.6, 'epoch': 9.0}
{'loss': 2.6573, 'learning_rate': 5e-06, 'epoch': 9.0}
{'eval_loss': 2.591785430908203, 'eval_accuracy@nld.rst.nldt': 0.32326283987915405, 'eval_f1@nld.rst.nldt': 0.041258925113498365, 'eval_precision@nld.rst.nldt': 0.03493726258420094, 'eval_recall@nld.rst.nldt': 0.06918053319019501, 'eval_loss@nld.rst.nldt': 2.591785430908203, 'eval_runtime': 4.3445, 'eval_samples_per_second': 76.188, 'eval_steps_per_second': 2.532, 'epoch': 9.0}
{'train@nld.rst.nldt_loss': 2.611358404159546, 'train@nld.rst.nldt_accuracy@nld.rst.nldt': 0.31156716417910446, 'train@nld.rst.nldt_f1@nld.rst.nldt': 0.03747615586551882, 'train@nld.rst.nldt_precision@nld.rst.nldt': 0.027857131911355658, 'train@nld.rst.nldt_recall@nld.rst.nldt': 0.06058064892623716, 'train@nld.rst.nldt_loss@nld.rst.nldt': 2.611358404159546, 'train@nld.rst.nldt_runtime': 19.6353, 'train@nld.rst.nldt_samples_per_second': 81.893, 'train@nld.rst.nldt_steps_per_second': 2.597, 'epoch': 10.0}
{'loss': 2.6436, 'learning_rate': 3.3333333333333333e-06, 'epoch': 10.0}
{'eval_loss': 2.5831637382507324, 'eval_accuracy@nld.rst.nldt': 0.32628398791540786, 'eval_f1@nld.rst.nldt': 0.04160101900842642, 'eval_precision@nld.rst.nldt': 0.03519433572926478, 'eval_recall@nld.rst.nldt': 0.06958310967972804, 'eval_loss@nld.rst.nldt': 2.5831642150878906, 'eval_runtime': 4.3434, 'eval_samples_per_second': 76.208, 'eval_steps_per_second': 2.533, 'epoch': 10.0}
{'train@nld.rst.nldt_loss': 2.604379892349243, 'train@nld.rst.nldt_accuracy@nld.rst.nldt': 0.31343283582089554, 'train@nld.rst.nldt_f1@nld.rst.nldt': 0.037892733974795666, 'train@nld.rst.nldt_precision@nld.rst.nldt': 0.027984525645312064, 'train@nld.rst.nldt_recall@nld.rst.nldt': 0.061557539682539686, 'train@nld.rst.nldt_loss@nld.rst.nldt': 2.604379892349243, 'train@nld.rst.nldt_runtime': 19.6151, 'train@nld.rst.nldt_samples_per_second': 81.978, 'train@nld.rst.nldt_steps_per_second': 2.6, 'epoch': 11.0}
{'loss': 2.6318, 'learning_rate': 1.6666666666666667e-06, 'epoch': 11.0}
{'eval_loss': 2.57796573638916, 'eval_accuracy@nld.rst.nldt': 0.32326283987915405, 'eval_f1@nld.rst.nldt': 0.041258925113498365, 'eval_precision@nld.rst.nldt': 0.03493726258420094, 'eval_recall@nld.rst.nldt': 0.06918053319019501, 'eval_loss@nld.rst.nldt': 2.5779659748077393, 'eval_runtime': 4.3579, 'eval_samples_per_second': 75.953, 'eval_steps_per_second': 2.524, 'epoch': 11.0}
{'train@nld.rst.nldt_loss': 2.601659059524536, 'train@nld.rst.nldt_accuracy@nld.rst.nldt': 0.31218905472636815, 'train@nld.rst.nldt_f1@nld.rst.nldt': 0.03771219892414225, 'train@nld.rst.nldt_precision@nld.rst.nldt': 0.027836088725577482, 'train@nld.rst.nldt_recall@nld.rst.nldt': 0.06141048085901027, 'train@nld.rst.nldt_loss@nld.rst.nldt': 2.601658821105957, 'train@nld.rst.nldt_runtime': 19.6299, 'train@nld.rst.nldt_samples_per_second': 81.916, 'train@nld.rst.nldt_steps_per_second': 2.598, 'epoch': 12.0}
{'loss': 2.6322, 'learning_rate': 0.0, 'epoch': 12.0}
{'eval_loss': 2.576415538787842, 'eval_accuracy@nld.rst.nldt': 0.32326283987915405, 'eval_f1@nld.rst.nldt': 0.041258925113498365, 'eval_precision@nld.rst.nldt': 0.03493726258420094, 'eval_recall@nld.rst.nldt': 0.06918053319019501, 'eval_loss@nld.rst.nldt': 2.576415538787842, 'eval_runtime': 4.3464, 'eval_samples_per_second': 76.155, 'eval_steps_per_second': 2.531, 'epoch': 12.0}
{'train_runtime': 777.36, 'train_samples_per_second': 24.822, 'train_steps_per_second': 0.787, 'train_loss': 2.804501801534416, 'epoch': 12.0}
***** train_log metrics *****
  epoch                    =       12.0
  train_loss               =     2.8045
  train_runtime            = 0:12:57.36
  train_samples_per_second =     24.822
  train_steps_per_second   =      0.787
{'train@eng.rst.rstdt_loss': 1.717619776725769, 'train@eng.rst.rstdt_accuracy@eng.rst.rstdt': 0.5189351331083615, 'train@eng.rst.rstdt_f1@eng.rst.rstdt': 0.09051531577239033, 'train@eng.rst.rstdt_precision@eng.rst.rstdt': 0.0997135052666494, 'train@eng.rst.rstdt_recall@eng.rst.rstdt': 0.11422141468603687, 'train@eng.rst.rstdt_loss@eng.rst.rstdt': 1.717619776725769, 'train@eng.rst.rstdt_runtime': 189.1951, 'train@eng.rst.rstdt_samples_per_second': 84.579, 'train@eng.rst.rstdt_steps_per_second': 2.648, 'epoch': 1.0}
{'loss': 2.1792, 'learning_rate': 1.8333333333333333e-05, 'epoch': 1.0}
{'eval_loss': 1.6954851150512695, 'eval_accuracy@eng.rst.rstdt': 0.5311536088834053, 'eval_f1@eng.rst.rstdt': 0.09059159761278042, 'eval_precision@eng.rst.rstdt': 0.09579185259574012, 'eval_recall@eng.rst.rstdt': 0.11349568812830257, 'eval_loss@eng.rst.rstdt': 1.6954851150512695, 'eval_runtime': 19.5727, 'eval_samples_per_second': 82.82, 'eval_steps_per_second': 2.606, 'epoch': 1.0}
{'train@eng.rst.rstdt_loss': 1.4047285318374634, 'train@eng.rst.rstdt_accuracy@eng.rst.rstdt': 0.6082989626296713, 'train@eng.rst.rstdt_f1@eng.rst.rstdt': 0.21358991395829346, 'train@eng.rst.rstdt_precision@eng.rst.rstdt': 0.3142717953841025, 'train@eng.rst.rstdt_recall@eng.rst.rstdt': 0.22075946858765022, 'train@eng.rst.rstdt_loss@eng.rst.rstdt': 1.4047285318374634, 'train@eng.rst.rstdt_runtime': 189.2951, 'train@eng.rst.rstdt_samples_per_second': 84.535, 'train@eng.rst.rstdt_steps_per_second': 2.647, 'epoch': 2.0}
{'loss': 1.5855, 'learning_rate': 1.6666666666666667e-05, 'epoch': 2.0}
{'eval_loss': 1.4062472581863403, 'eval_accuracy@eng.rst.rstdt': 0.621838371375694, 'eval_f1@eng.rst.rstdt': 0.23417574604910515, 'eval_precision@eng.rst.rstdt': 0.3824631489072483, 'eval_recall@eng.rst.rstdt': 0.2346191519717941, 'eval_loss@eng.rst.rstdt': 1.4062471389770508, 'eval_runtime': 19.514, 'eval_samples_per_second': 83.069, 'eval_steps_per_second': 2.614, 'epoch': 2.0}
{'train@eng.rst.rstdt_loss': 1.2929517030715942, 'train@eng.rst.rstdt_accuracy@eng.rst.rstdt': 0.6372328458942632, 'train@eng.rst.rstdt_f1@eng.rst.rstdt': 0.2965922970356631, 'train@eng.rst.rstdt_precision@eng.rst.rstdt': 0.43649213232687395, 'train@eng.rst.rstdt_recall@eng.rst.rstdt': 0.2817301543119621, 'train@eng.rst.rstdt_loss@eng.rst.rstdt': 1.2929517030715942, 'train@eng.rst.rstdt_runtime': 188.4467, 'train@eng.rst.rstdt_samples_per_second': 84.915, 'train@eng.rst.rstdt_steps_per_second': 2.659, 'epoch': 3.0}
{'loss': 1.4037, 'learning_rate': 1.5000000000000002e-05, 'epoch': 3.0}
{'eval_loss': 1.3221009969711304, 'eval_accuracy@eng.rst.rstdt': 0.6378778531770513, 'eval_f1@eng.rst.rstdt': 0.2926483634954244, 'eval_precision@eng.rst.rstdt': 0.44354846135237364, 'eval_recall@eng.rst.rstdt': 0.2834415294210394, 'eval_loss@eng.rst.rstdt': 1.3221007585525513, 'eval_runtime': 19.4513, 'eval_samples_per_second': 83.336, 'eval_steps_per_second': 2.622, 'epoch': 3.0}
{'train@eng.rst.rstdt_loss': 1.2266602516174316, 'train@eng.rst.rstdt_accuracy@eng.rst.rstdt': 0.6496687914010749, 'train@eng.rst.rstdt_f1@eng.rst.rstdt': 0.34022417735243843, 'train@eng.rst.rstdt_precision@eng.rst.rstdt': 0.43885735588359925, 'train@eng.rst.rstdt_recall@eng.rst.rstdt': 0.32324227649373144, 'train@eng.rst.rstdt_loss@eng.rst.rstdt': 1.2266602516174316, 'train@eng.rst.rstdt_runtime': 189.1801, 'train@eng.rst.rstdt_samples_per_second': 84.586, 'train@eng.rst.rstdt_steps_per_second': 2.648, 'epoch': 4.0}
{'loss': 1.3117, 'learning_rate': 1.3333333333333333e-05, 'epoch': 4.0}
{'eval_loss': 1.2838068008422852, 'eval_accuracy@eng.rst.rstdt': 0.634793337446021, 'eval_f1@eng.rst.rstdt': 0.3275537071691833, 'eval_precision@eng.rst.rstdt': 0.43751883375802364, 'eval_recall@eng.rst.rstdt': 0.3232670832772831, 'eval_loss@eng.rst.rstdt': 1.2838069200515747, 'eval_runtime': 19.5358, 'eval_samples_per_second': 82.976, 'eval_steps_per_second': 2.611, 'epoch': 4.0}
{'train@eng.rst.rstdt_loss': 1.180574893951416, 'train@eng.rst.rstdt_accuracy@eng.rst.rstdt': 0.6603549556305461, 'train@eng.rst.rstdt_f1@eng.rst.rstdt': 0.3623039605398013, 'train@eng.rst.rstdt_precision@eng.rst.rstdt': 0.5329032824787995, 'train@eng.rst.rstdt_recall@eng.rst.rstdt': 0.33751906675487153, 'train@eng.rst.rstdt_loss@eng.rst.rstdt': 1.180574893951416, 'train@eng.rst.rstdt_runtime': 189.2955, 'train@eng.rst.rstdt_samples_per_second': 84.534, 'train@eng.rst.rstdt_steps_per_second': 2.647, 'epoch': 5.0}
{'loss': 1.2552, 'learning_rate': 1.1666666666666668e-05, 'epoch': 5.0}
{'eval_loss': 1.2451634407043457, 'eval_accuracy@eng.rst.rstdt': 0.6421961752004935, 'eval_f1@eng.rst.rstdt': 0.3337358062755733, 'eval_precision@eng.rst.rstdt': 0.46583696525404783, 'eval_recall@eng.rst.rstdt': 0.3320484961051622, 'eval_loss@eng.rst.rstdt': 1.2451633214950562, 'eval_runtime': 19.5213, 'eval_samples_per_second': 83.037, 'eval_steps_per_second': 2.613, 'epoch': 5.0}
{'train@eng.rst.rstdt_loss': 1.1504685878753662, 'train@eng.rst.rstdt_accuracy@eng.rst.rstdt': 0.6654168228971379, 'train@eng.rst.rstdt_f1@eng.rst.rstdt': 0.3796766449255252, 'train@eng.rst.rstdt_precision@eng.rst.rstdt': 0.5080073686020519, 'train@eng.rst.rstdt_recall@eng.rst.rstdt': 0.3486835708988908, 'train@eng.rst.rstdt_loss@eng.rst.rstdt': 1.1504685878753662, 'train@eng.rst.rstdt_runtime': 188.5549, 'train@eng.rst.rstdt_samples_per_second': 84.867, 'train@eng.rst.rstdt_steps_per_second': 2.657, 'epoch': 6.0}
{'loss': 1.2139, 'learning_rate': 1e-05, 'epoch': 6.0}
{'eval_loss': 1.2231365442276, 'eval_accuracy@eng.rst.rstdt': 0.6434299814929056, 'eval_f1@eng.rst.rstdt': 0.3384838348193307, 'eval_precision@eng.rst.rstdt': 0.46767068873061557, 'eval_recall@eng.rst.rstdt': 0.33409962998235077, 'eval_loss@eng.rst.rstdt': 1.2231366634368896, 'eval_runtime': 19.4811, 'eval_samples_per_second': 83.209, 'eval_steps_per_second': 2.618, 'epoch': 6.0}
{'train@eng.rst.rstdt_loss': 1.1322181224822998, 'train@eng.rst.rstdt_accuracy@eng.rst.rstdt': 0.6679165104361955, 'train@eng.rst.rstdt_f1@eng.rst.rstdt': 0.39008248910203475, 'train@eng.rst.rstdt_precision@eng.rst.rstdt': 0.5963924957426969, 'train@eng.rst.rstdt_recall@eng.rst.rstdt': 0.3572417390849591, 'train@eng.rst.rstdt_loss@eng.rst.rstdt': 1.1322181224822998, 'train@eng.rst.rstdt_runtime': 189.1141, 'train@eng.rst.rstdt_samples_per_second': 84.616, 'train@eng.rst.rstdt_steps_per_second': 2.649, 'epoch': 7.0}
{'loss': 1.1918, 'learning_rate': 8.333333333333334e-06, 'epoch': 7.0}
{'eval_loss': 1.2111432552337646, 'eval_accuracy@eng.rst.rstdt': 0.6471314003701419, 'eval_f1@eng.rst.rstdt': 0.35525178315950123, 'eval_precision@eng.rst.rstdt': 0.5239752330180786, 'eval_recall@eng.rst.rstdt': 0.34615169830248604, 'eval_loss@eng.rst.rstdt': 1.2111432552337646, 'eval_runtime': 19.5226, 'eval_samples_per_second': 83.032, 'eval_steps_per_second': 2.612, 'epoch': 7.0}
{'train@eng.rst.rstdt_loss': 1.1159168481826782, 'train@eng.rst.rstdt_accuracy@eng.rst.rstdt': 0.6700412448443944, 'train@eng.rst.rstdt_f1@eng.rst.rstdt': 0.3991099916244371, 'train@eng.rst.rstdt_precision@eng.rst.rstdt': 0.5883578615352697, 'train@eng.rst.rstdt_recall@eng.rst.rstdt': 0.3661859752403151, 'train@eng.rst.rstdt_loss@eng.rst.rstdt': 1.1159167289733887, 'train@eng.rst.rstdt_runtime': 188.7486, 'train@eng.rst.rstdt_samples_per_second': 84.779, 'train@eng.rst.rstdt_steps_per_second': 2.654, 'epoch': 8.0}
{'loss': 1.1743, 'learning_rate': 6.666666666666667e-06, 'epoch': 8.0}
{'eval_loss': 1.2054561376571655, 'eval_accuracy@eng.rst.rstdt': 0.6539173349784084, 'eval_f1@eng.rst.rstdt': 0.3722457664005177, 'eval_precision@eng.rst.rstdt': 0.5317925910662197, 'eval_recall@eng.rst.rstdt': 0.36120990693596816, 'eval_loss@eng.rst.rstdt': 1.2054561376571655, 'eval_runtime': 19.492, 'eval_samples_per_second': 83.162, 'eval_steps_per_second': 2.616, 'epoch': 8.0}
{'train@eng.rst.rstdt_loss': 1.1047459840774536, 'train@eng.rst.rstdt_accuracy@eng.rst.rstdt': 0.6721659792525935, 'train@eng.rst.rstdt_f1@eng.rst.rstdt': 0.4043875768173548, 'train@eng.rst.rstdt_precision@eng.rst.rstdt': 0.6378210374958553, 'train@eng.rst.rstdt_recall@eng.rst.rstdt': 0.3695023672777243, 'train@eng.rst.rstdt_loss@eng.rst.rstdt': 1.1047459840774536, 'train@eng.rst.rstdt_runtime': 189.172, 'train@eng.rst.rstdt_samples_per_second': 84.59, 'train@eng.rst.rstdt_steps_per_second': 2.648, 'epoch': 9.0}
{'loss': 1.1618, 'learning_rate': 5e-06, 'epoch': 9.0}
{'eval_loss': 1.1970628499984741, 'eval_accuracy@eng.rst.rstdt': 0.6502159161011721, 'eval_f1@eng.rst.rstdt': 0.3658683138744258, 'eval_precision@eng.rst.rstdt': 0.5265951432109652, 'eval_recall@eng.rst.rstdt': 0.35461945782139015, 'eval_loss@eng.rst.rstdt': 1.1970628499984741, 'eval_runtime': 19.5444, 'eval_samples_per_second': 82.939, 'eval_steps_per_second': 2.609, 'epoch': 9.0}
{'train@eng.rst.rstdt_loss': 1.099088430404663, 'train@eng.rst.rstdt_accuracy@eng.rst.rstdt': 0.6718535183102112, 'train@eng.rst.rstdt_f1@eng.rst.rstdt': 0.4203046024317043, 'train@eng.rst.rstdt_precision@eng.rst.rstdt': 0.631911076447703, 'train@eng.rst.rstdt_recall@eng.rst.rstdt': 0.38346172177359156, 'train@eng.rst.rstdt_loss@eng.rst.rstdt': 1.099088430404663, 'train@eng.rst.rstdt_runtime': 189.2309, 'train@eng.rst.rstdt_samples_per_second': 84.563, 'train@eng.rst.rstdt_steps_per_second': 2.648, 'epoch': 10.0}
{'loss': 1.1514, 'learning_rate': 3.3333333333333333e-06, 'epoch': 10.0}
{'eval_loss': 1.2015814781188965, 'eval_accuracy@eng.rst.rstdt': 0.6446637877853177, 'eval_f1@eng.rst.rstdt': 0.37360780957871104, 'eval_precision@eng.rst.rstdt': 0.5240487906292042, 'eval_recall@eng.rst.rstdt': 0.3639538058009516, 'eval_loss@eng.rst.rstdt': 1.2015814781188965, 'eval_runtime': 19.5298, 'eval_samples_per_second': 83.001, 'eval_steps_per_second': 2.611, 'epoch': 10.0}
{'train@eng.rst.rstdt_loss': 1.0933693647384644, 'train@eng.rst.rstdt_accuracy@eng.rst.rstdt': 0.6744156980377453, 'train@eng.rst.rstdt_f1@eng.rst.rstdt': 0.41815659693364876, 'train@eng.rst.rstdt_precision@eng.rst.rstdt': 0.6401744179392431, 'train@eng.rst.rstdt_recall@eng.rst.rstdt': 0.37996874493091076, 'train@eng.rst.rstdt_loss@eng.rst.rstdt': 1.093369483947754, 'train@eng.rst.rstdt_runtime': 188.5587, 'train@eng.rst.rstdt_samples_per_second': 84.865, 'train@eng.rst.rstdt_steps_per_second': 2.657, 'epoch': 11.0}
{'loss': 1.1442, 'learning_rate': 1.6666666666666667e-06, 'epoch': 11.0}
{'eval_loss': 1.194583535194397, 'eval_accuracy@eng.rst.rstdt': 0.6483652066625539, 'eval_f1@eng.rst.rstdt': 0.3697964188689627, 'eval_precision@eng.rst.rstdt': 0.522675588609177, 'eval_recall@eng.rst.rstdt': 0.3583543833415228, 'eval_loss@eng.rst.rstdt': 1.194583535194397, 'eval_runtime': 19.4966, 'eval_samples_per_second': 83.143, 'eval_steps_per_second': 2.616, 'epoch': 11.0}
{'train@eng.rst.rstdt_loss': 1.0920192003250122, 'train@eng.rst.rstdt_accuracy@eng.rst.rstdt': 0.6749781277340332, 'train@eng.rst.rstdt_f1@eng.rst.rstdt': 0.41898727080697473, 'train@eng.rst.rstdt_precision@eng.rst.rstdt': 0.6399945778536067, 'train@eng.rst.rstdt_recall@eng.rst.rstdt': 0.38059313292035585, 'train@eng.rst.rstdt_loss@eng.rst.rstdt': 1.0920190811157227, 'train@eng.rst.rstdt_runtime': 189.1963, 'train@eng.rst.rstdt_samples_per_second': 84.579, 'train@eng.rst.rstdt_steps_per_second': 2.648, 'epoch': 12.0}
{'loss': 1.1413, 'learning_rate': 0.0, 'epoch': 12.0}
{'eval_loss': 1.1935455799102783, 'eval_accuracy@eng.rst.rstdt': 0.6477483035163479, 'eval_f1@eng.rst.rstdt': 0.3705804561508527, 'eval_precision@eng.rst.rstdt': 0.5260685066836723, 'eval_recall@eng.rst.rstdt': 0.3589704351277201, 'eval_loss@eng.rst.rstdt': 1.1935454607009888, 'eval_runtime': 19.5101, 'eval_samples_per_second': 83.085, 'eval_steps_per_second': 2.614, 'epoch': 12.0}
{'train_runtime': 7279.5349, 'train_samples_per_second': 26.379, 'train_steps_per_second': 0.826, 'train_loss': 1.3261606770995769, 'epoch': 12.0}
***** train_logger metrics *****
  epoch                    =       12.0
  train_loss               =     2.8045
  train_runtime            = 0:12:57.36
  train_samples_per_second =     24.822
  train_steps_per_second   =      0.787
-------------------------------------------------------------------
Lang1:  por.rst.cstn    Lang2:  eng.rst.rstdt
Saving run to:  runs/full_shot/FullShot=v4_por.rst.cstn_eng.rst.rstdt_lrfunc=Adafactor_lr=2e-05_plm=bert-base-multilingual-cased
Running with params: BERT_MODEL=bert-base-multilingual-cased lr=2e-05
read 4148 examples
read 573 examples
read 272 examples
read 16002 examples
read 1621 examples
read 2155 examples
Total prediction labels:  39
BertAdapterModel(
  (shared_parameters): ModuleDict()
  (bert): BertModel(
    (shared_parameters): ModuleDict()
    (invertible_adapters): ModuleDict(
      (pt): NICECouplingBlock(
        (F): Sequential(
          (0): Linear(in_features=384, out_features=192, bias=True)
          (1): Activation_Function_Class(
            (f): NewGELUActivation()
          )
          (2): Linear(in_features=192, out_features=384, bias=True)
        )
        (G): Sequential(
          (0): Linear(in_features=384, out_features=192, bias=True)
          (1): Activation_Function_Class(
            (f): NewGELUActivation()
          )
          (2): Linear(in_features=192, out_features=384, bias=True)
        )
      )
      (en): NICECouplingBlock(
        (F): Sequential(
          (0): Linear(in_features=384, out_features=192, bias=True)
          (1): Activation_Function_Class(
            (f): ReLU()
          )
          (2): Linear(in_features=192, out_features=384, bias=True)
        )
        (G): Sequential(
          (0): Linear(in_features=384, out_features=192, bias=True)
          (1): Activation_Function_Class(
            (f): ReLU()
          )
          (2): Linear(in_features=192, out_features=384, bias=True)
        )
      )
    )
    (embeddings): BertEmbeddings(
      (word_embeddings): Embedding(119547, 768, padding_idx=0)
      (position_embeddings): Embedding(512, 768)
      (token_type_embeddings): Embedding(2, 768)
      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
      (dropout): Dropout(p=0.1, inplace=False)
    )
    (encoder): BertEncoder(
      (layer): ModuleList(
        (0): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (pt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (en): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (1): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (pt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (en): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (2): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (pt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (en): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (3): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (pt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (en): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (4): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (pt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (en): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (5): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (pt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (en): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (6): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (pt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (en): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (7): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (pt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (en): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (8): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (pt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (en): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (9): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (pt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (en): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (10): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (pt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (en): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (11): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (pt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (en): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
      )
    )
    (pooler): BertPooler(
      (dense): Linear(in_features=768, out_features=768, bias=True)
      (activation): Tanh()
    )
    (prefix_tuning): PrefixTuningPool(
      (prefix_tunings): ModuleDict()
    )
  )
  (heads): ModuleDict(
    (pt): BertStyleMaskedLMHead(
      (0): Linear(in_features=768, out_features=768, bias=True)
      (1): Activation_Function_Class(
        (f): GELUActivation()
      )
      (2): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
      (3): Linear(in_features=768, out_features=119547, bias=True)
    )
    (disrpt): ClassificationHead(
      (0): Dropout(p=0.1, inplace=False)
      (1): Linear(in_features=768, out_features=768, bias=True)
      (2): Activation_Function_Class(
        (f): Tanh()
      )
      (3): Dropout(p=0.1, inplace=False)
      (4): Linear(in_features=768, out_features=39, bias=True)
    )
  )
)
{'train@por.rst.cstn_loss': 2.4416167736053467, 'train@por.rst.cstn_accuracy@por.rst.cstn': 0.2796528447444552, 'train@por.rst.cstn_f1@por.rst.cstn': 0.01507514921929655, 'train@por.rst.cstn_precision@por.rst.cstn': 0.039940202947571876, 'train@por.rst.cstn_recall@por.rst.cstn': 0.032006048387096774, 'train@por.rst.cstn_loss@por.rst.cstn': 2.4416165351867676, 'train@por.rst.cstn_runtime': 49.9286, 'train@por.rst.cstn_samples_per_second': 83.079, 'train@por.rst.cstn_steps_per_second': 2.604, 'epoch': 1.0}
{'loss': 2.9767, 'learning_rate': 1.8333333333333333e-05, 'epoch': 1.0}
{'eval_loss': 2.5408084392547607, 'eval_accuracy@por.rst.cstn': 0.28097731239092494, 'eval_f1@por.rst.cstn': 0.019940549913301957, 'eval_precision@por.rst.cstn': 0.012771696017769315, 'eval_recall@por.rst.cstn': 0.045454545454545456, 'eval_loss@por.rst.cstn': 2.5408082008361816, 'eval_runtime': 7.2479, 'eval_samples_per_second': 79.057, 'eval_steps_per_second': 2.483, 'epoch': 1.0}
{'train@por.rst.cstn_loss': 2.1811206340789795, 'train@por.rst.cstn_accuracy@por.rst.cstn': 0.38645130183220827, 'train@por.rst.cstn_f1@por.rst.cstn': 0.05843646552399232, 'train@por.rst.cstn_precision@por.rst.cstn': 0.07560671037388372, 'train@por.rst.cstn_recall@por.rst.cstn': 0.06716302328354802, 'train@por.rst.cstn_loss@por.rst.cstn': 2.1811208724975586, 'train@por.rst.cstn_runtime': 50.0344, 'train@por.rst.cstn_samples_per_second': 82.903, 'train@por.rst.cstn_steps_per_second': 2.598, 'epoch': 2.0}
{'loss': 2.3508, 'learning_rate': 1.6666666666666667e-05, 'epoch': 2.0}
{'eval_loss': 2.297647476196289, 'eval_accuracy@por.rst.cstn': 0.3333333333333333, 'eval_f1@por.rst.cstn': 0.07314584658572337, 'eval_precision@por.rst.cstn': 0.08208575641599151, 'eval_recall@por.rst.cstn': 0.09068879078760501, 'eval_loss@por.rst.cstn': 2.29764723777771, 'eval_runtime': 7.2316, 'eval_samples_per_second': 79.235, 'eval_steps_per_second': 2.489, 'epoch': 2.0}
{'train@por.rst.cstn_loss': 1.958350419998169, 'train@por.rst.cstn_accuracy@por.rst.cstn': 0.45973963355834135, 'train@por.rst.cstn_f1@por.rst.cstn': 0.07284084154111034, 'train@por.rst.cstn_precision@por.rst.cstn': 0.07163966911942593, 'train@por.rst.cstn_recall@por.rst.cstn': 0.08870937650113028, 'train@por.rst.cstn_loss@por.rst.cstn': 1.958350419998169, 'train@por.rst.cstn_runtime': 50.0492, 'train@por.rst.cstn_samples_per_second': 82.878, 'train@por.rst.cstn_steps_per_second': 2.597, 'epoch': 3.0}
{'loss': 2.1253, 'learning_rate': 1.5000000000000002e-05, 'epoch': 3.0}
{'eval_loss': 2.091475248336792, 'eval_accuracy@por.rst.cstn': 0.36649214659685864, 'eval_f1@por.rst.cstn': 0.08745054008933976, 'eval_precision@por.rst.cstn': 0.0764081562377017, 'eval_recall@por.rst.cstn': 0.12257820710597911, 'eval_loss@por.rst.cstn': 2.091475009918213, 'eval_runtime': 7.2168, 'eval_samples_per_second': 79.399, 'eval_steps_per_second': 2.494, 'epoch': 3.0}
{'train@por.rst.cstn_loss': 1.806330680847168, 'train@por.rst.cstn_accuracy@por.rst.cstn': 0.5207328833172613, 'train@por.rst.cstn_f1@por.rst.cstn': 0.10289373407786358, 'train@por.rst.cstn_precision@por.rst.cstn': 0.1346012374866276, 'train@por.rst.cstn_recall@por.rst.cstn': 0.1121639121358958, 'train@por.rst.cstn_loss@por.rst.cstn': 1.8063308000564575, 'train@por.rst.cstn_runtime': 49.92, 'train@por.rst.cstn_samples_per_second': 83.093, 'train@por.rst.cstn_steps_per_second': 2.604, 'epoch': 4.0}
{'loss': 1.9452, 'learning_rate': 1.3333333333333333e-05, 'epoch': 4.0}
{'eval_loss': 1.9578876495361328, 'eval_accuracy@por.rst.cstn': 0.44153577661431065, 'eval_f1@por.rst.cstn': 0.14303007907843, 'eval_precision@por.rst.cstn': 0.18731291542360945, 'eval_recall@por.rst.cstn': 0.16372893021270388, 'eval_loss@por.rst.cstn': 1.9578876495361328, 'eval_runtime': 7.205, 'eval_samples_per_second': 79.528, 'eval_steps_per_second': 2.498, 'epoch': 4.0}
{'train@por.rst.cstn_loss': 1.7092150449752808, 'train@por.rst.cstn_accuracy@por.rst.cstn': 0.5368852459016393, 'train@por.rst.cstn_f1@por.rst.cstn': 0.11813563743986105, 'train@por.rst.cstn_precision@por.rst.cstn': 0.13057857432026043, 'train@por.rst.cstn_recall@por.rst.cstn': 0.1282044068504804, 'train@por.rst.cstn_loss@por.rst.cstn': 1.7092152833938599, 'train@por.rst.cstn_runtime': 49.9271, 'train@por.rst.cstn_samples_per_second': 83.081, 'train@por.rst.cstn_steps_per_second': 2.604, 'epoch': 5.0}
{'loss': 1.8205, 'learning_rate': 1.1666666666666668e-05, 'epoch': 5.0}
{'eval_loss': 1.8623790740966797, 'eval_accuracy@por.rst.cstn': 0.4537521815008726, 'eval_f1@por.rst.cstn': 0.16017941356374205, 'eval_precision@por.rst.cstn': 0.17443727941366677, 'eval_recall@por.rst.cstn': 0.17792984152199878, 'eval_loss@por.rst.cstn': 1.8623790740966797, 'eval_runtime': 7.2365, 'eval_samples_per_second': 79.182, 'eval_steps_per_second': 2.487, 'epoch': 5.0}
{'train@por.rst.cstn_loss': 1.6462665796279907, 'train@por.rst.cstn_accuracy@por.rst.cstn': 0.5658148505303761, 'train@por.rst.cstn_f1@por.rst.cstn': 0.12783359310799794, 'train@por.rst.cstn_precision@por.rst.cstn': 0.13176766150757468, 'train@por.rst.cstn_recall@por.rst.cstn': 0.13930264986473195, 'train@por.rst.cstn_loss@por.rst.cstn': 1.6462668180465698, 'train@por.rst.cstn_runtime': 49.929, 'train@por.rst.cstn_samples_per_second': 83.078, 'train@por.rst.cstn_steps_per_second': 2.604, 'epoch': 6.0}
{'loss': 1.7374, 'learning_rate': 1e-05, 'epoch': 6.0}
{'eval_loss': 1.8002128601074219, 'eval_accuracy@por.rst.cstn': 0.4851657940663176, 'eval_f1@por.rst.cstn': 0.17473286124106224, 'eval_precision@por.rst.cstn': 0.1781675819504456, 'eval_recall@por.rst.cstn': 0.19368086493632572, 'eval_loss@por.rst.cstn': 1.800213098526001, 'eval_runtime': 7.1863, 'eval_samples_per_second': 79.735, 'eval_steps_per_second': 2.505, 'epoch': 6.0}
{'train@por.rst.cstn_loss': 1.6008586883544922, 'train@por.rst.cstn_accuracy@por.rst.cstn': 0.577145612343298, 'train@por.rst.cstn_f1@por.rst.cstn': 0.13445016021967376, 'train@por.rst.cstn_precision@por.rst.cstn': 0.13210178187872443, 'train@por.rst.cstn_recall@por.rst.cstn': 0.14638422075745677, 'train@por.rst.cstn_loss@por.rst.cstn': 1.6008586883544922, 'train@por.rst.cstn_runtime': 49.9698, 'train@por.rst.cstn_samples_per_second': 83.01, 'train@por.rst.cstn_steps_per_second': 2.602, 'epoch': 7.0}
{'loss': 1.6816, 'learning_rate': 8.333333333333334e-06, 'epoch': 7.0}
{'eval_loss': 1.7544610500335693, 'eval_accuracy@por.rst.cstn': 0.5043630017452007, 'eval_f1@por.rst.cstn': 0.18320935901126426, 'eval_precision@por.rst.cstn': 0.18213494659451995, 'eval_recall@por.rst.cstn': 0.2015965623725141, 'eval_loss@por.rst.cstn': 1.7544610500335693, 'eval_runtime': 7.2288, 'eval_samples_per_second': 79.266, 'eval_steps_per_second': 2.49, 'epoch': 7.0}
{'train@por.rst.cstn_loss': 1.574295997619629, 'train@por.rst.cstn_accuracy@por.rst.cstn': 0.5826904532304725, 'train@por.rst.cstn_f1@por.rst.cstn': 0.13678241647966577, 'train@por.rst.cstn_precision@por.rst.cstn': 0.13366189309361065, 'train@por.rst.cstn_recall@por.rst.cstn': 0.1483268385985943, 'train@por.rst.cstn_loss@por.rst.cstn': 1.5742961168289185, 'train@por.rst.cstn_runtime': 50.0417, 'train@por.rst.cstn_samples_per_second': 82.891, 'train@por.rst.cstn_steps_per_second': 2.598, 'epoch': 8.0}
{'loss': 1.6492, 'learning_rate': 6.666666666666667e-06, 'epoch': 8.0}
{'eval_loss': 1.7339024543762207, 'eval_accuracy@por.rst.cstn': 0.5078534031413613, 'eval_f1@por.rst.cstn': 0.18556217201019995, 'eval_precision@por.rst.cstn': 0.18510162399938146, 'eval_recall@por.rst.cstn': 0.20220451340952694, 'eval_loss@por.rst.cstn': 1.7339024543762207, 'eval_runtime': 7.2384, 'eval_samples_per_second': 79.161, 'eval_steps_per_second': 2.487, 'epoch': 8.0}
{'train@por.rst.cstn_loss': 1.5543442964553833, 'train@por.rst.cstn_accuracy@por.rst.cstn': 0.5872709739633558, 'train@por.rst.cstn_f1@por.rst.cstn': 0.13977448500791748, 'train@por.rst.cstn_precision@por.rst.cstn': 0.13584028464704034, 'train@por.rst.cstn_recall@por.rst.cstn': 0.15089819132356816, 'train@por.rst.cstn_loss@por.rst.cstn': 1.5543440580368042, 'train@por.rst.cstn_runtime': 50.0626, 'train@por.rst.cstn_samples_per_second': 82.856, 'train@por.rst.cstn_steps_per_second': 2.597, 'epoch': 9.0}
{'loss': 1.6232, 'learning_rate': 5e-06, 'epoch': 9.0}
{'eval_loss': 1.716675877571106, 'eval_accuracy@por.rst.cstn': 0.5148342059336823, 'eval_f1@por.rst.cstn': 0.19093674681385675, 'eval_precision@por.rst.cstn': 0.18949471934275375, 'eval_recall@por.rst.cstn': 0.20662370532871888, 'eval_loss@por.rst.cstn': 1.716676115989685, 'eval_runtime': 7.2239, 'eval_samples_per_second': 79.32, 'eval_steps_per_second': 2.492, 'epoch': 9.0}
{'train@por.rst.cstn_loss': 1.5370088815689087, 'train@por.rst.cstn_accuracy@por.rst.cstn': 0.5935390549662488, 'train@por.rst.cstn_f1@por.rst.cstn': 0.14350849433782945, 'train@por.rst.cstn_precision@por.rst.cstn': 0.15631136951481583, 'train@por.rst.cstn_recall@por.rst.cstn': 0.15545012918976203, 'train@por.rst.cstn_loss@por.rst.cstn': 1.5370087623596191, 'train@por.rst.cstn_runtime': 50.1124, 'train@por.rst.cstn_samples_per_second': 82.774, 'train@por.rst.cstn_steps_per_second': 2.594, 'epoch': 10.0}
{'loss': 1.5994, 'learning_rate': 3.3333333333333333e-06, 'epoch': 10.0}
{'eval_loss': 1.69807767868042, 'eval_accuracy@por.rst.cstn': 0.5148342059336823, 'eval_f1@por.rst.cstn': 0.19525134789457477, 'eval_precision@por.rst.cstn': 0.1988611986334167, 'eval_recall@por.rst.cstn': 0.21114803042460625, 'eval_loss@por.rst.cstn': 1.6980775594711304, 'eval_runtime': 7.2222, 'eval_samples_per_second': 79.338, 'eval_steps_per_second': 2.492, 'epoch': 10.0}
{'train@por.rst.cstn_loss': 1.5286508798599243, 'train@por.rst.cstn_accuracy@por.rst.cstn': 0.5935390549662488, 'train@por.rst.cstn_f1@por.rst.cstn': 0.1445303626978082, 'train@por.rst.cstn_precision@por.rst.cstn': 0.151629758514338, 'train@por.rst.cstn_recall@por.rst.cstn': 0.15609380394291938, 'train@por.rst.cstn_loss@por.rst.cstn': 1.5286508798599243, 'train@por.rst.cstn_runtime': 50.0719, 'train@por.rst.cstn_samples_per_second': 82.841, 'train@por.rst.cstn_steps_per_second': 2.596, 'epoch': 11.0}
{'loss': 1.5902, 'learning_rate': 1.6666666666666667e-06, 'epoch': 11.0}
{'eval_loss': 1.6919808387756348, 'eval_accuracy@por.rst.cstn': 0.5078534031413613, 'eval_f1@por.rst.cstn': 0.19376561215223062, 'eval_precision@por.rst.cstn': 0.19739679386532663, 'eval_recall@por.rst.cstn': 0.20923415482652014, 'eval_loss@por.rst.cstn': 1.6919808387756348, 'eval_runtime': 7.2215, 'eval_samples_per_second': 79.346, 'eval_steps_per_second': 2.493, 'epoch': 11.0}
{'train@por.rst.cstn_loss': 1.526015043258667, 'train@por.rst.cstn_accuracy@por.rst.cstn': 0.5949855351976856, 'train@por.rst.cstn_f1@por.rst.cstn': 0.14542506619000228, 'train@por.rst.cstn_precision@por.rst.cstn': 0.15547326629848734, 'train@por.rst.cstn_recall@por.rst.cstn': 0.15694491254866547, 'train@por.rst.cstn_loss@por.rst.cstn': 1.526015043258667, 'train@por.rst.cstn_runtime': 50.0834, 'train@por.rst.cstn_samples_per_second': 82.822, 'train@por.rst.cstn_steps_per_second': 2.596, 'epoch': 12.0}
{'loss': 1.5835, 'learning_rate': 0.0, 'epoch': 12.0}
{'eval_loss': 1.6887431144714355, 'eval_accuracy@por.rst.cstn': 0.5078534031413613, 'eval_f1@por.rst.cstn': 0.19420985476037594, 'eval_precision@por.rst.cstn': 0.19801312103403826, 'eval_recall@por.rst.cstn': 0.20923415482652014, 'eval_loss@por.rst.cstn': 1.688743233680725, 'eval_runtime': 7.2446, 'eval_samples_per_second': 79.094, 'eval_steps_per_second': 2.485, 'epoch': 12.0}
{'train_runtime': 1953.4757, 'train_samples_per_second': 25.481, 'train_steps_per_second': 0.799, 'train_loss': 1.8902545733329577, 'epoch': 12.0}
***** train_log metrics *****
  epoch                    =       12.0
  train_loss               =     1.8903
  train_runtime            = 0:32:33.47
  train_samples_per_second =     25.481
  train_steps_per_second   =      0.799
{'train@eng.rst.rstdt_loss': 1.5414220094680786, 'train@eng.rst.rstdt_accuracy@eng.rst.rstdt': 0.5712410948631421, 'train@eng.rst.rstdt_f1@eng.rst.rstdt': 0.16612503470586515, 'train@eng.rst.rstdt_precision@eng.rst.rstdt': 0.17421908344324574, 'train@eng.rst.rstdt_recall@eng.rst.rstdt': 0.17819249483953897, 'train@eng.rst.rstdt_loss@eng.rst.rstdt': 1.5414220094680786, 'train@eng.rst.rstdt_runtime': 188.3387, 'train@eng.rst.rstdt_samples_per_second': 84.964, 'train@eng.rst.rstdt_steps_per_second': 2.66, 'epoch': 1.0}
{'loss': 2.0654, 'learning_rate': 1.8333333333333333e-05, 'epoch': 1.0}
{'eval_loss': 1.546199083328247, 'eval_accuracy@eng.rst.rstdt': 0.5731030228254164, 'eval_f1@eng.rst.rstdt': 0.16504954319020887, 'eval_precision@eng.rst.rstdt': 0.163697132537982, 'eval_recall@eng.rst.rstdt': 0.17526172231160472, 'eval_loss@eng.rst.rstdt': 1.5461989641189575, 'eval_runtime': 19.4534, 'eval_samples_per_second': 83.327, 'eval_steps_per_second': 2.622, 'epoch': 1.0}
{'train@eng.rst.rstdt_loss': 1.3536685705184937, 'train@eng.rst.rstdt_accuracy@eng.rst.rstdt': 0.6246094238220222, 'train@eng.rst.rstdt_f1@eng.rst.rstdt': 0.26552376390887666, 'train@eng.rst.rstdt_precision@eng.rst.rstdt': 0.4090827433598318, 'train@eng.rst.rstdt_recall@eng.rst.rstdt': 0.25860209430010583, 'train@eng.rst.rstdt_loss@eng.rst.rstdt': 1.3536685705184937, 'train@eng.rst.rstdt_runtime': 189.0323, 'train@eng.rst.rstdt_samples_per_second': 84.652, 'train@eng.rst.rstdt_steps_per_second': 2.65, 'epoch': 2.0}
{'loss': 1.4872, 'learning_rate': 1.6666666666666667e-05, 'epoch': 2.0}
{'eval_loss': 1.3769069910049438, 'eval_accuracy@eng.rst.rstdt': 0.6162862430598396, 'eval_f1@eng.rst.rstdt': 0.251406996988777, 'eval_precision@eng.rst.rstdt': 0.3763307518284134, 'eval_recall@eng.rst.rstdt': 0.2505744549499535, 'eval_loss@eng.rst.rstdt': 1.3769069910049438, 'eval_runtime': 19.5114, 'eval_samples_per_second': 83.08, 'eval_steps_per_second': 2.614, 'epoch': 2.0}
{'train@eng.rst.rstdt_loss': 1.258914589881897, 'train@eng.rst.rstdt_accuracy@eng.rst.rstdt': 0.6477940257467817, 'train@eng.rst.rstdt_f1@eng.rst.rstdt': 0.3404443458276271, 'train@eng.rst.rstdt_precision@eng.rst.rstdt': 0.4405744496097853, 'train@eng.rst.rstdt_recall@eng.rst.rstdt': 0.31866978431127785, 'train@eng.rst.rstdt_loss@eng.rst.rstdt': 1.258914589881897, 'train@eng.rst.rstdt_runtime': 188.965, 'train@eng.rst.rstdt_samples_per_second': 84.682, 'train@eng.rst.rstdt_steps_per_second': 2.651, 'epoch': 3.0}
{'loss': 1.362, 'learning_rate': 1.5000000000000002e-05, 'epoch': 3.0}
{'eval_loss': 1.3055531978607178, 'eval_accuracy@eng.rst.rstdt': 0.6391116594694632, 'eval_f1@eng.rst.rstdt': 0.32141932799930906, 'eval_precision@eng.rst.rstdt': 0.4099761590820681, 'eval_recall@eng.rst.rstdt': 0.31420734507969733, 'eval_loss@eng.rst.rstdt': 1.3055530786514282, 'eval_runtime': 19.4708, 'eval_samples_per_second': 83.253, 'eval_steps_per_second': 2.619, 'epoch': 3.0}
{'train@eng.rst.rstdt_loss': 1.2040657997131348, 'train@eng.rst.rstdt_accuracy@eng.rst.rstdt': 0.6556055493063367, 'train@eng.rst.rstdt_f1@eng.rst.rstdt': 0.365251310328782, 'train@eng.rst.rstdt_precision@eng.rst.rstdt': 0.4804445732347093, 'train@eng.rst.rstdt_recall@eng.rst.rstdt': 0.34389294684619204, 'train@eng.rst.rstdt_loss@eng.rst.rstdt': 1.2040659189224243, 'train@eng.rst.rstdt_runtime': 188.4771, 'train@eng.rst.rstdt_samples_per_second': 84.902, 'train@eng.rst.rstdt_steps_per_second': 2.658, 'epoch': 4.0}
{'loss': 1.2823, 'learning_rate': 1.3333333333333333e-05, 'epoch': 4.0}
{'eval_loss': 1.2730916738510132, 'eval_accuracy@eng.rst.rstdt': 0.6329426280074029, 'eval_f1@eng.rst.rstdt': 0.3361689558259117, 'eval_precision@eng.rst.rstdt': 0.40260815306957637, 'eval_recall@eng.rst.rstdt': 0.333809921440264, 'eval_loss@eng.rst.rstdt': 1.2730917930603027, 'eval_runtime': 19.7032, 'eval_samples_per_second': 82.271, 'eval_steps_per_second': 2.588, 'epoch': 4.0}
{'train@eng.rst.rstdt_loss': 1.1656367778778076, 'train@eng.rst.rstdt_accuracy@eng.rst.rstdt': 0.6639170103737033, 'train@eng.rst.rstdt_f1@eng.rst.rstdt': 0.3824944020879867, 'train@eng.rst.rstdt_precision@eng.rst.rstdt': 0.5077231036459592, 'train@eng.rst.rstdt_recall@eng.rst.rstdt': 0.3543504645124959, 'train@eng.rst.rstdt_loss@eng.rst.rstdt': 1.165636658668518, 'train@eng.rst.rstdt_runtime': 189.171, 'train@eng.rst.rstdt_samples_per_second': 84.59, 'train@eng.rst.rstdt_steps_per_second': 2.648, 'epoch': 5.0}
{'loss': 1.2346, 'learning_rate': 1.1666666666666668e-05, 'epoch': 5.0}
{'eval_loss': 1.240095615386963, 'eval_accuracy@eng.rst.rstdt': 0.6415792720542874, 'eval_f1@eng.rst.rstdt': 0.3423773913463164, 'eval_precision@eng.rst.rstdt': 0.4222802173436155, 'eval_recall@eng.rst.rstdt': 0.33684735800121307, 'eval_loss@eng.rst.rstdt': 1.2400957345962524, 'eval_runtime': 19.5476, 'eval_samples_per_second': 82.926, 'eval_steps_per_second': 2.609, 'epoch': 5.0}
{'train@eng.rst.rstdt_loss': 1.1378003358840942, 'train@eng.rst.rstdt_accuracy@eng.rst.rstdt': 0.6687289088863893, 'train@eng.rst.rstdt_f1@eng.rst.rstdt': 0.39431278866924246, 'train@eng.rst.rstdt_precision@eng.rst.rstdt': 0.515133048509645, 'train@eng.rst.rstdt_recall@eng.rst.rstdt': 0.3622418098797055, 'train@eng.rst.rstdt_loss@eng.rst.rstdt': 1.1378004550933838, 'train@eng.rst.rstdt_runtime': 188.7131, 'train@eng.rst.rstdt_samples_per_second': 84.795, 'train@eng.rst.rstdt_steps_per_second': 2.655, 'epoch': 6.0}
{'loss': 1.1994, 'learning_rate': 1e-05, 'epoch': 6.0}
{'eval_loss': 1.2167472839355469, 'eval_accuracy@eng.rst.rstdt': 0.6421961752004935, 'eval_f1@eng.rst.rstdt': 0.3585376019543384, 'eval_precision@eng.rst.rstdt': 0.46837926318031103, 'eval_recall@eng.rst.rstdt': 0.34722380087027005, 'eval_loss@eng.rst.rstdt': 1.2167471647262573, 'eval_runtime': 19.4416, 'eval_samples_per_second': 83.378, 'eval_steps_per_second': 2.623, 'epoch': 6.0}
{'train@eng.rst.rstdt_loss': 1.1229755878448486, 'train@eng.rst.rstdt_accuracy@eng.rst.rstdt': 0.672665916760405, 'train@eng.rst.rstdt_f1@eng.rst.rstdt': 0.4015018507258101, 'train@eng.rst.rstdt_precision@eng.rst.rstdt': 0.5907194513769887, 'train@eng.rst.rstdt_recall@eng.rst.rstdt': 0.3659823116045407, 'train@eng.rst.rstdt_loss@eng.rst.rstdt': 1.122975468635559, 'train@eng.rst.rstdt_runtime': 189.1621, 'train@eng.rst.rstdt_samples_per_second': 84.594, 'train@eng.rst.rstdt_steps_per_second': 2.649, 'epoch': 7.0}
{'loss': 1.1834, 'learning_rate': 8.333333333333334e-06, 'epoch': 7.0}
{'eval_loss': 1.2046760320663452, 'eval_accuracy@eng.rst.rstdt': 0.6477483035163479, 'eval_f1@eng.rst.rstdt': 0.3656510622582041, 'eval_precision@eng.rst.rstdt': 0.473578060799573, 'eval_recall@eng.rst.rstdt': 0.34890824378574137, 'eval_loss@eng.rst.rstdt': 1.2046760320663452, 'eval_runtime': 19.5351, 'eval_samples_per_second': 82.979, 'eval_steps_per_second': 2.611, 'epoch': 7.0}
{'train@eng.rst.rstdt_loss': 1.1069928407669067, 'train@eng.rst.rstdt_accuracy@eng.rst.rstdt': 0.6723534558180227, 'train@eng.rst.rstdt_f1@eng.rst.rstdt': 0.406523939173248, 'train@eng.rst.rstdt_precision@eng.rst.rstdt': 0.5808847773576353, 'train@eng.rst.rstdt_recall@eng.rst.rstdt': 0.3753520645492412, 'train@eng.rst.rstdt_loss@eng.rst.rstdt': 1.1069929599761963, 'train@eng.rst.rstdt_runtime': 189.1377, 'train@eng.rst.rstdt_samples_per_second': 84.605, 'train@eng.rst.rstdt_steps_per_second': 2.649, 'epoch': 8.0}
{'loss': 1.1639, 'learning_rate': 6.666666666666667e-06, 'epoch': 8.0}
{'eval_loss': 1.204380750656128, 'eval_accuracy@eng.rst.rstdt': 0.6458975940777298, 'eval_f1@eng.rst.rstdt': 0.37287374423188313, 'eval_precision@eng.rst.rstdt': 0.47253181552588697, 'eval_recall@eng.rst.rstdt': 0.35922556575336806, 'eval_loss@eng.rst.rstdt': 1.2043806314468384, 'eval_runtime': 19.5781, 'eval_samples_per_second': 82.796, 'eval_steps_per_second': 2.605, 'epoch': 8.0}
{'train@eng.rst.rstdt_loss': 1.0976848602294922, 'train@eng.rst.rstdt_accuracy@eng.rst.rstdt': 0.6757280339957505, 'train@eng.rst.rstdt_f1@eng.rst.rstdt': 0.4113438165751216, 'train@eng.rst.rstdt_precision@eng.rst.rstdt': 0.5825111874900086, 'train@eng.rst.rstdt_recall@eng.rst.rstdt': 0.3778957567044666, 'train@eng.rst.rstdt_loss@eng.rst.rstdt': 1.0976849794387817, 'train@eng.rst.rstdt_runtime': 188.4957, 'train@eng.rst.rstdt_samples_per_second': 84.893, 'train@eng.rst.rstdt_steps_per_second': 2.658, 'epoch': 9.0}
{'loss': 1.1508, 'learning_rate': 5e-06, 'epoch': 9.0}
{'eval_loss': 1.1948838233947754, 'eval_accuracy@eng.rst.rstdt': 0.6477483035163479, 'eval_f1@eng.rst.rstdt': 0.37016691460384366, 'eval_precision@eng.rst.rstdt': 0.47478929645472606, 'eval_recall@eng.rst.rstdt': 0.35701890590734386, 'eval_loss@eng.rst.rstdt': 1.1948840618133545, 'eval_runtime': 19.4643, 'eval_samples_per_second': 83.281, 'eval_steps_per_second': 2.62, 'epoch': 9.0}
{'train@eng.rst.rstdt_loss': 1.0941414833068848, 'train@eng.rst.rstdt_accuracy@eng.rst.rstdt': 0.6737907761529809, 'train@eng.rst.rstdt_f1@eng.rst.rstdt': 0.4185278501782984, 'train@eng.rst.rstdt_precision@eng.rst.rstdt': 0.5671190015316243, 'train@eng.rst.rstdt_recall@eng.rst.rstdt': 0.3854474828303478, 'train@eng.rst.rstdt_loss@eng.rst.rstdt': 1.0941414833068848, 'train@eng.rst.rstdt_runtime': 188.5242, 'train@eng.rst.rstdt_samples_per_second': 84.88, 'train@eng.rst.rstdt_steps_per_second': 2.657, 'epoch': 10.0}
{'loss': 1.1426, 'learning_rate': 3.3333333333333333e-06, 'epoch': 10.0}
{'eval_loss': 1.2019187211990356, 'eval_accuracy@eng.rst.rstdt': 0.6397285626156693, 'eval_f1@eng.rst.rstdt': 0.3761505659109447, 'eval_precision@eng.rst.rstdt': 0.49727405395556845, 'eval_recall@eng.rst.rstdt': 0.36266041176558683, 'eval_loss@eng.rst.rstdt': 1.2019187211990356, 'eval_runtime': 19.4564, 'eval_samples_per_second': 83.315, 'eval_steps_per_second': 2.621, 'epoch': 10.0}
{'train@eng.rst.rstdt_loss': 1.087226390838623, 'train@eng.rst.rstdt_accuracy@eng.rst.rstdt': 0.6761029871266092, 'train@eng.rst.rstdt_f1@eng.rst.rstdt': 0.418956100793576, 'train@eng.rst.rstdt_precision@eng.rst.rstdt': 0.5785978307221049, 'train@eng.rst.rstdt_recall@eng.rst.rstdt': 0.38422567472483693, 'train@eng.rst.rstdt_loss@eng.rst.rstdt': 1.087226390838623, 'train@eng.rst.rstdt_runtime': 188.9588, 'train@eng.rst.rstdt_samples_per_second': 84.685, 'train@eng.rst.rstdt_steps_per_second': 2.651, 'epoch': 11.0}
{'loss': 1.1408, 'learning_rate': 1.6666666666666667e-06, 'epoch': 11.0}
{'eval_loss': 1.1929916143417358, 'eval_accuracy@eng.rst.rstdt': 0.6440468846391116, 'eval_f1@eng.rst.rstdt': 0.374365441935241, 'eval_precision@eng.rst.rstdt': 0.4726292174744188, 'eval_recall@eng.rst.rstdt': 0.36039140380397927, 'eval_loss@eng.rst.rstdt': 1.1929916143417358, 'eval_runtime': 19.4728, 'eval_samples_per_second': 83.244, 'eval_steps_per_second': 2.619, 'epoch': 11.0}
{'train@eng.rst.rstdt_loss': 1.085717797279358, 'train@eng.rst.rstdt_accuracy@eng.rst.rstdt': 0.6764779402574678, 'train@eng.rst.rstdt_f1@eng.rst.rstdt': 0.41926232277631054, 'train@eng.rst.rstdt_precision@eng.rst.rstdt': 0.6387272716245592, 'train@eng.rst.rstdt_recall@eng.rst.rstdt': 0.3837196659006482, 'train@eng.rst.rstdt_loss@eng.rst.rstdt': 1.0857176780700684, 'train@eng.rst.rstdt_runtime': 188.5486, 'train@eng.rst.rstdt_samples_per_second': 84.869, 'train@eng.rst.rstdt_steps_per_second': 2.657, 'epoch': 12.0}
{'loss': 1.1358, 'learning_rate': 0.0, 'epoch': 12.0}
{'eval_loss': 1.1906828880310059, 'eval_accuracy@eng.rst.rstdt': 0.6446637877853177, 'eval_f1@eng.rst.rstdt': 0.37454204518677664, 'eval_precision@eng.rst.rstdt': 0.47416784018116026, 'eval_recall@eng.rst.rstdt': 0.3599656931139833, 'eval_loss@eng.rst.rstdt': 1.1906828880310059, 'eval_runtime': 19.4781, 'eval_samples_per_second': 83.222, 'eval_steps_per_second': 2.618, 'epoch': 12.0}
{'train_runtime': 7305.591, 'train_samples_per_second': 26.285, 'train_steps_per_second': 0.823, 'train_loss': 1.295675281517044, 'epoch': 12.0}
***** train_logger metrics *****
  epoch                    =       12.0
  train_loss               =     1.8903
  train_runtime            = 0:32:33.47
  train_samples_per_second =     25.481
  train_steps_per_second   =      0.799
-------------------------------------------------------------------
Lang1:  rus.rst.rrt    Lang2:  eng.rst.rstdt
Saving run to:  runs/full_shot/FullShot=v4_rus.rst.rrt_eng.rst.rstdt_lrfunc=Adafactor_lr=2e-05_plm=bert-base-multilingual-cased
Running with params: BERT_MODEL=bert-base-multilingual-cased lr=2e-05
read 28822 examples
read 2855 examples
read 2843 examples
read 16002 examples
read 1621 examples
read 2155 examples
Total prediction labels:  30
BertAdapterModel(
  (shared_parameters): ModuleDict()
  (bert): BertModel(
    (shared_parameters): ModuleDict()
    (invertible_adapters): ModuleDict(
      (ru): NICECouplingBlock(
        (F): Sequential(
          (0): Linear(in_features=384, out_features=192, bias=True)
          (1): Activation_Function_Class(
            (f): NewGELUActivation()
          )
          (2): Linear(in_features=192, out_features=384, bias=True)
        )
        (G): Sequential(
          (0): Linear(in_features=384, out_features=192, bias=True)
          (1): Activation_Function_Class(
            (f): NewGELUActivation()
          )
          (2): Linear(in_features=192, out_features=384, bias=True)
        )
      )
      (en): NICECouplingBlock(
        (F): Sequential(
          (0): Linear(in_features=384, out_features=192, bias=True)
          (1): Activation_Function_Class(
            (f): ReLU()
          )
          (2): Linear(in_features=192, out_features=384, bias=True)
        )
        (G): Sequential(
          (0): Linear(in_features=384, out_features=192, bias=True)
          (1): Activation_Function_Class(
            (f): ReLU()
          )
          (2): Linear(in_features=192, out_features=384, bias=True)
        )
      )
    )
    (embeddings): BertEmbeddings(
      (word_embeddings): Embedding(119547, 768, padding_idx=0)
      (position_embeddings): Embedding(512, 768)
      (token_type_embeddings): Embedding(2, 768)
      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
      (dropout): Dropout(p=0.1, inplace=False)
    )
    (encoder): BertEncoder(
      (layer): ModuleList(
        (0): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (ru): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (en): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (1): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (ru): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (en): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (2): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (ru): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (en): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (3): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (ru): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (en): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (4): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (ru): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (en): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (5): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (ru): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (en): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (6): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (ru): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (en): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (7): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (ru): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (en): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (8): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (ru): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (en): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (9): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (ru): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (en): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (10): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (ru): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (en): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (11): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (ru): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (en): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
      )
    )
    (pooler): BertPooler(
      (dense): Linear(in_features=768, out_features=768, bias=True)
      (activation): Tanh()
    )
    (prefix_tuning): PrefixTuningPool(
      (prefix_tunings): ModuleDict()
    )
  )
  (heads): ModuleDict(
    (disrpt): ClassificationHead(
      (0): Dropout(p=0.1, inplace=False)
      (1): Linear(in_features=768, out_features=768, bias=True)
      (2): Activation_Function_Class(
        (f): Tanh()
      )
      (3): Dropout(p=0.1, inplace=False)
      (4): Linear(in_features=768, out_features=30, bias=True)
    )
  )
)
{'train@rus.rst.rrt_loss': 1.732812523841858, 'train@rus.rst.rrt_accuracy@rus.rst.rrt': 0.49101380889598223, 'train@rus.rst.rrt_f1@rus.rst.rrt': 0.17593264692650942, 'train@rus.rst.rrt_precision@rus.rst.rrt': 0.21570371456654172, 'train@rus.rst.rrt_recall@rus.rst.rrt': 0.19108379242066986, 'train@rus.rst.rrt_loss@rus.rst.rrt': 1.732812523841858, 'train@rus.rst.rrt_runtime': 346.2446, 'train@rus.rst.rrt_samples_per_second': 83.242, 'train@rus.rst.rrt_steps_per_second': 2.602, 'epoch': 1.0}
{'loss': 2.1547, 'learning_rate': 1.8333333333333333e-05, 'epoch': 1.0}
{'eval_loss': 1.7690850496292114, 'eval_accuracy@rus.rst.rrt': 0.47390542907180383, 'eval_f1@rus.rst.rrt': 0.19720406326570258, 'eval_precision@rus.rst.rrt': 0.20168702939863062, 'eval_recall@rus.rst.rrt': 0.2128061487461087, 'eval_loss@rus.rst.rrt': 1.7690850496292114, 'eval_runtime': 34.5564, 'eval_samples_per_second': 82.619, 'eval_steps_per_second': 2.604, 'epoch': 1.0}
{'train@rus.rst.rrt_loss': 1.507079839706421, 'train@rus.rst.rrt_accuracy@rus.rst.rrt': 0.5401082506418708, 'train@rus.rst.rrt_f1@rus.rst.rrt': 0.22431306317993424, 'train@rus.rst.rrt_precision@rus.rst.rrt': 0.31709557645947134, 'train@rus.rst.rrt_recall@rus.rst.rrt': 0.2304247141777279, 'train@rus.rst.rrt_loss@rus.rst.rrt': 1.5070797204971313, 'train@rus.rst.rrt_runtime': 345.7542, 'train@rus.rst.rrt_samples_per_second': 83.36, 'train@rus.rst.rrt_steps_per_second': 2.606, 'epoch': 2.0}
{'loss': 1.6468, 'learning_rate': 1.6666666666666667e-05, 'epoch': 2.0}
{'eval_loss': 1.575371265411377, 'eval_accuracy@rus.rst.rrt': 0.5148861646234676, 'eval_f1@rus.rst.rrt': 0.2471269974397369, 'eval_precision@rus.rst.rrt': 0.34941030058775274, 'eval_recall@rus.rst.rrt': 0.2535727748487017, 'eval_loss@rus.rst.rrt': 1.575371265411377, 'eval_runtime': 34.5708, 'eval_samples_per_second': 82.584, 'eval_steps_per_second': 2.603, 'epoch': 2.0}
{'train@rus.rst.rrt_loss': 1.4276694059371948, 'train@rus.rst.rrt_accuracy@rus.rst.rrt': 0.5642217750329609, 'train@rus.rst.rrt_f1@rus.rst.rrt': 0.27600801060538793, 'train@rus.rst.rrt_precision@rus.rst.rrt': 0.41502326606203205, 'train@rus.rst.rrt_recall@rus.rst.rrt': 0.27252789550876383, 'train@rus.rst.rrt_loss@rus.rst.rrt': 1.4276694059371948, 'train@rus.rst.rrt_runtime': 346.5737, 'train@rus.rst.rrt_samples_per_second': 83.163, 'train@rus.rst.rrt_steps_per_second': 2.6, 'epoch': 3.0}
{'loss': 1.5139, 'learning_rate': 1.5000000000000002e-05, 'epoch': 3.0}
{'eval_loss': 1.4999736547470093, 'eval_accuracy@rus.rst.rrt': 0.5362521891418564, 'eval_f1@rus.rst.rrt': 0.29683234030435424, 'eval_precision@rus.rst.rrt': 0.3829551556091064, 'eval_recall@rus.rst.rrt': 0.29640255011457833, 'eval_loss@rus.rst.rrt': 1.4999736547470093, 'eval_runtime': 34.6348, 'eval_samples_per_second': 82.432, 'eval_steps_per_second': 2.599, 'epoch': 3.0}
{'train@rus.rst.rrt_loss': 1.3701649904251099, 'train@rus.rst.rrt_accuracy@rus.rst.rrt': 0.5814308514329332, 'train@rus.rst.rrt_f1@rus.rst.rrt': 0.3085507140848624, 'train@rus.rst.rrt_precision@rus.rst.rrt': 0.42234383518600976, 'train@rus.rst.rrt_recall@rus.rst.rrt': 0.29422638506875987, 'train@rus.rst.rrt_loss@rus.rst.rrt': 1.3701651096343994, 'train@rus.rst.rrt_runtime': 346.6692, 'train@rus.rst.rrt_samples_per_second': 83.14, 'train@rus.rst.rrt_steps_per_second': 2.599, 'epoch': 4.0}
{'loss': 1.4483, 'learning_rate': 1.3333333333333333e-05, 'epoch': 4.0}
{'eval_loss': 1.4455876350402832, 'eval_accuracy@rus.rst.rrt': 0.5632224168126094, 'eval_f1@rus.rst.rrt': 0.35343171818322466, 'eval_precision@rus.rst.rrt': 0.5007806074193096, 'eval_recall@rus.rst.rrt': 0.33832484344668334, 'eval_loss@rus.rst.rrt': 1.4455876350402832, 'eval_runtime': 34.6532, 'eval_samples_per_second': 82.388, 'eval_steps_per_second': 2.597, 'epoch': 4.0}
{'train@rus.rst.rrt_loss': 1.3354265689849854, 'train@rus.rst.rrt_accuracy@rus.rst.rrt': 0.5907639997224342, 'train@rus.rst.rrt_f1@rus.rst.rrt': 0.3264220649340002, 'train@rus.rst.rrt_precision@rus.rst.rrt': 0.44014454272256154, 'train@rus.rst.rrt_recall@rus.rst.rrt': 0.3090934256752011, 'train@rus.rst.rrt_loss@rus.rst.rrt': 1.335426688194275, 'train@rus.rst.rrt_runtime': 345.8796, 'train@rus.rst.rrt_samples_per_second': 83.33, 'train@rus.rst.rrt_steps_per_second': 2.605, 'epoch': 5.0}
{'loss': 1.4063, 'learning_rate': 1.1666666666666668e-05, 'epoch': 5.0}
{'eval_loss': 1.4183142185211182, 'eval_accuracy@rus.rst.rrt': 0.5642732049036777, 'eval_f1@rus.rst.rrt': 0.3657647284567145, 'eval_precision@rus.rst.rrt': 0.4819022062791576, 'eval_recall@rus.rst.rrt': 0.35172781017891014, 'eval_loss@rus.rst.rrt': 1.4183142185211182, 'eval_runtime': 34.631, 'eval_samples_per_second': 82.441, 'eval_steps_per_second': 2.599, 'epoch': 5.0}
{'train@rus.rst.rrt_loss': 1.3120030164718628, 'train@rus.rst.rrt_accuracy@rus.rst.rrt': 0.5965581847200055, 'train@rus.rst.rrt_f1@rus.rst.rrt': 0.34071286549406227, 'train@rus.rst.rrt_precision@rus.rst.rrt': 0.44561272331337226, 'train@rus.rst.rrt_recall@rus.rst.rrt': 0.3222386971462, 'train@rus.rst.rrt_loss@rus.rst.rrt': 1.3120030164718628, 'train@rus.rst.rrt_runtime': 346.6695, 'train@rus.rst.rrt_samples_per_second': 83.14, 'train@rus.rst.rrt_steps_per_second': 2.599, 'epoch': 6.0}
{'loss': 1.379, 'learning_rate': 1e-05, 'epoch': 6.0}
{'eval_loss': 1.397505760192871, 'eval_accuracy@rus.rst.rrt': 0.574430823117338, 'eval_f1@rus.rst.rrt': 0.3884895116795023, 'eval_precision@rus.rst.rrt': 0.4810834211855312, 'eval_recall@rus.rst.rrt': 0.3728548443077314, 'eval_loss@rus.rst.rrt': 1.3975058794021606, 'eval_runtime': 34.5511, 'eval_samples_per_second': 82.631, 'eval_steps_per_second': 2.605, 'epoch': 6.0}
{'train@rus.rst.rrt_loss': 1.2963361740112305, 'train@rus.rst.rrt_accuracy@rus.rst.rrt': 0.6011033238498369, 'train@rus.rst.rrt_f1@rus.rst.rrt': 0.3507394576091199, 'train@rus.rst.rrt_precision@rus.rst.rrt': 0.4793874323373863, 'train@rus.rst.rrt_recall@rus.rst.rrt': 0.33238647628181706, 'train@rus.rst.rrt_loss@rus.rst.rrt': 1.29633629322052, 'train@rus.rst.rrt_runtime': 346.9293, 'train@rus.rst.rrt_samples_per_second': 83.077, 'train@rus.rst.rrt_steps_per_second': 2.597, 'epoch': 7.0}
{'loss': 1.3559, 'learning_rate': 8.333333333333334e-06, 'epoch': 7.0}
{'eval_loss': 1.3900197744369507, 'eval_accuracy@rus.rst.rrt': 0.5761821366024519, 'eval_f1@rus.rst.rrt': 0.3981229219219027, 'eval_precision@rus.rst.rrt': 0.49263517494895537, 'eval_recall@rus.rst.rrt': 0.3817510984260009, 'eval_loss@rus.rst.rrt': 1.3900197744369507, 'eval_runtime': 34.6349, 'eval_samples_per_second': 82.431, 'eval_steps_per_second': 2.599, 'epoch': 7.0}
{'train@rus.rst.rrt_loss': 1.2830537557601929, 'train@rus.rst.rrt_accuracy@rus.rst.rrt': 0.6033585455554784, 'train@rus.rst.rrt_f1@rus.rst.rrt': 0.35865953839951237, 'train@rus.rst.rrt_precision@rus.rst.rrt': 0.4401868001516794, 'train@rus.rst.rrt_recall@rus.rst.rrt': 0.34248132448393215, 'train@rus.rst.rrt_loss@rus.rst.rrt': 1.2830537557601929, 'train@rus.rst.rrt_runtime': 346.1543, 'train@rus.rst.rrt_samples_per_second': 83.263, 'train@rus.rst.rrt_steps_per_second': 2.603, 'epoch': 8.0}
{'loss': 1.34, 'learning_rate': 6.666666666666667e-06, 'epoch': 8.0}
{'eval_loss': 1.3768973350524902, 'eval_accuracy@rus.rst.rrt': 0.5761821366024519, 'eval_f1@rus.rst.rrt': 0.40738860838006863, 'eval_precision@rus.rst.rrt': 0.48302614782954506, 'eval_recall@rus.rst.rrt': 0.39674691004767876, 'eval_loss@rus.rst.rrt': 1.3768973350524902, 'eval_runtime': 34.5937, 'eval_samples_per_second': 82.529, 'eval_steps_per_second': 2.602, 'epoch': 8.0}
{'train@rus.rst.rrt_loss': 1.2707970142364502, 'train@rus.rst.rrt_accuracy@rus.rst.rrt': 0.6059954201651516, 'train@rus.rst.rrt_f1@rus.rst.rrt': 0.36162238834887533, 'train@rus.rst.rrt_precision@rus.rst.rrt': 0.4445696619670098, 'train@rus.rst.rrt_recall@rus.rst.rrt': 0.34383118707152155, 'train@rus.rst.rrt_loss@rus.rst.rrt': 1.2707970142364502, 'train@rus.rst.rrt_runtime': 345.7266, 'train@rus.rst.rrt_samples_per_second': 83.366, 'train@rus.rst.rrt_steps_per_second': 2.606, 'epoch': 9.0}
{'loss': 1.3305, 'learning_rate': 5e-06, 'epoch': 9.0}
{'eval_loss': 1.3699629306793213, 'eval_accuracy@rus.rst.rrt': 0.5768826619964974, 'eval_f1@rus.rst.rrt': 0.40584646486980147, 'eval_precision@rus.rst.rrt': 0.4857636091130384, 'eval_recall@rus.rst.rrt': 0.3921373172716913, 'eval_loss@rus.rst.rrt': 1.3699629306793213, 'eval_runtime': 34.588, 'eval_samples_per_second': 82.543, 'eval_steps_per_second': 2.602, 'epoch': 9.0}
{'train@rus.rst.rrt_loss': 1.2642000913619995, 'train@rus.rst.rrt_accuracy@rus.rst.rrt': 0.6082853375893414, 'train@rus.rst.rrt_f1@rus.rst.rrt': 0.3639979432899214, 'train@rus.rst.rrt_precision@rus.rst.rrt': 0.46413178583746467, 'train@rus.rst.rrt_recall@rus.rst.rrt': 0.3430642851551325, 'train@rus.rst.rrt_loss@rus.rst.rrt': 1.2642000913619995, 'train@rus.rst.rrt_runtime': 346.7411, 'train@rus.rst.rrt_samples_per_second': 83.123, 'train@rus.rst.rrt_steps_per_second': 2.598, 'epoch': 10.0}
{'loss': 1.3201, 'learning_rate': 3.3333333333333333e-06, 'epoch': 10.0}
{'eval_loss': 1.3669568300247192, 'eval_accuracy@rus.rst.rrt': 0.5793345008756567, 'eval_f1@rus.rst.rrt': 0.4101349754608594, 'eval_precision@rus.rst.rrt': 0.49591980217810855, 'eval_recall@rus.rst.rrt': 0.3933361280992728, 'eval_loss@rus.rst.rrt': 1.3669568300247192, 'eval_runtime': 34.5885, 'eval_samples_per_second': 82.542, 'eval_steps_per_second': 2.602, 'epoch': 10.0}
{'train@rus.rst.rrt_loss': 1.2608492374420166, 'train@rus.rst.rrt_accuracy@rus.rst.rrt': 0.6077995975296648, 'train@rus.rst.rrt_f1@rus.rst.rrt': 0.3656303474141987, 'train@rus.rst.rrt_precision@rus.rst.rrt': 0.4461239987672428, 'train@rus.rst.rrt_recall@rus.rst.rrt': 0.3471951431954314, 'train@rus.rst.rrt_loss@rus.rst.rrt': 1.2608492374420166, 'train@rus.rst.rrt_runtime': 346.4983, 'train@rus.rst.rrt_samples_per_second': 83.181, 'train@rus.rst.rrt_steps_per_second': 2.6, 'epoch': 11.0}
{'loss': 1.3148, 'learning_rate': 1.6666666666666667e-06, 'epoch': 11.0}
{'eval_loss': 1.367308497428894, 'eval_accuracy@rus.rst.rrt': 0.5775831873905429, 'eval_f1@rus.rst.rrt': 0.4135455789203036, 'eval_precision@rus.rst.rrt': 0.493659249467929, 'eval_recall@rus.rst.rrt': 0.3994208901068689, 'eval_loss@rus.rst.rrt': 1.367308497428894, 'eval_runtime': 35.3802, 'eval_samples_per_second': 80.695, 'eval_steps_per_second': 2.544, 'epoch': 11.0}
{'train@rus.rst.rrt_loss': 1.2590819597244263, 'train@rus.rst.rrt_accuracy@rus.rst.rrt': 0.6090486433974047, 'train@rus.rst.rrt_f1@rus.rst.rrt': 0.36596831836432514, 'train@rus.rst.rrt_precision@rus.rst.rrt': 0.44938957448871997, 'train@rus.rst.rrt_recall@rus.rst.rrt': 0.34736726563781417, 'train@rus.rst.rrt_loss@rus.rst.rrt': 1.2590821981430054, 'train@rus.rst.rrt_runtime': 346.0276, 'train@rus.rst.rrt_samples_per_second': 83.294, 'train@rus.rst.rrt_steps_per_second': 2.604, 'epoch': 12.0}
{'loss': 1.3077, 'learning_rate': 0.0, 'epoch': 12.0}
{'eval_loss': 1.3653135299682617, 'eval_accuracy@rus.rst.rrt': 0.5772329246935202, 'eval_f1@rus.rst.rrt': 0.4097127474994517, 'eval_precision@rus.rst.rrt': 0.48787930140666197, 'eval_recall@rus.rst.rrt': 0.39661797329731774, 'eval_loss@rus.rst.rrt': 1.3653134107589722, 'eval_runtime': 34.6202, 'eval_samples_per_second': 82.466, 'eval_steps_per_second': 2.6, 'epoch': 12.0}
{'train_runtime': 13344.7192, 'train_samples_per_second': 25.918, 'train_steps_per_second': 0.81, 'train_loss': 1.459830907552806, 'epoch': 12.0}
***** train_log metrics *****
  epoch                    =       12.0
  train_loss               =     1.4598
  train_runtime            = 3:42:24.71
  train_samples_per_second =     25.918
  train_steps_per_second   =       0.81
{'train@eng.rst.rstdt_loss': 1.3857524394989014, 'train@eng.rst.rstdt_accuracy@eng.rst.rstdt': 0.6194225721784777, 'train@eng.rst.rstdt_f1@eng.rst.rstdt': 0.2447632389566587, 'train@eng.rst.rstdt_precision@eng.rst.rstdt': 0.3559814432986612, 'train@eng.rst.rstdt_recall@eng.rst.rstdt': 0.2522970404074599, 'train@eng.rst.rstdt_loss@eng.rst.rstdt': 1.3857524394989014, 'train@eng.rst.rstdt_runtime': 189.1726, 'train@eng.rst.rstdt_samples_per_second': 84.589, 'train@eng.rst.rstdt_steps_per_second': 2.648, 'epoch': 1.0}
{'loss': 1.8443, 'learning_rate': 1.8333333333333333e-05, 'epoch': 1.0}
{'eval_loss': 1.4155787229537964, 'eval_accuracy@eng.rst.rstdt': 0.6125848241826033, 'eval_f1@eng.rst.rstdt': 0.23304056651180258, 'eval_precision@eng.rst.rstdt': 0.29437680242707265, 'eval_recall@eng.rst.rstdt': 0.2448684168423356, 'eval_loss@eng.rst.rstdt': 1.415578842163086, 'eval_runtime': 19.455, 'eval_samples_per_second': 83.321, 'eval_steps_per_second': 2.621, 'epoch': 1.0}
{'train@eng.rst.rstdt_loss': 1.2481184005737305, 'train@eng.rst.rstdt_accuracy@eng.rst.rstdt': 0.6521684789401325, 'train@eng.rst.rstdt_f1@eng.rst.rstdt': 0.3536345468354855, 'train@eng.rst.rstdt_precision@eng.rst.rstdt': 0.4410331188659593, 'train@eng.rst.rstdt_recall@eng.rst.rstdt': 0.33346393615968334, 'train@eng.rst.rstdt_loss@eng.rst.rstdt': 1.2481184005737305, 'train@eng.rst.rstdt_runtime': 188.6319, 'train@eng.rst.rstdt_samples_per_second': 84.832, 'train@eng.rst.rstdt_steps_per_second': 2.656, 'epoch': 2.0}
{'loss': 1.3693, 'learning_rate': 1.6666666666666667e-05, 'epoch': 2.0}
{'eval_loss': 1.2889740467071533, 'eval_accuracy@eng.rst.rstdt': 0.6446637877853177, 'eval_f1@eng.rst.rstdt': 0.3330866406246071, 'eval_precision@eng.rst.rstdt': 0.4228089165636309, 'eval_recall@eng.rst.rstdt': 0.32087243283687383, 'eval_loss@eng.rst.rstdt': 1.2889739274978638, 'eval_runtime': 19.4284, 'eval_samples_per_second': 83.435, 'eval_steps_per_second': 2.625, 'epoch': 2.0}
{'train@eng.rst.rstdt_loss': 1.1825625896453857, 'train@eng.rst.rstdt_accuracy@eng.rst.rstdt': 0.6621672290963629, 'train@eng.rst.rstdt_f1@eng.rst.rstdt': 0.3733539872085818, 'train@eng.rst.rstdt_precision@eng.rst.rstdt': 0.5084642563222015, 'train@eng.rst.rstdt_recall@eng.rst.rstdt': 0.35433336502996327, 'train@eng.rst.rstdt_loss@eng.rst.rstdt': 1.1825624704360962, 'train@eng.rst.rstdt_runtime': 188.5602, 'train@eng.rst.rstdt_samples_per_second': 84.864, 'train@eng.rst.rstdt_steps_per_second': 2.657, 'epoch': 3.0}
{'loss': 1.2757, 'learning_rate': 1.5000000000000002e-05, 'epoch': 3.0}
{'eval_loss': 1.2369946241378784, 'eval_accuracy@eng.rst.rstdt': 0.6428130783466995, 'eval_f1@eng.rst.rstdt': 0.346449758939161, 'eval_precision@eng.rst.rstdt': 0.42653396146343225, 'eval_recall@eng.rst.rstdt': 0.33653683604795764, 'eval_loss@eng.rst.rstdt': 1.2369945049285889, 'eval_runtime': 19.3893, 'eval_samples_per_second': 83.603, 'eval_steps_per_second': 2.63, 'epoch': 3.0}
{'train@eng.rst.rstdt_loss': 1.146591305732727, 'train@eng.rst.rstdt_accuracy@eng.rst.rstdt': 0.6652293463317085, 'train@eng.rst.rstdt_f1@eng.rst.rstdt': 0.3934690489958498, 'train@eng.rst.rstdt_precision@eng.rst.rstdt': 0.5580566698228444, 'train@eng.rst.rstdt_recall@eng.rst.rstdt': 0.3684975425709685, 'train@eng.rst.rstdt_loss@eng.rst.rstdt': 1.1465915441513062, 'train@eng.rst.rstdt_runtime': 189.3436, 'train@eng.rst.rstdt_samples_per_second': 84.513, 'train@eng.rst.rstdt_steps_per_second': 2.646, 'epoch': 4.0}
{'loss': 1.2189, 'learning_rate': 1.3333333333333333e-05, 'epoch': 4.0}
{'eval_loss': 1.2176300287246704, 'eval_accuracy@eng.rst.rstdt': 0.6366440468846392, 'eval_f1@eng.rst.rstdt': 0.3589884908703772, 'eval_precision@eng.rst.rstdt': 0.4666917095967184, 'eval_recall@eng.rst.rstdt': 0.35426292127076087, 'eval_loss@eng.rst.rstdt': 1.2176300287246704, 'eval_runtime': 19.4741, 'eval_samples_per_second': 83.239, 'eval_steps_per_second': 2.619, 'epoch': 4.0}
{'train@eng.rst.rstdt_loss': 1.1170207262039185, 'train@eng.rst.rstdt_accuracy@eng.rst.rstdt': 0.672540932383452, 'train@eng.rst.rstdt_f1@eng.rst.rstdt': 0.4098900433255478, 'train@eng.rst.rstdt_precision@eng.rst.rstdt': 0.5706635245856897, 'train@eng.rst.rstdt_recall@eng.rst.rstdt': 0.3768020022631046, 'train@eng.rst.rstdt_loss@eng.rst.rstdt': 1.117020606994629, 'train@eng.rst.rstdt_runtime': 188.5024, 'train@eng.rst.rstdt_samples_per_second': 84.89, 'train@eng.rst.rstdt_steps_per_second': 2.658, 'epoch': 5.0}
{'loss': 1.1815, 'learning_rate': 1.1666666666666668e-05, 'epoch': 5.0}
{'eval_loss': 1.1922180652618408, 'eval_accuracy@eng.rst.rstdt': 0.6576187538556446, 'eval_f1@eng.rst.rstdt': 0.3715450136433227, 'eval_precision@eng.rst.rstdt': 0.446897674987673, 'eval_recall@eng.rst.rstdt': 0.36028292012035423, 'eval_loss@eng.rst.rstdt': 1.1922180652618408, 'eval_runtime': 19.4097, 'eval_samples_per_second': 83.515, 'eval_steps_per_second': 2.628, 'epoch': 5.0}
{'train@eng.rst.rstdt_loss': 1.096537709236145, 'train@eng.rst.rstdt_accuracy@eng.rst.rstdt': 0.6784151981002374, 'train@eng.rst.rstdt_f1@eng.rst.rstdt': 0.4248906273250836, 'train@eng.rst.rstdt_precision@eng.rst.rstdt': 0.5744318746372276, 'train@eng.rst.rstdt_recall@eng.rst.rstdt': 0.38634579242663136, 'train@eng.rst.rstdt_loss@eng.rst.rstdt': 1.0965375900268555, 'train@eng.rst.rstdt_runtime': 188.5183, 'train@eng.rst.rstdt_samples_per_second': 84.883, 'train@eng.rst.rstdt_steps_per_second': 2.658, 'epoch': 6.0}
{'loss': 1.158, 'learning_rate': 1e-05, 'epoch': 6.0}
{'eval_loss': 1.1738948822021484, 'eval_accuracy@eng.rst.rstdt': 0.6533004318322023, 'eval_f1@eng.rst.rstdt': 0.3789944538089966, 'eval_precision@eng.rst.rstdt': 0.5032491960588384, 'eval_recall@eng.rst.rstdt': 0.36284969378521276, 'eval_loss@eng.rst.rstdt': 1.1738948822021484, 'eval_runtime': 19.3665, 'eval_samples_per_second': 83.701, 'eval_steps_per_second': 2.633, 'epoch': 6.0}
{'train@eng.rst.rstdt_loss': 1.084446668624878, 'train@eng.rst.rstdt_accuracy@eng.rst.rstdt': 0.6809148856392951, 'train@eng.rst.rstdt_f1@eng.rst.rstdt': 0.4342571140249062, 'train@eng.rst.rstdt_precision@eng.rst.rstdt': 0.5737606126516512, 'train@eng.rst.rstdt_recall@eng.rst.rstdt': 0.3947681091353805, 'train@eng.rst.rstdt_loss@eng.rst.rstdt': 1.084446668624878, 'train@eng.rst.rstdt_runtime': 189.0677, 'train@eng.rst.rstdt_samples_per_second': 84.636, 'train@eng.rst.rstdt_steps_per_second': 2.65, 'epoch': 7.0}
{'loss': 1.1428, 'learning_rate': 8.333333333333334e-06, 'epoch': 7.0}
{'eval_loss': 1.1672453880310059, 'eval_accuracy@eng.rst.rstdt': 0.6533004318322023, 'eval_f1@eng.rst.rstdt': 0.38443479930402014, 'eval_precision@eng.rst.rstdt': 0.5002538126884317, 'eval_recall@eng.rst.rstdt': 0.3669768025412533, 'eval_loss@eng.rst.rstdt': 1.1672453880310059, 'eval_runtime': 19.4065, 'eval_samples_per_second': 83.529, 'eval_steps_per_second': 2.628, 'epoch': 7.0}
{'train@eng.rst.rstdt_loss': 1.072575330734253, 'train@eng.rst.rstdt_accuracy@eng.rst.rstdt': 0.6824146981627297, 'train@eng.rst.rstdt_f1@eng.rst.rstdt': 0.4436438550414688, 'train@eng.rst.rstdt_precision@eng.rst.rstdt': 0.6303198268002008, 'train@eng.rst.rstdt_recall@eng.rst.rstdt': 0.4041587591182071, 'train@eng.rst.rstdt_loss@eng.rst.rstdt': 1.0725752115249634, 'train@eng.rst.rstdt_runtime': 188.9664, 'train@eng.rst.rstdt_samples_per_second': 84.682, 'train@eng.rst.rstdt_steps_per_second': 2.651, 'epoch': 8.0}
{'loss': 1.1284, 'learning_rate': 6.666666666666667e-06, 'epoch': 8.0}
{'eval_loss': 1.1642227172851562, 'eval_accuracy@eng.rst.rstdt': 0.6526835286859963, 'eval_f1@eng.rst.rstdt': 0.3883151827276969, 'eval_precision@eng.rst.rstdt': 0.4884609976977434, 'eval_recall@eng.rst.rstdt': 0.37257875610845853, 'eval_loss@eng.rst.rstdt': 1.1642229557037354, 'eval_runtime': 19.4786, 'eval_samples_per_second': 83.22, 'eval_steps_per_second': 2.618, 'epoch': 8.0}
{'train@eng.rst.rstdt_loss': 1.0637949705123901, 'train@eng.rst.rstdt_accuracy@eng.rst.rstdt': 0.6852893388326459, 'train@eng.rst.rstdt_f1@eng.rst.rstdt': 0.45047900460177587, 'train@eng.rst.rstdt_precision@eng.rst.rstdt': 0.6341454304090709, 'train@eng.rst.rstdt_recall@eng.rst.rstdt': 0.407438472683344, 'train@eng.rst.rstdt_loss@eng.rst.rstdt': 1.0637949705123901, 'train@eng.rst.rstdt_runtime': 189.1447, 'train@eng.rst.rstdt_samples_per_second': 84.602, 'train@eng.rst.rstdt_steps_per_second': 2.649, 'epoch': 9.0}
{'loss': 1.1173, 'learning_rate': 5e-06, 'epoch': 9.0}
{'eval_loss': 1.158018946647644, 'eval_accuracy@eng.rst.rstdt': 0.6514497223935842, 'eval_f1@eng.rst.rstdt': 0.3867089506708436, 'eval_precision@eng.rst.rstdt': 0.4913906148104803, 'eval_recall@eng.rst.rstdt': 0.3701028912744436, 'eval_loss@eng.rst.rstdt': 1.158018946647644, 'eval_runtime': 19.4836, 'eval_samples_per_second': 83.198, 'eval_steps_per_second': 2.618, 'epoch': 9.0}
{'train@eng.rst.rstdt_loss': 1.061310887336731, 'train@eng.rst.rstdt_accuracy@eng.rst.rstdt': 0.6836645419322585, 'train@eng.rst.rstdt_f1@eng.rst.rstdt': 0.46208998257626055, 'train@eng.rst.rstdt_precision@eng.rst.rstdt': 0.6220133398917888, 'train@eng.rst.rstdt_recall@eng.rst.rstdt': 0.42108040932978996, 'train@eng.rst.rstdt_loss@eng.rst.rstdt': 1.061310887336731, 'train@eng.rst.rstdt_runtime': 188.69, 'train@eng.rst.rstdt_samples_per_second': 84.806, 'train@eng.rst.rstdt_steps_per_second': 2.655, 'epoch': 10.0}
{'loss': 1.1132, 'learning_rate': 3.3333333333333333e-06, 'epoch': 10.0}
{'eval_loss': 1.1644797325134277, 'eval_accuracy@eng.rst.rstdt': 0.64898210980876, 'eval_f1@eng.rst.rstdt': 0.39825412584113673, 'eval_precision@eng.rst.rstdt': 0.5306892353689985, 'eval_recall@eng.rst.rstdt': 0.3833625625428183, 'eval_loss@eng.rst.rstdt': 1.1644797325134277, 'eval_runtime': 19.4224, 'eval_samples_per_second': 83.46, 'eval_steps_per_second': 2.626, 'epoch': 10.0}
{'train@eng.rst.rstdt_loss': 1.055137038230896, 'train@eng.rst.rstdt_accuracy@eng.rst.rstdt': 0.6858517685289339, 'train@eng.rst.rstdt_f1@eng.rst.rstdt': 0.4573495143905053, 'train@eng.rst.rstdt_precision@eng.rst.rstdt': 0.6306279329451372, 'train@eng.rst.rstdt_recall@eng.rst.rstdt': 0.4147637217794409, 'train@eng.rst.rstdt_loss@eng.rst.rstdt': 1.055137038230896, 'train@eng.rst.rstdt_runtime': 188.8698, 'train@eng.rst.rstdt_samples_per_second': 84.725, 'train@eng.rst.rstdt_steps_per_second': 2.653, 'epoch': 11.0}
{'loss': 1.1038, 'learning_rate': 1.6666666666666667e-06, 'epoch': 11.0}
{'eval_loss': 1.1553038358688354, 'eval_accuracy@eng.rst.rstdt': 0.6545342381246144, 'eval_f1@eng.rst.rstdt': 0.4005341517548728, 'eval_precision@eng.rst.rstdt': 0.5473196611515019, 'eval_recall@eng.rst.rstdt': 0.3807521181339828, 'eval_loss@eng.rst.rstdt': 1.1553038358688354, 'eval_runtime': 19.4392, 'eval_samples_per_second': 83.388, 'eval_steps_per_second': 2.624, 'epoch': 11.0}
{'train@eng.rst.rstdt_loss': 1.054473876953125, 'train@eng.rst.rstdt_accuracy@eng.rst.rstdt': 0.6861017372828396, 'train@eng.rst.rstdt_f1@eng.rst.rstdt': 0.4589821420749076, 'train@eng.rst.rstdt_precision@eng.rst.rstdt': 0.6299857208729527, 'train@eng.rst.rstdt_recall@eng.rst.rstdt': 0.41563324094831733, 'train@eng.rst.rstdt_loss@eng.rst.rstdt': 1.054473876953125, 'train@eng.rst.rstdt_runtime': 188.6901, 'train@eng.rst.rstdt_samples_per_second': 84.806, 'train@eng.rst.rstdt_steps_per_second': 2.655, 'epoch': 12.0}
{'loss': 1.1036, 'learning_rate': 0.0, 'epoch': 12.0}
{'eval_loss': 1.1552010774612427, 'eval_accuracy@eng.rst.rstdt': 0.6551511412708205, 'eval_f1@eng.rst.rstdt': 0.40067826003680007, 'eval_precision@eng.rst.rstdt': 0.5474952510092197, 'eval_recall@eng.rst.rstdt': 0.3808424768581176, 'eval_loss@eng.rst.rstdt': 1.1552010774612427, 'eval_runtime': 19.4168, 'eval_samples_per_second': 83.485, 'eval_steps_per_second': 2.627, 'epoch': 12.0}
{'train_runtime': 7284.6555, 'train_samples_per_second': 26.36, 'train_steps_per_second': 0.825, 'train_loss': 1.229744317923398, 'epoch': 12.0}
***** train_logger metrics *****
  epoch                    =       12.0
  train_loss               =     1.4598
  train_runtime            = 3:42:24.71
  train_samples_per_second =     25.918
  train_steps_per_second   =       0.81
-------------------------------------------------------------------
Lang1:  spa.rst.rststb    Lang2:  eng.rst.rstdt
Saving run to:  runs/full_shot/FullShot=v4_spa.rst.rststb_eng.rst.rstdt_lrfunc=Adafactor_lr=2e-05_plm=bert-base-multilingual-cased
Running with params: BERT_MODEL=bert-base-multilingual-cased lr=2e-05
read 2240 examples
read 383 examples
read 426 examples
read 16002 examples
read 1621 examples
read 2155 examples
Total prediction labels:  37
BertAdapterModel(
  (shared_parameters): ModuleDict()
  (bert): BertModel(
    (shared_parameters): ModuleDict()
    (invertible_adapters): ModuleDict(
      (es): NICECouplingBlock(
        (F): Sequential(
          (0): Linear(in_features=384, out_features=192, bias=True)
          (1): Activation_Function_Class(
            (f): NewGELUActivation()
          )
          (2): Linear(in_features=192, out_features=384, bias=True)
        )
        (G): Sequential(
          (0): Linear(in_features=384, out_features=192, bias=True)
          (1): Activation_Function_Class(
            (f): NewGELUActivation()
          )
          (2): Linear(in_features=192, out_features=384, bias=True)
        )
      )
      (en): NICECouplingBlock(
        (F): Sequential(
          (0): Linear(in_features=384, out_features=192, bias=True)
          (1): Activation_Function_Class(
            (f): ReLU()
          )
          (2): Linear(in_features=192, out_features=384, bias=True)
        )
        (G): Sequential(
          (0): Linear(in_features=384, out_features=192, bias=True)
          (1): Activation_Function_Class(
            (f): ReLU()
          )
          (2): Linear(in_features=192, out_features=384, bias=True)
        )
      )
    )
    (embeddings): BertEmbeddings(
      (word_embeddings): Embedding(119547, 768, padding_idx=0)
      (position_embeddings): Embedding(512, 768)
      (token_type_embeddings): Embedding(2, 768)
      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
      (dropout): Dropout(p=0.1, inplace=False)
    )
    (encoder): BertEncoder(
      (layer): ModuleList(
        (0): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (es): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (en): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (1): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (es): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (en): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (2): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (es): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (en): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (3): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (es): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (en): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (4): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (es): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (en): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (5): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (es): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (en): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (6): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (es): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (en): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (7): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (es): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (en): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (8): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (es): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (en): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (9): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (es): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (en): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (10): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (es): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (en): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (11): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (es): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (en): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
      )
    )
    (pooler): BertPooler(
      (dense): Linear(in_features=768, out_features=768, bias=True)
      (activation): Tanh()
    )
    (prefix_tuning): PrefixTuningPool(
      (prefix_tunings): ModuleDict()
    )
  )
  (heads): ModuleDict(
    (disrpt): ClassificationHead(
      (0): Dropout(p=0.1, inplace=False)
      (1): Linear(in_features=768, out_features=768, bias=True)
      (2): Activation_Function_Class(
        (f): Tanh()
      )
      (3): Dropout(p=0.1, inplace=False)
      (4): Linear(in_features=768, out_features=37, bias=True)
    )
  )
)
{'train@spa.rst.rststb_loss': 2.825916290283203, 'train@spa.rst.rststb_accuracy@spa.rst.rststb': 0.23883928571428573, 'train@spa.rst.rststb_f1@spa.rst.rststb': 0.029022983800695467, 'train@spa.rst.rststb_precision@spa.rst.rststb': 0.04291797896678743, 'train@spa.rst.rststb_recall@spa.rst.rststb': 0.046110087307016996, 'train@spa.rst.rststb_loss@spa.rst.rststb': 2.8259165287017822, 'train@spa.rst.rststb_runtime': 27.0589, 'train@spa.rst.rststb_samples_per_second': 82.782, 'train@spa.rst.rststb_steps_per_second': 2.587, 'epoch': 1.0}
{'loss': 3.2257, 'learning_rate': 1.8333333333333333e-05, 'epoch': 1.0}
{'eval_loss': 2.8988661766052246, 'eval_accuracy@spa.rst.rststb': 0.21148825065274152, 'eval_f1@spa.rst.rststb': 0.018915486077186128, 'eval_precision@spa.rst.rststb': 0.02387740555951532, 'eval_recall@spa.rst.rststb': 0.045454545454545456, 'eval_loss@spa.rst.rststb': 2.8988659381866455, 'eval_runtime': 4.9268, 'eval_samples_per_second': 77.739, 'eval_steps_per_second': 2.436, 'epoch': 1.0}
{'train@spa.rst.rststb_loss': 2.565695285797119, 'train@spa.rst.rststb_accuracy@spa.rst.rststb': 0.284375, 'train@spa.rst.rststb_f1@spa.rst.rststb': 0.04232834346278005, 'train@spa.rst.rststb_precision@spa.rst.rststb': 0.04567386344789448, 'train@spa.rst.rststb_recall@spa.rst.rststb': 0.05813934653013449, 'train@spa.rst.rststb_loss@spa.rst.rststb': 2.565695285797119, 'train@spa.rst.rststb_runtime': 27.1113, 'train@spa.rst.rststb_samples_per_second': 82.622, 'train@spa.rst.rststb_steps_per_second': 2.582, 'epoch': 2.0}
{'loss': 2.7096, 'learning_rate': 1.6666666666666667e-05, 'epoch': 2.0}
{'eval_loss': 2.70033597946167, 'eval_accuracy@spa.rst.rststb': 0.2349869451697128, 'eval_f1@spa.rst.rststb': 0.032356670514006945, 'eval_precision@spa.rst.rststb': 0.04321496084256009, 'eval_recall@spa.rst.rststb': 0.05376797156957992, 'eval_loss@spa.rst.rststb': 2.7003355026245117, 'eval_runtime': 4.9192, 'eval_samples_per_second': 77.858, 'eval_steps_per_second': 2.439, 'epoch': 2.0}
{'train@spa.rst.rststb_loss': 2.440337657928467, 'train@spa.rst.rststb_accuracy@spa.rst.rststb': 0.31160714285714286, 'train@spa.rst.rststb_f1@spa.rst.rststb': 0.04845085622194468, 'train@spa.rst.rststb_precision@spa.rst.rststb': 0.0800160730463165, 'train@spa.rst.rststb_recall@spa.rst.rststb': 0.06837449109363801, 'train@spa.rst.rststb_loss@spa.rst.rststb': 2.440337657928467, 'train@spa.rst.rststb_runtime': 27.1447, 'train@spa.rst.rststb_samples_per_second': 82.521, 'train@spa.rst.rststb_steps_per_second': 2.579, 'epoch': 3.0}
{'loss': 2.5396, 'learning_rate': 1.5000000000000002e-05, 'epoch': 3.0}
{'eval_loss': 2.6032731533050537, 'eval_accuracy@spa.rst.rststb': 0.27154046997389036, 'eval_f1@spa.rst.rststb': 0.04669608336380537, 'eval_precision@spa.rst.rststb': 0.03900096413473931, 'eval_recall@spa.rst.rststb': 0.06926610505783984, 'eval_loss@spa.rst.rststb': 2.6032726764678955, 'eval_runtime': 4.9528, 'eval_samples_per_second': 77.33, 'eval_steps_per_second': 2.423, 'epoch': 3.0}
{'train@spa.rst.rststb_loss': 2.343663215637207, 'train@spa.rst.rststb_accuracy@spa.rst.rststb': 0.35267857142857145, 'train@spa.rst.rststb_f1@spa.rst.rststb': 0.07770685068142982, 'train@spa.rst.rststb_precision@spa.rst.rststb': 0.08467394196771019, 'train@spa.rst.rststb_recall@spa.rst.rststb': 0.09093768447438269, 'train@spa.rst.rststb_loss@spa.rst.rststb': 2.343663454055786, 'train@spa.rst.rststb_runtime': 27.1199, 'train@spa.rst.rststb_samples_per_second': 82.596, 'train@spa.rst.rststb_steps_per_second': 2.581, 'epoch': 4.0}
{'loss': 2.4376, 'learning_rate': 1.3333333333333333e-05, 'epoch': 4.0}
{'eval_loss': 2.532963991165161, 'eval_accuracy@spa.rst.rststb': 0.29765013054830286, 'eval_f1@spa.rst.rststb': 0.08191796982443184, 'eval_precision@spa.rst.rststb': 0.09817112044968843, 'eval_recall@spa.rst.rststb': 0.09435177374888559, 'eval_loss@spa.rst.rststb': 2.5329642295837402, 'eval_runtime': 4.9151, 'eval_samples_per_second': 77.923, 'eval_steps_per_second': 2.441, 'epoch': 4.0}
{'train@spa.rst.rststb_loss': 2.259807586669922, 'train@spa.rst.rststb_accuracy@spa.rst.rststb': 0.3924107142857143, 'train@spa.rst.rststb_f1@spa.rst.rststb': 0.09387285518613067, 'train@spa.rst.rststb_precision@spa.rst.rststb': 0.08474300661022749, 'train@spa.rst.rststb_recall@spa.rst.rststb': 0.11576063394204338, 'train@spa.rst.rststb_loss@spa.rst.rststb': 2.2598073482513428, 'train@spa.rst.rststb_runtime': 27.0906, 'train@spa.rst.rststb_samples_per_second': 82.686, 'train@spa.rst.rststb_steps_per_second': 2.584, 'epoch': 5.0}
{'loss': 2.3513, 'learning_rate': 1.1666666666666668e-05, 'epoch': 5.0}
{'eval_loss': 2.4719934463500977, 'eval_accuracy@spa.rst.rststb': 0.3394255874673629, 'eval_f1@spa.rst.rststb': 0.0926852869228442, 'eval_precision@spa.rst.rststb': 0.0858831248020806, 'eval_recall@spa.rst.rststb': 0.11500328358422315, 'eval_loss@spa.rst.rststb': 2.4719934463500977, 'eval_runtime': 4.9167, 'eval_samples_per_second': 77.898, 'eval_steps_per_second': 2.441, 'epoch': 5.0}
{'train@spa.rst.rststb_loss': 2.194824695587158, 'train@spa.rst.rststb_accuracy@spa.rst.rststb': 0.3964285714285714, 'train@spa.rst.rststb_f1@spa.rst.rststb': 0.09425633198445268, 'train@spa.rst.rststb_precision@spa.rst.rststb': 0.08488395096401571, 'train@spa.rst.rststb_recall@spa.rst.rststb': 0.11978291730373726, 'train@spa.rst.rststb_loss@spa.rst.rststb': 2.1948249340057373, 'train@spa.rst.rststb_runtime': 27.1337, 'train@spa.rst.rststb_samples_per_second': 82.554, 'train@spa.rst.rststb_steps_per_second': 2.58, 'epoch': 6.0}
{'loss': 2.2726, 'learning_rate': 1e-05, 'epoch': 6.0}
{'eval_loss': 2.4231932163238525, 'eval_accuracy@spa.rst.rststb': 0.3629242819843342, 'eval_f1@spa.rst.rststb': 0.10356847830532043, 'eval_precision@spa.rst.rststb': 0.09595815718494252, 'eval_recall@spa.rst.rststb': 0.1306301121315209, 'eval_loss@spa.rst.rststb': 2.4231929779052734, 'eval_runtime': 4.9105, 'eval_samples_per_second': 77.996, 'eval_steps_per_second': 2.444, 'epoch': 6.0}
{'train@spa.rst.rststb_loss': 2.1406280994415283, 'train@spa.rst.rststb_accuracy@spa.rst.rststb': 0.4089285714285714, 'train@spa.rst.rststb_f1@spa.rst.rststb': 0.09964235332172092, 'train@spa.rst.rststb_precision@spa.rst.rststb': 0.10401938255051875, 'train@spa.rst.rststb_recall@spa.rst.rststb': 0.12674757166662795, 'train@spa.rst.rststb_loss@spa.rst.rststb': 2.140627861022949, 'train@spa.rst.rststb_runtime': 27.195, 'train@spa.rst.rststb_samples_per_second': 82.368, 'train@spa.rst.rststb_steps_per_second': 2.574, 'epoch': 7.0}
{'loss': 2.224, 'learning_rate': 8.333333333333334e-06, 'epoch': 7.0}
{'eval_loss': 2.3856382369995117, 'eval_accuracy@spa.rst.rststb': 0.36553524804177545, 'eval_f1@spa.rst.rststb': 0.10265899929474914, 'eval_precision@spa.rst.rststb': 0.09304567277463946, 'eval_recall@spa.rst.rststb': 0.13308677292692278, 'eval_loss@spa.rst.rststb': 2.3856379985809326, 'eval_runtime': 4.9593, 'eval_samples_per_second': 77.229, 'eval_steps_per_second': 2.42, 'epoch': 7.0}
{'train@spa.rst.rststb_loss': 2.100802183151245, 'train@spa.rst.rststb_accuracy@spa.rst.rststb': 0.41517857142857145, 'train@spa.rst.rststb_f1@spa.rst.rststb': 0.10151358313323706, 'train@spa.rst.rststb_precision@spa.rst.rststb': 0.1023428864490323, 'train@spa.rst.rststb_recall@spa.rst.rststb': 0.12925619006678937, 'train@spa.rst.rststb_loss@spa.rst.rststb': 2.100802183151245, 'train@spa.rst.rststb_runtime': 27.1563, 'train@spa.rst.rststb_samples_per_second': 82.485, 'train@spa.rst.rststb_steps_per_second': 2.578, 'epoch': 8.0}
{'loss': 2.175, 'learning_rate': 6.666666666666667e-06, 'epoch': 8.0}
{'eval_loss': 2.3559298515319824, 'eval_accuracy@spa.rst.rststb': 0.370757180156658, 'eval_f1@spa.rst.rststb': 0.10626884398986848, 'eval_precision@spa.rst.rststb': 0.09539741212446526, 'eval_recall@spa.rst.rststb': 0.13950969782810854, 'eval_loss@spa.rst.rststb': 2.3559298515319824, 'eval_runtime': 4.9585, 'eval_samples_per_second': 77.241, 'eval_steps_per_second': 2.42, 'epoch': 8.0}
{'train@spa.rst.rststb_loss': 2.0708045959472656, 'train@spa.rst.rststb_accuracy@spa.rst.rststb': 0.4205357142857143, 'train@spa.rst.rststb_f1@spa.rst.rststb': 0.10265984315805388, 'train@spa.rst.rststb_precision@spa.rst.rststb': 0.1360407409992414, 'train@spa.rst.rststb_recall@spa.rst.rststb': 0.13293113433352838, 'train@spa.rst.rststb_loss@spa.rst.rststb': 2.0708048343658447, 'train@spa.rst.rststb_runtime': 27.1696, 'train@spa.rst.rststb_samples_per_second': 82.445, 'train@spa.rst.rststb_steps_per_second': 2.576, 'epoch': 9.0}
{'loss': 2.1435, 'learning_rate': 5e-06, 'epoch': 9.0}
{'eval_loss': 2.3302993774414062, 'eval_accuracy@spa.rst.rststb': 0.3733681462140992, 'eval_f1@spa.rst.rststb': 0.10432765201159594, 'eval_precision@spa.rst.rststb': 0.09053140839569608, 'eval_recall@spa.rst.rststb': 0.14135820746171576, 'eval_loss@spa.rst.rststb': 2.330298900604248, 'eval_runtime': 4.9398, 'eval_samples_per_second': 77.534, 'eval_steps_per_second': 2.429, 'epoch': 9.0}
{'train@spa.rst.rststb_loss': 2.0499846935272217, 'train@spa.rst.rststb_accuracy@spa.rst.rststb': 0.4276785714285714, 'train@spa.rst.rststb_f1@spa.rst.rststb': 0.10683747223770138, 'train@spa.rst.rststb_precision@spa.rst.rststb': 0.1411821304273689, 'train@spa.rst.rststb_recall@spa.rst.rststb': 0.13550055995388036, 'train@spa.rst.rststb_loss@spa.rst.rststb': 2.049984931945801, 'train@spa.rst.rststb_runtime': 27.189, 'train@spa.rst.rststb_samples_per_second': 82.386, 'train@spa.rst.rststb_steps_per_second': 2.575, 'epoch': 10.0}
{'loss': 2.1142, 'learning_rate': 3.3333333333333333e-06, 'epoch': 10.0}
{'eval_loss': 2.316941976547241, 'eval_accuracy@spa.rst.rststb': 0.381201044386423, 'eval_f1@spa.rst.rststb': 0.10751474260124312, 'eval_precision@spa.rst.rststb': 0.09495077917539618, 'eval_recall@spa.rst.rststb': 0.143107852193478, 'eval_loss@spa.rst.rststb': 2.316941976547241, 'eval_runtime': 4.9172, 'eval_samples_per_second': 77.889, 'eval_steps_per_second': 2.44, 'epoch': 10.0}
{'train@spa.rst.rststb_loss': 2.0378174781799316, 'train@spa.rst.rststb_accuracy@spa.rst.rststb': 0.4299107142857143, 'train@spa.rst.rststb_f1@spa.rst.rststb': 0.10766090386003248, 'train@spa.rst.rststb_precision@spa.rst.rststb': 0.12434491412262463, 'train@spa.rst.rststb_recall@spa.rst.rststb': 0.13621078341587728, 'train@spa.rst.rststb_loss@spa.rst.rststb': 2.0378172397613525, 'train@spa.rst.rststb_runtime': 27.1758, 'train@spa.rst.rststb_samples_per_second': 82.426, 'train@spa.rst.rststb_steps_per_second': 2.576, 'epoch': 11.0}
{'loss': 2.0964, 'learning_rate': 1.6666666666666667e-06, 'epoch': 11.0}
{'eval_loss': 2.306875467300415, 'eval_accuracy@spa.rst.rststb': 0.38642297650130547, 'eval_f1@spa.rst.rststb': 0.1097925621334146, 'eval_precision@spa.rst.rststb': 0.09800413742896588, 'eval_recall@spa.rst.rststb': 0.14440571072689784, 'eval_loss@spa.rst.rststb': 2.306875467300415, 'eval_runtime': 4.9298, 'eval_samples_per_second': 77.69, 'eval_steps_per_second': 2.434, 'epoch': 11.0}
{'train@spa.rst.rststb_loss': 2.0338990688323975, 'train@spa.rst.rststb_accuracy@spa.rst.rststb': 0.4299107142857143, 'train@spa.rst.rststb_f1@spa.rst.rststb': 0.10782982782778607, 'train@spa.rst.rststb_precision@spa.rst.rststb': 0.12439446664699554, 'train@spa.rst.rststb_recall@spa.rst.rststb': 0.1362840598272447, 'train@spa.rst.rststb_loss@spa.rst.rststb': 2.0338990688323975, 'train@spa.rst.rststb_runtime': 27.1984, 'train@spa.rst.rststb_samples_per_second': 82.358, 'train@spa.rst.rststb_steps_per_second': 2.574, 'epoch': 12.0}
{'loss': 2.0782, 'learning_rate': 0.0, 'epoch': 12.0}
{'eval_loss': 2.304079055786133, 'eval_accuracy@spa.rst.rststb': 0.38642297650130547, 'eval_f1@spa.rst.rststb': 0.11012088224364439, 'eval_precision@spa.rst.rststb': 0.09869030401522845, 'eval_recall@spa.rst.rststb': 0.14440571072689784, 'eval_loss@spa.rst.rststb': 2.3040788173675537, 'eval_runtime': 4.9398, 'eval_samples_per_second': 77.533, 'eval_steps_per_second': 2.429, 'epoch': 12.0}
{'train_runtime': 1066.2292, 'train_samples_per_second': 25.21, 'train_steps_per_second': 0.788, 'train_loss': 2.3639615013485864, 'epoch': 12.0}
***** train_log metrics *****
  epoch                    =       12.0
  train_loss               =      2.364
  train_runtime            = 0:17:46.22
  train_samples_per_second =      25.21
  train_steps_per_second   =      0.788
{'train@eng.rst.rstdt_loss': 1.602033257484436, 'train@eng.rst.rstdt_accuracy@eng.rst.rstdt': 0.5545556805399325, 'train@eng.rst.rstdt_f1@eng.rst.rstdt': 0.12065775187366436, 'train@eng.rst.rstdt_precision@eng.rst.rstdt': 0.1792443170212991, 'train@eng.rst.rstdt_recall@eng.rst.rstdt': 0.13903265121132538, 'train@eng.rst.rstdt_loss@eng.rst.rstdt': 1.602033257484436, 'train@eng.rst.rstdt_runtime': 189.1055, 'train@eng.rst.rstdt_samples_per_second': 84.619, 'train@eng.rst.rstdt_steps_per_second': 2.649, 'epoch': 1.0}
{'loss': 2.0838, 'learning_rate': 1.8333333333333333e-05, 'epoch': 1.0}
{'eval_loss': 1.5904979705810547, 'eval_accuracy@eng.rst.rstdt': 0.5595311536088834, 'eval_f1@eng.rst.rstdt': 0.11936425470673223, 'eval_precision@eng.rst.rstdt': 0.17817233355097026, 'eval_recall@eng.rst.rstdt': 0.13708992446135643, 'eval_loss@eng.rst.rstdt': 1.5904982089996338, 'eval_runtime': 19.4852, 'eval_samples_per_second': 83.192, 'eval_steps_per_second': 2.617, 'epoch': 1.0}
{'train@eng.rst.rstdt_loss': 1.3763231039047241, 'train@eng.rst.rstdt_accuracy@eng.rst.rstdt': 0.6161104861892264, 'train@eng.rst.rstdt_f1@eng.rst.rstdt': 0.22657823801344798, 'train@eng.rst.rstdt_precision@eng.rst.rstdt': 0.2711782718451248, 'train@eng.rst.rstdt_recall@eng.rst.rstdt': 0.23992224473632873, 'train@eng.rst.rstdt_loss@eng.rst.rstdt': 1.3763233423233032, 'train@eng.rst.rstdt_runtime': 188.6309, 'train@eng.rst.rstdt_samples_per_second': 84.832, 'train@eng.rst.rstdt_steps_per_second': 2.656, 'epoch': 2.0}
{'loss': 1.5214, 'learning_rate': 1.6666666666666667e-05, 'epoch': 2.0}
{'eval_loss': 1.3874799013137817, 'eval_accuracy@eng.rst.rstdt': 0.6187538556446638, 'eval_f1@eng.rst.rstdt': 0.2195746281778, 'eval_precision@eng.rst.rstdt': 0.2559273432584761, 'eval_recall@eng.rst.rstdt': 0.23388520135073343, 'eval_loss@eng.rst.rstdt': 1.3874799013137817, 'eval_runtime': 19.4249, 'eval_samples_per_second': 83.45, 'eval_steps_per_second': 2.625, 'epoch': 2.0}
{'train@eng.rst.rstdt_loss': 1.2857279777526855, 'train@eng.rst.rstdt_accuracy@eng.rst.rstdt': 0.6377327834020747, 'train@eng.rst.rstdt_f1@eng.rst.rstdt': 0.3000516107062144, 'train@eng.rst.rstdt_precision@eng.rst.rstdt': 0.44260894786971544, 'train@eng.rst.rstdt_recall@eng.rst.rstdt': 0.28561227904856457, 'train@eng.rst.rstdt_loss@eng.rst.rstdt': 1.285728096961975, 'train@eng.rst.rstdt_runtime': 188.9869, 'train@eng.rst.rstdt_samples_per_second': 84.673, 'train@eng.rst.rstdt_steps_per_second': 2.651, 'epoch': 3.0}
{'loss': 1.3895, 'learning_rate': 1.5000000000000002e-05, 'epoch': 3.0}
{'eval_loss': 1.3144320249557495, 'eval_accuracy@eng.rst.rstdt': 0.634793337446021, 'eval_f1@eng.rst.rstdt': 0.28144441104116874, 'eval_precision@eng.rst.rstdt': 0.3360670914176797, 'eval_recall@eng.rst.rstdt': 0.2765583982416287, 'eval_loss@eng.rst.rstdt': 1.3144320249557495, 'eval_runtime': 19.5221, 'eval_samples_per_second': 83.034, 'eval_steps_per_second': 2.612, 'epoch': 3.0}
{'train@eng.rst.rstdt_loss': 1.2281076908111572, 'train@eng.rst.rstdt_accuracy@eng.rst.rstdt': 0.6508561429821272, 'train@eng.rst.rstdt_f1@eng.rst.rstdt': 0.34175583587428277, 'train@eng.rst.rstdt_precision@eng.rst.rstdt': 0.43521233923766633, 'train@eng.rst.rstdt_recall@eng.rst.rstdt': 0.32450954023869694, 'train@eng.rst.rstdt_loss@eng.rst.rstdt': 1.2281075716018677, 'train@eng.rst.rstdt_runtime': 188.6286, 'train@eng.rst.rstdt_samples_per_second': 84.833, 'train@eng.rst.rstdt_steps_per_second': 2.656, 'epoch': 4.0}
{'loss': 1.3051, 'learning_rate': 1.3333333333333333e-05, 'epoch': 4.0}
{'eval_loss': 1.282382607460022, 'eval_accuracy@eng.rst.rstdt': 0.6384947563232573, 'eval_f1@eng.rst.rstdt': 0.32643832611257284, 'eval_precision@eng.rst.rstdt': 0.40321862325713687, 'eval_recall@eng.rst.rstdt': 0.3202501820278856, 'eval_loss@eng.rst.rstdt': 1.282382607460022, 'eval_runtime': 19.4615, 'eval_samples_per_second': 83.293, 'eval_steps_per_second': 2.621, 'epoch': 4.0}
{'train@eng.rst.rstdt_loss': 1.1843022108078003, 'train@eng.rst.rstdt_accuracy@eng.rst.rstdt': 0.6568553930758655, 'train@eng.rst.rstdt_f1@eng.rst.rstdt': 0.3538594911504371, 'train@eng.rst.rstdt_precision@eng.rst.rstdt': 0.5317899008508219, 'train@eng.rst.rstdt_recall@eng.rst.rstdt': 0.33205973025873486, 'train@eng.rst.rstdt_loss@eng.rst.rstdt': 1.1843022108078003, 'train@eng.rst.rstdt_runtime': 189.1862, 'train@eng.rst.rstdt_samples_per_second': 84.583, 'train@eng.rst.rstdt_steps_per_second': 2.648, 'epoch': 5.0}
{'loss': 1.2583, 'learning_rate': 1.1666666666666668e-05, 'epoch': 5.0}
{'eval_loss': 1.2478253841400146, 'eval_accuracy@eng.rst.rstdt': 0.6434299814929056, 'eval_f1@eng.rst.rstdt': 0.32673932436881953, 'eval_precision@eng.rst.rstdt': 0.40283864117588153, 'eval_recall@eng.rst.rstdt': 0.32315699776874435, 'eval_loss@eng.rst.rstdt': 1.2478253841400146, 'eval_runtime': 19.4918, 'eval_samples_per_second': 83.163, 'eval_steps_per_second': 2.616, 'epoch': 5.0}
{'train@eng.rst.rstdt_loss': 1.153483271598816, 'train@eng.rst.rstdt_accuracy@eng.rst.rstdt': 0.6624796900387452, 'train@eng.rst.rstdt_f1@eng.rst.rstdt': 0.3676223170390397, 'train@eng.rst.rstdt_precision@eng.rst.rstdt': 0.5887769484235392, 'train@eng.rst.rstdt_recall@eng.rst.rstdt': 0.34196290994472833, 'train@eng.rst.rstdt_loss@eng.rst.rstdt': 1.1534831523895264, 'train@eng.rst.rstdt_runtime': 188.4744, 'train@eng.rst.rstdt_samples_per_second': 84.903, 'train@eng.rst.rstdt_steps_per_second': 2.658, 'epoch': 6.0}
{'loss': 1.2197, 'learning_rate': 1e-05, 'epoch': 6.0}
{'eval_loss': 1.2251256704330444, 'eval_accuracy@eng.rst.rstdt': 0.6520666255397902, 'eval_f1@eng.rst.rstdt': 0.3367848796676787, 'eval_precision@eng.rst.rstdt': 0.41717351219958687, 'eval_recall@eng.rst.rstdt': 0.3306814086299517, 'eval_loss@eng.rst.rstdt': 1.225125789642334, 'eval_runtime': 19.4416, 'eval_samples_per_second': 83.378, 'eval_steps_per_second': 2.623, 'epoch': 6.0}
{'train@eng.rst.rstdt_loss': 1.1353296041488647, 'train@eng.rst.rstdt_accuracy@eng.rst.rstdt': 0.6666666666666666, 'train@eng.rst.rstdt_f1@eng.rst.rstdt': 0.38536986780111854, 'train@eng.rst.rstdt_precision@eng.rst.rstdt': 0.6103109231059602, 'train@eng.rst.rstdt_recall@eng.rst.rstdt': 0.3524691112428089, 'train@eng.rst.rstdt_loss@eng.rst.rstdt': 1.1353294849395752, 'train@eng.rst.rstdt_runtime': 189.016, 'train@eng.rst.rstdt_samples_per_second': 84.659, 'train@eng.rst.rstdt_steps_per_second': 2.651, 'epoch': 7.0}
{'loss': 1.1979, 'learning_rate': 8.333333333333334e-06, 'epoch': 7.0}
{'eval_loss': 1.2114101648330688, 'eval_accuracy@eng.rst.rstdt': 0.6539173349784084, 'eval_f1@eng.rst.rstdt': 0.35331753237561814, 'eval_precision@eng.rst.rstdt': 0.5296548845181888, 'eval_recall@eng.rst.rstdt': 0.3402798382355491, 'eval_loss@eng.rst.rstdt': 1.2114101648330688, 'eval_runtime': 19.5041, 'eval_samples_per_second': 83.111, 'eval_steps_per_second': 2.615, 'epoch': 7.0}
{'train@eng.rst.rstdt_loss': 1.1189934015274048, 'train@eng.rst.rstdt_accuracy@eng.rst.rstdt': 0.6690413698287714, 'train@eng.rst.rstdt_f1@eng.rst.rstdt': 0.4019972774748263, 'train@eng.rst.rstdt_precision@eng.rst.rstdt': 0.6439528509716461, 'train@eng.rst.rstdt_recall@eng.rst.rstdt': 0.3662985615626966, 'train@eng.rst.rstdt_loss@eng.rst.rstdt': 1.1189935207366943, 'train@eng.rst.rstdt_runtime': 188.9854, 'train@eng.rst.rstdt_samples_per_second': 84.673, 'train@eng.rst.rstdt_steps_per_second': 2.651, 'epoch': 8.0}
{'loss': 1.1781, 'learning_rate': 6.666666666666667e-06, 'epoch': 8.0}
{'eval_loss': 1.206492304801941, 'eval_accuracy@eng.rst.rstdt': 0.6551511412708205, 'eval_f1@eng.rst.rstdt': 0.36232736158567236, 'eval_precision@eng.rst.rstdt': 0.5321016819259909, 'eval_recall@eng.rst.rstdt': 0.34844278403102713, 'eval_loss@eng.rst.rstdt': 1.2064924240112305, 'eval_runtime': 19.495, 'eval_samples_per_second': 83.15, 'eval_steps_per_second': 2.616, 'epoch': 8.0}
{'train@eng.rst.rstdt_loss': 1.1075619459152222, 'train@eng.rst.rstdt_accuracy@eng.rst.rstdt': 0.6719160104986877, 'train@eng.rst.rstdt_f1@eng.rst.rstdt': 0.41159262246499284, 'train@eng.rst.rstdt_precision@eng.rst.rstdt': 0.6434879727846322, 'train@eng.rst.rstdt_recall@eng.rst.rstdt': 0.37118161391895077, 'train@eng.rst.rstdt_loss@eng.rst.rstdt': 1.1075619459152222, 'train@eng.rst.rstdt_runtime': 188.4347, 'train@eng.rst.rstdt_samples_per_second': 84.921, 'train@eng.rst.rstdt_steps_per_second': 2.659, 'epoch': 9.0}
{'loss': 1.164, 'learning_rate': 5e-06, 'epoch': 9.0}
{'eval_loss': 1.1960725784301758, 'eval_accuracy@eng.rst.rstdt': 0.6533004318322023, 'eval_f1@eng.rst.rstdt': 0.3620975825387302, 'eval_precision@eng.rst.rstdt': 0.5324799573848886, 'eval_recall@eng.rst.rstdt': 0.3460464185167113, 'eval_loss@eng.rst.rstdt': 1.1960725784301758, 'eval_runtime': 19.4354, 'eval_samples_per_second': 83.404, 'eval_steps_per_second': 2.624, 'epoch': 9.0}
{'train@eng.rst.rstdt_loss': 1.102839708328247, 'train@eng.rst.rstdt_accuracy@eng.rst.rstdt': 0.6729158855143107, 'train@eng.rst.rstdt_f1@eng.rst.rstdt': 0.42734529625395623, 'train@eng.rst.rstdt_precision@eng.rst.rstdt': 0.6158802325217637, 'train@eng.rst.rstdt_recall@eng.rst.rstdt': 0.3869488989518814, 'train@eng.rst.rstdt_loss@eng.rst.rstdt': 1.1028395891189575, 'train@eng.rst.rstdt_runtime': 189.0064, 'train@eng.rst.rstdt_samples_per_second': 84.664, 'train@eng.rst.rstdt_steps_per_second': 2.651, 'epoch': 10.0}
{'loss': 1.1545, 'learning_rate': 3.3333333333333333e-06, 'epoch': 10.0}
{'eval_loss': 1.201598882675171, 'eval_accuracy@eng.rst.rstdt': 0.6471314003701419, 'eval_f1@eng.rst.rstdt': 0.3684151360083964, 'eval_precision@eng.rst.rstdt': 0.5224547934810562, 'eval_recall@eng.rst.rstdt': 0.3538681556506947, 'eval_loss@eng.rst.rstdt': 1.201598882675171, 'eval_runtime': 19.4951, 'eval_samples_per_second': 83.149, 'eval_steps_per_second': 2.616, 'epoch': 10.0}
{'train@eng.rst.rstdt_loss': 1.0955737829208374, 'train@eng.rst.rstdt_accuracy@eng.rst.rstdt': 0.6734783152105986, 'train@eng.rst.rstdt_f1@eng.rst.rstdt': 0.422954525394931, 'train@eng.rst.rstdt_precision@eng.rst.rstdt': 0.6309129457062206, 'train@eng.rst.rstdt_recall@eng.rst.rstdt': 0.3805379823311, 'train@eng.rst.rstdt_loss@eng.rst.rstdt': 1.0955737829208374, 'train@eng.rst.rstdt_runtime': 189.0027, 'train@eng.rst.rstdt_samples_per_second': 84.665, 'train@eng.rst.rstdt_steps_per_second': 2.651, 'epoch': 11.0}
{'loss': 1.1519, 'learning_rate': 1.6666666666666667e-06, 'epoch': 11.0}
{'eval_loss': 1.192394733428955, 'eval_accuracy@eng.rst.rstdt': 0.6508328192473781, 'eval_f1@eng.rst.rstdt': 0.36941189654315154, 'eval_precision@eng.rst.rstdt': 0.5317184655411002, 'eval_recall@eng.rst.rstdt': 0.3527271741363751, 'eval_loss@eng.rst.rstdt': 1.192394733428955, 'eval_runtime': 19.4531, 'eval_samples_per_second': 83.329, 'eval_steps_per_second': 2.622, 'epoch': 11.0}
{'train@eng.rst.rstdt_loss': 1.0942001342773438, 'train@eng.rst.rstdt_accuracy@eng.rst.rstdt': 0.6738532683414573, 'train@eng.rst.rstdt_f1@eng.rst.rstdt': 0.4243974222915734, 'train@eng.rst.rstdt_precision@eng.rst.rstdt': 0.6323538522795784, 'train@eng.rst.rstdt_recall@eng.rst.rstdt': 0.3817958993287451, 'train@eng.rst.rstdt_loss@eng.rst.rstdt': 1.0942000150680542, 'train@eng.rst.rstdt_runtime': 188.4314, 'train@eng.rst.rstdt_samples_per_second': 84.922, 'train@eng.rst.rstdt_steps_per_second': 2.659, 'epoch': 12.0}
{'loss': 1.1461, 'learning_rate': 0.0, 'epoch': 12.0}
{'eval_loss': 1.1908845901489258, 'eval_accuracy@eng.rst.rstdt': 0.6526835286859963, 'eval_f1@eng.rst.rstdt': 0.37038944433533566, 'eval_precision@eng.rst.rstdt': 0.5331208773591998, 'eval_recall@eng.rst.rstdt': 0.3532709997908902, 'eval_loss@eng.rst.rstdt': 1.1908844709396362, 'eval_runtime': 19.4127, 'eval_samples_per_second': 83.502, 'eval_steps_per_second': 2.627, 'epoch': 12.0}
{'train_runtime': 7281.8392, 'train_samples_per_second': 26.37, 'train_steps_per_second': 0.826, 'train_loss': 1.3141940707614719, 'epoch': 12.0}
***** train_logger metrics *****
  epoch                    =       12.0
  train_loss               =      2.364
  train_runtime            = 0:17:46.22
  train_samples_per_second =      25.21
  train_steps_per_second   =      0.788
-------------------------------------------------------------------
Lang1:  spa.rst.sctb    Lang2:  eng.rst.rstdt
Saving run to:  runs/full_shot/FullShot=v4_spa.rst.sctb_eng.rst.rstdt_lrfunc=Adafactor_lr=2e-05_plm=bert-base-multilingual-cased
Running with params: BERT_MODEL=bert-base-multilingual-cased lr=2e-05
read 439 examples
read 94 examples
read 159 examples
read 16002 examples
read 1621 examples
read 2155 examples
Total prediction labels:  33
BertAdapterModel(
  (shared_parameters): ModuleDict()
  (bert): BertModel(
    (shared_parameters): ModuleDict()
    (invertible_adapters): ModuleDict(
      (es): NICECouplingBlock(
        (F): Sequential(
          (0): Linear(in_features=384, out_features=192, bias=True)
          (1): Activation_Function_Class(
            (f): NewGELUActivation()
          )
          (2): Linear(in_features=192, out_features=384, bias=True)
        )
        (G): Sequential(
          (0): Linear(in_features=384, out_features=192, bias=True)
          (1): Activation_Function_Class(
            (f): NewGELUActivation()
          )
          (2): Linear(in_features=192, out_features=384, bias=True)
        )
      )
      (en): NICECouplingBlock(
        (F): Sequential(
          (0): Linear(in_features=384, out_features=192, bias=True)
          (1): Activation_Function_Class(
            (f): ReLU()
          )
          (2): Linear(in_features=192, out_features=384, bias=True)
        )
        (G): Sequential(
          (0): Linear(in_features=384, out_features=192, bias=True)
          (1): Activation_Function_Class(
            (f): ReLU()
          )
          (2): Linear(in_features=192, out_features=384, bias=True)
        )
      )
    )
    (embeddings): BertEmbeddings(
      (word_embeddings): Embedding(119547, 768, padding_idx=0)
      (position_embeddings): Embedding(512, 768)
      (token_type_embeddings): Embedding(2, 768)
      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
      (dropout): Dropout(p=0.1, inplace=False)
    )
    (encoder): BertEncoder(
      (layer): ModuleList(
        (0): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (es): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (en): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (1): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (es): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (en): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (2): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (es): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (en): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (3): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (es): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (en): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (4): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (es): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (en): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (5): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (es): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (en): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (6): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (es): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (en): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (7): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (es): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (en): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (8): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (es): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (en): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (9): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (es): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (en): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (10): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (es): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (en): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (11): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (es): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (en): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
      )
    )
    (pooler): BertPooler(
      (dense): Linear(in_features=768, out_features=768, bias=True)
      (activation): Tanh()
    )
    (prefix_tuning): PrefixTuningPool(
      (prefix_tunings): ModuleDict()
    )
  )
  (heads): ModuleDict(
    (disrpt): ClassificationHead(
      (0): Dropout(p=0.1, inplace=False)
      (1): Linear(in_features=768, out_features=768, bias=True)
      (2): Activation_Function_Class(
        (f): Tanh()
      )
      (3): Dropout(p=0.1, inplace=False)
      (4): Linear(in_features=768, out_features=33, bias=True)
    )
  )
)
{'train@spa.rst.sctb_loss': 3.2064733505249023, 'train@spa.rst.sctb_accuracy@spa.rst.sctb': 0.3416856492027335, 'train@spa.rst.sctb_f1@spa.rst.sctb': 0.021222410865874366, 'train@spa.rst.sctb_precision@spa.rst.sctb': 0.014236902050113895, 'train@spa.rst.sctb_recall@spa.rst.sctb': 0.041666666666666664, 'train@spa.rst.sctb_loss@spa.rst.sctb': 3.2064733505249023, 'train@spa.rst.sctb_runtime': 5.5151, 'train@spa.rst.sctb_samples_per_second': 79.6, 'train@spa.rst.sctb_steps_per_second': 2.538, 'epoch': 1.0}
{'loss': 3.3802, 'learning_rate': 1.8333333333333333e-05, 'epoch': 1.0}
{'eval_loss': 3.198488712310791, 'eval_accuracy@spa.rst.sctb': 0.35106382978723405, 'eval_f1@spa.rst.sctb': 0.03056970819823993, 'eval_precision@spa.rst.sctb': 0.02065081351689612, 'eval_recall@spa.rst.sctb': 0.058823529411764705, 'eval_loss@spa.rst.sctb': 3.1984879970550537, 'eval_runtime': 1.4337, 'eval_samples_per_second': 65.563, 'eval_steps_per_second': 2.092, 'epoch': 1.0}
{'train@spa.rst.sctb_loss': 2.930215835571289, 'train@spa.rst.sctb_accuracy@spa.rst.sctb': 0.3416856492027335, 'train@spa.rst.sctb_f1@spa.rst.sctb': 0.021222410865874366, 'train@spa.rst.sctb_precision@spa.rst.sctb': 0.014236902050113895, 'train@spa.rst.sctb_recall@spa.rst.sctb': 0.041666666666666664, 'train@spa.rst.sctb_loss@spa.rst.sctb': 2.9302165508270264, 'train@spa.rst.sctb_runtime': 5.5245, 'train@spa.rst.sctb_samples_per_second': 79.464, 'train@spa.rst.sctb_steps_per_second': 2.534, 'epoch': 2.0}
{'loss': 3.0901, 'learning_rate': 1.6666666666666667e-05, 'epoch': 2.0}
{'eval_loss': 2.9199588298797607, 'eval_accuracy@spa.rst.sctb': 0.35106382978723405, 'eval_f1@spa.rst.sctb': 0.03056970819823993, 'eval_precision@spa.rst.sctb': 0.02065081351689612, 'eval_recall@spa.rst.sctb': 0.058823529411764705, 'eval_loss@spa.rst.sctb': 2.9199583530426025, 'eval_runtime': 1.4398, 'eval_samples_per_second': 65.285, 'eval_steps_per_second': 2.084, 'epoch': 2.0}
{'train@spa.rst.sctb_loss': 2.677529811859131, 'train@spa.rst.sctb_accuracy@spa.rst.sctb': 0.3416856492027335, 'train@spa.rst.sctb_f1@spa.rst.sctb': 0.021222410865874366, 'train@spa.rst.sctb_precision@spa.rst.sctb': 0.014236902050113895, 'train@spa.rst.sctb_recall@spa.rst.sctb': 0.041666666666666664, 'train@spa.rst.sctb_loss@spa.rst.sctb': 2.6775290966033936, 'train@spa.rst.sctb_runtime': 5.5371, 'train@spa.rst.sctb_samples_per_second': 79.284, 'train@spa.rst.sctb_steps_per_second': 2.528, 'epoch': 3.0}
{'loss': 2.8449, 'learning_rate': 1.5000000000000002e-05, 'epoch': 3.0}
{'eval_loss': 2.6697099208831787, 'eval_accuracy@spa.rst.sctb': 0.35106382978723405, 'eval_f1@spa.rst.sctb': 0.03056970819823993, 'eval_precision@spa.rst.sctb': 0.02065081351689612, 'eval_recall@spa.rst.sctb': 0.058823529411764705, 'eval_loss@spa.rst.sctb': 2.6697099208831787, 'eval_runtime': 1.4521, 'eval_samples_per_second': 64.734, 'eval_steps_per_second': 2.066, 'epoch': 3.0}
{'train@spa.rst.sctb_loss': 2.4894137382507324, 'train@spa.rst.sctb_accuracy@spa.rst.sctb': 0.3416856492027335, 'train@spa.rst.sctb_f1@spa.rst.sctb': 0.021222410865874366, 'train@spa.rst.sctb_precision@spa.rst.sctb': 0.014236902050113895, 'train@spa.rst.sctb_recall@spa.rst.sctb': 0.041666666666666664, 'train@spa.rst.sctb_loss@spa.rst.sctb': 2.4894137382507324, 'train@spa.rst.sctb_runtime': 5.5341, 'train@spa.rst.sctb_samples_per_second': 79.326, 'train@spa.rst.sctb_steps_per_second': 2.53, 'epoch': 4.0}
{'loss': 2.6136, 'learning_rate': 1.3333333333333333e-05, 'epoch': 4.0}
{'eval_loss': 2.490508794784546, 'eval_accuracy@spa.rst.sctb': 0.35106382978723405, 'eval_f1@spa.rst.sctb': 0.03056970819823993, 'eval_precision@spa.rst.sctb': 0.02065081351689612, 'eval_recall@spa.rst.sctb': 0.058823529411764705, 'eval_loss@spa.rst.sctb': 2.490508794784546, 'eval_runtime': 1.4305, 'eval_samples_per_second': 65.709, 'eval_steps_per_second': 2.097, 'epoch': 4.0}
{'train@spa.rst.sctb_loss': 2.3702681064605713, 'train@spa.rst.sctb_accuracy@spa.rst.sctb': 0.3416856492027335, 'train@spa.rst.sctb_f1@spa.rst.sctb': 0.021222410865874366, 'train@spa.rst.sctb_precision@spa.rst.sctb': 0.014236902050113895, 'train@spa.rst.sctb_recall@spa.rst.sctb': 0.041666666666666664, 'train@spa.rst.sctb_loss@spa.rst.sctb': 2.3702683448791504, 'train@spa.rst.sctb_runtime': 5.5898, 'train@spa.rst.sctb_samples_per_second': 78.536, 'train@spa.rst.sctb_steps_per_second': 2.505, 'epoch': 5.0}
{'loss': 2.4603, 'learning_rate': 1.1666666666666668e-05, 'epoch': 5.0}
{'eval_loss': 2.3793928623199463, 'eval_accuracy@spa.rst.sctb': 0.35106382978723405, 'eval_f1@spa.rst.sctb': 0.03056970819823993, 'eval_precision@spa.rst.sctb': 0.02065081351689612, 'eval_recall@spa.rst.sctb': 0.058823529411764705, 'eval_loss@spa.rst.sctb': 2.3793931007385254, 'eval_runtime': 1.4324, 'eval_samples_per_second': 65.623, 'eval_steps_per_second': 2.094, 'epoch': 5.0}
{'train@spa.rst.sctb_loss': 2.297788143157959, 'train@spa.rst.sctb_accuracy@spa.rst.sctb': 0.3416856492027335, 'train@spa.rst.sctb_f1@spa.rst.sctb': 0.021222410865874366, 'train@spa.rst.sctb_precision@spa.rst.sctb': 0.014236902050113895, 'train@spa.rst.sctb_recall@spa.rst.sctb': 0.041666666666666664, 'train@spa.rst.sctb_loss@spa.rst.sctb': 2.297788381576538, 'train@spa.rst.sctb_runtime': 5.546, 'train@spa.rst.sctb_samples_per_second': 79.156, 'train@spa.rst.sctb_steps_per_second': 2.524, 'epoch': 6.0}
{'loss': 2.3792, 'learning_rate': 1e-05, 'epoch': 6.0}
{'eval_loss': 2.3164992332458496, 'eval_accuracy@spa.rst.sctb': 0.35106382978723405, 'eval_f1@spa.rst.sctb': 0.03056970819823993, 'eval_precision@spa.rst.sctb': 0.02065081351689612, 'eval_recall@spa.rst.sctb': 0.058823529411764705, 'eval_loss@spa.rst.sctb': 2.3164994716644287, 'eval_runtime': 1.4429, 'eval_samples_per_second': 65.147, 'eval_steps_per_second': 2.079, 'epoch': 6.0}
{'train@spa.rst.sctb_loss': 2.2503411769866943, 'train@spa.rst.sctb_accuracy@spa.rst.sctb': 0.3462414578587699, 'train@spa.rst.sctb_f1@spa.rst.sctb': 0.023085734425940613, 'train@spa.rst.sctb_precision@spa.rst.sctb': 0.035201149425287355, 'train@spa.rst.sctb_recall@spa.rst.sctb': 0.04256272401433692, 'train@spa.rst.sctb_loss@spa.rst.sctb': 2.2503411769866943, 'train@spa.rst.sctb_runtime': 5.5676, 'train@spa.rst.sctb_samples_per_second': 78.849, 'train@spa.rst.sctb_steps_per_second': 2.515, 'epoch': 7.0}
{'loss': 2.3085, 'learning_rate': 8.333333333333334e-06, 'epoch': 7.0}
{'eval_loss': 2.2784171104431152, 'eval_accuracy@spa.rst.sctb': 0.35106382978723405, 'eval_f1@spa.rst.sctb': 0.03056970819823993, 'eval_precision@spa.rst.sctb': 0.02065081351689612, 'eval_recall@spa.rst.sctb': 0.058823529411764705, 'eval_loss@spa.rst.sctb': 2.278416872024536, 'eval_runtime': 1.426, 'eval_samples_per_second': 65.917, 'eval_steps_per_second': 2.104, 'epoch': 7.0}
{'train@spa.rst.sctb_loss': 2.2218732833862305, 'train@spa.rst.sctb_accuracy@spa.rst.sctb': 0.3530751708428246, 'train@spa.rst.sctb_f1@spa.rst.sctb': 0.02563203213116558, 'train@spa.rst.sctb_precision@spa.rst.sctb': 0.03199811345303149, 'train@spa.rst.sctb_recall@spa.rst.sctb': 0.04390681003584229, 'train@spa.rst.sctb_loss@spa.rst.sctb': 2.2218730449676514, 'train@spa.rst.sctb_runtime': 5.5844, 'train@spa.rst.sctb_samples_per_second': 78.611, 'train@spa.rst.sctb_steps_per_second': 2.507, 'epoch': 8.0}
{'loss': 2.2612, 'learning_rate': 6.666666666666667e-06, 'epoch': 8.0}
{'eval_loss': 2.257032871246338, 'eval_accuracy@spa.rst.sctb': 0.3829787234042553, 'eval_f1@spa.rst.sctb': 0.04947209653092006, 'eval_precision@spa.rst.sctb': 0.05524968608132908, 'eval_recall@spa.rst.sctb': 0.06942489914626138, 'eval_loss@spa.rst.sctb': 2.257032871246338, 'eval_runtime': 1.4373, 'eval_samples_per_second': 65.402, 'eval_steps_per_second': 2.087, 'epoch': 8.0}
{'train@spa.rst.sctb_loss': 2.2023351192474365, 'train@spa.rst.sctb_accuracy@spa.rst.sctb': 0.3621867881548975, 'train@spa.rst.sctb_f1@spa.rst.sctb': 0.02914525982158832, 'train@spa.rst.sctb_precision@spa.rst.sctb': 0.03382748346777124, 'train@spa.rst.sctb_recall@spa.rst.sctb': 0.04586917562724014, 'train@spa.rst.sctb_loss@spa.rst.sctb': 2.2023351192474365, 'train@spa.rst.sctb_runtime': 5.5617, 'train@spa.rst.sctb_samples_per_second': 78.933, 'train@spa.rst.sctb_steps_per_second': 2.517, 'epoch': 9.0}
{'loss': 2.2513, 'learning_rate': 5e-06, 'epoch': 9.0}
{'eval_loss': 2.242086887359619, 'eval_accuracy@spa.rst.sctb': 0.40425531914893614, 'eval_f1@spa.rst.sctb': 0.05598377281947261, 'eval_precision@spa.rst.sctb': 0.054764512595837894, 'eval_recall@spa.rst.sctb': 0.07561684961065766, 'eval_loss@spa.rst.sctb': 2.242086410522461, 'eval_runtime': 1.4405, 'eval_samples_per_second': 65.255, 'eval_steps_per_second': 2.083, 'epoch': 9.0}
{'train@spa.rst.sctb_loss': 2.1893351078033447, 'train@spa.rst.sctb_accuracy@spa.rst.sctb': 0.37813211845102507, 'train@spa.rst.sctb_f1@spa.rst.sctb': 0.033729899016342574, 'train@spa.rst.sctb_precision@spa.rst.sctb': 0.038790410214615594, 'train@spa.rst.sctb_recall@spa.rst.sctb': 0.04900537634408602, 'train@spa.rst.sctb_loss@spa.rst.sctb': 2.1893351078033447, 'train@spa.rst.sctb_runtime': 5.5698, 'train@spa.rst.sctb_samples_per_second': 78.818, 'train@spa.rst.sctb_steps_per_second': 2.514, 'epoch': 10.0}
{'loss': 2.2353, 'learning_rate': 3.3333333333333333e-06, 'epoch': 10.0}
{'eval_loss': 2.2329719066619873, 'eval_accuracy@spa.rst.sctb': 0.4148936170212766, 'eval_f1@spa.rst.sctb': 0.058759029927760574, 'eval_precision@spa.rst.sctb': 0.05491313334450589, 'eval_recall@spa.rst.sctb': 0.0787128248428558, 'eval_loss@spa.rst.sctb': 2.2329719066619873, 'eval_runtime': 1.4436, 'eval_samples_per_second': 65.113, 'eval_steps_per_second': 2.078, 'epoch': 10.0}
{'train@spa.rst.sctb_loss': 2.182271718978882, 'train@spa.rst.sctb_accuracy@spa.rst.sctb': 0.3826879271070615, 'train@spa.rst.sctb_f1@spa.rst.sctb': 0.03483956397342224, 'train@spa.rst.sctb_precision@spa.rst.sctb': 0.03861353183248608, 'train@spa.rst.sctb_recall@spa.rst.sctb': 0.049901433691756265, 'train@spa.rst.sctb_loss@spa.rst.sctb': 2.182271957397461, 'train@spa.rst.sctb_runtime': 5.5544, 'train@spa.rst.sctb_samples_per_second': 79.036, 'train@spa.rst.sctb_steps_per_second': 2.521, 'epoch': 11.0}
{'loss': 2.2261, 'learning_rate': 1.6666666666666667e-06, 'epoch': 11.0}
{'eval_loss': 2.2275590896606445, 'eval_accuracy@spa.rst.sctb': 0.425531914893617, 'eval_f1@spa.rst.sctb': 0.06129510627780523, 'eval_precision@spa.rst.sctb': 0.05519980143956317, 'eval_recall@spa.rst.sctb': 0.08180880007505395, 'eval_loss@spa.rst.sctb': 2.2275590896606445, 'eval_runtime': 1.4391, 'eval_samples_per_second': 65.318, 'eval_steps_per_second': 2.085, 'epoch': 11.0}
{'train@spa.rst.sctb_loss': 2.1798765659332275, 'train@spa.rst.sctb_accuracy@spa.rst.sctb': 0.38496583143507973, 'train@spa.rst.sctb_f1@spa.rst.sctb': 0.035681818181818176, 'train@spa.rst.sctb_precision@spa.rst.sctb': 0.0378525641025641, 'train@spa.rst.sctb_recall@spa.rst.sctb': 0.050519713261648745, 'train@spa.rst.sctb_loss@spa.rst.sctb': 2.1798768043518066, 'train@spa.rst.sctb_runtime': 5.5786, 'train@spa.rst.sctb_samples_per_second': 78.694, 'train@spa.rst.sctb_steps_per_second': 2.51, 'epoch': 12.0}
{'loss': 2.2227, 'learning_rate': 0.0, 'epoch': 12.0}
{'eval_loss': 2.2260239124298096, 'eval_accuracy@spa.rst.sctb': 0.425531914893617, 'eval_f1@spa.rst.sctb': 0.06129510627780523, 'eval_precision@spa.rst.sctb': 0.05519980143956317, 'eval_recall@spa.rst.sctb': 0.08180880007505395, 'eval_loss@spa.rst.sctb': 2.2260241508483887, 'eval_runtime': 1.4483, 'eval_samples_per_second': 64.905, 'eval_steps_per_second': 2.071, 'epoch': 12.0}
{'train_runtime': 217.5005, 'train_samples_per_second': 24.221, 'train_steps_per_second': 0.772, 'train_loss': 2.522788195382981, 'epoch': 12.0}
***** train_log metrics *****
  epoch                    =       12.0
  train_loss               =     2.5228
  train_runtime            = 0:03:37.50
  train_samples_per_second =     24.221
  train_steps_per_second   =      0.772
{'train@eng.rst.rstdt_loss': 1.7172132730484009, 'train@eng.rst.rstdt_accuracy@eng.rst.rstdt': 0.5161229846269216, 'train@eng.rst.rstdt_f1@eng.rst.rstdt': 0.08882091266122177, 'train@eng.rst.rstdt_precision@eng.rst.rstdt': 0.09797832813890124, 'train@eng.rst.rstdt_recall@eng.rst.rstdt': 0.11322807311390874, 'train@eng.rst.rstdt_loss@eng.rst.rstdt': 1.7172131538391113, 'train@eng.rst.rstdt_runtime': 189.0317, 'train@eng.rst.rstdt_samples_per_second': 84.652, 'train@eng.rst.rstdt_steps_per_second': 2.65, 'epoch': 1.0}
{'loss': 2.1408, 'learning_rate': 1.8333333333333333e-05, 'epoch': 1.0}
{'eval_loss': 1.6956201791763306, 'eval_accuracy@eng.rst.rstdt': 0.5286859962985812, 'eval_f1@eng.rst.rstdt': 0.09017106962230872, 'eval_precision@eng.rst.rstdt': 0.09560790540115374, 'eval_recall@eng.rst.rstdt': 0.11271867341881642, 'eval_loss@eng.rst.rstdt': 1.6956201791763306, 'eval_runtime': 19.4917, 'eval_samples_per_second': 83.163, 'eval_steps_per_second': 2.616, 'epoch': 1.0}
{'train@eng.rst.rstdt_loss': 1.408388376235962, 'train@eng.rst.rstdt_accuracy@eng.rst.rstdt': 0.6084239470066242, 'train@eng.rst.rstdt_f1@eng.rst.rstdt': 0.20608574818731124, 'train@eng.rst.rstdt_precision@eng.rst.rstdt': 0.29242211681050584, 'train@eng.rst.rstdt_recall@eng.rst.rstdt': 0.22003332790623062, 'train@eng.rst.rstdt_loss@eng.rst.rstdt': 1.408388376235962, 'train@eng.rst.rstdt_runtime': 188.3945, 'train@eng.rst.rstdt_samples_per_second': 84.939, 'train@eng.rst.rstdt_steps_per_second': 2.659, 'epoch': 2.0}
{'loss': 1.5843, 'learning_rate': 1.6666666666666667e-05, 'epoch': 2.0}
{'eval_loss': 1.4126126766204834, 'eval_accuracy@eng.rst.rstdt': 0.6113510178901912, 'eval_f1@eng.rst.rstdt': 0.20604354977181416, 'eval_precision@eng.rst.rstdt': 0.26430898285240023, 'eval_recall@eng.rst.rstdt': 0.21851793175365294, 'eval_loss@eng.rst.rstdt': 1.4126126766204834, 'eval_runtime': 19.4303, 'eval_samples_per_second': 83.426, 'eval_steps_per_second': 2.625, 'epoch': 2.0}
{'train@eng.rst.rstdt_loss': 1.3055579662322998, 'train@eng.rst.rstdt_accuracy@eng.rst.rstdt': 0.6326709161354831, 'train@eng.rst.rstdt_f1@eng.rst.rstdt': 0.27913484769101105, 'train@eng.rst.rstdt_precision@eng.rst.rstdt': 0.45090539885047726, 'train@eng.rst.rstdt_recall@eng.rst.rstdt': 0.2688824343733451, 'train@eng.rst.rstdt_loss@eng.rst.rstdt': 1.3055579662322998, 'train@eng.rst.rstdt_runtime': 188.9554, 'train@eng.rst.rstdt_samples_per_second': 84.687, 'train@eng.rst.rstdt_steps_per_second': 2.651, 'epoch': 3.0}
{'loss': 1.4129, 'learning_rate': 1.5000000000000002e-05, 'epoch': 3.0}
{'eval_loss': 1.3318541049957275, 'eval_accuracy@eng.rst.rstdt': 0.6310919185687847, 'eval_f1@eng.rst.rstdt': 0.2721954365495781, 'eval_precision@eng.rst.rstdt': 0.4007478799186918, 'eval_recall@eng.rst.rstdt': 0.26496495701986017, 'eval_loss@eng.rst.rstdt': 1.3318541049957275, 'eval_runtime': 19.4952, 'eval_samples_per_second': 83.149, 'eval_steps_per_second': 2.616, 'epoch': 3.0}
{'train@eng.rst.rstdt_loss': 1.2400325536727905, 'train@eng.rst.rstdt_accuracy@eng.rst.rstdt': 0.6459192600924885, 'train@eng.rst.rstdt_f1@eng.rst.rstdt': 0.33496126991708464, 'train@eng.rst.rstdt_precision@eng.rst.rstdt': 0.49748544373913256, 'train@eng.rst.rstdt_recall@eng.rst.rstdt': 0.3175068006917452, 'train@eng.rst.rstdt_loss@eng.rst.rstdt': 1.24003267288208, 'train@eng.rst.rstdt_runtime': 188.7676, 'train@eng.rst.rstdt_samples_per_second': 84.771, 'train@eng.rst.rstdt_steps_per_second': 2.654, 'epoch': 4.0}
{'loss': 1.3214, 'learning_rate': 1.3333333333333333e-05, 'epoch': 4.0}
{'eval_loss': 1.2948719263076782, 'eval_accuracy@eng.rst.rstdt': 0.634176434299815, 'eval_f1@eng.rst.rstdt': 0.3177561554907869, 'eval_precision@eng.rst.rstdt': 0.39863866776136464, 'eval_recall@eng.rst.rstdt': 0.31383352393797087, 'eval_loss@eng.rst.rstdt': 1.2948719263076782, 'eval_runtime': 19.4083, 'eval_samples_per_second': 83.521, 'eval_steps_per_second': 2.628, 'epoch': 4.0}
{'train@eng.rst.rstdt_loss': 1.1915109157562256, 'train@eng.rst.rstdt_accuracy@eng.rst.rstdt': 0.6589801274840645, 'train@eng.rst.rstdt_f1@eng.rst.rstdt': 0.36361886776642827, 'train@eng.rst.rstdt_precision@eng.rst.rstdt': 0.5406578723751025, 'train@eng.rst.rstdt_recall@eng.rst.rstdt': 0.33680262860486776, 'train@eng.rst.rstdt_loss@eng.rst.rstdt': 1.1915106773376465, 'train@eng.rst.rstdt_runtime': 188.3288, 'train@eng.rst.rstdt_samples_per_second': 84.968, 'train@eng.rst.rstdt_steps_per_second': 2.66, 'epoch': 5.0}
{'loss': 1.2665, 'learning_rate': 1.1666666666666668e-05, 'epoch': 5.0}
{'eval_loss': 1.2533106803894043, 'eval_accuracy@eng.rst.rstdt': 0.6458975940777298, 'eval_f1@eng.rst.rstdt': 0.32712678837398823, 'eval_precision@eng.rst.rstdt': 0.4208977626484569, 'eval_recall@eng.rst.rstdt': 0.32127454032502367, 'eval_loss@eng.rst.rstdt': 1.2533106803894043, 'eval_runtime': 19.3917, 'eval_samples_per_second': 83.592, 'eval_steps_per_second': 2.63, 'epoch': 5.0}
{'train@eng.rst.rstdt_loss': 1.158576250076294, 'train@eng.rst.rstdt_accuracy@eng.rst.rstdt': 0.6637295338082739, 'train@eng.rst.rstdt_f1@eng.rst.rstdt': 0.3804942450831319, 'train@eng.rst.rstdt_precision@eng.rst.rstdt': 0.529766379520653, 'train@eng.rst.rstdt_recall@eng.rst.rstdt': 0.34924936433723897, 'train@eng.rst.rstdt_loss@eng.rst.rstdt': 1.158576250076294, 'train@eng.rst.rstdt_runtime': 188.9179, 'train@eng.rst.rstdt_samples_per_second': 84.703, 'train@eng.rst.rstdt_steps_per_second': 2.652, 'epoch': 6.0}
{'loss': 1.2248, 'learning_rate': 1e-05, 'epoch': 6.0}
{'eval_loss': 1.22774076461792, 'eval_accuracy@eng.rst.rstdt': 0.6409623689080814, 'eval_f1@eng.rst.rstdt': 0.3448085256603432, 'eval_precision@eng.rst.rstdt': 0.4677166300787287, 'eval_recall@eng.rst.rstdt': 0.3317348444862116, 'eval_loss@eng.rst.rstdt': 1.22774076461792, 'eval_runtime': 19.4569, 'eval_samples_per_second': 83.312, 'eval_steps_per_second': 2.621, 'epoch': 6.0}
{'train@eng.rst.rstdt_loss': 1.1395330429077148, 'train@eng.rst.rstdt_accuracy@eng.rst.rstdt': 0.6680414948131483, 'train@eng.rst.rstdt_f1@eng.rst.rstdt': 0.39223175071939465, 'train@eng.rst.rstdt_precision@eng.rst.rstdt': 0.53795511532656, 'train@eng.rst.rstdt_recall@eng.rst.rstdt': 0.358446276281149, 'train@eng.rst.rstdt_loss@eng.rst.rstdt': 1.1395330429077148, 'train@eng.rst.rstdt_runtime': 188.4532, 'train@eng.rst.rstdt_samples_per_second': 84.912, 'train@eng.rst.rstdt_steps_per_second': 2.658, 'epoch': 7.0}
{'loss': 1.2013, 'learning_rate': 8.333333333333334e-06, 'epoch': 7.0}
{'eval_loss': 1.2125293016433716, 'eval_accuracy@eng.rst.rstdt': 0.6440468846391116, 'eval_f1@eng.rst.rstdt': 0.3576325821083908, 'eval_precision@eng.rst.rstdt': 0.47029777476390383, 'eval_recall@eng.rst.rstdt': 0.3434452018323368, 'eval_loss@eng.rst.rstdt': 1.2125293016433716, 'eval_runtime': 19.425, 'eval_samples_per_second': 83.449, 'eval_steps_per_second': 2.625, 'epoch': 7.0}
{'train@eng.rst.rstdt_loss': 1.1233667135238647, 'train@eng.rst.rstdt_accuracy@eng.rst.rstdt': 0.669978752655918, 'train@eng.rst.rstdt_f1@eng.rst.rstdt': 0.399950187072738, 'train@eng.rst.rstdt_precision@eng.rst.rstdt': 0.5854162687257506, 'train@eng.rst.rstdt_recall@eng.rst.rstdt': 0.36734528109305936, 'train@eng.rst.rstdt_loss@eng.rst.rstdt': 1.1233667135238647, 'train@eng.rst.rstdt_runtime': 189.0618, 'train@eng.rst.rstdt_samples_per_second': 84.639, 'train@eng.rst.rstdt_steps_per_second': 2.65, 'epoch': 8.0}
{'loss': 1.1816, 'learning_rate': 6.666666666666667e-06, 'epoch': 8.0}
{'eval_loss': 1.207256555557251, 'eval_accuracy@eng.rst.rstdt': 0.6458975940777298, 'eval_f1@eng.rst.rstdt': 0.3577214913651555, 'eval_precision@eng.rst.rstdt': 0.4624978115328144, 'eval_recall@eng.rst.rstdt': 0.34908978905727617, 'eval_loss@eng.rst.rstdt': 1.2072566747665405, 'eval_runtime': 19.5045, 'eval_samples_per_second': 83.109, 'eval_steps_per_second': 2.615, 'epoch': 8.0}
{'train@eng.rst.rstdt_loss': 1.1117534637451172, 'train@eng.rst.rstdt_accuracy@eng.rst.rstdt': 0.6713535808023997, 'train@eng.rst.rstdt_f1@eng.rst.rstdt': 0.4037404388626736, 'train@eng.rst.rstdt_precision@eng.rst.rstdt': 0.5780195963543262, 'train@eng.rst.rstdt_recall@eng.rst.rstdt': 0.37005264527188786, 'train@eng.rst.rstdt_loss@eng.rst.rstdt': 1.1117534637451172, 'train@eng.rst.rstdt_runtime': 189.1872, 'train@eng.rst.rstdt_samples_per_second': 84.583, 'train@eng.rst.rstdt_steps_per_second': 2.648, 'epoch': 9.0}
{'loss': 1.1677, 'learning_rate': 5e-06, 'epoch': 9.0}
{'eval_loss': 1.1980817317962646, 'eval_accuracy@eng.rst.rstdt': 0.6477483035163479, 'eval_f1@eng.rst.rstdt': 0.3582271260634659, 'eval_precision@eng.rst.rstdt': 0.4625970692575717, 'eval_recall@eng.rst.rstdt': 0.3490543824709589, 'eval_loss@eng.rst.rstdt': 1.1980818510055542, 'eval_runtime': 19.4655, 'eval_samples_per_second': 83.275, 'eval_steps_per_second': 2.62, 'epoch': 9.0}
{'train@eng.rst.rstdt_loss': 1.1072636842727661, 'train@eng.rst.rstdt_accuracy@eng.rst.rstdt': 0.6719785026871641, 'train@eng.rst.rstdt_f1@eng.rst.rstdt': 0.4111194625330983, 'train@eng.rst.rstdt_precision@eng.rst.rstdt': 0.5699984628510677, 'train@eng.rst.rstdt_recall@eng.rst.rstdt': 0.37871390217939666, 'train@eng.rst.rstdt_loss@eng.rst.rstdt': 1.1072636842727661, 'train@eng.rst.rstdt_runtime': 188.4736, 'train@eng.rst.rstdt_samples_per_second': 84.903, 'train@eng.rst.rstdt_steps_per_second': 2.658, 'epoch': 10.0}
{'loss': 1.1596, 'learning_rate': 3.3333333333333333e-06, 'epoch': 10.0}
{'eval_loss': 1.2033594846725464, 'eval_accuracy@eng.rst.rstdt': 0.6421961752004935, 'eval_f1@eng.rst.rstdt': 0.36379421251889954, 'eval_precision@eng.rst.rstdt': 0.49221198401256766, 'eval_recall@eng.rst.rstdt': 0.3550382100582478, 'eval_loss@eng.rst.rstdt': 1.2033594846725464, 'eval_runtime': 19.6436, 'eval_samples_per_second': 82.521, 'eval_steps_per_second': 2.596, 'epoch': 10.0}
{'train@eng.rst.rstdt_loss': 1.1009498834609985, 'train@eng.rst.rstdt_accuracy@eng.rst.rstdt': 0.6732908386451694, 'train@eng.rst.rstdt_f1@eng.rst.rstdt': 0.4095931397952448, 'train@eng.rst.rstdt_precision@eng.rst.rstdt': 0.5792816053807935, 'train@eng.rst.rstdt_recall@eng.rst.rstdt': 0.3749533538723663, 'train@eng.rst.rstdt_loss@eng.rst.rstdt': 1.1009498834609985, 'train@eng.rst.rstdt_runtime': 189.0183, 'train@eng.rst.rstdt_samples_per_second': 84.658, 'train@eng.rst.rstdt_steps_per_second': 2.651, 'epoch': 11.0}
{'loss': 1.1512, 'learning_rate': 1.6666666666666667e-06, 'epoch': 11.0}
{'eval_loss': 1.1953669786453247, 'eval_accuracy@eng.rst.rstdt': 0.6465144972239358, 'eval_f1@eng.rst.rstdt': 0.36451054349895884, 'eval_precision@eng.rst.rstdt': 0.5202135115048032, 'eval_recall@eng.rst.rstdt': 0.3542604791461637, 'eval_loss@eng.rst.rstdt': 1.1953669786453247, 'eval_runtime': 19.4644, 'eval_samples_per_second': 83.28, 'eval_steps_per_second': 2.62, 'epoch': 11.0}
{'train@eng.rst.rstdt_loss': 1.0996416807174683, 'train@eng.rst.rstdt_accuracy@eng.rst.rstdt': 0.6739782527184102, 'train@eng.rst.rstdt_f1@eng.rst.rstdt': 0.4101579609028685, 'train@eng.rst.rstdt_precision@eng.rst.rstdt': 0.5792658634032817, 'train@eng.rst.rstdt_recall@eng.rst.rstdt': 0.3752586909169086, 'train@eng.rst.rstdt_loss@eng.rst.rstdt': 1.0996416807174683, 'train@eng.rst.rstdt_runtime': 188.8565, 'train@eng.rst.rstdt_samples_per_second': 84.731, 'train@eng.rst.rstdt_steps_per_second': 2.653, 'epoch': 12.0}
{'loss': 1.1481, 'learning_rate': 0.0, 'epoch': 12.0}
{'eval_loss': 1.1938081979751587, 'eval_accuracy@eng.rst.rstdt': 0.6477483035163479, 'eval_f1@eng.rst.rstdt': 0.3655469793688482, 'eval_precision@eng.rst.rstdt': 0.5244579876354177, 'eval_recall@eng.rst.rstdt': 0.35490428717274525, 'eval_loss@eng.rst.rstdt': 1.1938081979751587, 'eval_runtime': 19.4233, 'eval_samples_per_second': 83.457, 'eval_steps_per_second': 2.626, 'epoch': 12.0}
{'train_runtime': 7280.7668, 'train_samples_per_second': 26.374, 'train_steps_per_second': 0.826, 'train_loss': 1.3300196482035929, 'epoch': 12.0}
***** train_logger metrics *****
  epoch                    =       12.0
  train_loss               =     2.5228
  train_runtime            = 0:03:37.50
  train_samples_per_second =     24.221
  train_steps_per_second   =      0.772
-------------------------------------------------------------------
Lang1:  tur.pdtb.tdb    Lang2:  eng.rst.rstdt
Saving run to:  runs/full_shot/FullShot=v4_tur.pdtb.tdb_eng.rst.rstdt_lrfunc=Adafactor_lr=2e-05_plm=bert-base-multilingual-cased
Running with params: BERT_MODEL=bert-base-multilingual-cased lr=2e-05
read 2451 examples
read 312 examples
read 422 examples
read 16002 examples
read 1621 examples
read 2155 examples
Total prediction labels:  40
BertAdapterModel(
  (shared_parameters): ModuleDict()
  (bert): BertModel(
    (shared_parameters): ModuleDict()
    (invertible_adapters): ModuleDict(
      (tr): NICECouplingBlock(
        (F): Sequential(
          (0): Linear(in_features=384, out_features=192, bias=True)
          (1): Activation_Function_Class(
            (f): NewGELUActivation()
          )
          (2): Linear(in_features=192, out_features=384, bias=True)
        )
        (G): Sequential(
          (0): Linear(in_features=384, out_features=192, bias=True)
          (1): Activation_Function_Class(
            (f): NewGELUActivation()
          )
          (2): Linear(in_features=192, out_features=384, bias=True)
        )
      )
      (en): NICECouplingBlock(
        (F): Sequential(
          (0): Linear(in_features=384, out_features=192, bias=True)
          (1): Activation_Function_Class(
            (f): ReLU()
          )
          (2): Linear(in_features=192, out_features=384, bias=True)
        )
        (G): Sequential(
          (0): Linear(in_features=384, out_features=192, bias=True)
          (1): Activation_Function_Class(
            (f): ReLU()
          )
          (2): Linear(in_features=192, out_features=384, bias=True)
        )
      )
    )
    (embeddings): BertEmbeddings(
      (word_embeddings): Embedding(119547, 768, padding_idx=0)
      (position_embeddings): Embedding(512, 768)
      (token_type_embeddings): Embedding(2, 768)
      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
      (dropout): Dropout(p=0.1, inplace=False)
    )
    (encoder): BertEncoder(
      (layer): ModuleList(
        (0): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (tr): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (en): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (1): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (tr): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (en): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (2): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (tr): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (en): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (3): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (tr): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (en): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (4): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (tr): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (en): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (5): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (tr): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (en): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (6): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (tr): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (en): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (7): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (tr): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (en): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (8): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (tr): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (en): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (9): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (tr): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (en): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (10): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (tr): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (en): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (11): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (tr): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (en): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
      )
    )
    (pooler): BertPooler(
      (dense): Linear(in_features=768, out_features=768, bias=True)
      (activation): Tanh()
    )
    (prefix_tuning): PrefixTuningPool(
      (prefix_tunings): ModuleDict()
    )
  )
  (heads): ModuleDict(
    (tr): BertStyleMaskedLMHead(
      (0): Linear(in_features=768, out_features=768, bias=True)
      (1): Activation_Function_Class(
        (f): GELUActivation()
      )
      (2): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
      (3): Linear(in_features=768, out_features=119547, bias=True)
    )
    (disrpt): ClassificationHead(
      (0): Dropout(p=0.1, inplace=False)
      (1): Linear(in_features=768, out_features=768, bias=True)
      (2): Activation_Function_Class(
        (f): Tanh()
      )
      (3): Dropout(p=0.1, inplace=False)
      (4): Linear(in_features=768, out_features=40, bias=True)
    )
  )
)
{'train@tur.pdtb.tdb_loss': 2.828228235244751, 'train@tur.pdtb.tdb_accuracy@tur.pdtb.tdb': 0.25050999592003265, 'train@tur.pdtb.tdb_f1@tur.pdtb.tdb': 0.01741967515426626, 'train@tur.pdtb.tdb_precision@tur.pdtb.tdb': 0.010891738953044898, 'train@tur.pdtb.tdb_recall@tur.pdtb.tdb': 0.043478260869565216, 'train@tur.pdtb.tdb_loss@tur.pdtb.tdb': 2.828227996826172, 'train@tur.pdtb.tdb_runtime': 29.5857, 'train@tur.pdtb.tdb_samples_per_second': 82.844, 'train@tur.pdtb.tdb_steps_per_second': 2.603, 'epoch': 1.0}
{'loss': 3.2515, 'learning_rate': 1.8333333333333333e-05, 'epoch': 1.0}
{'eval_loss': 2.7602131366729736, 'eval_accuracy@tur.pdtb.tdb': 0.266025641025641, 'eval_f1@tur.pdtb.tdb': 0.019102416570771, 'eval_precision@tur.pdtb.tdb': 0.012092074592074592, 'eval_recall@tur.pdtb.tdb': 0.045454545454545456, 'eval_loss@tur.pdtb.tdb': 2.7602131366729736, 'eval_runtime': 4.0941, 'eval_samples_per_second': 76.207, 'eval_steps_per_second': 2.443, 'epoch': 1.0}
{'train@tur.pdtb.tdb_loss': 2.436363697052002, 'train@tur.pdtb.tdb_accuracy@tur.pdtb.tdb': 0.25050999592003265, 'train@tur.pdtb.tdb_f1@tur.pdtb.tdb': 0.01741967515426626, 'train@tur.pdtb.tdb_precision@tur.pdtb.tdb': 0.010891738953044898, 'train@tur.pdtb.tdb_recall@tur.pdtb.tdb': 0.043478260869565216, 'train@tur.pdtb.tdb_loss@tur.pdtb.tdb': 2.436363458633423, 'train@tur.pdtb.tdb_runtime': 29.6231, 'train@tur.pdtb.tdb_samples_per_second': 82.739, 'train@tur.pdtb.tdb_steps_per_second': 2.599, 'epoch': 2.0}
{'loss': 2.5987, 'learning_rate': 1.6666666666666667e-05, 'epoch': 2.0}
{'eval_loss': 2.3320257663726807, 'eval_accuracy@tur.pdtb.tdb': 0.266025641025641, 'eval_f1@tur.pdtb.tdb': 0.019102416570771, 'eval_precision@tur.pdtb.tdb': 0.012092074592074592, 'eval_recall@tur.pdtb.tdb': 0.045454545454545456, 'eval_loss@tur.pdtb.tdb': 2.3320257663726807, 'eval_runtime': 4.0814, 'eval_samples_per_second': 76.445, 'eval_steps_per_second': 2.45, 'epoch': 2.0}
{'train@tur.pdtb.tdb_loss': 2.3574397563934326, 'train@tur.pdtb.tdb_accuracy@tur.pdtb.tdb': 0.25091799265605874, 'train@tur.pdtb.tdb_f1@tur.pdtb.tdb': 0.017798769811805042, 'train@tur.pdtb.tdb_precision@tur.pdtb.tdb': 0.025406756964935356, 'train@tur.pdtb.tdb_recall@tur.pdtb.tdb': 0.04365572315882875, 'train@tur.pdtb.tdb_loss@tur.pdtb.tdb': 2.3574395179748535, 'train@tur.pdtb.tdb_runtime': 29.6262, 'train@tur.pdtb.tdb_samples_per_second': 82.731, 'train@tur.pdtb.tdb_steps_per_second': 2.599, 'epoch': 3.0}
{'loss': 2.4165, 'learning_rate': 1.5000000000000002e-05, 'epoch': 3.0}
{'eval_loss': 2.2870936393737793, 'eval_accuracy@tur.pdtb.tdb': 0.266025641025641, 'eval_f1@tur.pdtb.tdb': 0.01915089986155976, 'eval_precision@tur.pdtb.tdb': 0.012130955860859398, 'eval_recall@tur.pdtb.tdb': 0.045454545454545456, 'eval_loss@tur.pdtb.tdb': 2.2870938777923584, 'eval_runtime': 4.092, 'eval_samples_per_second': 76.246, 'eval_steps_per_second': 2.444, 'epoch': 3.0}
{'train@tur.pdtb.tdb_loss': 2.294119358062744, 'train@tur.pdtb.tdb_accuracy@tur.pdtb.tdb': 0.2807017543859649, 'train@tur.pdtb.tdb_f1@tur.pdtb.tdb': 0.03775618456382175, 'train@tur.pdtb.tdb_precision@tur.pdtb.tdb': 0.04384497772250185, 'train@tur.pdtb.tdb_recall@tur.pdtb.tdb': 0.05827818803487972, 'train@tur.pdtb.tdb_loss@tur.pdtb.tdb': 2.2941195964813232, 'train@tur.pdtb.tdb_runtime': 29.638, 'train@tur.pdtb.tdb_samples_per_second': 82.698, 'train@tur.pdtb.tdb_steps_per_second': 2.598, 'epoch': 4.0}
{'loss': 2.3512, 'learning_rate': 1.3333333333333333e-05, 'epoch': 4.0}
{'eval_loss': 2.249734878540039, 'eval_accuracy@tur.pdtb.tdb': 0.26282051282051283, 'eval_f1@tur.pdtb.tdb': 0.026408702947215604, 'eval_precision@tur.pdtb.tdb': 0.027773251076708184, 'eval_recall@tur.pdtb.tdb': 0.047989641274545594, 'eval_loss@tur.pdtb.tdb': 2.249735116958618, 'eval_runtime': 4.0983, 'eval_samples_per_second': 76.128, 'eval_steps_per_second': 2.44, 'epoch': 4.0}
{'train@tur.pdtb.tdb_loss': 2.2447798252105713, 'train@tur.pdtb.tdb_accuracy@tur.pdtb.tdb': 0.3108935128518972, 'train@tur.pdtb.tdb_f1@tur.pdtb.tdb': 0.07743292710607365, 'train@tur.pdtb.tdb_precision@tur.pdtb.tdb': 0.09768085149291647, 'train@tur.pdtb.tdb_recall@tur.pdtb.tdb': 0.0900058864249958, 'train@tur.pdtb.tdb_loss@tur.pdtb.tdb': 2.244779586791992, 'train@tur.pdtb.tdb_runtime': 29.6618, 'train@tur.pdtb.tdb_samples_per_second': 82.632, 'train@tur.pdtb.tdb_steps_per_second': 2.596, 'epoch': 5.0}
{'loss': 2.2998, 'learning_rate': 1.1666666666666668e-05, 'epoch': 5.0}
{'eval_loss': 2.2157068252563477, 'eval_accuracy@tur.pdtb.tdb': 0.2948717948717949, 'eval_f1@tur.pdtb.tdb': 0.07119732484513314, 'eval_precision@tur.pdtb.tdb': 0.06376262626262627, 'eval_recall@tur.pdtb.tdb': 0.09913540078316832, 'eval_loss@tur.pdtb.tdb': 2.2157068252563477, 'eval_runtime': 4.1034, 'eval_samples_per_second': 76.035, 'eval_steps_per_second': 2.437, 'epoch': 5.0}
{'train@tur.pdtb.tdb_loss': 2.2000033855438232, 'train@tur.pdtb.tdb_accuracy@tur.pdtb.tdb': 0.3215014279885761, 'train@tur.pdtb.tdb_f1@tur.pdtb.tdb': 0.08150526852812102, 'train@tur.pdtb.tdb_precision@tur.pdtb.tdb': 0.08724401016689488, 'train@tur.pdtb.tdb_recall@tur.pdtb.tdb': 0.10086185472751778, 'train@tur.pdtb.tdb_loss@tur.pdtb.tdb': 2.2000033855438232, 'train@tur.pdtb.tdb_runtime': 29.7459, 'train@tur.pdtb.tdb_samples_per_second': 82.398, 'train@tur.pdtb.tdb_steps_per_second': 2.589, 'epoch': 6.0}
{'loss': 2.2622, 'learning_rate': 1e-05, 'epoch': 6.0}
{'eval_loss': 2.1894400119781494, 'eval_accuracy@tur.pdtb.tdb': 0.28525641025641024, 'eval_f1@tur.pdtb.tdb': 0.07140985973441599, 'eval_precision@tur.pdtb.tdb': 0.06217945055921596, 'eval_recall@tur.pdtb.tdb': 0.09835599080815238, 'eval_loss@tur.pdtb.tdb': 2.1894402503967285, 'eval_runtime': 4.1222, 'eval_samples_per_second': 75.688, 'eval_steps_per_second': 2.426, 'epoch': 6.0}
{'train@tur.pdtb.tdb_loss': 2.1710143089294434, 'train@tur.pdtb.tdb_accuracy@tur.pdtb.tdb': 0.32721338229294167, 'train@tur.pdtb.tdb_f1@tur.pdtb.tdb': 0.0861200847127474, 'train@tur.pdtb.tdb_precision@tur.pdtb.tdb': 0.08078596311437404, 'train@tur.pdtb.tdb_recall@tur.pdtb.tdb': 0.10740211086528825, 'train@tur.pdtb.tdb_loss@tur.pdtb.tdb': 2.1710143089294434, 'train@tur.pdtb.tdb_runtime': 29.6318, 'train@tur.pdtb.tdb_samples_per_second': 82.715, 'train@tur.pdtb.tdb_steps_per_second': 2.599, 'epoch': 7.0}
{'loss': 2.2281, 'learning_rate': 8.333333333333334e-06, 'epoch': 7.0}
{'eval_loss': 2.1718363761901855, 'eval_accuracy@tur.pdtb.tdb': 0.2916666666666667, 'eval_f1@tur.pdtb.tdb': 0.07104334094779954, 'eval_precision@tur.pdtb.tdb': 0.05899252262888626, 'eval_recall@tur.pdtb.tdb': 0.10117833186578756, 'eval_loss@tur.pdtb.tdb': 2.1718363761901855, 'eval_runtime': 4.1295, 'eval_samples_per_second': 75.555, 'eval_steps_per_second': 2.422, 'epoch': 7.0}
{'train@tur.pdtb.tdb_loss': 2.155251979827881, 'train@tur.pdtb.tdb_accuracy@tur.pdtb.tdb': 0.3288453692370461, 'train@tur.pdtb.tdb_f1@tur.pdtb.tdb': 0.08736518270530237, 'train@tur.pdtb.tdb_precision@tur.pdtb.tdb': 0.0808782452572466, 'train@tur.pdtb.tdb_recall@tur.pdtb.tdb': 0.1082249457813903, 'train@tur.pdtb.tdb_loss@tur.pdtb.tdb': 2.15525221824646, 'train@tur.pdtb.tdb_runtime': 29.7129, 'train@tur.pdtb.tdb_samples_per_second': 82.489, 'train@tur.pdtb.tdb_steps_per_second': 2.591, 'epoch': 8.0}
{'loss': 2.2095, 'learning_rate': 6.666666666666667e-06, 'epoch': 8.0}
{'eval_loss': 2.16044020652771, 'eval_accuracy@tur.pdtb.tdb': 0.2980769230769231, 'eval_f1@tur.pdtb.tdb': 0.07479495967878141, 'eval_precision@tur.pdtb.tdb': 0.06901967846173855, 'eval_recall@tur.pdtb.tdb': 0.10315461645076782, 'eval_loss@tur.pdtb.tdb': 2.16044020652771, 'eval_runtime': 4.1185, 'eval_samples_per_second': 75.756, 'eval_steps_per_second': 2.428, 'epoch': 8.0}
{'train@tur.pdtb.tdb_loss': 2.1326777935028076, 'train@tur.pdtb.tdb_accuracy@tur.pdtb.tdb': 0.3329253365973072, 'train@tur.pdtb.tdb_f1@tur.pdtb.tdb': 0.08854140412917025, 'train@tur.pdtb.tdb_precision@tur.pdtb.tdb': 0.09509074296337258, 'train@tur.pdtb.tdb_recall@tur.pdtb.tdb': 0.11134006823726388, 'train@tur.pdtb.tdb_loss@tur.pdtb.tdb': 2.1326777935028076, 'train@tur.pdtb.tdb_runtime': 29.75, 'train@tur.pdtb.tdb_samples_per_second': 82.387, 'train@tur.pdtb.tdb_steps_per_second': 2.588, 'epoch': 9.0}
{'loss': 2.1852, 'learning_rate': 5e-06, 'epoch': 9.0}
{'eval_loss': 2.1449666023254395, 'eval_accuracy@tur.pdtb.tdb': 0.3108974358974359, 'eval_f1@tur.pdtb.tdb': 0.07684827157336373, 'eval_precision@tur.pdtb.tdb': 0.07184934623792226, 'eval_recall@tur.pdtb.tdb': 0.10850221003365551, 'eval_loss@tur.pdtb.tdb': 2.1449666023254395, 'eval_runtime': 4.1169, 'eval_samples_per_second': 75.786, 'eval_steps_per_second': 2.429, 'epoch': 9.0}
{'train@tur.pdtb.tdb_loss': 2.1221811771392822, 'train@tur.pdtb.tdb_accuracy@tur.pdtb.tdb': 0.3382292941656467, 'train@tur.pdtb.tdb_f1@tur.pdtb.tdb': 0.09094972624440213, 'train@tur.pdtb.tdb_precision@tur.pdtb.tdb': 0.08777925263889684, 'train@tur.pdtb.tdb_recall@tur.pdtb.tdb': 0.11328065794672929, 'train@tur.pdtb.tdb_loss@tur.pdtb.tdb': 2.1221811771392822, 'train@tur.pdtb.tdb_runtime': 29.703, 'train@tur.pdtb.tdb_samples_per_second': 82.517, 'train@tur.pdtb.tdb_steps_per_second': 2.592, 'epoch': 10.0}
{'loss': 2.1723, 'learning_rate': 3.3333333333333333e-06, 'epoch': 10.0}
{'eval_loss': 2.137101888656616, 'eval_accuracy@tur.pdtb.tdb': 0.3108974358974359, 'eval_f1@tur.pdtb.tdb': 0.07802870412767991, 'eval_precision@tur.pdtb.tdb': 0.07048554431953641, 'eval_recall@tur.pdtb.tdb': 0.10835366576746418, 'eval_loss@tur.pdtb.tdb': 2.137101888656616, 'eval_runtime': 4.1108, 'eval_samples_per_second': 75.897, 'eval_steps_per_second': 2.433, 'epoch': 10.0}
{'train@tur.pdtb.tdb_loss': 2.1169679164886475, 'train@tur.pdtb.tdb_accuracy@tur.pdtb.tdb': 0.33741330069359443, 'train@tur.pdtb.tdb_f1@tur.pdtb.tdb': 0.0905103237232812, 'train@tur.pdtb.tdb_precision@tur.pdtb.tdb': 0.08698976649327186, 'train@tur.pdtb.tdb_recall@tur.pdtb.tdb': 0.1128590080885952, 'train@tur.pdtb.tdb_loss@tur.pdtb.tdb': 2.1169679164886475, 'train@tur.pdtb.tdb_runtime': 29.716, 'train@tur.pdtb.tdb_samples_per_second': 82.481, 'train@tur.pdtb.tdb_steps_per_second': 2.591, 'epoch': 11.0}
{'loss': 2.1698, 'learning_rate': 1.6666666666666667e-06, 'epoch': 11.0}
{'eval_loss': 2.1343283653259277, 'eval_accuracy@tur.pdtb.tdb': 0.3141025641025641, 'eval_f1@tur.pdtb.tdb': 0.07962543026210314, 'eval_precision@tur.pdtb.tdb': 0.07288412295220464, 'eval_recall@tur.pdtb.tdb': 0.10961629203009045, 'eval_loss@tur.pdtb.tdb': 2.1343281269073486, 'eval_runtime': 4.1047, 'eval_samples_per_second': 76.01, 'eval_steps_per_second': 2.436, 'epoch': 11.0}
{'train@tur.pdtb.tdb_loss': 2.114126682281494, 'train@tur.pdtb.tdb_accuracy@tur.pdtb.tdb': 0.3390452876376989, 'train@tur.pdtb.tdb_f1@tur.pdtb.tdb': 0.09149270012508899, 'train@tur.pdtb.tdb_precision@tur.pdtb.tdb': 0.09314174820764701, 'train@tur.pdtb.tdb_recall@tur.pdtb.tdb': 0.11363543486708193, 'train@tur.pdtb.tdb_loss@tur.pdtb.tdb': 2.114126443862915, 'train@tur.pdtb.tdb_runtime': 29.7336, 'train@tur.pdtb.tdb_samples_per_second': 82.432, 'train@tur.pdtb.tdb_steps_per_second': 2.59, 'epoch': 12.0}
{'loss': 2.1564, 'learning_rate': 0.0, 'epoch': 12.0}
{'eval_loss': 2.1320226192474365, 'eval_accuracy@tur.pdtb.tdb': 0.3141025641025641, 'eval_f1@tur.pdtb.tdb': 0.07953919238887379, 'eval_precision@tur.pdtb.tdb': 0.07189623507805326, 'eval_recall@tur.pdtb.tdb': 0.10961629203009045, 'eval_loss@tur.pdtb.tdb': 2.1320223808288574, 'eval_runtime': 4.1143, 'eval_samples_per_second': 75.832, 'eval_steps_per_second': 2.431, 'epoch': 12.0}
{'train_runtime': 1153.1209, 'train_samples_per_second': 25.506, 'train_steps_per_second': 0.801, 'train_loss': 2.358438103746026, 'epoch': 12.0}
***** train_log metrics *****
  epoch                    =       12.0
  train_loss               =     2.3584
  train_runtime            = 0:19:13.12
  train_samples_per_second =     25.506
  train_steps_per_second   =      0.801
{'train@eng.rst.rstdt_loss': 1.6278208494186401, 'train@eng.rst.rstdt_accuracy@eng.rst.rstdt': 0.5516185476815398, 'train@eng.rst.rstdt_f1@eng.rst.rstdt': 0.12070037145628465, 'train@eng.rst.rstdt_precision@eng.rst.rstdt': 0.12516041608875544, 'train@eng.rst.rstdt_recall@eng.rst.rstdt': 0.13937359417173067, 'train@eng.rst.rstdt_loss@eng.rst.rstdt': 1.6278208494186401, 'train@eng.rst.rstdt_runtime': 188.4794, 'train@eng.rst.rstdt_samples_per_second': 84.901, 'train@eng.rst.rstdt_steps_per_second': 2.658, 'epoch': 1.0}
{'loss': 2.0187, 'learning_rate': 1.8333333333333333e-05, 'epoch': 1.0}
{'eval_loss': 1.626607060432434, 'eval_accuracy@eng.rst.rstdt': 0.5508945095619988, 'eval_f1@eng.rst.rstdt': 0.11805259106859581, 'eval_precision@eng.rst.rstdt': 0.11899241949318386, 'eval_recall@eng.rst.rstdt': 0.13647802290691677, 'eval_loss@eng.rst.rstdt': 1.626607060432434, 'eval_runtime': 19.4635, 'eval_samples_per_second': 83.284, 'eval_steps_per_second': 2.62, 'epoch': 1.0}
{'train@eng.rst.rstdt_loss': 1.4068812131881714, 'train@eng.rst.rstdt_accuracy@eng.rst.rstdt': 0.6068616422947132, 'train@eng.rst.rstdt_f1@eng.rst.rstdt': 0.20918951754975867, 'train@eng.rst.rstdt_precision@eng.rst.rstdt': 0.29814347994059676, 'train@eng.rst.rstdt_recall@eng.rst.rstdt': 0.2178483484403796, 'train@eng.rst.rstdt_loss@eng.rst.rstdt': 1.4068810939788818, 'train@eng.rst.rstdt_runtime': 189.2494, 'train@eng.rst.rstdt_samples_per_second': 84.555, 'train@eng.rst.rstdt_steps_per_second': 2.647, 'epoch': 2.0}
{'loss': 1.5535, 'learning_rate': 1.6666666666666667e-05, 'epoch': 2.0}
{'eval_loss': 1.4231563806533813, 'eval_accuracy@eng.rst.rstdt': 0.6033312769895126, 'eval_f1@eng.rst.rstdt': 0.20013068417965063, 'eval_precision@eng.rst.rstdt': 0.22338088385966057, 'eval_recall@eng.rst.rstdt': 0.21144717850058678, 'eval_loss@eng.rst.rstdt': 1.4231563806533813, 'eval_runtime': 19.552, 'eval_samples_per_second': 82.907, 'eval_steps_per_second': 2.608, 'epoch': 2.0}
{'train@eng.rst.rstdt_loss': 1.3157011270523071, 'train@eng.rst.rstdt_accuracy@eng.rst.rstdt': 0.6315460567429071, 'train@eng.rst.rstdt_f1@eng.rst.rstdt': 0.28071785099335694, 'train@eng.rst.rstdt_precision@eng.rst.rstdt': 0.4400462483633553, 'train@eng.rst.rstdt_recall@eng.rst.rstdt': 0.2706312981948675, 'train@eng.rst.rstdt_loss@eng.rst.rstdt': 1.3157011270523071, 'train@eng.rst.rstdt_runtime': 188.9354, 'train@eng.rst.rstdt_samples_per_second': 84.696, 'train@eng.rst.rstdt_steps_per_second': 2.652, 'epoch': 3.0}
{'loss': 1.4212, 'learning_rate': 1.5000000000000002e-05, 'epoch': 3.0}
{'eval_loss': 1.358230710029602, 'eval_accuracy@eng.rst.rstdt': 0.6249228871067243, 'eval_f1@eng.rst.rstdt': 0.2799273403381976, 'eval_precision@eng.rst.rstdt': 0.4063052538198057, 'eval_recall@eng.rst.rstdt': 0.27277415082476025, 'eval_loss@eng.rst.rstdt': 1.3582308292388916, 'eval_runtime': 19.5153, 'eval_samples_per_second': 83.063, 'eval_steps_per_second': 2.613, 'epoch': 3.0}
{'train@eng.rst.rstdt_loss': 1.2590664625167847, 'train@eng.rst.rstdt_accuracy@eng.rst.rstdt': 0.644294463192101, 'train@eng.rst.rstdt_f1@eng.rst.rstdt': 0.3340479268588504, 'train@eng.rst.rstdt_precision@eng.rst.rstdt': 0.4322866586932257, 'train@eng.rst.rstdt_recall@eng.rst.rstdt': 0.3177438304316982, 'train@eng.rst.rstdt_loss@eng.rst.rstdt': 1.2590664625167847, 'train@eng.rst.rstdt_runtime': 188.4684, 'train@eng.rst.rstdt_samples_per_second': 84.905, 'train@eng.rst.rstdt_steps_per_second': 2.658, 'epoch': 4.0}
{'loss': 1.341, 'learning_rate': 1.3333333333333333e-05, 'epoch': 4.0}
{'eval_loss': 1.3245505094528198, 'eval_accuracy@eng.rst.rstdt': 0.6329426280074029, 'eval_f1@eng.rst.rstdt': 0.3310132976814447, 'eval_precision@eng.rst.rstdt': 0.4575042064078805, 'eval_recall@eng.rst.rstdt': 0.32264768660832893, 'eval_loss@eng.rst.rstdt': 1.3245505094528198, 'eval_runtime': 19.5027, 'eval_samples_per_second': 83.117, 'eval_steps_per_second': 2.615, 'epoch': 4.0}
{'train@eng.rst.rstdt_loss': 1.2124253511428833, 'train@eng.rst.rstdt_accuracy@eng.rst.rstdt': 0.6544806899137607, 'train@eng.rst.rstdt_f1@eng.rst.rstdt': 0.3482920471336366, 'train@eng.rst.rstdt_precision@eng.rst.rstdt': 0.555960993937592, 'train@eng.rst.rstdt_recall@eng.rst.rstdt': 0.3274998740385676, 'train@eng.rst.rstdt_loss@eng.rst.rstdt': 1.2124253511428833, 'train@eng.rst.rstdt_runtime': 189.0439, 'train@eng.rst.rstdt_samples_per_second': 84.647, 'train@eng.rst.rstdt_steps_per_second': 2.65, 'epoch': 5.0}
{'loss': 1.2903, 'learning_rate': 1.1666666666666668e-05, 'epoch': 5.0}
{'eval_loss': 1.2827571630477905, 'eval_accuracy@eng.rst.rstdt': 0.6366440468846392, 'eval_f1@eng.rst.rstdt': 0.3257958647502883, 'eval_precision@eng.rst.rstdt': 0.4398749765414931, 'eval_recall@eng.rst.rstdt': 0.31664470286387925, 'eval_loss@eng.rst.rstdt': 1.282757043838501, 'eval_runtime': 19.5132, 'eval_samples_per_second': 83.072, 'eval_steps_per_second': 2.614, 'epoch': 5.0}
{'train@eng.rst.rstdt_loss': 1.178946614265442, 'train@eng.rst.rstdt_accuracy@eng.rst.rstdt': 0.6607924009498812, 'train@eng.rst.rstdt_f1@eng.rst.rstdt': 0.3698346189782753, 'train@eng.rst.rstdt_precision@eng.rst.rstdt': 0.5455521719639626, 'train@eng.rst.rstdt_recall@eng.rst.rstdt': 0.34171139780362686, 'train@eng.rst.rstdt_loss@eng.rst.rstdt': 1.178946614265442, 'train@eng.rst.rstdt_runtime': 189.1327, 'train@eng.rst.rstdt_samples_per_second': 84.607, 'train@eng.rst.rstdt_steps_per_second': 2.649, 'epoch': 6.0}
{'loss': 1.2464, 'learning_rate': 1e-05, 'epoch': 6.0}
{'eval_loss': 1.2569576501846313, 'eval_accuracy@eng.rst.rstdt': 0.6428130783466995, 'eval_f1@eng.rst.rstdt': 0.3328750841636917, 'eval_precision@eng.rst.rstdt': 0.4309122144782827, 'eval_recall@eng.rst.rstdt': 0.32298872740342627, 'eval_loss@eng.rst.rstdt': 1.2569575309753418, 'eval_runtime': 19.4964, 'eval_samples_per_second': 83.144, 'eval_steps_per_second': 2.616, 'epoch': 6.0}
{'train@eng.rst.rstdt_loss': 1.1616911888122559, 'train@eng.rst.rstdt_accuracy@eng.rst.rstdt': 0.6649793775778028, 'train@eng.rst.rstdt_f1@eng.rst.rstdt': 0.3839908303680405, 'train@eng.rst.rstdt_precision@eng.rst.rstdt': 0.5513877096550311, 'train@eng.rst.rstdt_recall@eng.rst.rstdt': 0.3499723942323018, 'train@eng.rst.rstdt_loss@eng.rst.rstdt': 1.1616910696029663, 'train@eng.rst.rstdt_runtime': 188.6433, 'train@eng.rst.rstdt_samples_per_second': 84.827, 'train@eng.rst.rstdt_steps_per_second': 2.656, 'epoch': 7.0}
{'loss': 1.2269, 'learning_rate': 8.333333333333334e-06, 'epoch': 7.0}
{'eval_loss': 1.2400273084640503, 'eval_accuracy@eng.rst.rstdt': 0.6458975940777298, 'eval_f1@eng.rst.rstdt': 0.3438264538052327, 'eval_precision@eng.rst.rstdt': 0.4851670421473865, 'eval_recall@eng.rst.rstdt': 0.32751229371472484, 'eval_loss@eng.rst.rstdt': 1.2400274276733398, 'eval_runtime': 19.4954, 'eval_samples_per_second': 83.148, 'eval_steps_per_second': 2.616, 'epoch': 7.0}
{'train@eng.rst.rstdt_loss': 1.1459527015686035, 'train@eng.rst.rstdt_accuracy@eng.rst.rstdt': 0.6645419322584677, 'train@eng.rst.rstdt_f1@eng.rst.rstdt': 0.39052172725169143, 'train@eng.rst.rstdt_precision@eng.rst.rstdt': 0.551643797995795, 'train@eng.rst.rstdt_recall@eng.rst.rstdt': 0.35966898073017567, 'train@eng.rst.rstdt_loss@eng.rst.rstdt': 1.145952820777893, 'train@eng.rst.rstdt_runtime': 189.2528, 'train@eng.rst.rstdt_samples_per_second': 84.554, 'train@eng.rst.rstdt_steps_per_second': 2.647, 'epoch': 8.0}
{'loss': 1.2051, 'learning_rate': 6.666666666666667e-06, 'epoch': 8.0}
{'eval_loss': 1.2357348203659058, 'eval_accuracy@eng.rst.rstdt': 0.6421961752004935, 'eval_f1@eng.rst.rstdt': 0.3487294522220289, 'eval_precision@eng.rst.rstdt': 0.46764594988901964, 'eval_recall@eng.rst.rstdt': 0.3357551669394984, 'eval_loss@eng.rst.rstdt': 1.2357348203659058, 'eval_runtime': 19.5519, 'eval_samples_per_second': 82.907, 'eval_steps_per_second': 2.608, 'epoch': 8.0}
{'train@eng.rst.rstdt_loss': 1.132514476776123, 'train@eng.rst.rstdt_accuracy@eng.rst.rstdt': 0.6682289713785777, 'train@eng.rst.rstdt_f1@eng.rst.rstdt': 0.3980934057349486, 'train@eng.rst.rstdt_precision@eng.rst.rstdt': 0.554398703374475, 'train@eng.rst.rstdt_recall@eng.rst.rstdt': 0.3645805721798477, 'train@eng.rst.rstdt_loss@eng.rst.rstdt': 1.1325147151947021, 'train@eng.rst.rstdt_runtime': 188.7434, 'train@eng.rst.rstdt_samples_per_second': 84.782, 'train@eng.rst.rstdt_steps_per_second': 2.654, 'epoch': 9.0}
{'loss': 1.1898, 'learning_rate': 5e-06, 'epoch': 9.0}
{'eval_loss': 1.2255865335464478, 'eval_accuracy@eng.rst.rstdt': 0.6477483035163479, 'eval_f1@eng.rst.rstdt': 0.36064572931322286, 'eval_precision@eng.rst.rstdt': 0.4819831947638916, 'eval_recall@eng.rst.rstdt': 0.34594526937343384, 'eval_loss@eng.rst.rstdt': 1.2255864143371582, 'eval_runtime': 19.4768, 'eval_samples_per_second': 83.227, 'eval_steps_per_second': 2.618, 'epoch': 9.0}
{'train@eng.rst.rstdt_loss': 1.1281352043151855, 'train@eng.rst.rstdt_accuracy@eng.rst.rstdt': 0.6688538932633421, 'train@eng.rst.rstdt_f1@eng.rst.rstdt': 0.40752693324975786, 'train@eng.rst.rstdt_precision@eng.rst.rstdt': 0.5421766067048969, 'train@eng.rst.rstdt_recall@eng.rst.rstdt': 0.37517588935304275, 'train@eng.rst.rstdt_loss@eng.rst.rstdt': 1.128135085105896, 'train@eng.rst.rstdt_runtime': 189.4341, 'train@eng.rst.rstdt_samples_per_second': 84.473, 'train@eng.rst.rstdt_steps_per_second': 2.645, 'epoch': 10.0}
{'loss': 1.1831, 'learning_rate': 3.3333333333333333e-06, 'epoch': 10.0}
{'eval_loss': 1.2280863523483276, 'eval_accuracy@eng.rst.rstdt': 0.6403454657618753, 'eval_f1@eng.rst.rstdt': 0.3604308372061647, 'eval_precision@eng.rst.rstdt': 0.46161776476225985, 'eval_recall@eng.rst.rstdt': 0.3490291032223529, 'eval_loss@eng.rst.rstdt': 1.228086233139038, 'eval_runtime': 19.5373, 'eval_samples_per_second': 82.97, 'eval_steps_per_second': 2.61, 'epoch': 10.0}
{'train@eng.rst.rstdt_loss': 1.1206161975860596, 'train@eng.rst.rstdt_accuracy@eng.rst.rstdt': 0.6697287839020123, 'train@eng.rst.rstdt_f1@eng.rst.rstdt': 0.4063924808120979, 'train@eng.rst.rstdt_precision@eng.rst.rstdt': 0.5481366926341156, 'train@eng.rst.rstdt_recall@eng.rst.rstdt': 0.37119500316225285, 'train@eng.rst.rstdt_loss@eng.rst.rstdt': 1.12061607837677, 'train@eng.rst.rstdt_runtime': 188.8724, 'train@eng.rst.rstdt_samples_per_second': 84.724, 'train@eng.rst.rstdt_steps_per_second': 2.653, 'epoch': 11.0}
{'loss': 1.1761, 'learning_rate': 1.6666666666666667e-06, 'epoch': 11.0}
{'eval_loss': 1.2197681665420532, 'eval_accuracy@eng.rst.rstdt': 0.6415792720542874, 'eval_f1@eng.rst.rstdt': 0.35653876247401645, 'eval_precision@eng.rst.rstdt': 0.46358539390293246, 'eval_recall@eng.rst.rstdt': 0.34405770008523195, 'eval_loss@eng.rst.rstdt': 1.2197681665420532, 'eval_runtime': 19.5294, 'eval_samples_per_second': 83.003, 'eval_steps_per_second': 2.611, 'epoch': 11.0}
{'train@eng.rst.rstdt_loss': 1.1190251111984253, 'train@eng.rst.rstdt_accuracy@eng.rst.rstdt': 0.6702912135983002, 'train@eng.rst.rstdt_f1@eng.rst.rstdt': 0.4071118262776338, 'train@eng.rst.rstdt_precision@eng.rst.rstdt': 0.5508978183398501, 'train@eng.rst.rstdt_recall@eng.rst.rstdt': 0.37164304943802984, 'train@eng.rst.rstdt_loss@eng.rst.rstdt': 1.1190251111984253, 'train@eng.rst.rstdt_runtime': 189.1588, 'train@eng.rst.rstdt_samples_per_second': 84.596, 'train@eng.rst.rstdt_steps_per_second': 2.649, 'epoch': 12.0}
{'loss': 1.172, 'learning_rate': 0.0, 'epoch': 12.0}
{'eval_loss': 1.2181686162948608, 'eval_accuracy@eng.rst.rstdt': 0.6421961752004935, 'eval_f1@eng.rst.rstdt': 0.3597665112261524, 'eval_precision@eng.rst.rstdt': 0.4728347827065241, 'eval_recall@eng.rst.rstdt': 0.34567419320102943, 'eval_loss@eng.rst.rstdt': 1.2181683778762817, 'eval_runtime': 19.5378, 'eval_samples_per_second': 82.968, 'eval_steps_per_second': 2.61, 'epoch': 12.0}
{'train_runtime': 7306.2048, 'train_samples_per_second': 26.282, 'train_steps_per_second': 0.823, 'train_loss': 1.3353376965957409, 'epoch': 12.0}
***** train_logger metrics *****
  epoch                    =       12.0
  train_loss               =     2.3584
  train_runtime            = 0:19:13.12
  train_samples_per_second =     25.506
  train_steps_per_second   =      0.801
-------------------------------------------------------------------
Lang1:  zho.rst.sctb    Lang2:  eng.rst.rstdt
Saving run to:  runs/full_shot/FullShot=v4_zho.rst.sctb_eng.rst.rstdt_lrfunc=Adafactor_lr=2e-05_plm=bert-base-multilingual-cased
Running with params: BERT_MODEL=bert-base-multilingual-cased lr=2e-05
read 439 examples
read 94 examples
read 159 examples
read 16002 examples
read 1621 examples
read 2155 examples
Total prediction labels:  34
BertAdapterModel(
  (shared_parameters): ModuleDict()
  (bert): BertModel(
    (shared_parameters): ModuleDict()
    (invertible_adapters): ModuleDict(
      (zh): NICECouplingBlock(
        (F): Sequential(
          (0): Linear(in_features=384, out_features=192, bias=True)
          (1): Activation_Function_Class(
            (f): ReLU()
          )
          (2): Linear(in_features=192, out_features=384, bias=True)
        )
        (G): Sequential(
          (0): Linear(in_features=384, out_features=192, bias=True)
          (1): Activation_Function_Class(
            (f): ReLU()
          )
          (2): Linear(in_features=192, out_features=384, bias=True)
        )
      )
      (en): NICECouplingBlock(
        (F): Sequential(
          (0): Linear(in_features=384, out_features=192, bias=True)
          (1): Activation_Function_Class(
            (f): ReLU()
          )
          (2): Linear(in_features=192, out_features=384, bias=True)
        )
        (G): Sequential(
          (0): Linear(in_features=384, out_features=192, bias=True)
          (1): Activation_Function_Class(
            (f): ReLU()
          )
          (2): Linear(in_features=192, out_features=384, bias=True)
        )
      )
    )
    (embeddings): BertEmbeddings(
      (word_embeddings): Embedding(119547, 768, padding_idx=0)
      (position_embeddings): Embedding(512, 768)
      (token_type_embeddings): Embedding(2, 768)
      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
      (dropout): Dropout(p=0.1, inplace=False)
    )
    (encoder): BertEncoder(
      (layer): ModuleList(
        (0): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (zh): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (en): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (1): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (zh): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (en): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (2): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (zh): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (en): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (3): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (zh): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (en): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (4): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (zh): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (en): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (5): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (zh): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (en): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (6): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (zh): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (en): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (7): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (zh): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (en): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (8): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (zh): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (en): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (9): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (zh): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (en): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (10): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (zh): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (en): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (11): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (zh): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (en): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
      )
    )
    (pooler): BertPooler(
      (dense): Linear(in_features=768, out_features=768, bias=True)
      (activation): Tanh()
    )
    (prefix_tuning): PrefixTuningPool(
      (prefix_tunings): ModuleDict()
    )
  )
  (heads): ModuleDict(
    (disrpt): ClassificationHead(
      (0): Dropout(p=0.1, inplace=False)
      (1): Linear(in_features=768, out_features=768, bias=True)
      (2): Activation_Function_Class(
        (f): Tanh()
      )
      (3): Dropout(p=0.1, inplace=False)
      (4): Linear(in_features=768, out_features=34, bias=True)
    )
  )
)
{'train@zho.rst.sctb_loss': 3.3139686584472656, 'train@zho.rst.sctb_accuracy@zho.rst.sctb': 0.3325740318906606, 'train@zho.rst.sctb_f1@zho.rst.sctb': 0.019230769230769232, 'train@zho.rst.sctb_precision@zho.rst.sctb': 0.01282051282051282, 'train@zho.rst.sctb_recall@zho.rst.sctb': 0.038461538461538464, 'train@zho.rst.sctb_loss@zho.rst.sctb': 3.313969373703003, 'train@zho.rst.sctb_runtime': 5.4469, 'train@zho.rst.sctb_samples_per_second': 80.596, 'train@zho.rst.sctb_steps_per_second': 2.57, 'epoch': 1.0}
{'loss': 3.452, 'learning_rate': 1.8333333333333333e-05, 'epoch': 1.0}
{'eval_loss': 3.320845603942871, 'eval_accuracy@zho.rst.sctb': 0.32978723404255317, 'eval_f1@zho.rst.sctb': 0.026105263157894736, 'eval_precision@zho.rst.sctb': 0.017543859649122806, 'eval_recall@zho.rst.sctb': 0.05098684210526316, 'eval_loss@zho.rst.sctb': 3.320844888687134, 'eval_runtime': 1.4222, 'eval_samples_per_second': 66.096, 'eval_steps_per_second': 2.109, 'epoch': 1.0}
{'train@zho.rst.sctb_loss': 3.111785888671875, 'train@zho.rst.sctb_accuracy@zho.rst.sctb': 0.3348519362186788, 'train@zho.rst.sctb_f1@zho.rst.sctb': 0.020032051282051284, 'train@zho.rst.sctb_precision@zho.rst.sctb': 0.05128205128205128, 'train@zho.rst.sctb_recall@zho.rst.sctb': 0.038866396761133605, 'train@zho.rst.sctb_loss@zho.rst.sctb': 3.111785888671875, 'train@zho.rst.sctb_runtime': 5.4566, 'train@zho.rst.sctb_samples_per_second': 80.453, 'train@zho.rst.sctb_steps_per_second': 2.566, 'epoch': 2.0}
{'loss': 3.2278, 'learning_rate': 1.6666666666666667e-05, 'epoch': 2.0}
{'eval_loss': 3.1294615268707275, 'eval_accuracy@zho.rst.sctb': 0.32978723404255317, 'eval_f1@zho.rst.sctb': 0.026105263157894736, 'eval_precision@zho.rst.sctb': 0.017543859649122806, 'eval_recall@zho.rst.sctb': 0.05098684210526316, 'eval_loss@zho.rst.sctb': 3.1294608116149902, 'eval_runtime': 1.435, 'eval_samples_per_second': 65.503, 'eval_steps_per_second': 2.091, 'epoch': 2.0}
{'train@zho.rst.sctb_loss': 2.9293720722198486, 'train@zho.rst.sctb_accuracy@zho.rst.sctb': 0.3348519362186788, 'train@zho.rst.sctb_f1@zho.rst.sctb': 0.020032051282051284, 'train@zho.rst.sctb_precision@zho.rst.sctb': 0.05128205128205128, 'train@zho.rst.sctb_recall@zho.rst.sctb': 0.038866396761133605, 'train@zho.rst.sctb_loss@zho.rst.sctb': 2.9293720722198486, 'train@zho.rst.sctb_runtime': 5.4733, 'train@zho.rst.sctb_samples_per_second': 80.207, 'train@zho.rst.sctb_steps_per_second': 2.558, 'epoch': 3.0}
{'loss': 3.0535, 'learning_rate': 1.5000000000000002e-05, 'epoch': 3.0}
{'eval_loss': 2.9570655822753906, 'eval_accuracy@zho.rst.sctb': 0.32978723404255317, 'eval_f1@zho.rst.sctb': 0.026105263157894736, 'eval_precision@zho.rst.sctb': 0.017543859649122806, 'eval_recall@zho.rst.sctb': 0.05098684210526316, 'eval_loss@zho.rst.sctb': 2.957066059112549, 'eval_runtime': 1.4258, 'eval_samples_per_second': 65.928, 'eval_steps_per_second': 2.104, 'epoch': 3.0}
{'train@zho.rst.sctb_loss': 2.7731621265411377, 'train@zho.rst.sctb_accuracy@zho.rst.sctb': 0.33712984054669703, 'train@zho.rst.sctb_f1@zho.rst.sctb': 0.02084979793596794, 'train@zho.rst.sctb_precision@zho.rst.sctb': 0.051311388839992964, 'train@zho.rst.sctb_recall@zho.rst.sctb': 0.039271255060728746, 'train@zho.rst.sctb_loss@zho.rst.sctb': 2.773162603378296, 'train@zho.rst.sctb_runtime': 5.485, 'train@zho.rst.sctb_samples_per_second': 80.036, 'train@zho.rst.sctb_steps_per_second': 2.552, 'epoch': 4.0}
{'loss': 2.8781, 'learning_rate': 1.3333333333333333e-05, 'epoch': 4.0}
{'eval_loss': 2.8113043308258057, 'eval_accuracy@zho.rst.sctb': 0.32978723404255317, 'eval_f1@zho.rst.sctb': 0.026105263157894736, 'eval_precision@zho.rst.sctb': 0.017543859649122806, 'eval_recall@zho.rst.sctb': 0.05098684210526316, 'eval_loss@zho.rst.sctb': 2.8113045692443848, 'eval_runtime': 1.4294, 'eval_samples_per_second': 65.761, 'eval_steps_per_second': 2.099, 'epoch': 4.0}
{'train@zho.rst.sctb_loss': 2.6447770595550537, 'train@zho.rst.sctb_accuracy@zho.rst.sctb': 0.33712984054669703, 'train@zho.rst.sctb_f1@zho.rst.sctb': 0.020866713061116596, 'train@zho.rst.sctb_precision@zho.rst.sctb': 0.038520348153375676, 'train@zho.rst.sctb_recall@zho.rst.sctb': 0.039271255060728746, 'train@zho.rst.sctb_loss@zho.rst.sctb': 2.6447765827178955, 'train@zho.rst.sctb_runtime': 5.5025, 'train@zho.rst.sctb_samples_per_second': 79.782, 'train@zho.rst.sctb_steps_per_second': 2.544, 'epoch': 5.0}
{'loss': 2.7504, 'learning_rate': 1.1666666666666668e-05, 'epoch': 5.0}
{'eval_loss': 2.694270372390747, 'eval_accuracy@zho.rst.sctb': 0.32978723404255317, 'eval_f1@zho.rst.sctb': 0.026105263157894736, 'eval_precision@zho.rst.sctb': 0.017543859649122806, 'eval_recall@zho.rst.sctb': 0.05098684210526316, 'eval_loss@zho.rst.sctb': 2.6942708492279053, 'eval_runtime': 1.4265, 'eval_samples_per_second': 65.894, 'eval_steps_per_second': 2.103, 'epoch': 5.0}
{'train@zho.rst.sctb_loss': 2.5399036407470703, 'train@zho.rst.sctb_accuracy@zho.rst.sctb': 0.33940774487471526, 'train@zho.rst.sctb_f1@zho.rst.sctb': 0.0216710875331565, 'train@zho.rst.sctb_precision@zho.rst.sctb': 0.03601559730591988, 'train@zho.rst.sctb_recall@zho.rst.sctb': 0.03967611336032389, 'train@zho.rst.sctb_loss@zho.rst.sctb': 2.5399038791656494, 'train@zho.rst.sctb_runtime': 5.5091, 'train@zho.rst.sctb_samples_per_second': 79.686, 'train@zho.rst.sctb_steps_per_second': 2.541, 'epoch': 6.0}
{'loss': 2.633, 'learning_rate': 1e-05, 'epoch': 6.0}
{'eval_loss': 2.602546453475952, 'eval_accuracy@zho.rst.sctb': 0.32978723404255317, 'eval_f1@zho.rst.sctb': 0.026105263157894736, 'eval_precision@zho.rst.sctb': 0.017543859649122806, 'eval_recall@zho.rst.sctb': 0.05098684210526316, 'eval_loss@zho.rst.sctb': 2.6025469303131104, 'eval_runtime': 1.4587, 'eval_samples_per_second': 64.439, 'eval_steps_per_second': 2.057, 'epoch': 6.0}
{'train@zho.rst.sctb_loss': 2.4604766368865967, 'train@zho.rst.sctb_accuracy@zho.rst.sctb': 0.3462414578587699, 'train@zho.rst.sctb_f1@zho.rst.sctb': 0.023935733070348457, 'train@zho.rst.sctb_precision@zho.rst.sctb': 0.03870005963029218, 'train@zho.rst.sctb_recall@zho.rst.sctb': 0.04089068825910931, 'train@zho.rst.sctb_loss@zho.rst.sctb': 2.4604763984680176, 'train@zho.rst.sctb_runtime': 5.4567, 'train@zho.rst.sctb_samples_per_second': 80.451, 'train@zho.rst.sctb_steps_per_second': 2.566, 'epoch': 7.0}
{'loss': 2.5459, 'learning_rate': 8.333333333333334e-06, 'epoch': 7.0}
{'eval_loss': 2.535393476486206, 'eval_accuracy@zho.rst.sctb': 0.3404255319148936, 'eval_f1@zho.rst.sctb': 0.03185595567867036, 'eval_precision@zho.rst.sctb': 0.044050343249427915, 'eval_recall@zho.rst.sctb': 0.0540828173374613, 'eval_loss@zho.rst.sctb': 2.535393476486206, 'eval_runtime': 1.4335, 'eval_samples_per_second': 65.573, 'eval_steps_per_second': 2.093, 'epoch': 7.0}
{'train@zho.rst.sctb_loss': 2.4069740772247314, 'train@zho.rst.sctb_accuracy@zho.rst.sctb': 0.34851936218678814, 'train@zho.rst.sctb_f1@zho.rst.sctb': 0.024659977703455963, 'train@zho.rst.sctb_precision@zho.rst.sctb': 0.04001255155101309, 'train@zho.rst.sctb_recall@zho.rst.sctb': 0.04129554655870445, 'train@zho.rst.sctb_loss@zho.rst.sctb': 2.4069738388061523, 'train@zho.rst.sctb_runtime': 5.5085, 'train@zho.rst.sctb_samples_per_second': 79.695, 'train@zho.rst.sctb_steps_per_second': 2.542, 'epoch': 8.0}
{'loss': 2.4717, 'learning_rate': 6.666666666666667e-06, 'epoch': 8.0}
{'eval_loss': 2.492154121398926, 'eval_accuracy@zho.rst.sctb': 0.3617021276595745, 'eval_f1@zho.rst.sctb': 0.04132231404958678, 'eval_precision@zho.rst.sctb': 0.04991129509166174, 'eval_recall@zho.rst.sctb': 0.06027476780185759, 'eval_loss@zho.rst.sctb': 2.492154121398926, 'eval_runtime': 1.4257, 'eval_samples_per_second': 65.933, 'eval_steps_per_second': 2.104, 'epoch': 8.0}
{'train@zho.rst.sctb_loss': 2.3698434829711914, 'train@zho.rst.sctb_accuracy@zho.rst.sctb': 0.36674259681093396, 'train@zho.rst.sctb_f1@zho.rst.sctb': 0.030330844786371495, 'train@zho.rst.sctb_precision@zho.rst.sctb': 0.041345944223642066, 'train@zho.rst.sctb_recall@zho.rst.sctb': 0.044675836060118684, 'train@zho.rst.sctb_loss@zho.rst.sctb': 2.3698439598083496, 'train@zho.rst.sctb_runtime': 5.4929, 'train@zho.rst.sctb_samples_per_second': 79.922, 'train@zho.rst.sctb_steps_per_second': 2.549, 'epoch': 9.0}
{'loss': 2.4171, 'learning_rate': 5e-06, 'epoch': 9.0}
{'eval_loss': 2.4628875255584717, 'eval_accuracy@zho.rst.sctb': 0.35106382978723405, 'eval_f1@zho.rst.sctb': 0.039694825298540466, 'eval_precision@zho.rst.sctb': 0.04070521130412238, 'eval_recall@zho.rst.sctb': 0.058630030959752326, 'eval_loss@zho.rst.sctb': 2.46288800239563, 'eval_runtime': 1.4379, 'eval_samples_per_second': 65.373, 'eval_steps_per_second': 2.086, 'epoch': 9.0}
{'train@zho.rst.sctb_loss': 2.345651149749756, 'train@zho.rst.sctb_accuracy@zho.rst.sctb': 0.37813211845102507, 'train@zho.rst.sctb_f1@zho.rst.sctb': 0.03308816966278094, 'train@zho.rst.sctb_precision@zho.rst.sctb': 0.0414537102930711, 'train@zho.rst.sctb_recall@zho.rst.sctb': 0.04670012755809439, 'train@zho.rst.sctb_loss@zho.rst.sctb': 2.345651149749756, 'train@zho.rst.sctb_runtime': 5.4844, 'train@zho.rst.sctb_samples_per_second': 80.045, 'train@zho.rst.sctb_steps_per_second': 2.553, 'epoch': 10.0}
{'loss': 2.3858, 'learning_rate': 3.3333333333333333e-06, 'epoch': 10.0}
{'eval_loss': 2.445014476776123, 'eval_accuracy@zho.rst.sctb': 0.3404255319148936, 'eval_f1@zho.rst.sctb': 0.03850133809099019, 'eval_precision@zho.rst.sctb': 0.03748470012239902, 'eval_recall@zho.rst.sctb': 0.05698529411764706, 'eval_loss@zho.rst.sctb': 2.445014238357544, 'eval_runtime': 1.4305, 'eval_samples_per_second': 65.713, 'eval_steps_per_second': 2.097, 'epoch': 10.0}
{'train@zho.rst.sctb_loss': 2.333080768585205, 'train@zho.rst.sctb_accuracy@zho.rst.sctb': 0.3826879271070615, 'train@zho.rst.sctb_f1@zho.rst.sctb': 0.034028358416945376, 'train@zho.rst.sctb_precision@zho.rst.sctb': 0.040542790542790544, 'train@zho.rst.sctb_recall@zho.rst.sctb': 0.04750984415728468, 'train@zho.rst.sctb_loss@zho.rst.sctb': 2.333081007003784, 'train@zho.rst.sctb_runtime': 5.482, 'train@zho.rst.sctb_samples_per_second': 80.08, 'train@zho.rst.sctb_steps_per_second': 2.554, 'epoch': 11.0}
{'loss': 2.3727, 'learning_rate': 1.6666666666666667e-06, 'epoch': 11.0}
{'eval_loss': 2.4356062412261963, 'eval_accuracy@zho.rst.sctb': 0.3404255319148936, 'eval_f1@zho.rst.sctb': 0.03850133809099019, 'eval_precision@zho.rst.sctb': 0.03748470012239902, 'eval_recall@zho.rst.sctb': 0.05698529411764706, 'eval_loss@zho.rst.sctb': 2.4356071949005127, 'eval_runtime': 1.4407, 'eval_samples_per_second': 65.247, 'eval_steps_per_second': 2.082, 'epoch': 11.0}
{'train@zho.rst.sctb_loss': 2.329036235809326, 'train@zho.rst.sctb_accuracy@zho.rst.sctb': 0.3826879271070615, 'train@zho.rst.sctb_f1@zho.rst.sctb': 0.034028358416945376, 'train@zho.rst.sctb_precision@zho.rst.sctb': 0.040542790542790544, 'train@zho.rst.sctb_recall@zho.rst.sctb': 0.04750984415728468, 'train@zho.rst.sctb_loss@zho.rst.sctb': 2.3290367126464844, 'train@zho.rst.sctb_runtime': 5.5092, 'train@zho.rst.sctb_samples_per_second': 79.685, 'train@zho.rst.sctb_steps_per_second': 2.541, 'epoch': 12.0}
{'loss': 2.3558, 'learning_rate': 0.0, 'epoch': 12.0}
{'eval_loss': 2.432636260986328, 'eval_accuracy@zho.rst.sctb': 0.3404255319148936, 'eval_f1@zho.rst.sctb': 0.03850133809099019, 'eval_precision@zho.rst.sctb': 0.03748470012239902, 'eval_recall@zho.rst.sctb': 0.05698529411764706, 'eval_loss@zho.rst.sctb': 2.432636260986328, 'eval_runtime': 1.4579, 'eval_samples_per_second': 64.477, 'eval_steps_per_second': 2.058, 'epoch': 12.0}
{'train_runtime': 214.2555, 'train_samples_per_second': 24.587, 'train_steps_per_second': 0.784, 'train_loss': 2.711998848688035, 'epoch': 12.0}
***** train_log metrics *****
  epoch                    =       12.0
  train_loss               =      2.712
  train_runtime            = 0:03:34.25
  train_samples_per_second =     24.587
  train_steps_per_second   =      0.784
{'train@eng.rst.rstdt_loss': 1.7438278198242188, 'train@eng.rst.rstdt_accuracy@eng.rst.rstdt': 0.514185726784152, 'train@eng.rst.rstdt_f1@eng.rst.rstdt': 0.08591349243901542, 'train@eng.rst.rstdt_precision@eng.rst.rstdt': 0.15741367179671145, 'train@eng.rst.rstdt_recall@eng.rst.rstdt': 0.11182299089757664, 'train@eng.rst.rstdt_loss@eng.rst.rstdt': 1.7438278198242188, 'train@eng.rst.rstdt_runtime': 188.5585, 'train@eng.rst.rstdt_samples_per_second': 84.865, 'train@eng.rst.rstdt_steps_per_second': 2.657, 'epoch': 1.0}
{'loss': 2.169, 'learning_rate': 1.8333333333333333e-05, 'epoch': 1.0}
{'eval_loss': 1.7264212369918823, 'eval_accuracy@eng.rst.rstdt': 0.526218383713757, 'eval_f1@eng.rst.rstdt': 0.08725626278833871, 'eval_precision@eng.rst.rstdt': 0.09762085329236936, 'eval_recall@eng.rst.rstdt': 0.1109891873852033, 'eval_loss@eng.rst.rstdt': 1.7264208793640137, 'eval_runtime': 19.4258, 'eval_samples_per_second': 83.446, 'eval_steps_per_second': 2.625, 'epoch': 1.0}
{'train@eng.rst.rstdt_loss': 1.4244706630706787, 'train@eng.rst.rstdt_accuracy@eng.rst.rstdt': 0.6051118610173728, 'train@eng.rst.rstdt_f1@eng.rst.rstdt': 0.2049288057264358, 'train@eng.rst.rstdt_precision@eng.rst.rstdt': 0.3113908012529989, 'train@eng.rst.rstdt_recall@eng.rst.rstdt': 0.21093678190511123, 'train@eng.rst.rstdt_loss@eng.rst.rstdt': 1.4244707822799683, 'train@eng.rst.rstdt_runtime': 189.1793, 'train@eng.rst.rstdt_samples_per_second': 84.586, 'train@eng.rst.rstdt_steps_per_second': 2.648, 'epoch': 2.0}
{'loss': 1.6073, 'learning_rate': 1.6666666666666667e-05, 'epoch': 2.0}
{'eval_loss': 1.4307069778442383, 'eval_accuracy@eng.rst.rstdt': 0.6070326958667489, 'eval_f1@eng.rst.rstdt': 0.19944140564245375, 'eval_precision@eng.rst.rstdt': 0.22277232608774616, 'eval_recall@eng.rst.rstdt': 0.20681642949989196, 'eval_loss@eng.rst.rstdt': 1.4307070970535278, 'eval_runtime': 19.5087, 'eval_samples_per_second': 83.091, 'eval_steps_per_second': 2.614, 'epoch': 2.0}
{'train@eng.rst.rstdt_loss': 1.319498896598816, 'train@eng.rst.rstdt_accuracy@eng.rst.rstdt': 0.6326084239470067, 'train@eng.rst.rstdt_f1@eng.rst.rstdt': 0.2844458822649266, 'train@eng.rst.rstdt_precision@eng.rst.rstdt': 0.39550829940455356, 'train@eng.rst.rstdt_recall@eng.rst.rstdt': 0.2705158671579718, 'train@eng.rst.rstdt_loss@eng.rst.rstdt': 1.3194990158081055, 'train@eng.rst.rstdt_runtime': 188.9204, 'train@eng.rst.rstdt_samples_per_second': 84.702, 'train@eng.rst.rstdt_steps_per_second': 2.652, 'epoch': 3.0}
{'loss': 1.4266, 'learning_rate': 1.5000000000000002e-05, 'epoch': 3.0}
{'eval_loss': 1.3477661609649658, 'eval_accuracy@eng.rst.rstdt': 0.6323257248611968, 'eval_f1@eng.rst.rstdt': 0.28735895195369404, 'eval_precision@eng.rst.rstdt': 0.4102369057920351, 'eval_recall@eng.rst.rstdt': 0.27726288412248246, 'eval_loss@eng.rst.rstdt': 1.347766399383545, 'eval_runtime': 19.4253, 'eval_samples_per_second': 83.448, 'eval_steps_per_second': 2.625, 'epoch': 3.0}
{'train@eng.rst.rstdt_loss': 1.250733494758606, 'train@eng.rst.rstdt_accuracy@eng.rst.rstdt': 0.6470441194850644, 'train@eng.rst.rstdt_f1@eng.rst.rstdt': 0.33244789741185815, 'train@eng.rst.rstdt_precision@eng.rst.rstdt': 0.44171369814472505, 'train@eng.rst.rstdt_recall@eng.rst.rstdt': 0.31523444539629264, 'train@eng.rst.rstdt_loss@eng.rst.rstdt': 1.2507336139678955, 'train@eng.rst.rstdt_runtime': 188.74, 'train@eng.rst.rstdt_samples_per_second': 84.783, 'train@eng.rst.rstdt_steps_per_second': 2.654, 'epoch': 4.0}
{'loss': 1.3348, 'learning_rate': 1.3333333333333333e-05, 'epoch': 4.0}
{'eval_loss': 1.3069912195205688, 'eval_accuracy@eng.rst.rstdt': 0.6366440468846392, 'eval_f1@eng.rst.rstdt': 0.32042084929158293, 'eval_precision@eng.rst.rstdt': 0.4163859060267895, 'eval_recall@eng.rst.rstdt': 0.31571916442349446, 'eval_loss@eng.rst.rstdt': 1.3069912195205688, 'eval_runtime': 19.4499, 'eval_samples_per_second': 83.342, 'eval_steps_per_second': 2.622, 'epoch': 4.0}
{'train@eng.rst.rstdt_loss': 1.2017009258270264, 'train@eng.rst.rstdt_accuracy@eng.rst.rstdt': 0.6557305336832896, 'train@eng.rst.rstdt_f1@eng.rst.rstdt': 0.3498256860071652, 'train@eng.rst.rstdt_precision@eng.rst.rstdt': 0.4464194118818997, 'train@eng.rst.rstdt_recall@eng.rst.rstdt': 0.32948374029270505, 'train@eng.rst.rstdt_loss@eng.rst.rstdt': 1.2017009258270264, 'train@eng.rst.rstdt_runtime': 189.1786, 'train@eng.rst.rstdt_samples_per_second': 84.587, 'train@eng.rst.rstdt_steps_per_second': 2.648, 'epoch': 5.0}
{'loss': 1.28, 'learning_rate': 1.1666666666666668e-05, 'epoch': 5.0}
{'eval_loss': 1.266265869140625, 'eval_accuracy@eng.rst.rstdt': 0.6434299814929056, 'eval_f1@eng.rst.rstdt': 0.3250623667443362, 'eval_precision@eng.rst.rstdt': 0.4203484463630655, 'eval_recall@eng.rst.rstdt': 0.32091326425080247, 'eval_loss@eng.rst.rstdt': 1.2662657499313354, 'eval_runtime': 19.4803, 'eval_samples_per_second': 83.212, 'eval_steps_per_second': 2.618, 'epoch': 5.0}
{'train@eng.rst.rstdt_loss': 1.1669800281524658, 'train@eng.rst.rstdt_accuracy@eng.rst.rstdt': 0.6622922134733158, 'train@eng.rst.rstdt_f1@eng.rst.rstdt': 0.3668978984852746, 'train@eng.rst.rstdt_precision@eng.rst.rstdt': 0.5051988859963352, 'train@eng.rst.rstdt_recall@eng.rst.rstdt': 0.34070547054312555, 'train@eng.rst.rstdt_loss@eng.rst.rstdt': 1.1669800281524658, 'train@eng.rst.rstdt_runtime': 188.6023, 'train@eng.rst.rstdt_samples_per_second': 84.845, 'train@eng.rst.rstdt_steps_per_second': 2.656, 'epoch': 6.0}
{'loss': 1.2337, 'learning_rate': 1e-05, 'epoch': 6.0}
{'eval_loss': 1.239493727684021, 'eval_accuracy@eng.rst.rstdt': 0.6452806909315237, 'eval_f1@eng.rst.rstdt': 0.32993660559277044, 'eval_precision@eng.rst.rstdt': 0.41328219202457855, 'eval_recall@eng.rst.rstdt': 0.3262138419452638, 'eval_loss@eng.rst.rstdt': 1.239493727684021, 'eval_runtime': 19.4419, 'eval_samples_per_second': 83.377, 'eval_steps_per_second': 2.623, 'epoch': 6.0}
{'train@eng.rst.rstdt_loss': 1.145716667175293, 'train@eng.rst.rstdt_accuracy@eng.rst.rstdt': 0.6652293463317085, 'train@eng.rst.rstdt_f1@eng.rst.rstdt': 0.3795784207388981, 'train@eng.rst.rstdt_precision@eng.rst.rstdt': 0.5000424749722125, 'train@eng.rst.rstdt_recall@eng.rst.rstdt': 0.3495738329256346, 'train@eng.rst.rstdt_loss@eng.rst.rstdt': 1.1457167863845825, 'train@eng.rst.rstdt_runtime': 189.1422, 'train@eng.rst.rstdt_samples_per_second': 84.603, 'train@eng.rst.rstdt_steps_per_second': 2.649, 'epoch': 7.0}
{'loss': 1.2114, 'learning_rate': 8.333333333333334e-06, 'epoch': 7.0}
{'eval_loss': 1.2208753824234009, 'eval_accuracy@eng.rst.rstdt': 0.6440468846391116, 'eval_f1@eng.rst.rstdt': 0.3328288501252182, 'eval_precision@eng.rst.rstdt': 0.4125969888030909, 'eval_recall@eng.rst.rstdt': 0.32828056540783784, 'eval_loss@eng.rst.rstdt': 1.2208753824234009, 'eval_runtime': 19.4847, 'eval_samples_per_second': 83.193, 'eval_steps_per_second': 2.617, 'epoch': 7.0}
{'train@eng.rst.rstdt_loss': 1.1290665864944458, 'train@eng.rst.rstdt_accuracy@eng.rst.rstdt': 0.6674790651168604, 'train@eng.rst.rstdt_f1@eng.rst.rstdt': 0.38960480008055337, 'train@eng.rst.rstdt_precision@eng.rst.rstdt': 0.5219911006649288, 'train@eng.rst.rstdt_recall@eng.rst.rstdt': 0.3599946166359672, 'train@eng.rst.rstdt_loss@eng.rst.rstdt': 1.1290664672851562, 'train@eng.rst.rstdt_runtime': 189.0537, 'train@eng.rst.rstdt_samples_per_second': 84.643, 'train@eng.rst.rstdt_steps_per_second': 2.65, 'epoch': 8.0}
{'loss': 1.1893, 'learning_rate': 6.666666666666667e-06, 'epoch': 8.0}
{'eval_loss': 1.2150882482528687, 'eval_accuracy@eng.rst.rstdt': 0.6446637877853177, 'eval_f1@eng.rst.rstdt': 0.3487922717040115, 'eval_precision@eng.rst.rstdt': 0.4752911296481608, 'eval_recall@eng.rst.rstdt': 0.3398913112963988, 'eval_loss@eng.rst.rstdt': 1.2150882482528687, 'eval_runtime': 19.499, 'eval_samples_per_second': 83.133, 'eval_steps_per_second': 2.616, 'epoch': 8.0}
{'train@eng.rst.rstdt_loss': 1.1170949935913086, 'train@eng.rst.rstdt_accuracy@eng.rst.rstdt': 0.6703537057867767, 'train@eng.rst.rstdt_f1@eng.rst.rstdt': 0.39475330289030053, 'train@eng.rst.rstdt_precision@eng.rst.rstdt': 0.5856146866702069, 'train@eng.rst.rstdt_recall@eng.rst.rstdt': 0.3628791194114252, 'train@eng.rst.rstdt_loss@eng.rst.rstdt': 1.1170951128005981, 'train@eng.rst.rstdt_runtime': 188.4766, 'train@eng.rst.rstdt_samples_per_second': 84.902, 'train@eng.rst.rstdt_steps_per_second': 2.658, 'epoch': 9.0}
{'loss': 1.1729, 'learning_rate': 5e-06, 'epoch': 9.0}
{'eval_loss': 1.2042672634124756, 'eval_accuracy@eng.rst.rstdt': 0.6465144972239358, 'eval_f1@eng.rst.rstdt': 0.3537582518588147, 'eval_precision@eng.rst.rstdt': 0.5278732956432681, 'eval_recall@eng.rst.rstdt': 0.3427272233416175, 'eval_loss@eng.rst.rstdt': 1.2042672634124756, 'eval_runtime': 19.4063, 'eval_samples_per_second': 83.529, 'eval_steps_per_second': 2.628, 'epoch': 9.0}
{'train@eng.rst.rstdt_loss': 1.1120548248291016, 'train@eng.rst.rstdt_accuracy@eng.rst.rstdt': 0.6693538307711536, 'train@eng.rst.rstdt_f1@eng.rst.rstdt': 0.40120639543870124, 'train@eng.rst.rstdt_precision@eng.rst.rstdt': 0.566605076445496, 'train@eng.rst.rstdt_recall@eng.rst.rstdt': 0.37226892296405256, 'train@eng.rst.rstdt_loss@eng.rst.rstdt': 1.1120548248291016, 'train@eng.rst.rstdt_runtime': 189.2008, 'train@eng.rst.rstdt_samples_per_second': 84.577, 'train@eng.rst.rstdt_steps_per_second': 2.648, 'epoch': 10.0}
{'loss': 1.165, 'learning_rate': 3.3333333333333333e-06, 'epoch': 10.0}
{'eval_loss': 1.208701252937317, 'eval_accuracy@eng.rst.rstdt': 0.6378778531770513, 'eval_f1@eng.rst.rstdt': 0.3529606664682579, 'eval_precision@eng.rst.rstdt': 0.45483735779989515, 'eval_recall@eng.rst.rstdt': 0.3457073813566244, 'eval_loss@eng.rst.rstdt': 1.2087011337280273, 'eval_runtime': 19.4713, 'eval_samples_per_second': 83.251, 'eval_steps_per_second': 2.619, 'epoch': 10.0}
{'train@eng.rst.rstdt_loss': 1.1051362752914429, 'train@eng.rst.rstdt_accuracy@eng.rst.rstdt': 0.6705411823522059, 'train@eng.rst.rstdt_f1@eng.rst.rstdt': 0.4000810777944215, 'train@eng.rst.rstdt_precision@eng.rst.rstdt': 0.5761037071010542, 'train@eng.rst.rstdt_recall@eng.rst.rstdt': 0.3684591424463987, 'train@eng.rst.rstdt_loss@eng.rst.rstdt': 1.1051362752914429, 'train@eng.rst.rstdt_runtime': 189.0674, 'train@eng.rst.rstdt_samples_per_second': 84.636, 'train@eng.rst.rstdt_steps_per_second': 2.65, 'epoch': 11.0}
{'loss': 1.1569, 'learning_rate': 1.6666666666666667e-06, 'epoch': 11.0}
{'eval_loss': 1.199275016784668, 'eval_accuracy@eng.rst.rstdt': 0.6452806909315237, 'eval_f1@eng.rst.rstdt': 0.357373787708929, 'eval_precision@eng.rst.rstdt': 0.5225356272836823, 'eval_recall@eng.rst.rstdt': 0.3458417219364614, 'eval_loss@eng.rst.rstdt': 1.199275016784668, 'eval_runtime': 19.4262, 'eval_samples_per_second': 83.444, 'eval_steps_per_second': 2.625, 'epoch': 11.0}
{'train@eng.rst.rstdt_loss': 1.1036614179611206, 'train@eng.rst.rstdt_accuracy@eng.rst.rstdt': 0.6709161354830646, 'train@eng.rst.rstdt_f1@eng.rst.rstdt': 0.40179830166450825, 'train@eng.rst.rstdt_precision@eng.rst.rstdt': 0.5763063350720811, 'train@eng.rst.rstdt_recall@eng.rst.rstdt': 0.3697109647606467, 'train@eng.rst.rstdt_loss@eng.rst.rstdt': 1.1036614179611206, 'train@eng.rst.rstdt_runtime': 194.1408, 'train@eng.rst.rstdt_samples_per_second': 82.425, 'train@eng.rst.rstdt_steps_per_second': 2.581, 'epoch': 12.0}
{'loss': 1.1521, 'learning_rate': 0.0, 'epoch': 12.0}
{'eval_loss': 1.1976814270019531, 'eval_accuracy@eng.rst.rstdt': 0.6446637877853177, 'eval_f1@eng.rst.rstdt': 0.3572659641571046, 'eval_precision@eng.rst.rstdt': 0.4929684571389655, 'eval_recall@eng.rst.rstdt': 0.3457513632123266, 'eval_loss@eng.rst.rstdt': 1.1976814270019531, 'eval_runtime': 19.4949, 'eval_samples_per_second': 83.15, 'eval_steps_per_second': 2.616, 'epoch': 12.0}
{'train_runtime': 7288.0126, 'train_samples_per_second': 26.348, 'train_steps_per_second': 0.825, 'train_loss': 1.3415906021615622, 'epoch': 12.0}
***** train_logger metrics *****
  epoch                    =       12.0
  train_loss               =      2.712
  train_runtime            = 0:03:34.25
  train_samples_per_second =     24.587
  train_steps_per_second   =      0.784

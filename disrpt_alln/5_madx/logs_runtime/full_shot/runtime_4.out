-------------------------------------------------------------------
Lang1:  deu.rst.pcc    Lang2:  eng.sdrt.stac
Saving run to:  runs/full_shot/FullShot=v4_deu.rst.pcc_eng.sdrt.stac_lrfunc=Adafactor_lr=2e-05_plm=bert-base-multilingual-cased
Running with params: BERT_MODEL=bert-base-multilingual-cased lr=2e-05
read 2164 examples
read 241 examples
read 260 examples
read 9580 examples
read 1145 examples
read 1510 examples
Total prediction labels:  42
BertAdapterModel(
  (shared_parameters): ModuleDict()
  (bert): BertModel(
    (shared_parameters): ModuleDict()
    (invertible_adapters): ModuleDict(
      (de): NICECouplingBlock(
        (F): Sequential(
          (0): Linear(in_features=384, out_features=192, bias=True)
          (1): Activation_Function_Class(
            (f): NewGELUActivation()
          )
          (2): Linear(in_features=192, out_features=384, bias=True)
        )
        (G): Sequential(
          (0): Linear(in_features=384, out_features=192, bias=True)
          (1): Activation_Function_Class(
            (f): NewGELUActivation()
          )
          (2): Linear(in_features=192, out_features=384, bias=True)
        )
      )
      (en): NICECouplingBlock(
        (F): Sequential(
          (0): Linear(in_features=384, out_features=192, bias=True)
          (1): Activation_Function_Class(
            (f): ReLU()
          )
          (2): Linear(in_features=192, out_features=384, bias=True)
        )
        (G): Sequential(
          (0): Linear(in_features=384, out_features=192, bias=True)
          (1): Activation_Function_Class(
            (f): ReLU()
          )
          (2): Linear(in_features=192, out_features=384, bias=True)
        )
      )
    )
    (embeddings): BertEmbeddings(
      (word_embeddings): Embedding(119547, 768, padding_idx=0)
      (position_embeddings): Embedding(512, 768)
      (token_type_embeddings): Embedding(2, 768)
      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
      (dropout): Dropout(p=0.1, inplace=False)
    )
    (encoder): BertEncoder(
      (layer): ModuleList(
        (0): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (de): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (en): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (1): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (de): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (en): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (2): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (de): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (en): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (3): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (de): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (en): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (4): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (de): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (en): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (5): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (de): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (en): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (6): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (de): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (en): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (7): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (de): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (en): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (8): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (de): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (en): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (9): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (de): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (en): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (10): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (de): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (en): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (11): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (de): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (en): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
      )
    )
    (pooler): BertPooler(
      (dense): Linear(in_features=768, out_features=768, bias=True)
      (activation): Tanh()
    )
    (prefix_tuning): PrefixTuningPool(
      (prefix_tunings): ModuleDict()
    )
  )
  (heads): ModuleDict(
    (disrpt): ClassificationHead(
      (0): Dropout(p=0.1, inplace=False)
      (1): Linear(in_features=768, out_features=768, bias=True)
      (2): Activation_Function_Class(
        (f): Tanh()
      )
      (3): Dropout(p=0.1, inplace=False)
      (4): Linear(in_features=768, out_features=42, bias=True)
    )
  )
)
{'train@deu.rst.pcc_loss': 3.2917287349700928, 'train@deu.rst.pcc_accuracy@deu.rst.pcc': 0.09981515711645102, 'train@deu.rst.pcc_f1@deu.rst.pcc': 0.009162006621678428, 'train@deu.rst.pcc_precision@deu.rst.pcc': 0.028161463260801007, 'train@deu.rst.pcc_recall@deu.rst.pcc': 0.03961229843773647, 'train@deu.rst.pcc_loss@deu.rst.pcc': 3.291729211807251, 'train@deu.rst.pcc_runtime': 27.0337, 'train@deu.rst.pcc_samples_per_second': 80.048, 'train@deu.rst.pcc_steps_per_second': 2.515, 'epoch': 1.0}
{'loss': 3.52, 'learning_rate': 1.8333333333333333e-05, 'epoch': 1.0}
{'eval_loss': 3.3014299869537354, 'eval_accuracy@deu.rst.pcc': 0.11618257261410789, 'eval_f1@deu.rst.pcc': 0.008974358974358975, 'eval_precision@deu.rst.pcc': 0.005028735632183908, 'eval_recall@deu.rst.pcc': 0.041666666666666664, 'eval_loss@deu.rst.pcc': 3.301429510116577, 'eval_runtime': 3.4358, 'eval_samples_per_second': 70.145, 'eval_steps_per_second': 2.328, 'epoch': 1.0}
{'train@deu.rst.pcc_loss': 3.0039680004119873, 'train@deu.rst.pcc_accuracy@deu.rst.pcc': 0.11044362292051756, 'train@deu.rst.pcc_f1@deu.rst.pcc': 0.016182566675372653, 'train@deu.rst.pcc_precision@deu.rst.pcc': 0.0425750344154642, 'train@deu.rst.pcc_recall@deu.rst.pcc': 0.044246532495776605, 'train@deu.rst.pcc_loss@deu.rst.pcc': 3.0039680004119873, 'train@deu.rst.pcc_runtime': 26.759, 'train@deu.rst.pcc_samples_per_second': 80.87, 'train@deu.rst.pcc_steps_per_second': 2.541, 'epoch': 2.0}
{'loss': 3.1405, 'learning_rate': 1.6666666666666667e-05, 'epoch': 2.0}
{'eval_loss': 3.030216693878174, 'eval_accuracy@deu.rst.pcc': 0.12448132780082988, 'eval_f1@deu.rst.pcc': 0.01359180035650624, 'eval_precision@deu.rst.pcc': 0.00876068376068376, 'eval_recall@deu.rst.pcc': 0.0462962962962963, 'eval_loss@deu.rst.pcc': 3.030216932296753, 'eval_runtime': 3.45, 'eval_samples_per_second': 69.855, 'eval_steps_per_second': 2.319, 'epoch': 2.0}
{'train@deu.rst.pcc_loss': 2.918858766555786, 'train@deu.rst.pcc_accuracy@deu.rst.pcc': 0.13123844731977818, 'train@deu.rst.pcc_f1@deu.rst.pcc': 0.029790536056808474, 'train@deu.rst.pcc_precision@deu.rst.pcc': 0.05190961794747769, 'train@deu.rst.pcc_recall@deu.rst.pcc': 0.05436113878342097, 'train@deu.rst.pcc_loss@deu.rst.pcc': 2.9188590049743652, 'train@deu.rst.pcc_runtime': 26.6571, 'train@deu.rst.pcc_samples_per_second': 81.179, 'train@deu.rst.pcc_steps_per_second': 2.551, 'epoch': 3.0}
{'loss': 2.9891, 'learning_rate': 1.5000000000000002e-05, 'epoch': 3.0}
{'eval_loss': 2.9607560634613037, 'eval_accuracy@deu.rst.pcc': 0.14107883817427386, 'eval_f1@deu.rst.pcc': 0.024034371327280788, 'eval_precision@deu.rst.pcc': 0.01707153607551448, 'eval_recall@deu.rst.pcc': 0.05644586894586895, 'eval_loss@deu.rst.pcc': 2.9607560634613037, 'eval_runtime': 3.43, 'eval_samples_per_second': 70.263, 'eval_steps_per_second': 2.332, 'epoch': 3.0}
{'train@deu.rst.pcc_loss': 2.857097864151001, 'train@deu.rst.pcc_accuracy@deu.rst.pcc': 0.18345656192236598, 'train@deu.rst.pcc_f1@deu.rst.pcc': 0.07115146325891733, 'train@deu.rst.pcc_precision@deu.rst.pcc': 0.10736200769611795, 'train@deu.rst.pcc_recall@deu.rst.pcc': 0.09729494956954514, 'train@deu.rst.pcc_loss@deu.rst.pcc': 2.857097864151001, 'train@deu.rst.pcc_runtime': 26.6668, 'train@deu.rst.pcc_samples_per_second': 81.149, 'train@deu.rst.pcc_steps_per_second': 2.55, 'epoch': 4.0}
{'loss': 2.9203, 'learning_rate': 1.3333333333333333e-05, 'epoch': 4.0}
{'eval_loss': 2.9075515270233154, 'eval_accuracy@deu.rst.pcc': 0.17012448132780084, 'eval_f1@deu.rst.pcc': 0.053559515473828694, 'eval_precision@deu.rst.pcc': 0.04325882450882451, 'eval_recall@deu.rst.pcc': 0.0888341982091982, 'eval_loss@deu.rst.pcc': 2.9075515270233154, 'eval_runtime': 3.3702, 'eval_samples_per_second': 71.508, 'eval_steps_per_second': 2.374, 'epoch': 4.0}
{'train@deu.rst.pcc_loss': 2.808685302734375, 'train@deu.rst.pcc_accuracy@deu.rst.pcc': 0.19963031423290203, 'train@deu.rst.pcc_f1@deu.rst.pcc': 0.07354701320278406, 'train@deu.rst.pcc_precision@deu.rst.pcc': 0.12549139573489634, 'train@deu.rst.pcc_recall@deu.rst.pcc': 0.11477336424437572, 'train@deu.rst.pcc_loss@deu.rst.pcc': 2.808685302734375, 'train@deu.rst.pcc_runtime': 26.7017, 'train@deu.rst.pcc_samples_per_second': 81.043, 'train@deu.rst.pcc_steps_per_second': 2.547, 'epoch': 5.0}
{'loss': 2.8647, 'learning_rate': 1.1666666666666668e-05, 'epoch': 5.0}
{'eval_loss': 2.8651928901672363, 'eval_accuracy@deu.rst.pcc': 0.17842323651452283, 'eval_f1@deu.rst.pcc': 0.060964800343637236, 'eval_precision@deu.rst.pcc': 0.08248103185920096, 'eval_recall@deu.rst.pcc': 0.10964845339845342, 'eval_loss@deu.rst.pcc': 2.8651926517486572, 'eval_runtime': 3.411, 'eval_samples_per_second': 70.654, 'eval_steps_per_second': 2.345, 'epoch': 5.0}
{'train@deu.rst.pcc_loss': 2.76735258102417, 'train@deu.rst.pcc_accuracy@deu.rst.pcc': 0.20378927911275416, 'train@deu.rst.pcc_f1@deu.rst.pcc': 0.07513590086407669, 'train@deu.rst.pcc_precision@deu.rst.pcc': 0.10384185770184882, 'train@deu.rst.pcc_recall@deu.rst.pcc': 0.12049894329076824, 'train@deu.rst.pcc_loss@deu.rst.pcc': 2.767352819442749, 'train@deu.rst.pcc_runtime': 26.7243, 'train@deu.rst.pcc_samples_per_second': 80.975, 'train@deu.rst.pcc_steps_per_second': 2.545, 'epoch': 6.0}
{'loss': 2.8231, 'learning_rate': 1e-05, 'epoch': 6.0}
{'eval_loss': 2.8280727863311768, 'eval_accuracy@deu.rst.pcc': 0.1908713692946058, 'eval_f1@deu.rst.pcc': 0.06567357408913387, 'eval_precision@deu.rst.pcc': 0.08098520646736655, 'eval_recall@deu.rst.pcc': 0.12416056166056166, 'eval_loss@deu.rst.pcc': 2.828073024749756, 'eval_runtime': 3.3795, 'eval_samples_per_second': 71.312, 'eval_steps_per_second': 2.367, 'epoch': 6.0}
{'train@deu.rst.pcc_loss': 2.7361602783203125, 'train@deu.rst.pcc_accuracy@deu.rst.pcc': 0.20841035120147874, 'train@deu.rst.pcc_f1@deu.rst.pcc': 0.07766379254639973, 'train@deu.rst.pcc_precision@deu.rst.pcc': 0.09093979825258199, 'train@deu.rst.pcc_recall@deu.rst.pcc': 0.12340975313742097, 'train@deu.rst.pcc_loss@deu.rst.pcc': 2.7361602783203125, 'train@deu.rst.pcc_runtime': 26.7466, 'train@deu.rst.pcc_samples_per_second': 80.908, 'train@deu.rst.pcc_steps_per_second': 2.542, 'epoch': 7.0}
{'loss': 2.7897, 'learning_rate': 8.333333333333334e-06, 'epoch': 7.0}
{'eval_loss': 2.8016929626464844, 'eval_accuracy@deu.rst.pcc': 0.18672199170124482, 'eval_f1@deu.rst.pcc': 0.06470982741126469, 'eval_precision@deu.rst.pcc': 0.0810485671594491, 'eval_recall@deu.rst.pcc': 0.12089183964183964, 'eval_loss@deu.rst.pcc': 2.8016929626464844, 'eval_runtime': 3.9801, 'eval_samples_per_second': 60.551, 'eval_steps_per_second': 2.01, 'epoch': 7.0}
{'train@deu.rst.pcc_loss': 2.712250232696533, 'train@deu.rst.pcc_accuracy@deu.rst.pcc': 0.21303142329020333, 'train@deu.rst.pcc_f1@deu.rst.pcc': 0.08104932479678796, 'train@deu.rst.pcc_precision@deu.rst.pcc': 0.10218206453123872, 'train@deu.rst.pcc_recall@deu.rst.pcc': 0.12654689771458869, 'train@deu.rst.pcc_loss@deu.rst.pcc': 2.712250232696533, 'train@deu.rst.pcc_runtime': 26.8624, 'train@deu.rst.pcc_samples_per_second': 80.559, 'train@deu.rst.pcc_steps_per_second': 2.531, 'epoch': 8.0}
{'loss': 2.7597, 'learning_rate': 6.666666666666667e-06, 'epoch': 8.0}
{'eval_loss': 2.7830188274383545, 'eval_accuracy@deu.rst.pcc': 0.1950207468879668, 'eval_f1@deu.rst.pcc': 0.06731687091710133, 'eval_precision@deu.rst.pcc': 0.06992846390379598, 'eval_recall@deu.rst.pcc': 0.12647537647537646, 'eval_loss@deu.rst.pcc': 2.7830188274383545, 'eval_runtime': 3.4225, 'eval_samples_per_second': 70.415, 'eval_steps_per_second': 2.337, 'epoch': 8.0}
{'train@deu.rst.pcc_loss': 2.6937742233276367, 'train@deu.rst.pcc_accuracy@deu.rst.pcc': 0.21903881700554528, 'train@deu.rst.pcc_f1@deu.rst.pcc': 0.08566885320424103, 'train@deu.rst.pcc_precision@deu.rst.pcc': 0.10769803205949993, 'train@deu.rst.pcc_recall@deu.rst.pcc': 0.1306830781718693, 'train@deu.rst.pcc_loss@deu.rst.pcc': 2.6937739849090576, 'train@deu.rst.pcc_runtime': 26.6586, 'train@deu.rst.pcc_samples_per_second': 81.174, 'train@deu.rst.pcc_steps_per_second': 2.551, 'epoch': 9.0}
{'loss': 2.7403, 'learning_rate': 5e-06, 'epoch': 9.0}
{'eval_loss': 2.7707033157348633, 'eval_accuracy@deu.rst.pcc': 0.1908713692946058, 'eval_f1@deu.rst.pcc': 0.06775827792628408, 'eval_precision@deu.rst.pcc': 0.06926070586086795, 'eval_recall@deu.rst.pcc': 0.12498728123728124, 'eval_loss@deu.rst.pcc': 2.7707035541534424, 'eval_runtime': 3.3956, 'eval_samples_per_second': 70.974, 'eval_steps_per_second': 2.356, 'epoch': 9.0}
{'train@deu.rst.pcc_loss': 2.6810765266418457, 'train@deu.rst.pcc_accuracy@deu.rst.pcc': 0.22273567467652494, 'train@deu.rst.pcc_f1@deu.rst.pcc': 0.08847769668288243, 'train@deu.rst.pcc_precision@deu.rst.pcc': 0.1074211691692202, 'train@deu.rst.pcc_recall@deu.rst.pcc': 0.13348572369089715, 'train@deu.rst.pcc_loss@deu.rst.pcc': 2.6810765266418457, 'train@deu.rst.pcc_runtime': 26.6036, 'train@deu.rst.pcc_samples_per_second': 81.342, 'train@deu.rst.pcc_steps_per_second': 2.556, 'epoch': 10.0}
{'loss': 2.7247, 'learning_rate': 3.3333333333333333e-06, 'epoch': 10.0}
{'eval_loss': 2.7578225135803223, 'eval_accuracy@deu.rst.pcc': 0.1991701244813278, 'eval_f1@deu.rst.pcc': 0.07278058147799939, 'eval_precision@deu.rst.pcc': 0.07306010530695595, 'eval_recall@deu.rst.pcc': 0.1296805046805047, 'eval_loss@deu.rst.pcc': 2.757822275161743, 'eval_runtime': 3.4447, 'eval_samples_per_second': 69.962, 'eval_steps_per_second': 2.322, 'epoch': 10.0}
{'train@deu.rst.pcc_loss': 2.673401355743408, 'train@deu.rst.pcc_accuracy@deu.rst.pcc': 0.2231977818853974, 'train@deu.rst.pcc_f1@deu.rst.pcc': 0.08887030076204594, 'train@deu.rst.pcc_precision@deu.rst.pcc': 0.10037560649310237, 'train@deu.rst.pcc_recall@deu.rst.pcc': 0.1339473416053558, 'train@deu.rst.pcc_loss@deu.rst.pcc': 2.673401355743408, 'train@deu.rst.pcc_runtime': 26.7171, 'train@deu.rst.pcc_samples_per_second': 80.997, 'train@deu.rst.pcc_steps_per_second': 2.545, 'epoch': 11.0}
{'loss': 2.7127, 'learning_rate': 1.6666666666666667e-06, 'epoch': 11.0}
{'eval_loss': 2.7518999576568604, 'eval_accuracy@deu.rst.pcc': 0.1991701244813278, 'eval_f1@deu.rst.pcc': 0.07334400682560062, 'eval_precision@deu.rst.pcc': 0.074009222333001, 'eval_recall@deu.rst.pcc': 0.12879019129019129, 'eval_loss@deu.rst.pcc': 2.7518999576568604, 'eval_runtime': 3.3663, 'eval_samples_per_second': 71.592, 'eval_steps_per_second': 2.377, 'epoch': 11.0}
{'train@deu.rst.pcc_loss': 2.67091703414917, 'train@deu.rst.pcc_accuracy@deu.rst.pcc': 0.2231977818853974, 'train@deu.rst.pcc_f1@deu.rst.pcc': 0.08935310042378758, 'train@deu.rst.pcc_precision@deu.rst.pcc': 0.10088201359747163, 'train@deu.rst.pcc_recall@deu.rst.pcc': 0.13443772022010506, 'train@deu.rst.pcc_loss@deu.rst.pcc': 2.67091703414917, 'train@deu.rst.pcc_runtime': 26.6373, 'train@deu.rst.pcc_samples_per_second': 81.239, 'train@deu.rst.pcc_steps_per_second': 2.553, 'epoch': 12.0}
{'loss': 2.7068, 'learning_rate': 0.0, 'epoch': 12.0}
{'eval_loss': 2.7493081092834473, 'eval_accuracy@deu.rst.pcc': 0.1991701244813278, 'eval_f1@deu.rst.pcc': 0.07369359263375491, 'eval_precision@deu.rst.pcc': 0.073911392042197, 'eval_recall@deu.rst.pcc': 0.1296805046805047, 'eval_loss@deu.rst.pcc': 2.7493083477020264, 'eval_runtime': 3.3997, 'eval_samples_per_second': 70.888, 'eval_steps_per_second': 2.353, 'epoch': 12.0}
{'train_runtime': 1040.6594, 'train_samples_per_second': 24.953, 'train_steps_per_second': 0.784, 'train_loss': 2.890965798321892, 'epoch': 12.0}
***** train_log metrics *****
  epoch                    =       12.0
  train_loss               =      2.891
  train_runtime            = 0:17:20.65
  train_samples_per_second =     24.953
  train_steps_per_second   =      0.784
{'train@eng.sdrt.stac_loss': 2.0646777153015137, 'train@eng.sdrt.stac_accuracy@eng.sdrt.stac': 0.35177453027139877, 'train@eng.sdrt.stac_f1@eng.sdrt.stac': 0.06827452513304882, 'train@eng.sdrt.stac_precision@eng.sdrt.stac': 0.13075078545236846, 'train@eng.sdrt.stac_recall@eng.sdrt.stac': 0.11058234799264802, 'train@eng.sdrt.stac_loss@eng.sdrt.stac': 2.0646774768829346, 'train@eng.sdrt.stac_runtime': 114.7277, 'train@eng.sdrt.stac_samples_per_second': 83.502, 'train@eng.sdrt.stac_steps_per_second': 2.615, 'epoch': 1.0}
{'loss': 2.4409, 'learning_rate': 1.8333333333333333e-05, 'epoch': 1.0}
{'eval_loss': 2.018702983856201, 'eval_accuracy@eng.sdrt.stac': 0.359825327510917, 'eval_f1@eng.sdrt.stac': 0.06753220611916264, 'eval_precision@eng.sdrt.stac': 0.06625481901906188, 'eval_recall@eng.sdrt.stac': 0.1123562482242085, 'eval_loss@eng.sdrt.stac': 2.018702983856201, 'eval_runtime': 14.2181, 'eval_samples_per_second': 80.531, 'eval_steps_per_second': 2.532, 'epoch': 1.0}
{'train@eng.sdrt.stac_loss': 1.7986702919006348, 'train@eng.sdrt.stac_accuracy@eng.sdrt.stac': 0.4364300626304802, 'train@eng.sdrt.stac_f1@eng.sdrt.stac': 0.1412588714367531, 'train@eng.sdrt.stac_precision@eng.sdrt.stac': 0.14820775875046444, 'train@eng.sdrt.stac_recall@eng.sdrt.stac': 0.18283095374970573, 'train@eng.sdrt.stac_loss@eng.sdrt.stac': 1.7986702919006348, 'train@eng.sdrt.stac_runtime': 114.3831, 'train@eng.sdrt.stac_samples_per_second': 83.754, 'train@eng.sdrt.stac_steps_per_second': 2.623, 'epoch': 2.0}
{'loss': 1.9516, 'learning_rate': 1.6666666666666667e-05, 'epoch': 2.0}
{'eval_loss': 1.755558967590332, 'eval_accuracy@eng.sdrt.stac': 0.4366812227074236, 'eval_f1@eng.sdrt.stac': 0.1329849838186265, 'eval_precision@eng.sdrt.stac': 0.13086315037856944, 'eval_recall@eng.sdrt.stac': 0.18004147794487954, 'eval_loss@eng.sdrt.stac': 1.755558967590332, 'eval_runtime': 14.1399, 'eval_samples_per_second': 80.977, 'eval_steps_per_second': 2.546, 'epoch': 2.0}
{'train@eng.sdrt.stac_loss': 1.6917768716812134, 'train@eng.sdrt.stac_accuracy@eng.sdrt.stac': 0.461482254697286, 'train@eng.sdrt.stac_f1@eng.sdrt.stac': 0.17799624278156703, 'train@eng.sdrt.stac_precision@eng.sdrt.stac': 0.1950815260737209, 'train@eng.sdrt.stac_recall@eng.sdrt.stac': 0.20912387499491536, 'train@eng.sdrt.stac_loss@eng.sdrt.stac': 1.6917767524719238, 'train@eng.sdrt.stac_runtime': 114.4045, 'train@eng.sdrt.stac_samples_per_second': 83.738, 'train@eng.sdrt.stac_steps_per_second': 2.622, 'epoch': 3.0}
{'loss': 1.7853, 'learning_rate': 1.5000000000000002e-05, 'epoch': 3.0}
{'eval_loss': 1.6493138074874878, 'eval_accuracy@eng.sdrt.stac': 0.462882096069869, 'eval_f1@eng.sdrt.stac': 0.16422349038966824, 'eval_precision@eng.sdrt.stac': 0.18171404835074678, 'eval_recall@eng.sdrt.stac': 0.19936614712195427, 'eval_loss@eng.sdrt.stac': 1.6493138074874878, 'eval_runtime': 14.143, 'eval_samples_per_second': 80.959, 'eval_steps_per_second': 2.545, 'epoch': 3.0}
{'train@eng.sdrt.stac_loss': 1.6262505054473877, 'train@eng.sdrt.stac_accuracy@eng.sdrt.stac': 0.49018789144050107, 'train@eng.sdrt.stac_f1@eng.sdrt.stac': 0.24664986408263312, 'train@eng.sdrt.stac_precision@eng.sdrt.stac': 0.29135503148291036, 'train@eng.sdrt.stac_recall@eng.sdrt.stac': 0.25775926984571573, 'train@eng.sdrt.stac_loss@eng.sdrt.stac': 1.6262506246566772, 'train@eng.sdrt.stac_runtime': 114.2707, 'train@eng.sdrt.stac_samples_per_second': 83.836, 'train@eng.sdrt.stac_steps_per_second': 2.625, 'epoch': 4.0}
{'loss': 1.7005, 'learning_rate': 1.3333333333333333e-05, 'epoch': 4.0}
{'eval_loss': 1.5841424465179443, 'eval_accuracy@eng.sdrt.stac': 0.5004366812227075, 'eval_f1@eng.sdrt.stac': 0.23595122475709937, 'eval_precision@eng.sdrt.stac': 0.2860361910910357, 'eval_recall@eng.sdrt.stac': 0.24882721550815934, 'eval_loss@eng.sdrt.stac': 1.5841423273086548, 'eval_runtime': 14.1863, 'eval_samples_per_second': 80.712, 'eval_steps_per_second': 2.538, 'epoch': 4.0}
{'train@eng.sdrt.stac_loss': 1.5786328315734863, 'train@eng.sdrt.stac_accuracy@eng.sdrt.stac': 0.5055323590814196, 'train@eng.sdrt.stac_f1@eng.sdrt.stac': 0.27746238195735917, 'train@eng.sdrt.stac_precision@eng.sdrt.stac': 0.32598766410764957, 'train@eng.sdrt.stac_recall@eng.sdrt.stac': 0.2931013821826923, 'train@eng.sdrt.stac_loss@eng.sdrt.stac': 1.5786328315734863, 'train@eng.sdrt.stac_runtime': 114.2516, 'train@eng.sdrt.stac_samples_per_second': 83.85, 'train@eng.sdrt.stac_steps_per_second': 2.626, 'epoch': 5.0}
{'loss': 1.6476, 'learning_rate': 1.1666666666666668e-05, 'epoch': 5.0}
{'eval_loss': 1.5446220636367798, 'eval_accuracy@eng.sdrt.stac': 0.5109170305676856, 'eval_f1@eng.sdrt.stac': 0.24883277853383956, 'eval_precision@eng.sdrt.stac': 0.25455602575261876, 'eval_recall@eng.sdrt.stac': 0.26624667150230896, 'eval_loss@eng.sdrt.stac': 1.5446221828460693, 'eval_runtime': 14.1074, 'eval_samples_per_second': 81.163, 'eval_steps_per_second': 2.552, 'epoch': 5.0}
{'train@eng.sdrt.stac_loss': 1.5413620471954346, 'train@eng.sdrt.stac_accuracy@eng.sdrt.stac': 0.5129436325678497, 'train@eng.sdrt.stac_f1@eng.sdrt.stac': 0.2950179449722969, 'train@eng.sdrt.stac_precision@eng.sdrt.stac': 0.4605973681034874, 'train@eng.sdrt.stac_recall@eng.sdrt.stac': 0.3040607618719064, 'train@eng.sdrt.stac_loss@eng.sdrt.stac': 1.541361927986145, 'train@eng.sdrt.stac_runtime': 115.0459, 'train@eng.sdrt.stac_samples_per_second': 83.271, 'train@eng.sdrt.stac_steps_per_second': 2.608, 'epoch': 6.0}
{'loss': 1.6034, 'learning_rate': 1e-05, 'epoch': 6.0}
{'eval_loss': 1.5269054174423218, 'eval_accuracy@eng.sdrt.stac': 0.5152838427947598, 'eval_f1@eng.sdrt.stac': 0.27390696753042454, 'eval_precision@eng.sdrt.stac': 0.3430531339938014, 'eval_recall@eng.sdrt.stac': 0.2834411889660693, 'eval_loss@eng.sdrt.stac': 1.5269054174423218, 'eval_runtime': 14.2165, 'eval_samples_per_second': 80.54, 'eval_steps_per_second': 2.532, 'epoch': 6.0}
{'train@eng.sdrt.stac_loss': 1.5211676359176636, 'train@eng.sdrt.stac_accuracy@eng.sdrt.stac': 0.5239039665970773, 'train@eng.sdrt.stac_f1@eng.sdrt.stac': 0.31373624279266066, 'train@eng.sdrt.stac_precision@eng.sdrt.stac': 0.4007370747077281, 'train@eng.sdrt.stac_recall@eng.sdrt.stac': 0.32506644192877077, 'train@eng.sdrt.stac_loss@eng.sdrt.stac': 1.521167516708374, 'train@eng.sdrt.stac_runtime': 114.4708, 'train@eng.sdrt.stac_samples_per_second': 83.689, 'train@eng.sdrt.stac_steps_per_second': 2.621, 'epoch': 7.0}
{'loss': 1.5706, 'learning_rate': 8.333333333333334e-06, 'epoch': 7.0}
{'eval_loss': 1.5099797248840332, 'eval_accuracy@eng.sdrt.stac': 0.5275109170305677, 'eval_f1@eng.sdrt.stac': 0.28340602746070725, 'eval_precision@eng.sdrt.stac': 0.3313414338508741, 'eval_recall@eng.sdrt.stac': 0.2955330300433875, 'eval_loss@eng.sdrt.stac': 1.5099796056747437, 'eval_runtime': 14.1613, 'eval_samples_per_second': 80.854, 'eval_steps_per_second': 2.542, 'epoch': 7.0}
{'train@eng.sdrt.stac_loss': 1.489703893661499, 'train@eng.sdrt.stac_accuracy@eng.sdrt.stac': 0.5298538622129436, 'train@eng.sdrt.stac_f1@eng.sdrt.stac': 0.32519538312121843, 'train@eng.sdrt.stac_precision@eng.sdrt.stac': 0.4209620441748416, 'train@eng.sdrt.stac_recall@eng.sdrt.stac': 0.33578055402700024, 'train@eng.sdrt.stac_loss@eng.sdrt.stac': 1.4897041320800781, 'train@eng.sdrt.stac_runtime': 114.487, 'train@eng.sdrt.stac_samples_per_second': 83.678, 'train@eng.sdrt.stac_steps_per_second': 2.62, 'epoch': 8.0}
{'loss': 1.5494, 'learning_rate': 6.666666666666667e-06, 'epoch': 8.0}
{'eval_loss': 1.478798270225525, 'eval_accuracy@eng.sdrt.stac': 0.5301310043668123, 'eval_f1@eng.sdrt.stac': 0.29073515611261747, 'eval_precision@eng.sdrt.stac': 0.4199177916840383, 'eval_recall@eng.sdrt.stac': 0.2985517249242385, 'eval_loss@eng.sdrt.stac': 1.478798270225525, 'eval_runtime': 14.1794, 'eval_samples_per_second': 80.751, 'eval_steps_per_second': 2.539, 'epoch': 8.0}
{'train@eng.sdrt.stac_loss': 1.4761583805084229, 'train@eng.sdrt.stac_accuracy@eng.sdrt.stac': 0.5360125260960334, 'train@eng.sdrt.stac_f1@eng.sdrt.stac': 0.3388823679210936, 'train@eng.sdrt.stac_precision@eng.sdrt.stac': 0.41747009644339295, 'train@eng.sdrt.stac_recall@eng.sdrt.stac': 0.34892519141437256, 'train@eng.sdrt.stac_loss@eng.sdrt.stac': 1.4761584997177124, 'train@eng.sdrt.stac_runtime': 114.1866, 'train@eng.sdrt.stac_samples_per_second': 83.898, 'train@eng.sdrt.stac_steps_per_second': 2.627, 'epoch': 9.0}
{'loss': 1.5285, 'learning_rate': 5e-06, 'epoch': 9.0}
{'eval_loss': 1.4704530239105225, 'eval_accuracy@eng.sdrt.stac': 0.5379912663755458, 'eval_f1@eng.sdrt.stac': 0.31691357181551477, 'eval_precision@eng.sdrt.stac': 0.4301634644565111, 'eval_recall@eng.sdrt.stac': 0.31952457173268484, 'eval_loss@eng.sdrt.stac': 1.470452904701233, 'eval_runtime': 14.033, 'eval_samples_per_second': 81.594, 'eval_steps_per_second': 2.565, 'epoch': 9.0}
{'train@eng.sdrt.stac_loss': 1.4629395008087158, 'train@eng.sdrt.stac_accuracy@eng.sdrt.stac': 0.5387265135699374, 'train@eng.sdrt.stac_f1@eng.sdrt.stac': 0.3456780704204695, 'train@eng.sdrt.stac_precision@eng.sdrt.stac': 0.41880502213320475, 'train@eng.sdrt.stac_recall@eng.sdrt.stac': 0.35516989534005944, 'train@eng.sdrt.stac_loss@eng.sdrt.stac': 1.4629395008087158, 'train@eng.sdrt.stac_runtime': 114.1513, 'train@eng.sdrt.stac_samples_per_second': 83.924, 'train@eng.sdrt.stac_steps_per_second': 2.628, 'epoch': 10.0}
{'loss': 1.5154, 'learning_rate': 3.3333333333333333e-06, 'epoch': 10.0}
{'eval_loss': 1.4613595008850098, 'eval_accuracy@eng.sdrt.stac': 0.5379912663755458, 'eval_f1@eng.sdrt.stac': 0.3341770765315953, 'eval_precision@eng.sdrt.stac': 0.4249321524619095, 'eval_recall@eng.sdrt.stac': 0.33908095774964697, 'eval_loss@eng.sdrt.stac': 1.4613593816757202, 'eval_runtime': 14.1735, 'eval_samples_per_second': 80.784, 'eval_steps_per_second': 2.54, 'epoch': 10.0}
{'train@eng.sdrt.stac_loss': 1.4550185203552246, 'train@eng.sdrt.stac_accuracy@eng.sdrt.stac': 0.5421711899791232, 'train@eng.sdrt.stac_f1@eng.sdrt.stac': 0.3519577022005645, 'train@eng.sdrt.stac_precision@eng.sdrt.stac': 0.4221469400184167, 'train@eng.sdrt.stac_recall@eng.sdrt.stac': 0.3617927894838282, 'train@eng.sdrt.stac_loss@eng.sdrt.stac': 1.455018401145935, 'train@eng.sdrt.stac_runtime': 114.6529, 'train@eng.sdrt.stac_samples_per_second': 83.557, 'train@eng.sdrt.stac_steps_per_second': 2.617, 'epoch': 11.0}
{'loss': 1.5054, 'learning_rate': 1.6666666666666667e-06, 'epoch': 11.0}
{'eval_loss': 1.4552435874938965, 'eval_accuracy@eng.sdrt.stac': 0.5406113537117904, 'eval_f1@eng.sdrt.stac': 0.33894769693264154, 'eval_precision@eng.sdrt.stac': 0.4284306877529723, 'eval_recall@eng.sdrt.stac': 0.34464381562931523, 'eval_loss@eng.sdrt.stac': 1.4552433490753174, 'eval_runtime': 14.225, 'eval_samples_per_second': 80.492, 'eval_steps_per_second': 2.531, 'epoch': 11.0}
{'train@eng.sdrt.stac_loss': 1.453416109085083, 'train@eng.sdrt.stac_accuracy@eng.sdrt.stac': 0.542901878914405, 'train@eng.sdrt.stac_f1@eng.sdrt.stac': 0.35264301675274734, 'train@eng.sdrt.stac_precision@eng.sdrt.stac': 0.4231797450860183, 'train@eng.sdrt.stac_recall@eng.sdrt.stac': 0.3619907710475371, 'train@eng.sdrt.stac_loss@eng.sdrt.stac': 1.4534159898757935, 'train@eng.sdrt.stac_runtime': 114.417, 'train@eng.sdrt.stac_samples_per_second': 83.729, 'train@eng.sdrt.stac_steps_per_second': 2.622, 'epoch': 12.0}
{'loss': 1.5026, 'learning_rate': 0.0, 'epoch': 12.0}
{'eval_loss': 1.454516887664795, 'eval_accuracy@eng.sdrt.stac': 0.5414847161572053, 'eval_f1@eng.sdrt.stac': 0.34038797492686323, 'eval_precision@eng.sdrt.stac': 0.4300154047095268, 'eval_recall@eng.sdrt.stac': 0.34566380158427223, 'eval_loss@eng.sdrt.stac': 1.454516887664795, 'eval_runtime': 14.1711, 'eval_samples_per_second': 80.799, 'eval_steps_per_second': 2.54, 'epoch': 12.0}
{'train_runtime': 4459.1886, 'train_samples_per_second': 25.78, 'train_steps_per_second': 0.807, 'train_loss': 1.6917671797010634, 'epoch': 12.0}
***** train_logger metrics *****
  epoch                    =       12.0
  train_loss               =      2.891
  train_runtime            = 0:17:20.65
  train_samples_per_second =     24.953
  train_steps_per_second   =      0.784
-------------------------------------------------------------------
Lang1:  eng.pdtb.pdtb    Lang2:  eng.sdrt.stac
Saving run to:  runs/full_shot/FullShot=v4_eng.pdtb.pdtb_eng.sdrt.stac_lrfunc=Adafactor_lr=2e-05_plm=bert-base-multilingual-cased
Running with params: BERT_MODEL=bert-base-multilingual-cased lr=2e-05
read 43920 examples
read 1674 examples
read 2257 examples
read 9580 examples
read 1145 examples
read 1510 examples
Total prediction labels:  39
BertAdapterModel(
  (shared_parameters): ModuleDict()
  (bert): BertModel(
    (shared_parameters): ModuleDict()
    (invertible_adapters): ModuleDict(
      (en): NICECouplingBlock(
        (F): Sequential(
          (0): Linear(in_features=384, out_features=192, bias=True)
          (1): Activation_Function_Class(
            (f): ReLU()
          )
          (2): Linear(in_features=192, out_features=384, bias=True)
        )
        (G): Sequential(
          (0): Linear(in_features=384, out_features=192, bias=True)
          (1): Activation_Function_Class(
            (f): ReLU()
          )
          (2): Linear(in_features=192, out_features=384, bias=True)
        )
      )
    )
    (embeddings): BertEmbeddings(
      (word_embeddings): Embedding(119547, 768, padding_idx=0)
      (position_embeddings): Embedding(512, 768)
      (token_type_embeddings): Embedding(2, 768)
      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
      (dropout): Dropout(p=0.1, inplace=False)
    )
    (encoder): BertEncoder(
      (layer): ModuleList(
        (0): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (en): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (1): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (en): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (2): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (en): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (3): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (en): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (4): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (en): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (5): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (en): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (6): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (en): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (7): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (en): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (8): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (en): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (9): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (en): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (10): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (en): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (11): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (en): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
      )
    )
    (pooler): BertPooler(
      (dense): Linear(in_features=768, out_features=768, bias=True)
      (activation): Tanh()
    )
    (prefix_tuning): PrefixTuningPool(
      (prefix_tunings): ModuleDict()
    )
  )
  (heads): ModuleDict(
    (disrpt): ClassificationHead(
      (0): Dropout(p=0.1, inplace=False)
      (1): Linear(in_features=768, out_features=768, bias=True)
      (2): Activation_Function_Class(
        (f): Tanh()
      )
      (3): Dropout(p=0.1, inplace=False)
      (4): Linear(in_features=768, out_features=39, bias=True)
    )
  )
)
{'train@eng.pdtb.pdtb_loss': 1.2864125967025757, 'train@eng.pdtb.pdtb_accuracy@eng.pdtb.pdtb': 0.5924408014571949, 'train@eng.pdtb.pdtb_f1@eng.pdtb.pdtb': 0.25269548498623423, 'train@eng.pdtb.pdtb_precision@eng.pdtb.pdtb': 0.3109786249370461, 'train@eng.pdtb.pdtb_recall@eng.pdtb.pdtb': 0.25174102410769456, 'train@eng.pdtb.pdtb_loss@eng.pdtb.pdtb': 1.2864125967025757, 'train@eng.pdtb.pdtb_runtime': 524.4441, 'train@eng.pdtb.pdtb_samples_per_second': 83.746, 'train@eng.pdtb.pdtb_steps_per_second': 2.618, 'epoch': 1.0}
{'loss': 1.8473, 'learning_rate': 1.8333333333333333e-05, 'epoch': 1.0}
{'eval_loss': 1.201423168182373, 'eval_accuracy@eng.pdtb.pdtb': 0.6290322580645161, 'eval_f1@eng.pdtb.pdtb': 0.3053501545021509, 'eval_precision@eng.pdtb.pdtb': 0.3336291759630097, 'eval_recall@eng.pdtb.pdtb': 0.3051715180379226, 'eval_loss@eng.pdtb.pdtb': 1.2014232873916626, 'eval_runtime': 20.419, 'eval_samples_per_second': 81.982, 'eval_steps_per_second': 2.596, 'epoch': 1.0}
{'train@eng.pdtb.pdtb_loss': 1.118650197982788, 'train@eng.pdtb.pdtb_accuracy@eng.pdtb.pdtb': 0.6330145719489981, 'train@eng.pdtb.pdtb_f1@eng.pdtb.pdtb': 0.3298029370581934, 'train@eng.pdtb.pdtb_precision@eng.pdtb.pdtb': 0.3648796495448245, 'train@eng.pdtb.pdtb_recall@eng.pdtb.pdtb': 0.323991337556422, 'train@eng.pdtb.pdtb_loss@eng.pdtb.pdtb': 1.118650197982788, 'train@eng.pdtb.pdtb_runtime': 524.064, 'train@eng.pdtb.pdtb_samples_per_second': 83.807, 'train@eng.pdtb.pdtb_steps_per_second': 2.62, 'epoch': 2.0}
{'loss': 1.247, 'learning_rate': 1.6666666666666667e-05, 'epoch': 2.0}
{'eval_loss': 1.048140048980713, 'eval_accuracy@eng.pdtb.pdtb': 0.6666666666666666, 'eval_f1@eng.pdtb.pdtb': 0.3964385761285497, 'eval_precision@eng.pdtb.pdtb': 0.44306978604618835, 'eval_recall@eng.pdtb.pdtb': 0.3889970286832224, 'eval_loss@eng.pdtb.pdtb': 1.048140048980713, 'eval_runtime': 20.5286, 'eval_samples_per_second': 81.545, 'eval_steps_per_second': 2.582, 'epoch': 2.0}
{'train@eng.pdtb.pdtb_loss': 1.0622563362121582, 'train@eng.pdtb.pdtb_accuracy@eng.pdtb.pdtb': 0.6489754098360656, 'train@eng.pdtb.pdtb_f1@eng.pdtb.pdtb': 0.40841072501805664, 'train@eng.pdtb.pdtb_precision@eng.pdtb.pdtb': 0.4627895921018253, 'train@eng.pdtb.pdtb_recall@eng.pdtb.pdtb': 0.38894234023629964, 'train@eng.pdtb.pdtb_loss@eng.pdtb.pdtb': 1.0622563362121582, 'train@eng.pdtb.pdtb_runtime': 523.7229, 'train@eng.pdtb.pdtb_samples_per_second': 83.861, 'train@eng.pdtb.pdtb_steps_per_second': 2.622, 'epoch': 3.0}
{'loss': 1.1429, 'learning_rate': 1.5000000000000002e-05, 'epoch': 3.0}
{'eval_loss': 1.0038851499557495, 'eval_accuracy@eng.pdtb.pdtb': 0.6732377538829152, 'eval_f1@eng.pdtb.pdtb': 0.4567325802586123, 'eval_precision@eng.pdtb.pdtb': 0.5153093189876399, 'eval_recall@eng.pdtb.pdtb': 0.4375616054945966, 'eval_loss@eng.pdtb.pdtb': 1.0038851499557495, 'eval_runtime': 20.4747, 'eval_samples_per_second': 81.76, 'eval_steps_per_second': 2.589, 'epoch': 3.0}
{'train@eng.pdtb.pdtb_loss': 1.0119211673736572, 'train@eng.pdtb.pdtb_accuracy@eng.pdtb.pdtb': 0.6635701275045537, 'train@eng.pdtb.pdtb_f1@eng.pdtb.pdtb': 0.4407240861393769, 'train@eng.pdtb.pdtb_precision@eng.pdtb.pdtb': 0.5162778438153324, 'train@eng.pdtb.pdtb_recall@eng.pdtb.pdtb': 0.42736885840130034, 'train@eng.pdtb.pdtb_loss@eng.pdtb.pdtb': 1.0119211673736572, 'train@eng.pdtb.pdtb_runtime': 525.6417, 'train@eng.pdtb.pdtb_samples_per_second': 83.555, 'train@eng.pdtb.pdtb_steps_per_second': 2.612, 'epoch': 4.0}
{'loss': 1.0903, 'learning_rate': 1.3333333333333333e-05, 'epoch': 4.0}
{'eval_loss': 0.9609497785568237, 'eval_accuracy@eng.pdtb.pdtb': 0.6821983273596177, 'eval_f1@eng.pdtb.pdtb': 0.48683919051186886, 'eval_precision@eng.pdtb.pdtb': 0.5341633801972967, 'eval_recall@eng.pdtb.pdtb': 0.46831859014093496, 'eval_loss@eng.pdtb.pdtb': 0.9609496593475342, 'eval_runtime': 20.4442, 'eval_samples_per_second': 81.881, 'eval_steps_per_second': 2.592, 'epoch': 4.0}
{'train@eng.pdtb.pdtb_loss': 0.9883981943130493, 'train@eng.pdtb.pdtb_accuracy@eng.pdtb.pdtb': 0.6701047358834245, 'train@eng.pdtb.pdtb_f1@eng.pdtb.pdtb': 0.44951480919726167, 'train@eng.pdtb.pdtb_precision@eng.pdtb.pdtb': 0.5183706587920818, 'train@eng.pdtb.pdtb_recall@eng.pdtb.pdtb': 0.4413347245206124, 'train@eng.pdtb.pdtb_loss@eng.pdtb.pdtb': 0.9883981347084045, 'train@eng.pdtb.pdtb_runtime': 524.6363, 'train@eng.pdtb.pdtb_samples_per_second': 83.715, 'train@eng.pdtb.pdtb_steps_per_second': 2.617, 'epoch': 5.0}
{'loss': 1.0583, 'learning_rate': 1.1666666666666668e-05, 'epoch': 5.0}
{'eval_loss': 0.9445145726203918, 'eval_accuracy@eng.pdtb.pdtb': 0.6881720430107527, 'eval_f1@eng.pdtb.pdtb': 0.49599567538832057, 'eval_precision@eng.pdtb.pdtb': 0.5371686781842198, 'eval_recall@eng.pdtb.pdtb': 0.48307318471039673, 'eval_loss@eng.pdtb.pdtb': 0.9445145726203918, 'eval_runtime': 20.4491, 'eval_samples_per_second': 81.862, 'eval_steps_per_second': 2.592, 'epoch': 5.0}
{'train@eng.pdtb.pdtb_loss': 0.9676516056060791, 'train@eng.pdtb.pdtb_accuracy@eng.pdtb.pdtb': 0.6758652094717669, 'train@eng.pdtb.pdtb_f1@eng.pdtb.pdtb': 0.4582268890659257, 'train@eng.pdtb.pdtb_precision@eng.pdtb.pdtb': 0.522767784402457, 'train@eng.pdtb.pdtb_recall@eng.pdtb.pdtb': 0.4508681190986135, 'train@eng.pdtb.pdtb_loss@eng.pdtb.pdtb': 0.9676516056060791, 'train@eng.pdtb.pdtb_runtime': 523.8268, 'train@eng.pdtb.pdtb_samples_per_second': 83.845, 'train@eng.pdtb.pdtb_steps_per_second': 2.621, 'epoch': 6.0}
{'loss': 1.0328, 'learning_rate': 1e-05, 'epoch': 6.0}
{'eval_loss': 0.9351603984832764, 'eval_accuracy@eng.pdtb.pdtb': 0.6929510155316607, 'eval_f1@eng.pdtb.pdtb': 0.5098857545396724, 'eval_precision@eng.pdtb.pdtb': 0.5551583542967327, 'eval_recall@eng.pdtb.pdtb': 0.49756658043768764, 'eval_loss@eng.pdtb.pdtb': 0.9351603388786316, 'eval_runtime': 20.4281, 'eval_samples_per_second': 81.946, 'eval_steps_per_second': 2.594, 'epoch': 6.0}
{'train@eng.pdtb.pdtb_loss': 0.9558260440826416, 'train@eng.pdtb.pdtb_accuracy@eng.pdtb.pdtb': 0.6775956284153005, 'train@eng.pdtb.pdtb_f1@eng.pdtb.pdtb': 0.4609416310517324, 'train@eng.pdtb.pdtb_precision@eng.pdtb.pdtb': 0.530780703306404, 'train@eng.pdtb.pdtb_recall@eng.pdtb.pdtb': 0.4503624254296269, 'train@eng.pdtb.pdtb_loss@eng.pdtb.pdtb': 0.955825924873352, 'train@eng.pdtb.pdtb_runtime': 524.2719, 'train@eng.pdtb.pdtb_samples_per_second': 83.773, 'train@eng.pdtb.pdtb_steps_per_second': 2.619, 'epoch': 7.0}
{'loss': 1.0157, 'learning_rate': 8.333333333333334e-06, 'epoch': 7.0}
{'eval_loss': 0.919802725315094, 'eval_accuracy@eng.pdtb.pdtb': 0.6863799283154122, 'eval_f1@eng.pdtb.pdtb': 0.5091804448789456, 'eval_precision@eng.pdtb.pdtb': 0.5679642014739918, 'eval_recall@eng.pdtb.pdtb': 0.4907506477982226, 'eval_loss@eng.pdtb.pdtb': 0.919802725315094, 'eval_runtime': 20.5155, 'eval_samples_per_second': 81.597, 'eval_steps_per_second': 2.583, 'epoch': 7.0}
{'train@eng.pdtb.pdtb_loss': 0.9429892301559448, 'train@eng.pdtb.pdtb_accuracy@eng.pdtb.pdtb': 0.68158014571949, 'train@eng.pdtb.pdtb_f1@eng.pdtb.pdtb': 0.46714706065763695, 'train@eng.pdtb.pdtb_precision@eng.pdtb.pdtb': 0.5174030934244047, 'train@eng.pdtb.pdtb_recall@eng.pdtb.pdtb': 0.4627896093311986, 'train@eng.pdtb.pdtb_loss@eng.pdtb.pdtb': 0.9429892301559448, 'train@eng.pdtb.pdtb_runtime': 518.298, 'train@eng.pdtb.pdtb_samples_per_second': 84.739, 'train@eng.pdtb.pdtb_steps_per_second': 2.649, 'epoch': 8.0}
{'loss': 1.0034, 'learning_rate': 6.666666666666667e-06, 'epoch': 8.0}
{'eval_loss': 0.9152242541313171, 'eval_accuracy@eng.pdtb.pdtb': 0.6917562724014337, 'eval_f1@eng.pdtb.pdtb': 0.5362555402047676, 'eval_precision@eng.pdtb.pdtb': 0.6072093393505741, 'eval_recall@eng.pdtb.pdtb': 0.5191697226033386, 'eval_loss@eng.pdtb.pdtb': 0.9152244329452515, 'eval_runtime': 20.0765, 'eval_samples_per_second': 83.381, 'eval_steps_per_second': 2.64, 'epoch': 8.0}
{'train@eng.pdtb.pdtb_loss': 0.9335508942604065, 'train@eng.pdtb.pdtb_accuracy@eng.pdtb.pdtb': 0.6848360655737705, 'train@eng.pdtb.pdtb_f1@eng.pdtb.pdtb': 0.4724346910972541, 'train@eng.pdtb.pdtb_precision@eng.pdtb.pdtb': 0.5237490482607656, 'train@eng.pdtb.pdtb_recall@eng.pdtb.pdtb': 0.4671621046528335, 'train@eng.pdtb.pdtb_loss@eng.pdtb.pdtb': 0.9335507750511169, 'train@eng.pdtb.pdtb_runtime': 517.5849, 'train@eng.pdtb.pdtb_samples_per_second': 84.856, 'train@eng.pdtb.pdtb_steps_per_second': 2.653, 'epoch': 9.0}
{'loss': 0.9929, 'learning_rate': 5e-06, 'epoch': 9.0}
{'eval_loss': 0.9122050404548645, 'eval_accuracy@eng.pdtb.pdtb': 0.6911589008363201, 'eval_f1@eng.pdtb.pdtb': 0.5350274699380297, 'eval_precision@eng.pdtb.pdtb': 0.6091910457675509, 'eval_recall@eng.pdtb.pdtb': 0.51551548657233, 'eval_loss@eng.pdtb.pdtb': 0.9122050404548645, 'eval_runtime': 20.0625, 'eval_samples_per_second': 83.439, 'eval_steps_per_second': 2.642, 'epoch': 9.0}
{'train@eng.pdtb.pdtb_loss': 0.9298778176307678, 'train@eng.pdtb.pdtb_accuracy@eng.pdtb.pdtb': 0.6857240437158469, 'train@eng.pdtb.pdtb_f1@eng.pdtb.pdtb': 0.473323651242762, 'train@eng.pdtb.pdtb_precision@eng.pdtb.pdtb': 0.525846830041443, 'train@eng.pdtb.pdtb_recall@eng.pdtb.pdtb': 0.46797882837927834, 'train@eng.pdtb.pdtb_loss@eng.pdtb.pdtb': 0.9298779964447021, 'train@eng.pdtb.pdtb_runtime': 518.3406, 'train@eng.pdtb.pdtb_samples_per_second': 84.732, 'train@eng.pdtb.pdtb_steps_per_second': 2.649, 'epoch': 10.0}
{'loss': 0.9878, 'learning_rate': 3.3333333333333333e-06, 'epoch': 10.0}
{'eval_loss': 0.9064856171607971, 'eval_accuracy@eng.pdtb.pdtb': 0.6893667861409797, 'eval_f1@eng.pdtb.pdtb': 0.5439599914880917, 'eval_precision@eng.pdtb.pdtb': 0.6174043510151839, 'eval_recall@eng.pdtb.pdtb': 0.5264161448666916, 'eval_loss@eng.pdtb.pdtb': 0.9064856171607971, 'eval_runtime': 20.1035, 'eval_samples_per_second': 83.269, 'eval_steps_per_second': 2.636, 'epoch': 10.0}
{'train@eng.pdtb.pdtb_loss': 0.9252228140830994, 'train@eng.pdtb.pdtb_accuracy@eng.pdtb.pdtb': 0.6881375227686704, 'train@eng.pdtb.pdtb_f1@eng.pdtb.pdtb': 0.4757402690336233, 'train@eng.pdtb.pdtb_precision@eng.pdtb.pdtb': 0.5236445246026824, 'train@eng.pdtb.pdtb_recall@eng.pdtb.pdtb': 0.4718013523937954, 'train@eng.pdtb.pdtb_loss@eng.pdtb.pdtb': 0.9252228736877441, 'train@eng.pdtb.pdtb_runtime': 517.6156, 'train@eng.pdtb.pdtb_samples_per_second': 84.851, 'train@eng.pdtb.pdtb_steps_per_second': 2.653, 'epoch': 11.0}
{'loss': 0.9807, 'learning_rate': 1.6666666666666667e-06, 'epoch': 11.0}
{'eval_loss': 0.9061813354492188, 'eval_accuracy@eng.pdtb.pdtb': 0.6923536439665472, 'eval_f1@eng.pdtb.pdtb': 0.5437487904519522, 'eval_precision@eng.pdtb.pdtb': 0.611530274447969, 'eval_recall@eng.pdtb.pdtb': 0.5264221126416356, 'eval_loss@eng.pdtb.pdtb': 0.9061813354492188, 'eval_runtime': 20.0803, 'eval_samples_per_second': 83.365, 'eval_steps_per_second': 2.639, 'epoch': 11.0}
{'train@eng.pdtb.pdtb_loss': 0.9240043759346008, 'train@eng.pdtb.pdtb_accuracy@eng.pdtb.pdtb': 0.6885701275045537, 'train@eng.pdtb.pdtb_f1@eng.pdtb.pdtb': 0.47577326094340167, 'train@eng.pdtb.pdtb_precision@eng.pdtb.pdtb': 0.5268393180420147, 'train@eng.pdtb.pdtb_recall@eng.pdtb.pdtb': 0.4701609816768817, 'train@eng.pdtb.pdtb_loss@eng.pdtb.pdtb': 0.9240043759346008, 'train@eng.pdtb.pdtb_runtime': 517.7112, 'train@eng.pdtb.pdtb_samples_per_second': 84.835, 'train@eng.pdtb.pdtb_steps_per_second': 2.652, 'epoch': 12.0}
{'loss': 0.9749, 'learning_rate': 0.0, 'epoch': 12.0}
{'eval_loss': 0.904157280921936, 'eval_accuracy@eng.pdtb.pdtb': 0.6935483870967742, 'eval_f1@eng.pdtb.pdtb': 0.5455992571302113, 'eval_precision@eng.pdtb.pdtb': 0.6154476852735302, 'eval_recall@eng.pdtb.pdtb': 0.5266270610683027, 'eval_loss@eng.pdtb.pdtb': 0.904157280921936, 'eval_runtime': 23.6971, 'eval_samples_per_second': 70.641, 'eval_steps_per_second': 2.237, 'epoch': 12.0}
{'train_runtime': 19722.61, 'train_samples_per_second': 26.723, 'train_steps_per_second': 0.835, 'train_loss': 1.1144852458808456, 'epoch': 12.0}
***** train_log metrics *****
  epoch                    =       12.0
  train_loss               =     1.1145
  train_runtime            = 5:28:42.61
  train_samples_per_second =     26.723
  train_steps_per_second   =      0.835
{'train@eng.sdrt.stac_loss': 1.89989173412323, 'train@eng.sdrt.stac_accuracy@eng.sdrt.stac': 0.4103340292275574, 'train@eng.sdrt.stac_f1@eng.sdrt.stac': 0.14038234301542057, 'train@eng.sdrt.stac_precision@eng.sdrt.stac': 0.18684435641154765, 'train@eng.sdrt.stac_recall@eng.sdrt.stac': 0.15670940553741855, 'train@eng.sdrt.stac_loss@eng.sdrt.stac': 1.8998914957046509, 'train@eng.sdrt.stac_runtime': 112.78, 'train@eng.sdrt.stac_samples_per_second': 84.944, 'train@eng.sdrt.stac_steps_per_second': 2.66, 'epoch': 1.0}
{'loss': 2.2903, 'learning_rate': 1.8333333333333333e-05, 'epoch': 1.0}
{'eval_loss': 1.870078206062317, 'eval_accuracy@eng.sdrt.stac': 0.43056768558951963, 'eval_f1@eng.sdrt.stac': 0.1539059082786364, 'eval_precision@eng.sdrt.stac': 0.22034673463563914, 'eval_recall@eng.sdrt.stac': 0.17062153171727656, 'eval_loss@eng.sdrt.stac': 1.8700783252716064, 'eval_runtime': 13.8262, 'eval_samples_per_second': 82.814, 'eval_steps_per_second': 2.604, 'epoch': 1.0}
{'train@eng.sdrt.stac_loss': 1.688849925994873, 'train@eng.sdrt.stac_accuracy@eng.sdrt.stac': 0.477035490605428, 'train@eng.sdrt.stac_f1@eng.sdrt.stac': 0.23843508471829172, 'train@eng.sdrt.stac_precision@eng.sdrt.stac': 0.2759962859431057, 'train@eng.sdrt.stac_recall@eng.sdrt.stac': 0.24791613947530283, 'train@eng.sdrt.stac_loss@eng.sdrt.stac': 1.688849925994873, 'train@eng.sdrt.stac_runtime': 112.8531, 'train@eng.sdrt.stac_samples_per_second': 84.889, 'train@eng.sdrt.stac_steps_per_second': 2.658, 'epoch': 2.0}
{'loss': 1.8348, 'learning_rate': 1.6666666666666667e-05, 'epoch': 2.0}
{'eval_loss': 1.6740208864212036, 'eval_accuracy@eng.sdrt.stac': 0.4882096069868996, 'eval_f1@eng.sdrt.stac': 0.22708400081883634, 'eval_precision@eng.sdrt.stac': 0.2861600076540154, 'eval_recall@eng.sdrt.stac': 0.2371181734548412, 'eval_loss@eng.sdrt.stac': 1.6740210056304932, 'eval_runtime': 13.8613, 'eval_samples_per_second': 82.604, 'eval_steps_per_second': 2.597, 'epoch': 2.0}
{'train@eng.sdrt.stac_loss': 1.597867727279663, 'train@eng.sdrt.stac_accuracy@eng.sdrt.stac': 0.4927974947807933, 'train@eng.sdrt.stac_f1@eng.sdrt.stac': 0.2861914408491623, 'train@eng.sdrt.stac_precision@eng.sdrt.stac': 0.3200081121163285, 'train@eng.sdrt.stac_recall@eng.sdrt.stac': 0.2932430735945071, 'train@eng.sdrt.stac_loss@eng.sdrt.stac': 1.597867727279663, 'train@eng.sdrt.stac_runtime': 112.8816, 'train@eng.sdrt.stac_samples_per_second': 84.868, 'train@eng.sdrt.stac_steps_per_second': 2.658, 'epoch': 3.0}
{'loss': 1.6958, 'learning_rate': 1.5000000000000002e-05, 'epoch': 3.0}
{'eval_loss': 1.6106699705123901, 'eval_accuracy@eng.sdrt.stac': 0.49344978165938863, 'eval_f1@eng.sdrt.stac': 0.264010368999574, 'eval_precision@eng.sdrt.stac': 0.3170143697818507, 'eval_recall@eng.sdrt.stac': 0.2707973912704984, 'eval_loss@eng.sdrt.stac': 1.6106698513031006, 'eval_runtime': 13.8592, 'eval_samples_per_second': 82.616, 'eval_steps_per_second': 2.598, 'epoch': 3.0}
{'train@eng.sdrt.stac_loss': 1.5379327535629272, 'train@eng.sdrt.stac_accuracy@eng.sdrt.stac': 0.5173277661795407, 'train@eng.sdrt.stac_f1@eng.sdrt.stac': 0.32046020741695513, 'train@eng.sdrt.stac_precision@eng.sdrt.stac': 0.32974570619964905, 'train@eng.sdrt.stac_recall@eng.sdrt.stac': 0.32983846481127377, 'train@eng.sdrt.stac_loss@eng.sdrt.stac': 1.5379328727722168, 'train@eng.sdrt.stac_runtime': 112.6555, 'train@eng.sdrt.stac_samples_per_second': 85.038, 'train@eng.sdrt.stac_steps_per_second': 2.663, 'epoch': 4.0}
{'loss': 1.6266, 'learning_rate': 1.3333333333333333e-05, 'epoch': 4.0}
{'eval_loss': 1.5440648794174194, 'eval_accuracy@eng.sdrt.stac': 0.525764192139738, 'eval_f1@eng.sdrt.stac': 0.3174595935907748, 'eval_precision@eng.sdrt.stac': 0.3266657688106045, 'eval_recall@eng.sdrt.stac': 0.3213369039896154, 'eval_loss@eng.sdrt.stac': 1.5440648794174194, 'eval_runtime': 13.8423, 'eval_samples_per_second': 82.717, 'eval_steps_per_second': 2.601, 'epoch': 4.0}
{'train@eng.sdrt.stac_loss': 1.512691617012024, 'train@eng.sdrt.stac_accuracy@eng.sdrt.stac': 0.5248434237995825, 'train@eng.sdrt.stac_f1@eng.sdrt.stac': 0.30745091804987557, 'train@eng.sdrt.stac_precision@eng.sdrt.stac': 0.3197835557850695, 'train@eng.sdrt.stac_recall@eng.sdrt.stac': 0.3225832865583724, 'train@eng.sdrt.stac_loss@eng.sdrt.stac': 1.5126914978027344, 'train@eng.sdrt.stac_runtime': 112.7375, 'train@eng.sdrt.stac_samples_per_second': 84.976, 'train@eng.sdrt.stac_steps_per_second': 2.661, 'epoch': 5.0}
{'loss': 1.587, 'learning_rate': 1.1666666666666668e-05, 'epoch': 5.0}
{'eval_loss': 1.5184342861175537, 'eval_accuracy@eng.sdrt.stac': 0.5318777292576419, 'eval_f1@eng.sdrt.stac': 0.32314041166991014, 'eval_precision@eng.sdrt.stac': 0.33046380930329716, 'eval_recall@eng.sdrt.stac': 0.3320512485536656, 'eval_loss@eng.sdrt.stac': 1.5184342861175537, 'eval_runtime': 13.808, 'eval_samples_per_second': 82.923, 'eval_steps_per_second': 2.607, 'epoch': 5.0}
{'train@eng.sdrt.stac_loss': 1.4681497812271118, 'train@eng.sdrt.stac_accuracy@eng.sdrt.stac': 0.5312108559498956, 'train@eng.sdrt.stac_f1@eng.sdrt.stac': 0.3360550757769928, 'train@eng.sdrt.stac_precision@eng.sdrt.stac': 0.34349648045891473, 'train@eng.sdrt.stac_recall@eng.sdrt.stac': 0.34560575783625913, 'train@eng.sdrt.stac_loss@eng.sdrt.stac': 1.4681497812271118, 'train@eng.sdrt.stac_runtime': 112.8956, 'train@eng.sdrt.stac_samples_per_second': 84.857, 'train@eng.sdrt.stac_steps_per_second': 2.657, 'epoch': 6.0}
{'loss': 1.5472, 'learning_rate': 1e-05, 'epoch': 6.0}
{'eval_loss': 1.4994771480560303, 'eval_accuracy@eng.sdrt.stac': 0.5318777292576419, 'eval_f1@eng.sdrt.stac': 0.3364681467015458, 'eval_precision@eng.sdrt.stac': 0.36153703615354554, 'eval_recall@eng.sdrt.stac': 0.33866243611837565, 'eval_loss@eng.sdrt.stac': 1.4994772672653198, 'eval_runtime': 13.8631, 'eval_samples_per_second': 82.594, 'eval_steps_per_second': 2.597, 'epoch': 6.0}
{'train@eng.sdrt.stac_loss': 1.4668323993682861, 'train@eng.sdrt.stac_accuracy@eng.sdrt.stac': 0.5358037578288101, 'train@eng.sdrt.stac_f1@eng.sdrt.stac': 0.338644805792518, 'train@eng.sdrt.stac_precision@eng.sdrt.stac': 0.4134340011430746, 'train@eng.sdrt.stac_recall@eng.sdrt.stac': 0.35401435653454055, 'train@eng.sdrt.stac_loss@eng.sdrt.stac': 1.4668322801589966, 'train@eng.sdrt.stac_runtime': 112.7465, 'train@eng.sdrt.stac_samples_per_second': 84.969, 'train@eng.sdrt.stac_steps_per_second': 2.661, 'epoch': 7.0}
{'loss': 1.5235, 'learning_rate': 8.333333333333334e-06, 'epoch': 7.0}
{'eval_loss': 1.4890106916427612, 'eval_accuracy@eng.sdrt.stac': 0.5397379912663756, 'eval_f1@eng.sdrt.stac': 0.3429331332091814, 'eval_precision@eng.sdrt.stac': 0.42014414867099503, 'eval_recall@eng.sdrt.stac': 0.34591630715923916, 'eval_loss@eng.sdrt.stac': 1.4890106916427612, 'eval_runtime': 13.8247, 'eval_samples_per_second': 82.823, 'eval_steps_per_second': 2.604, 'epoch': 7.0}
{'train@eng.sdrt.stac_loss': 1.434754490852356, 'train@eng.sdrt.stac_accuracy@eng.sdrt.stac': 0.5448851774530271, 'train@eng.sdrt.stac_f1@eng.sdrt.stac': 0.35608224018453305, 'train@eng.sdrt.stac_precision@eng.sdrt.stac': 0.42470220359753585, 'train@eng.sdrt.stac_recall@eng.sdrt.stac': 0.36640643490346003, 'train@eng.sdrt.stac_loss@eng.sdrt.stac': 1.4347543716430664, 'train@eng.sdrt.stac_runtime': 112.7799, 'train@eng.sdrt.stac_samples_per_second': 84.944, 'train@eng.sdrt.stac_steps_per_second': 2.66, 'epoch': 8.0}
{'loss': 1.5071, 'learning_rate': 6.666666666666667e-06, 'epoch': 8.0}
{'eval_loss': 1.4666714668273926, 'eval_accuracy@eng.sdrt.stac': 0.537117903930131, 'eval_f1@eng.sdrt.stac': 0.35536735713620415, 'eval_precision@eng.sdrt.stac': 0.4188103121872967, 'eval_recall@eng.sdrt.stac': 0.35657740107388897, 'eval_loss@eng.sdrt.stac': 1.4666714668273926, 'eval_runtime': 13.8687, 'eval_samples_per_second': 82.56, 'eval_steps_per_second': 2.596, 'epoch': 8.0}
{'train@eng.sdrt.stac_loss': 1.4271992444992065, 'train@eng.sdrt.stac_accuracy@eng.sdrt.stac': 0.5492693110647182, 'train@eng.sdrt.stac_f1@eng.sdrt.stac': 0.36400335407675233, 'train@eng.sdrt.stac_precision@eng.sdrt.stac': 0.41361384943295326, 'train@eng.sdrt.stac_recall@eng.sdrt.stac': 0.3756713006339407, 'train@eng.sdrt.stac_loss@eng.sdrt.stac': 1.427199363708496, 'train@eng.sdrt.stac_runtime': 112.8962, 'train@eng.sdrt.stac_samples_per_second': 84.857, 'train@eng.sdrt.stac_steps_per_second': 2.657, 'epoch': 9.0}
{'loss': 1.4975, 'learning_rate': 5e-06, 'epoch': 9.0}
{'eval_loss': 1.4571603536605835, 'eval_accuracy@eng.sdrt.stac': 0.5423580786026201, 'eval_f1@eng.sdrt.stac': 0.3557199895719965, 'eval_precision@eng.sdrt.stac': 0.39757805291741943, 'eval_recall@eng.sdrt.stac': 0.35778382903670936, 'eval_loss@eng.sdrt.stac': 1.457160472869873, 'eval_runtime': 13.9254, 'eval_samples_per_second': 82.224, 'eval_steps_per_second': 2.585, 'epoch': 9.0}
{'train@eng.sdrt.stac_loss': 1.4149317741394043, 'train@eng.sdrt.stac_accuracy@eng.sdrt.stac': 0.5528183716075157, 'train@eng.sdrt.stac_f1@eng.sdrt.stac': 0.3706627117394985, 'train@eng.sdrt.stac_precision@eng.sdrt.stac': 0.4216453547286346, 'train@eng.sdrt.stac_recall@eng.sdrt.stac': 0.3794879589248612, 'train@eng.sdrt.stac_loss@eng.sdrt.stac': 1.4149317741394043, 'train@eng.sdrt.stac_runtime': 112.6326, 'train@eng.sdrt.stac_samples_per_second': 85.055, 'train@eng.sdrt.stac_steps_per_second': 2.664, 'epoch': 10.0}
{'loss': 1.476, 'learning_rate': 3.3333333333333333e-06, 'epoch': 10.0}
{'eval_loss': 1.4512381553649902, 'eval_accuracy@eng.sdrt.stac': 0.5467248908296943, 'eval_f1@eng.sdrt.stac': 0.3621527325088111, 'eval_precision@eng.sdrt.stac': 0.4071990672869029, 'eval_recall@eng.sdrt.stac': 0.36363995617891054, 'eval_loss@eng.sdrt.stac': 1.4512381553649902, 'eval_runtime': 13.8277, 'eval_samples_per_second': 82.805, 'eval_steps_per_second': 2.603, 'epoch': 10.0}
{'train@eng.sdrt.stac_loss': 1.4074499607086182, 'train@eng.sdrt.stac_accuracy@eng.sdrt.stac': 0.5539665970772443, 'train@eng.sdrt.stac_f1@eng.sdrt.stac': 0.3759519791941671, 'train@eng.sdrt.stac_precision@eng.sdrt.stac': 0.41428194949927133, 'train@eng.sdrt.stac_recall@eng.sdrt.stac': 0.3839163610732712, 'train@eng.sdrt.stac_loss@eng.sdrt.stac': 1.4074500799179077, 'train@eng.sdrt.stac_runtime': 112.803, 'train@eng.sdrt.stac_samples_per_second': 84.927, 'train@eng.sdrt.stac_steps_per_second': 2.66, 'epoch': 11.0}
{'loss': 1.4743, 'learning_rate': 1.6666666666666667e-06, 'epoch': 11.0}
{'eval_loss': 1.4478849172592163, 'eval_accuracy@eng.sdrt.stac': 0.5449781659388646, 'eval_f1@eng.sdrt.stac': 0.3667036196049395, 'eval_precision@eng.sdrt.stac': 0.4073054660126493, 'eval_recall@eng.sdrt.stac': 0.36762969078742636, 'eval_loss@eng.sdrt.stac': 1.4478850364685059, 'eval_runtime': 13.824, 'eval_samples_per_second': 82.827, 'eval_steps_per_second': 2.604, 'epoch': 11.0}
{'train@eng.sdrt.stac_loss': 1.4074034690856934, 'train@eng.sdrt.stac_accuracy@eng.sdrt.stac': 0.5550104384133612, 'train@eng.sdrt.stac_f1@eng.sdrt.stac': 0.37578441197223095, 'train@eng.sdrt.stac_precision@eng.sdrt.stac': 0.4152628847710332, 'train@eng.sdrt.stac_recall@eng.sdrt.stac': 0.38454422982363856, 'train@eng.sdrt.stac_loss@eng.sdrt.stac': 1.407403588294983, 'train@eng.sdrt.stac_runtime': 112.9893, 'train@eng.sdrt.stac_samples_per_second': 84.787, 'train@eng.sdrt.stac_steps_per_second': 2.655, 'epoch': 12.0}
{'loss': 1.4708, 'learning_rate': 0.0, 'epoch': 12.0}
{'eval_loss': 1.4470291137695312, 'eval_accuracy@eng.sdrt.stac': 0.5493449781659389, 'eval_f1@eng.sdrt.stac': 0.36838950413354166, 'eval_precision@eng.sdrt.stac': 0.4089839988329051, 'eval_recall@eng.sdrt.stac': 0.3691323829966448, 'eval_loss@eng.sdrt.stac': 1.4470292329788208, 'eval_runtime': 13.9007, 'eval_samples_per_second': 82.37, 'eval_steps_per_second': 2.59, 'epoch': 12.0}
{'train_runtime': 4375.3417, 'train_samples_per_second': 26.275, 'train_steps_per_second': 0.823, 'train_loss': 1.6275662146674261, 'epoch': 12.0}
***** train_logger metrics *****
  epoch                    =       12.0
  train_loss               =     1.1145
  train_runtime            = 5:28:42.61
  train_samples_per_second =     26.723
  train_steps_per_second   =      0.835
-------------------------------------------------------------------
Lang1:  eng.rst.gum    Lang2:  eng.sdrt.stac
Saving run to:  runs/full_shot/FullShot=v4_eng.rst.gum_eng.sdrt.stac_lrfunc=Adafactor_lr=2e-05_plm=bert-base-multilingual-cased
Running with params: BERT_MODEL=bert-base-multilingual-cased lr=2e-05
read 13897 examples
read 2149 examples
read 2091 examples
read 9580 examples
read 1145 examples
read 1510 examples
Total prediction labels:  39
BertAdapterModel(
  (shared_parameters): ModuleDict()
  (bert): BertModel(
    (shared_parameters): ModuleDict()
    (invertible_adapters): ModuleDict(
      (en): NICECouplingBlock(
        (F): Sequential(
          (0): Linear(in_features=384, out_features=192, bias=True)
          (1): Activation_Function_Class(
            (f): ReLU()
          )
          (2): Linear(in_features=192, out_features=384, bias=True)
        )
        (G): Sequential(
          (0): Linear(in_features=384, out_features=192, bias=True)
          (1): Activation_Function_Class(
            (f): ReLU()
          )
          (2): Linear(in_features=192, out_features=384, bias=True)
        )
      )
    )
    (embeddings): BertEmbeddings(
      (word_embeddings): Embedding(119547, 768, padding_idx=0)
      (position_embeddings): Embedding(512, 768)
      (token_type_embeddings): Embedding(2, 768)
      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
      (dropout): Dropout(p=0.1, inplace=False)
    )
    (encoder): BertEncoder(
      (layer): ModuleList(
        (0): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (en): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (1): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (en): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (2): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (en): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (3): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (en): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (4): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (en): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (5): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (en): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (6): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (en): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (7): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (en): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (8): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (en): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (9): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (en): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (10): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (en): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (11): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (en): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
      )
    )
    (pooler): BertPooler(
      (dense): Linear(in_features=768, out_features=768, bias=True)
      (activation): Tanh()
    )
    (prefix_tuning): PrefixTuningPool(
      (prefix_tunings): ModuleDict()
    )
  )
  (heads): ModuleDict(
    (disrpt): ClassificationHead(
      (0): Dropout(p=0.1, inplace=False)
      (1): Linear(in_features=768, out_features=768, bias=True)
      (2): Activation_Function_Class(
        (f): Tanh()
      )
      (3): Dropout(p=0.1, inplace=False)
      (4): Linear(in_features=768, out_features=39, bias=True)
    )
  )
)
{'train@eng.rst.gum_loss': 2.554147481918335, 'train@eng.rst.gum_accuracy@eng.rst.gum': 0.23415125566669065, 'train@eng.rst.gum_f1@eng.rst.gum': 0.03270232301151008, 'train@eng.rst.gum_precision@eng.rst.gum': 0.06518081955419942, 'train@eng.rst.gum_recall@eng.rst.gum': 0.052828597152502335, 'train@eng.rst.gum_loss@eng.rst.gum': 2.554147481918335, 'train@eng.rst.gum_runtime': 164.0458, 'train@eng.rst.gum_samples_per_second': 84.714, 'train@eng.rst.gum_steps_per_second': 2.652, 'epoch': 1.0}
{'loss': 2.8358, 'learning_rate': 1.8333333333333333e-05, 'epoch': 1.0}
{'eval_loss': 2.6308984756469727, 'eval_accuracy@eng.rst.gum': 0.23266635644485809, 'eval_f1@eng.rst.gum': 0.036827938528507013, 'eval_precision@eng.rst.gum': 0.06357492127895441, 'eval_recall@eng.rst.gum': 0.05621162340424312, 'eval_loss@eng.rst.gum': 2.6308987140655518, 'eval_runtime': 25.7257, 'eval_samples_per_second': 83.535, 'eval_steps_per_second': 2.643, 'epoch': 1.0}
{'train@eng.rst.gum_loss': 2.1123058795928955, 'train@eng.rst.gum_accuracy@eng.rst.gum': 0.37727567100813125, 'train@eng.rst.gum_f1@eng.rst.gum': 0.13546381412669503, 'train@eng.rst.gum_precision@eng.rst.gum': 0.19215106003077892, 'train@eng.rst.gum_recall@eng.rst.gum': 0.1523239951742525, 'train@eng.rst.gum_loss@eng.rst.gum': 2.1123056411743164, 'train@eng.rst.gum_runtime': 164.4186, 'train@eng.rst.gum_samples_per_second': 84.522, 'train@eng.rst.gum_steps_per_second': 2.646, 'epoch': 2.0}
{'loss': 2.3948, 'learning_rate': 1.6666666666666667e-05, 'epoch': 2.0}
{'eval_loss': 2.2116940021514893, 'eval_accuracy@eng.rst.gum': 0.35318752908329454, 'eval_f1@eng.rst.gum': 0.133949077741968, 'eval_precision@eng.rst.gum': 0.19509979307437259, 'eval_recall@eng.rst.gum': 0.1550903254318463, 'eval_loss@eng.rst.gum': 2.21169376373291, 'eval_runtime': 25.7467, 'eval_samples_per_second': 83.467, 'eval_steps_per_second': 2.641, 'epoch': 2.0}
{'train@eng.rst.gum_loss': 1.8270338773727417, 'train@eng.rst.gum_accuracy@eng.rst.gum': 0.4736274015974671, 'train@eng.rst.gum_f1@eng.rst.gum': 0.24947785780576376, 'train@eng.rst.gum_precision@eng.rst.gum': 0.35513047536501074, 'train@eng.rst.gum_recall@eng.rst.gum': 0.26604826681055904, 'train@eng.rst.gum_loss@eng.rst.gum': 1.8270338773727417, 'train@eng.rst.gum_runtime': 163.9444, 'train@eng.rst.gum_samples_per_second': 84.767, 'train@eng.rst.gum_steps_per_second': 2.653, 'epoch': 3.0}
{'loss': 2.0304, 'learning_rate': 1.5000000000000002e-05, 'epoch': 3.0}
{'eval_loss': 1.9570844173431396, 'eval_accuracy@eng.rst.gum': 0.4448580735225686, 'eval_f1@eng.rst.gum': 0.24752247771295774, 'eval_precision@eng.rst.gum': 0.3036436507565335, 'eval_recall@eng.rst.gum': 0.26648529770877705, 'eval_loss@eng.rst.gum': 1.9570846557617188, 'eval_runtime': 25.7597, 'eval_samples_per_second': 83.425, 'eval_steps_per_second': 2.64, 'epoch': 3.0}
{'train@eng.rst.gum_loss': 1.6900979280471802, 'train@eng.rst.gum_accuracy@eng.rst.gum': 0.5019788443548967, 'train@eng.rst.gum_f1@eng.rst.gum': 0.2885563088282503, 'train@eng.rst.gum_precision@eng.rst.gum': 0.4342514274863633, 'train@eng.rst.gum_recall@eng.rst.gum': 0.3007638929394157, 'train@eng.rst.gum_loss@eng.rst.gum': 1.6900979280471802, 'train@eng.rst.gum_runtime': 163.8616, 'train@eng.rst.gum_samples_per_second': 84.809, 'train@eng.rst.gum_steps_per_second': 2.655, 'epoch': 4.0}
{'loss': 1.8317, 'learning_rate': 1.3333333333333333e-05, 'epoch': 4.0}
{'eval_loss': 1.852094054222107, 'eval_accuracy@eng.rst.gum': 0.46905537459283386, 'eval_f1@eng.rst.gum': 0.27848325423804837, 'eval_precision@eng.rst.gum': 0.31662579695079446, 'eval_recall@eng.rst.gum': 0.29438750433569083, 'eval_loss@eng.rst.gum': 1.8520941734313965, 'eval_runtime': 25.7273, 'eval_samples_per_second': 83.53, 'eval_steps_per_second': 2.643, 'epoch': 4.0}
{'train@eng.rst.gum_loss': 1.6105215549468994, 'train@eng.rst.gum_accuracy@eng.rst.gum': 0.5185291789594877, 'train@eng.rst.gum_f1@eng.rst.gum': 0.3328866627138608, 'train@eng.rst.gum_precision@eng.rst.gum': 0.4418577142315529, 'train@eng.rst.gum_recall@eng.rst.gum': 0.33889646169926235, 'train@eng.rst.gum_loss@eng.rst.gum': 1.6105215549468994, 'train@eng.rst.gum_runtime': 163.8581, 'train@eng.rst.gum_samples_per_second': 84.811, 'train@eng.rst.gum_steps_per_second': 2.655, 'epoch': 5.0}
{'loss': 1.7245, 'learning_rate': 1.1666666666666668e-05, 'epoch': 5.0}
{'eval_loss': 1.7885103225708008, 'eval_accuracy@eng.rst.gum': 0.4848766868310842, 'eval_f1@eng.rst.gum': 0.32369154466464145, 'eval_precision@eng.rst.gum': 0.3545697814438263, 'eval_recall@eng.rst.gum': 0.3358651992998586, 'eval_loss@eng.rst.gum': 1.7885104417800903, 'eval_runtime': 25.7058, 'eval_samples_per_second': 83.6, 'eval_steps_per_second': 2.645, 'epoch': 5.0}
{'train@eng.rst.gum_loss': 1.5574103593826294, 'train@eng.rst.gum_accuracy@eng.rst.gum': 0.5352953874937036, 'train@eng.rst.gum_f1@eng.rst.gum': 0.371947033715796, 'train@eng.rst.gum_precision@eng.rst.gum': 0.4884371153057674, 'train@eng.rst.gum_recall@eng.rst.gum': 0.3738658026377733, 'train@eng.rst.gum_loss@eng.rst.gum': 1.5574102401733398, 'train@eng.rst.gum_runtime': 163.8009, 'train@eng.rst.gum_samples_per_second': 84.841, 'train@eng.rst.gum_steps_per_second': 2.656, 'epoch': 6.0}
{'loss': 1.6626, 'learning_rate': 1e-05, 'epoch': 6.0}
{'eval_loss': 1.752628207206726, 'eval_accuracy@eng.rst.gum': 0.4965100046533271, 'eval_f1@eng.rst.gum': 0.3460287114565803, 'eval_precision@eng.rst.gum': 0.3720877422993536, 'eval_recall@eng.rst.gum': 0.35791275959195384, 'eval_loss@eng.rst.gum': 1.752628207206726, 'eval_runtime': 25.7418, 'eval_samples_per_second': 83.483, 'eval_steps_per_second': 2.642, 'epoch': 6.0}
{'train@eng.rst.gum_loss': 1.5205186605453491, 'train@eng.rst.gum_accuracy@eng.rst.gum': 0.5429948909836655, 'train@eng.rst.gum_f1@eng.rst.gum': 0.38394902996349123, 'train@eng.rst.gum_precision@eng.rst.gum': 0.54609737889994, 'train@eng.rst.gum_recall@eng.rst.gum': 0.3880502863272037, 'train@eng.rst.gum_loss@eng.rst.gum': 1.5205186605453491, 'train@eng.rst.gum_runtime': 163.8132, 'train@eng.rst.gum_samples_per_second': 84.834, 'train@eng.rst.gum_steps_per_second': 2.655, 'epoch': 7.0}
{'loss': 1.613, 'learning_rate': 8.333333333333334e-06, 'epoch': 7.0}
{'eval_loss': 1.7290616035461426, 'eval_accuracy@eng.rst.gum': 0.501628664495114, 'eval_f1@eng.rst.gum': 0.3540563625386698, 'eval_precision@eng.rst.gum': 0.4599771058140062, 'eval_recall@eng.rst.gum': 0.37044570098795104, 'eval_loss@eng.rst.gum': 1.7290617227554321, 'eval_runtime': 25.7017, 'eval_samples_per_second': 83.613, 'eval_steps_per_second': 2.646, 'epoch': 7.0}
{'train@eng.rst.gum_loss': 1.4954278469085693, 'train@eng.rst.gum_accuracy@eng.rst.gum': 0.5508383104267108, 'train@eng.rst.gum_f1@eng.rst.gum': 0.39482472173147753, 'train@eng.rst.gum_precision@eng.rst.gum': 0.5393930338500985, 'train@eng.rst.gum_recall@eng.rst.gum': 0.3990849710474747, 'train@eng.rst.gum_loss@eng.rst.gum': 1.4954279661178589, 'train@eng.rst.gum_runtime': 164.2114, 'train@eng.rst.gum_samples_per_second': 84.629, 'train@eng.rst.gum_steps_per_second': 2.649, 'epoch': 8.0}
{'loss': 1.5851, 'learning_rate': 6.666666666666667e-06, 'epoch': 8.0}
{'eval_loss': 1.7120555639266968, 'eval_accuracy@eng.rst.gum': 0.5076779897626803, 'eval_f1@eng.rst.gum': 0.36939476306226504, 'eval_precision@eng.rst.gum': 0.46052288027837796, 'eval_recall@eng.rst.gum': 0.3870612898477334, 'eval_loss@eng.rst.gum': 1.7120554447174072, 'eval_runtime': 25.7167, 'eval_samples_per_second': 83.564, 'eval_steps_per_second': 2.644, 'epoch': 8.0}
{'train@eng.rst.gum_loss': 1.475719928741455, 'train@eng.rst.gum_accuracy@eng.rst.gum': 0.5587536878462978, 'train@eng.rst.gum_f1@eng.rst.gum': 0.40715912666366444, 'train@eng.rst.gum_precision@eng.rst.gum': 0.5337817755143582, 'train@eng.rst.gum_recall@eng.rst.gum': 0.41147161949702454, 'train@eng.rst.gum_loss@eng.rst.gum': 1.4757198095321655, 'train@eng.rst.gum_runtime': 163.933, 'train@eng.rst.gum_samples_per_second': 84.772, 'train@eng.rst.gum_steps_per_second': 2.654, 'epoch': 9.0}
{'loss': 1.5621, 'learning_rate': 5e-06, 'epoch': 9.0}
{'eval_loss': 1.6952160596847534, 'eval_accuracy@eng.rst.gum': 0.5058166589111215, 'eval_f1@eng.rst.gum': 0.3708264158874944, 'eval_precision@eng.rst.gum': 0.4526694770508001, 'eval_recall@eng.rst.gum': 0.39169744536107687, 'eval_loss@eng.rst.gum': 1.6952159404754639, 'eval_runtime': 25.7358, 'eval_samples_per_second': 83.502, 'eval_steps_per_second': 2.642, 'epoch': 9.0}
{'train@eng.rst.gum_loss': 1.4602454900741577, 'train@eng.rst.gum_accuracy@eng.rst.gum': 0.5604806792832986, 'train@eng.rst.gum_f1@eng.rst.gum': 0.4140870664959605, 'train@eng.rst.gum_precision@eng.rst.gum': 0.5448733474928144, 'train@eng.rst.gum_recall@eng.rst.gum': 0.41312249059322687, 'train@eng.rst.gum_loss@eng.rst.gum': 1.4602454900741577, 'train@eng.rst.gum_runtime': 163.8957, 'train@eng.rst.gum_samples_per_second': 84.792, 'train@eng.rst.gum_steps_per_second': 2.654, 'epoch': 10.0}
{'loss': 1.5435, 'learning_rate': 3.3333333333333333e-06, 'epoch': 10.0}
{'eval_loss': 1.6868361234664917, 'eval_accuracy@eng.rst.gum': 0.5090739879013495, 'eval_f1@eng.rst.gum': 0.3747319722159873, 'eval_precision@eng.rst.gum': 0.4684220330379391, 'eval_recall@eng.rst.gum': 0.39030037928953976, 'eval_loss@eng.rst.gum': 1.6868361234664917, 'eval_runtime': 25.714, 'eval_samples_per_second': 83.573, 'eval_steps_per_second': 2.644, 'epoch': 10.0}
{'train@eng.rst.gum_loss': 1.4525998830795288, 'train@eng.rst.gum_accuracy@eng.rst.gum': 0.5632150823918831, 'train@eng.rst.gum_f1@eng.rst.gum': 0.4180207432242325, 'train@eng.rst.gum_precision@eng.rst.gum': 0.5371490248569276, 'train@eng.rst.gum_recall@eng.rst.gum': 0.4199445788020584, 'train@eng.rst.gum_loss@eng.rst.gum': 1.4525997638702393, 'train@eng.rst.gum_runtime': 163.9562, 'train@eng.rst.gum_samples_per_second': 84.76, 'train@eng.rst.gum_steps_per_second': 2.653, 'epoch': 11.0}
{'loss': 1.536, 'learning_rate': 1.6666666666666667e-06, 'epoch': 11.0}
{'eval_loss': 1.6807149648666382, 'eval_accuracy@eng.rst.gum': 0.5104699860400186, 'eval_f1@eng.rst.gum': 0.37861524317907824, 'eval_precision@eng.rst.gum': 0.4636597986213435, 'eval_recall@eng.rst.gum': 0.3952776677817004, 'eval_loss@eng.rst.gum': 1.6807152032852173, 'eval_runtime': 25.7368, 'eval_samples_per_second': 83.499, 'eval_steps_per_second': 2.642, 'epoch': 11.0}
{'train@eng.rst.gum_loss': 1.4495130777359009, 'train@eng.rst.gum_accuracy@eng.rst.gum': 0.5633589983449665, 'train@eng.rst.gum_f1@eng.rst.gum': 0.4180935658619899, 'train@eng.rst.gum_precision@eng.rst.gum': 0.5389737568201454, 'train@eng.rst.gum_recall@eng.rst.gum': 0.4193828622428034, 'train@eng.rst.gum_loss@eng.rst.gum': 1.4495129585266113, 'train@eng.rst.gum_runtime': 163.7106, 'train@eng.rst.gum_samples_per_second': 84.888, 'train@eng.rst.gum_steps_per_second': 2.657, 'epoch': 12.0}
{'loss': 1.5244, 'learning_rate': 0.0, 'epoch': 12.0}
{'eval_loss': 1.67905855178833, 'eval_accuracy@eng.rst.gum': 0.5132619823173569, 'eval_f1@eng.rst.gum': 0.38088787519780704, 'eval_precision@eng.rst.gum': 0.4719742505858879, 'eval_recall@eng.rst.gum': 0.396457792270213, 'eval_loss@eng.rst.gum': 1.67905855178833, 'eval_runtime': 25.699, 'eval_samples_per_second': 83.622, 'eval_steps_per_second': 2.646, 'epoch': 12.0}
{'train_runtime': 6417.2037, 'train_samples_per_second': 25.987, 'train_steps_per_second': 0.813, 'train_loss': 1.8203284954202585, 'epoch': 12.0}
***** train_log metrics *****
  epoch                    =       12.0
  train_loss               =     1.8203
  train_runtime            = 1:46:57.20
  train_samples_per_second =     25.987
  train_steps_per_second   =      0.813
{'train@eng.sdrt.stac_loss': 1.9303696155548096, 'train@eng.sdrt.stac_accuracy@eng.sdrt.stac': 0.38559498956158667, 'train@eng.sdrt.stac_f1@eng.sdrt.stac': 0.10691345473395507, 'train@eng.sdrt.stac_precision@eng.sdrt.stac': 0.13538294112147628, 'train@eng.sdrt.stac_recall@eng.sdrt.stac': 0.13239275122721367, 'train@eng.sdrt.stac_loss@eng.sdrt.stac': 1.9303696155548096, 'train@eng.sdrt.stac_runtime': 112.8222, 'train@eng.sdrt.stac_samples_per_second': 84.912, 'train@eng.sdrt.stac_steps_per_second': 2.659, 'epoch': 1.0}
{'loss': 2.2536, 'learning_rate': 1.8333333333333333e-05, 'epoch': 1.0}
{'eval_loss': 1.8967376947402954, 'eval_accuracy@eng.sdrt.stac': 0.4069868995633188, 'eval_f1@eng.sdrt.stac': 0.10755623593047953, 'eval_precision@eng.sdrt.stac': 0.11261458602241371, 'eval_recall@eng.sdrt.stac': 0.13571787403531535, 'eval_loss@eng.sdrt.stac': 1.8967379331588745, 'eval_runtime': 13.8503, 'eval_samples_per_second': 82.67, 'eval_steps_per_second': 2.599, 'epoch': 1.0}
{'train@eng.sdrt.stac_loss': 1.7015318870544434, 'train@eng.sdrt.stac_accuracy@eng.sdrt.stac': 0.45855949895615866, 'train@eng.sdrt.stac_f1@eng.sdrt.stac': 0.18870011854290775, 'train@eng.sdrt.stac_precision@eng.sdrt.stac': 0.2590185162704681, 'train@eng.sdrt.stac_recall@eng.sdrt.stac': 0.2026681532025512, 'train@eng.sdrt.stac_loss@eng.sdrt.stac': 1.7015318870544434, 'train@eng.sdrt.stac_runtime': 113.0738, 'train@eng.sdrt.stac_samples_per_second': 84.723, 'train@eng.sdrt.stac_steps_per_second': 2.653, 'epoch': 2.0}
{'loss': 1.856, 'learning_rate': 1.6666666666666667e-05, 'epoch': 2.0}
{'eval_loss': 1.67108952999115, 'eval_accuracy@eng.sdrt.stac': 0.45327510917030567, 'eval_f1@eng.sdrt.stac': 0.19178073732541034, 'eval_precision@eng.sdrt.stac': 0.23394019882042957, 'eval_recall@eng.sdrt.stac': 0.2146739313879239, 'eval_loss@eng.sdrt.stac': 1.67108952999115, 'eval_runtime': 13.8864, 'eval_samples_per_second': 82.455, 'eval_steps_per_second': 2.592, 'epoch': 2.0}
{'train@eng.sdrt.stac_loss': 1.6018939018249512, 'train@eng.sdrt.stac_accuracy@eng.sdrt.stac': 0.4966597077244259, 'train@eng.sdrt.stac_f1@eng.sdrt.stac': 0.2599590748616405, 'train@eng.sdrt.stac_precision@eng.sdrt.stac': 0.3178557710289332, 'train@eng.sdrt.stac_recall@eng.sdrt.stac': 0.2769986477499468, 'train@eng.sdrt.stac_loss@eng.sdrt.stac': 1.6018937826156616, 'train@eng.sdrt.stac_runtime': 112.8874, 'train@eng.sdrt.stac_samples_per_second': 84.863, 'train@eng.sdrt.stac_steps_per_second': 2.658, 'epoch': 3.0}
{'loss': 1.7054, 'learning_rate': 1.5000000000000002e-05, 'epoch': 3.0}
{'eval_loss': 1.5801281929016113, 'eval_accuracy@eng.sdrt.stac': 0.4882096069868996, 'eval_f1@eng.sdrt.stac': 0.2609124485322647, 'eval_precision@eng.sdrt.stac': 0.2930455833406739, 'eval_recall@eng.sdrt.stac': 0.28971346145326254, 'eval_loss@eng.sdrt.stac': 1.5801280736923218, 'eval_runtime': 13.8459, 'eval_samples_per_second': 82.696, 'eval_steps_per_second': 2.6, 'epoch': 3.0}
{'train@eng.sdrt.stac_loss': 1.5395950078964233, 'train@eng.sdrt.stac_accuracy@eng.sdrt.stac': 0.5175365344467641, 'train@eng.sdrt.stac_f1@eng.sdrt.stac': 0.3019281152678255, 'train@eng.sdrt.stac_precision@eng.sdrt.stac': 0.3900989350031529, 'train@eng.sdrt.stac_recall@eng.sdrt.stac': 0.31794169461330596, 'train@eng.sdrt.stac_loss@eng.sdrt.stac': 1.5395952463150024, 'train@eng.sdrt.stac_runtime': 112.8975, 'train@eng.sdrt.stac_samples_per_second': 84.856, 'train@eng.sdrt.stac_steps_per_second': 2.657, 'epoch': 4.0}
{'loss': 1.6211, 'learning_rate': 1.3333333333333333e-05, 'epoch': 4.0}
{'eval_loss': 1.5140941143035889, 'eval_accuracy@eng.sdrt.stac': 0.5117903930131005, 'eval_f1@eng.sdrt.stac': 0.305800451307375, 'eval_precision@eng.sdrt.stac': 0.3653924437306584, 'eval_recall@eng.sdrt.stac': 0.31566051637608394, 'eval_loss@eng.sdrt.stac': 1.5140941143035889, 'eval_runtime': 13.8387, 'eval_samples_per_second': 82.739, 'eval_steps_per_second': 2.601, 'epoch': 4.0}
{'train@eng.sdrt.stac_loss': 1.506610631942749, 'train@eng.sdrt.stac_accuracy@eng.sdrt.stac': 0.5302713987473904, 'train@eng.sdrt.stac_f1@eng.sdrt.stac': 0.3177524594422167, 'train@eng.sdrt.stac_precision@eng.sdrt.stac': 0.366434723136746, 'train@eng.sdrt.stac_recall@eng.sdrt.stac': 0.33585689649953865, 'train@eng.sdrt.stac_loss@eng.sdrt.stac': 1.5066107511520386, 'train@eng.sdrt.stac_runtime': 112.888, 'train@eng.sdrt.stac_samples_per_second': 84.863, 'train@eng.sdrt.stac_steps_per_second': 2.658, 'epoch': 5.0}
{'loss': 1.5732, 'learning_rate': 1.1666666666666668e-05, 'epoch': 5.0}
{'eval_loss': 1.4859567880630493, 'eval_accuracy@eng.sdrt.stac': 0.5292576419213973, 'eval_f1@eng.sdrt.stac': 0.313300904431482, 'eval_precision@eng.sdrt.stac': 0.3333460116670736, 'eval_recall@eng.sdrt.stac': 0.3308839518865921, 'eval_loss@eng.sdrt.stac': 1.4859566688537598, 'eval_runtime': 13.8648, 'eval_samples_per_second': 82.583, 'eval_steps_per_second': 2.597, 'epoch': 5.0}
{'train@eng.sdrt.stac_loss': 1.46879243850708, 'train@eng.sdrt.stac_accuracy@eng.sdrt.stac': 0.5375782881002088, 'train@eng.sdrt.stac_f1@eng.sdrt.stac': 0.33232081613851927, 'train@eng.sdrt.stac_precision@eng.sdrt.stac': 0.3742146446548423, 'train@eng.sdrt.stac_recall@eng.sdrt.stac': 0.34657844239298286, 'train@eng.sdrt.stac_loss@eng.sdrt.stac': 1.4687925577163696, 'train@eng.sdrt.stac_runtime': 112.7652, 'train@eng.sdrt.stac_samples_per_second': 84.955, 'train@eng.sdrt.stac_steps_per_second': 2.66, 'epoch': 6.0}
{'loss': 1.5358, 'learning_rate': 1e-05, 'epoch': 6.0}
{'eval_loss': 1.4759303331375122, 'eval_accuracy@eng.sdrt.stac': 0.519650655021834, 'eval_f1@eng.sdrt.stac': 0.31712849531162124, 'eval_precision@eng.sdrt.stac': 0.3472969770559773, 'eval_recall@eng.sdrt.stac': 0.3355849438957394, 'eval_loss@eng.sdrt.stac': 1.4759303331375122, 'eval_runtime': 13.8378, 'eval_samples_per_second': 82.745, 'eval_steps_per_second': 2.602, 'epoch': 6.0}
{'train@eng.sdrt.stac_loss': 1.457690715789795, 'train@eng.sdrt.stac_accuracy@eng.sdrt.stac': 0.544258872651357, 'train@eng.sdrt.stac_f1@eng.sdrt.stac': 0.34088987023238365, 'train@eng.sdrt.stac_precision@eng.sdrt.stac': 0.37762886844135446, 'train@eng.sdrt.stac_recall@eng.sdrt.stac': 0.35617534555691294, 'train@eng.sdrt.stac_loss@eng.sdrt.stac': 1.4576908349990845, 'train@eng.sdrt.stac_runtime': 113.0467, 'train@eng.sdrt.stac_samples_per_second': 84.744, 'train@eng.sdrt.stac_steps_per_second': 2.654, 'epoch': 7.0}
{'loss': 1.5136, 'learning_rate': 8.333333333333334e-06, 'epoch': 7.0}
{'eval_loss': 1.4627822637557983, 'eval_accuracy@eng.sdrt.stac': 0.5283842794759825, 'eval_f1@eng.sdrt.stac': 0.3306452278990149, 'eval_precision@eng.sdrt.stac': 0.3594970584086614, 'eval_recall@eng.sdrt.stac': 0.341354942823473, 'eval_loss@eng.sdrt.stac': 1.4627822637557983, 'eval_runtime': 13.9269, 'eval_samples_per_second': 82.215, 'eval_steps_per_second': 2.585, 'epoch': 7.0}
{'train@eng.sdrt.stac_loss': 1.433085560798645, 'train@eng.sdrt.stac_accuracy@eng.sdrt.stac': 0.5532359081419624, 'train@eng.sdrt.stac_f1@eng.sdrt.stac': 0.35536827229183127, 'train@eng.sdrt.stac_precision@eng.sdrt.stac': 0.38384951424025643, 'train@eng.sdrt.stac_recall@eng.sdrt.stac': 0.3671083884523527, 'train@eng.sdrt.stac_loss@eng.sdrt.stac': 1.433085560798645, 'train@eng.sdrt.stac_runtime': 112.8841, 'train@eng.sdrt.stac_samples_per_second': 84.866, 'train@eng.sdrt.stac_steps_per_second': 2.658, 'epoch': 8.0}
{'loss': 1.4927, 'learning_rate': 6.666666666666667e-06, 'epoch': 8.0}
{'eval_loss': 1.441140055656433, 'eval_accuracy@eng.sdrt.stac': 0.5406113537117904, 'eval_f1@eng.sdrt.stac': 0.35463700265495446, 'eval_precision@eng.sdrt.stac': 0.373976244207611, 'eval_recall@eng.sdrt.stac': 0.36737721490541675, 'eval_loss@eng.sdrt.stac': 1.441140055656433, 'eval_runtime': 13.8644, 'eval_samples_per_second': 82.586, 'eval_steps_per_second': 2.597, 'epoch': 8.0}
{'train@eng.sdrt.stac_loss': 1.4220863580703735, 'train@eng.sdrt.stac_accuracy@eng.sdrt.stac': 0.5554279749478079, 'train@eng.sdrt.stac_f1@eng.sdrt.stac': 0.3604480600764762, 'train@eng.sdrt.stac_precision@eng.sdrt.stac': 0.3820708432078379, 'train@eng.sdrt.stac_recall@eng.sdrt.stac': 0.37525035157450626, 'train@eng.sdrt.stac_loss@eng.sdrt.stac': 1.4220863580703735, 'train@eng.sdrt.stac_runtime': 112.9857, 'train@eng.sdrt.stac_samples_per_second': 84.789, 'train@eng.sdrt.stac_steps_per_second': 2.655, 'epoch': 9.0}
{'loss': 1.4753, 'learning_rate': 5e-06, 'epoch': 9.0}
{'eval_loss': 1.4298348426818848, 'eval_accuracy@eng.sdrt.stac': 0.5458515283842795, 'eval_f1@eng.sdrt.stac': 0.36158567957695376, 'eval_precision@eng.sdrt.stac': 0.3723901810636942, 'eval_recall@eng.sdrt.stac': 0.3730849525207989, 'eval_loss@eng.sdrt.stac': 1.4298347234725952, 'eval_runtime': 13.8754, 'eval_samples_per_second': 82.52, 'eval_steps_per_second': 2.595, 'epoch': 9.0}
{'train@eng.sdrt.stac_loss': 1.4101372957229614, 'train@eng.sdrt.stac_accuracy@eng.sdrt.stac': 0.5581419624217119, 'train@eng.sdrt.stac_f1@eng.sdrt.stac': 0.38659259714027694, 'train@eng.sdrt.stac_precision@eng.sdrt.stac': 0.4087548508796811, 'train@eng.sdrt.stac_recall@eng.sdrt.stac': 0.4010111584848937, 'train@eng.sdrt.stac_loss@eng.sdrt.stac': 1.410137414932251, 'train@eng.sdrt.stac_runtime': 113.0221, 'train@eng.sdrt.stac_samples_per_second': 84.762, 'train@eng.sdrt.stac_steps_per_second': 2.654, 'epoch': 10.0}
{'loss': 1.4601, 'learning_rate': 3.3333333333333333e-06, 'epoch': 10.0}
{'eval_loss': 1.4259989261627197, 'eval_accuracy@eng.sdrt.stac': 0.5475982532751091, 'eval_f1@eng.sdrt.stac': 0.3681034278017181, 'eval_precision@eng.sdrt.stac': 0.38049625769861123, 'eval_recall@eng.sdrt.stac': 0.3811105965243, 'eval_loss@eng.sdrt.stac': 1.4259988069534302, 'eval_runtime': 13.8894, 'eval_samples_per_second': 82.437, 'eval_steps_per_second': 2.592, 'epoch': 10.0}
{'train@eng.sdrt.stac_loss': 1.404212474822998, 'train@eng.sdrt.stac_accuracy@eng.sdrt.stac': 0.5590814196242171, 'train@eng.sdrt.stac_f1@eng.sdrt.stac': 0.3883702956693643, 'train@eng.sdrt.stac_precision@eng.sdrt.stac': 0.4067446743633564, 'train@eng.sdrt.stac_recall@eng.sdrt.stac': 0.40409612971132297, 'train@eng.sdrt.stac_loss@eng.sdrt.stac': 1.404212474822998, 'train@eng.sdrt.stac_runtime': 112.7922, 'train@eng.sdrt.stac_samples_per_second': 84.935, 'train@eng.sdrt.stac_steps_per_second': 2.66, 'epoch': 11.0}
{'loss': 1.4568, 'learning_rate': 1.6666666666666667e-06, 'epoch': 11.0}
{'eval_loss': 1.4230785369873047, 'eval_accuracy@eng.sdrt.stac': 0.5519650655021834, 'eval_f1@eng.sdrt.stac': 0.3711741515061474, 'eval_precision@eng.sdrt.stac': 0.3770894580770797, 'eval_recall@eng.sdrt.stac': 0.38524800401863235, 'eval_loss@eng.sdrt.stac': 1.4230784177780151, 'eval_runtime': 13.8573, 'eval_samples_per_second': 82.628, 'eval_steps_per_second': 2.598, 'epoch': 11.0}
{'train@eng.sdrt.stac_loss': 1.403498888015747, 'train@eng.sdrt.stac_accuracy@eng.sdrt.stac': 0.5606471816283924, 'train@eng.sdrt.stac_f1@eng.sdrt.stac': 0.38956655108530847, 'train@eng.sdrt.stac_precision@eng.sdrt.stac': 0.4070370838987265, 'train@eng.sdrt.stac_recall@eng.sdrt.stac': 0.40555101684966777, 'train@eng.sdrt.stac_loss@eng.sdrt.stac': 1.403498888015747, 'train@eng.sdrt.stac_runtime': 113.0394, 'train@eng.sdrt.stac_samples_per_second': 84.749, 'train@eng.sdrt.stac_steps_per_second': 2.654, 'epoch': 12.0}
{'loss': 1.455, 'learning_rate': 0.0, 'epoch': 12.0}
{'eval_loss': 1.4230490922927856, 'eval_accuracy@eng.sdrt.stac': 0.548471615720524, 'eval_f1@eng.sdrt.stac': 0.3680012224048379, 'eval_precision@eng.sdrt.stac': 0.3722498520594523, 'eval_recall@eng.sdrt.stac': 0.3833878263087792, 'eval_loss@eng.sdrt.stac': 1.4230490922927856, 'eval_runtime': 13.8718, 'eval_samples_per_second': 82.542, 'eval_steps_per_second': 2.595, 'epoch': 12.0}
{'train_runtime': 4377.6701, 'train_samples_per_second': 26.261, 'train_steps_per_second': 0.822, 'train_loss': 1.6165537940131294, 'epoch': 12.0}
***** train_logger metrics *****
  epoch                    =       12.0
  train_loss               =     1.8203
  train_runtime            = 1:46:57.20
  train_samples_per_second =     25.987
  train_steps_per_second   =      0.813
-------------------------------------------------------------------
Lang1:  eng.rst.rstdt    Lang2:  eng.sdrt.stac
Saving run to:  runs/full_shot/FullShot=v4_eng.rst.rstdt_eng.sdrt.stac_lrfunc=Adafactor_lr=2e-05_plm=bert-base-multilingual-cased
Running with params: BERT_MODEL=bert-base-multilingual-cased lr=2e-05
read 16002 examples
read 1621 examples
read 2155 examples
read 9580 examples
read 1145 examples
read 1510 examples
Total prediction labels:  33
BertAdapterModel(
  (shared_parameters): ModuleDict()
  (bert): BertModel(
    (shared_parameters): ModuleDict()
    (invertible_adapters): ModuleDict(
      (en): NICECouplingBlock(
        (F): Sequential(
          (0): Linear(in_features=384, out_features=192, bias=True)
          (1): Activation_Function_Class(
            (f): ReLU()
          )
          (2): Linear(in_features=192, out_features=384, bias=True)
        )
        (G): Sequential(
          (0): Linear(in_features=384, out_features=192, bias=True)
          (1): Activation_Function_Class(
            (f): ReLU()
          )
          (2): Linear(in_features=192, out_features=384, bias=True)
        )
      )
    )
    (embeddings): BertEmbeddings(
      (word_embeddings): Embedding(119547, 768, padding_idx=0)
      (position_embeddings): Embedding(512, 768)
      (token_type_embeddings): Embedding(2, 768)
      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
      (dropout): Dropout(p=0.1, inplace=False)
    )
    (encoder): BertEncoder(
      (layer): ModuleList(
        (0): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (en): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (1): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (en): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (2): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (en): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (3): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (en): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (4): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (en): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (5): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (en): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (6): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (en): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (7): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (en): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (8): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (en): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (9): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (en): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (10): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (en): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (11): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (en): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
      )
    )
    (pooler): BertPooler(
      (dense): Linear(in_features=768, out_features=768, bias=True)
      (activation): Tanh()
    )
    (prefix_tuning): PrefixTuningPool(
      (prefix_tunings): ModuleDict()
    )
  )
  (heads): ModuleDict(
    (disrpt): ClassificationHead(
      (0): Dropout(p=0.1, inplace=False)
      (1): Linear(in_features=768, out_features=768, bias=True)
      (2): Activation_Function_Class(
        (f): Tanh()
      )
      (3): Dropout(p=0.1, inplace=False)
      (4): Linear(in_features=768, out_features=33, bias=True)
    )
  )
)
{'train@eng.rst.rstdt_loss': 1.7251514196395874, 'train@eng.rst.rstdt_accuracy@eng.rst.rstdt': 0.5149356330458693, 'train@eng.rst.rstdt_f1@eng.rst.rstdt': 0.08453864701258439, 'train@eng.rst.rstdt_precision@eng.rst.rstdt': 0.06946387308921853, 'train@eng.rst.rstdt_recall@eng.rst.rstdt': 0.11115388058156878, 'train@eng.rst.rstdt_loss@eng.rst.rstdt': 1.7251514196395874, 'train@eng.rst.rstdt_runtime': 188.935, 'train@eng.rst.rstdt_samples_per_second': 84.696, 'train@eng.rst.rstdt_steps_per_second': 2.652, 'epoch': 1.0}
{'loss': 2.1776, 'learning_rate': 1.8333333333333333e-05, 'epoch': 1.0}
{'eval_loss': 1.703608512878418, 'eval_accuracy@eng.rst.rstdt': 0.526835286859963, 'eval_f1@eng.rst.rstdt': 0.08476476225805557, 'eval_precision@eng.rst.rstdt': 0.07049382716049382, 'eval_recall@eng.rst.rstdt': 0.10958157582098987, 'eval_loss@eng.rst.rstdt': 1.703608512878418, 'eval_runtime': 19.4626, 'eval_samples_per_second': 83.288, 'eval_steps_per_second': 2.62, 'epoch': 1.0}
{'train@eng.rst.rstdt_loss': 1.460900068283081, 'train@eng.rst.rstdt_accuracy@eng.rst.rstdt': 0.5819272590926134, 'train@eng.rst.rstdt_f1@eng.rst.rstdt': 0.17425381370212747, 'train@eng.rst.rstdt_precision@eng.rst.rstdt': 0.19906546921641027, 'train@eng.rst.rstdt_recall@eng.rst.rstdt': 0.18592844769048908, 'train@eng.rst.rstdt_loss@eng.rst.rstdt': 1.460900068283081, 'train@eng.rst.rstdt_runtime': 188.8458, 'train@eng.rst.rstdt_samples_per_second': 84.736, 'train@eng.rst.rstdt_steps_per_second': 2.653, 'epoch': 2.0}
{'loss': 1.622, 'learning_rate': 1.6666666666666667e-05, 'epoch': 2.0}
{'eval_loss': 1.466788411140442, 'eval_accuracy@eng.rst.rstdt': 0.5909932140653917, 'eval_f1@eng.rst.rstdt': 0.1771037098215981, 'eval_precision@eng.rst.rstdt': 0.18523890184347735, 'eval_recall@eng.rst.rstdt': 0.18544774006157191, 'eval_loss@eng.rst.rstdt': 1.466788649559021, 'eval_runtime': 19.476, 'eval_samples_per_second': 83.231, 'eval_steps_per_second': 2.619, 'epoch': 2.0}
{'train@eng.rst.rstdt_loss': 1.3420889377593994, 'train@eng.rst.rstdt_accuracy@eng.rst.rstdt': 0.6201099862517185, 'train@eng.rst.rstdt_f1@eng.rst.rstdt': 0.24797589614851484, 'train@eng.rst.rstdt_precision@eng.rst.rstdt': 0.41882133247637876, 'train@eng.rst.rstdt_recall@eng.rst.rstdt': 0.24948189491806616, 'train@eng.rst.rstdt_loss@eng.rst.rstdt': 1.3420889377593994, 'train@eng.rst.rstdt_runtime': 189.0091, 'train@eng.rst.rstdt_samples_per_second': 84.663, 'train@eng.rst.rstdt_steps_per_second': 2.651, 'epoch': 3.0}
{'loss': 1.4562, 'learning_rate': 1.5000000000000002e-05, 'epoch': 3.0}
{'eval_loss': 1.3762614727020264, 'eval_accuracy@eng.rst.rstdt': 0.6206045650832819, 'eval_f1@eng.rst.rstdt': 0.2278959819357226, 'eval_precision@eng.rst.rstdt': 0.2582151056400697, 'eval_recall@eng.rst.rstdt': 0.23619826260860693, 'eval_loss@eng.rst.rstdt': 1.3762617111206055, 'eval_runtime': 19.4999, 'eval_samples_per_second': 83.128, 'eval_steps_per_second': 2.615, 'epoch': 3.0}
{'train@eng.rst.rstdt_loss': 1.2694350481033325, 'train@eng.rst.rstdt_accuracy@eng.rst.rstdt': 0.6430446194225722, 'train@eng.rst.rstdt_f1@eng.rst.rstdt': 0.32661931085934987, 'train@eng.rst.rstdt_precision@eng.rst.rstdt': 0.5227115474828542, 'train@eng.rst.rstdt_recall@eng.rst.rstdt': 0.31123292858139046, 'train@eng.rst.rstdt_loss@eng.rst.rstdt': 1.2694350481033325, 'train@eng.rst.rstdt_runtime': 189.6984, 'train@eng.rst.rstdt_samples_per_second': 84.355, 'train@eng.rst.rstdt_steps_per_second': 2.641, 'epoch': 4.0}
{'loss': 1.3567, 'learning_rate': 1.3333333333333333e-05, 'epoch': 4.0}
{'eval_loss': 1.3286497592926025, 'eval_accuracy@eng.rst.rstdt': 0.6397285626156693, 'eval_f1@eng.rst.rstdt': 0.31933115264621853, 'eval_precision@eng.rst.rstdt': 0.3938634695053312, 'eval_recall@eng.rst.rstdt': 0.31419259642180924, 'eval_loss@eng.rst.rstdt': 1.3286497592926025, 'eval_runtime': 19.5691, 'eval_samples_per_second': 82.835, 'eval_steps_per_second': 2.606, 'epoch': 4.0}
{'train@eng.rst.rstdt_loss': 1.2178376913070679, 'train@eng.rst.rstdt_accuracy@eng.rst.rstdt': 0.6550431196100488, 'train@eng.rst.rstdt_f1@eng.rst.rstdt': 0.3546868215686882, 'train@eng.rst.rstdt_precision@eng.rst.rstdt': 0.5160545526107669, 'train@eng.rst.rstdt_recall@eng.rst.rstdt': 0.3294261053310223, 'train@eng.rst.rstdt_loss@eng.rst.rstdt': 1.2178375720977783, 'train@eng.rst.rstdt_runtime': 188.897, 'train@eng.rst.rstdt_samples_per_second': 84.713, 'train@eng.rst.rstdt_steps_per_second': 2.652, 'epoch': 5.0}
{'loss': 1.2987, 'learning_rate': 1.1666666666666668e-05, 'epoch': 5.0}
{'eval_loss': 1.2852801084518433, 'eval_accuracy@eng.rst.rstdt': 0.6403454657618753, 'eval_f1@eng.rst.rstdt': 0.32414300699678633, 'eval_precision@eng.rst.rstdt': 0.4066861990883673, 'eval_recall@eng.rst.rstdt': 0.3123872889079982, 'eval_loss@eng.rst.rstdt': 1.2852801084518433, 'eval_runtime': 19.4452, 'eval_samples_per_second': 83.362, 'eval_steps_per_second': 2.623, 'epoch': 5.0}
{'train@eng.rst.rstdt_loss': 1.1829978227615356, 'train@eng.rst.rstdt_accuracy@eng.rst.rstdt': 0.6608548931383577, 'train@eng.rst.rstdt_f1@eng.rst.rstdt': 0.3715904363635527, 'train@eng.rst.rstdt_precision@eng.rst.rstdt': 0.5193790431100578, 'train@eng.rst.rstdt_recall@eng.rst.rstdt': 0.3402672774296476, 'train@eng.rst.rstdt_loss@eng.rst.rstdt': 1.1829978227615356, 'train@eng.rst.rstdt_runtime': 188.9706, 'train@eng.rst.rstdt_samples_per_second': 84.68, 'train@eng.rst.rstdt_steps_per_second': 2.651, 'epoch': 6.0}
{'loss': 1.2509, 'learning_rate': 1e-05, 'epoch': 6.0}
{'eval_loss': 1.2564773559570312, 'eval_accuracy@eng.rst.rstdt': 0.64898210980876, 'eval_f1@eng.rst.rstdt': 0.3354013315904556, 'eval_precision@eng.rst.rstdt': 0.4191298978330076, 'eval_recall@eng.rst.rstdt': 0.3226250262542003, 'eval_loss@eng.rst.rstdt': 1.2564773559570312, 'eval_runtime': 19.4583, 'eval_samples_per_second': 83.306, 'eval_steps_per_second': 2.621, 'epoch': 6.0}
{'train@eng.rst.rstdt_loss': 1.1614094972610474, 'train@eng.rst.rstdt_accuracy@eng.rst.rstdt': 0.6656667916510436, 'train@eng.rst.rstdt_f1@eng.rst.rstdt': 0.38148036368814225, 'train@eng.rst.rstdt_precision@eng.rst.rstdt': 0.5216165641509057, 'train@eng.rst.rstdt_recall@eng.rst.rstdt': 0.3477308913693979, 'train@eng.rst.rstdt_loss@eng.rst.rstdt': 1.1614094972610474, 'train@eng.rst.rstdt_runtime': 188.8376, 'train@eng.rst.rstdt_samples_per_second': 84.739, 'train@eng.rst.rstdt_steps_per_second': 2.653, 'epoch': 7.0}
{'loss': 1.2281, 'learning_rate': 8.333333333333334e-06, 'epoch': 7.0}
{'eval_loss': 1.239853024482727, 'eval_accuracy@eng.rst.rstdt': 0.6483652066625539, 'eval_f1@eng.rst.rstdt': 0.3525155108018326, 'eval_precision@eng.rst.rstdt': 0.47560557191595526, 'eval_recall@eng.rst.rstdt': 0.3338476431290417, 'eval_loss@eng.rst.rstdt': 1.2398529052734375, 'eval_runtime': 19.4756, 'eval_samples_per_second': 83.232, 'eval_steps_per_second': 2.619, 'epoch': 7.0}
{'train@eng.rst.rstdt_loss': 1.1430002450942993, 'train@eng.rst.rstdt_accuracy@eng.rst.rstdt': 0.6679165104361955, 'train@eng.rst.rstdt_f1@eng.rst.rstdt': 0.3913147345379506, 'train@eng.rst.rstdt_precision@eng.rst.rstdt': 0.5230347586675034, 'train@eng.rst.rstdt_recall@eng.rst.rstdt': 0.36018088885428035, 'train@eng.rst.rstdt_loss@eng.rst.rstdt': 1.1430002450942993, 'train@eng.rst.rstdt_runtime': 189.0282, 'train@eng.rst.rstdt_samples_per_second': 84.654, 'train@eng.rst.rstdt_steps_per_second': 2.65, 'epoch': 8.0}
{'loss': 1.2059, 'learning_rate': 6.666666666666667e-06, 'epoch': 8.0}
{'eval_loss': 1.2325677871704102, 'eval_accuracy@eng.rst.rstdt': 0.6434299814929056, 'eval_f1@eng.rst.rstdt': 0.346307935453312, 'eval_precision@eng.rst.rstdt': 0.45458546069560635, 'eval_recall@eng.rst.rstdt': 0.3357156342128004, 'eval_loss@eng.rst.rstdt': 1.2325677871704102, 'eval_runtime': 19.5118, 'eval_samples_per_second': 83.078, 'eval_steps_per_second': 2.614, 'epoch': 8.0}
{'train@eng.rst.rstdt_loss': 1.1302520036697388, 'train@eng.rst.rstdt_accuracy@eng.rst.rstdt': 0.6707911511061118, 'train@eng.rst.rstdt_f1@eng.rst.rstdt': 0.3979627016809027, 'train@eng.rst.rstdt_precision@eng.rst.rstdt': 0.5808420300187409, 'train@eng.rst.rstdt_recall@eng.rst.rstdt': 0.3646628829840427, 'train@eng.rst.rstdt_loss@eng.rst.rstdt': 1.1302520036697388, 'train@eng.rst.rstdt_runtime': 188.9335, 'train@eng.rst.rstdt_samples_per_second': 84.696, 'train@eng.rst.rstdt_steps_per_second': 2.652, 'epoch': 9.0}
{'loss': 1.1889, 'learning_rate': 5e-06, 'epoch': 9.0}
{'eval_loss': 1.2214276790618896, 'eval_accuracy@eng.rst.rstdt': 0.6458975940777298, 'eval_f1@eng.rst.rstdt': 0.34811472703297247, 'eval_precision@eng.rst.rstdt': 0.43966448783847867, 'eval_recall@eng.rst.rstdt': 0.33923094659370795, 'eval_loss@eng.rst.rstdt': 1.2214276790618896, 'eval_runtime': 19.4718, 'eval_samples_per_second': 83.248, 'eval_steps_per_second': 2.619, 'epoch': 9.0}
{'train@eng.rst.rstdt_loss': 1.1232295036315918, 'train@eng.rst.rstdt_accuracy@eng.rst.rstdt': 0.6693538307711536, 'train@eng.rst.rstdt_f1@eng.rst.rstdt': 0.4028857865320005, 'train@eng.rst.rstdt_precision@eng.rst.rstdt': 0.5681215728694415, 'train@eng.rst.rstdt_recall@eng.rst.rstdt': 0.37126065880894493, 'train@eng.rst.rstdt_loss@eng.rst.rstdt': 1.1232295036315918, 'train@eng.rst.rstdt_runtime': 188.5691, 'train@eng.rst.rstdt_samples_per_second': 84.86, 'train@eng.rst.rstdt_steps_per_second': 2.657, 'epoch': 10.0}
{'loss': 1.176, 'learning_rate': 3.3333333333333333e-06, 'epoch': 10.0}
{'eval_loss': 1.221480131149292, 'eval_accuracy@eng.rst.rstdt': 0.6446637877853177, 'eval_f1@eng.rst.rstdt': 0.36495954798700003, 'eval_precision@eng.rst.rstdt': 0.48484691609617997, 'eval_recall@eng.rst.rstdt': 0.35086130314827146, 'eval_loss@eng.rst.rstdt': 1.2214800119400024, 'eval_runtime': 19.4472, 'eval_samples_per_second': 83.354, 'eval_steps_per_second': 2.622, 'epoch': 10.0}
{'train@eng.rst.rstdt_loss': 1.1171923875808716, 'train@eng.rst.rstdt_accuracy@eng.rst.rstdt': 0.6717910261217348, 'train@eng.rst.rstdt_f1@eng.rst.rstdt': 0.40339470458543286, 'train@eng.rst.rstdt_precision@eng.rst.rstdt': 0.5790724574820333, 'train@eng.rst.rstdt_recall@eng.rst.rstdt': 0.36964120757139196, 'train@eng.rst.rstdt_loss@eng.rst.rstdt': 1.1171923875808716, 'train@eng.rst.rstdt_runtime': 188.6967, 'train@eng.rst.rstdt_samples_per_second': 84.803, 'train@eng.rst.rstdt_steps_per_second': 2.655, 'epoch': 11.0}
{'loss': 1.1706, 'learning_rate': 1.6666666666666667e-06, 'epoch': 11.0}
{'eval_loss': 1.214755654335022, 'eval_accuracy@eng.rst.rstdt': 0.6446637877853177, 'eval_f1@eng.rst.rstdt': 0.35584972849435165, 'eval_precision@eng.rst.rstdt': 0.46835918611470323, 'eval_recall@eng.rst.rstdt': 0.3455739377371425, 'eval_loss@eng.rst.rstdt': 1.2147555351257324, 'eval_runtime': 19.4381, 'eval_samples_per_second': 83.393, 'eval_steps_per_second': 2.624, 'epoch': 11.0}
{'train@eng.rst.rstdt_loss': 1.1154112815856934, 'train@eng.rst.rstdt_accuracy@eng.rst.rstdt': 0.6724784401949756, 'train@eng.rst.rstdt_f1@eng.rst.rstdt': 0.4045251056786291, 'train@eng.rst.rstdt_precision@eng.rst.rstdt': 0.5797409404006397, 'train@eng.rst.rstdt_recall@eng.rst.rstdt': 0.3700837944383564, 'train@eng.rst.rstdt_loss@eng.rst.rstdt': 1.115411400794983, 'train@eng.rst.rstdt_runtime': 188.8368, 'train@eng.rst.rstdt_samples_per_second': 84.74, 'train@eng.rst.rstdt_steps_per_second': 2.653, 'epoch': 12.0}
{'loss': 1.1669, 'learning_rate': 0.0, 'epoch': 12.0}
{'eval_loss': 1.2126022577285767, 'eval_accuracy@eng.rst.rstdt': 0.6452806909315237, 'eval_f1@eng.rst.rstdt': 0.3596574504272807, 'eval_precision@eng.rst.rstdt': 0.4814112261609302, 'eval_recall@eng.rst.rstdt': 0.34995982910732554, 'eval_loss@eng.rst.rstdt': 1.2126022577285767, 'eval_runtime': 19.4768, 'eval_samples_per_second': 83.227, 'eval_steps_per_second': 2.618, 'epoch': 12.0}
{'train_runtime': 7269.8505, 'train_samples_per_second': 26.414, 'train_steps_per_second': 0.827, 'train_loss': 1.3582036569764118, 'epoch': 12.0}
***** train_log metrics *****
  epoch                    =       12.0
  train_loss               =     1.3582
  train_runtime            = 2:01:09.85
  train_samples_per_second =     26.414
  train_steps_per_second   =      0.827
{'train@eng.sdrt.stac_loss': 1.8694920539855957, 'train@eng.sdrt.stac_accuracy@eng.sdrt.stac': 0.3983298538622129, 'train@eng.sdrt.stac_f1@eng.sdrt.stac': 0.14806928902115543, 'train@eng.sdrt.stac_precision@eng.sdrt.stac': 0.2284287237414857, 'train@eng.sdrt.stac_recall@eng.sdrt.stac': 0.16166271744726607, 'train@eng.sdrt.stac_loss@eng.sdrt.stac': 1.8694920539855957, 'train@eng.sdrt.stac_runtime': 112.89, 'train@eng.sdrt.stac_samples_per_second': 84.861, 'train@eng.sdrt.stac_steps_per_second': 2.657, 'epoch': 1.0}
{'loss': 2.2432, 'learning_rate': 1.8333333333333333e-05, 'epoch': 1.0}
{'eval_loss': 1.7947193384170532, 'eval_accuracy@eng.sdrt.stac': 0.41746724890829695, 'eval_f1@eng.sdrt.stac': 0.13960337974472967, 'eval_precision@eng.sdrt.stac': 0.15597811232649283, 'eval_recall@eng.sdrt.stac': 0.1546985883975868, 'eval_loss@eng.sdrt.stac': 1.7947193384170532, 'eval_runtime': 13.8391, 'eval_samples_per_second': 82.737, 'eval_steps_per_second': 2.601, 'epoch': 1.0}
{'train@eng.sdrt.stac_loss': 1.6555763483047485, 'train@eng.sdrt.stac_accuracy@eng.sdrt.stac': 0.4710855949895616, 'train@eng.sdrt.stac_f1@eng.sdrt.stac': 0.25589538253886623, 'train@eng.sdrt.stac_precision@eng.sdrt.stac': 0.2937571560816483, 'train@eng.sdrt.stac_recall@eng.sdrt.stac': 0.275639191309242, 'train@eng.sdrt.stac_loss@eng.sdrt.stac': 1.6555763483047485, 'train@eng.sdrt.stac_runtime': 112.8832, 'train@eng.sdrt.stac_samples_per_second': 84.866, 'train@eng.sdrt.stac_steps_per_second': 2.658, 'epoch': 2.0}
{'loss': 1.7926, 'learning_rate': 1.6666666666666667e-05, 'epoch': 2.0}
{'eval_loss': 1.605470895767212, 'eval_accuracy@eng.sdrt.stac': 0.47685589519650656, 'eval_f1@eng.sdrt.stac': 0.2460470775304844, 'eval_precision@eng.sdrt.stac': 0.3357760465168009, 'eval_recall@eng.sdrt.stac': 0.2581958211258143, 'eval_loss@eng.sdrt.stac': 1.605470895767212, 'eval_runtime': 13.8033, 'eval_samples_per_second': 82.951, 'eval_steps_per_second': 2.608, 'epoch': 2.0}
{'train@eng.sdrt.stac_loss': 1.5772825479507446, 'train@eng.sdrt.stac_accuracy@eng.sdrt.stac': 0.48862212943632566, 'train@eng.sdrt.stac_f1@eng.sdrt.stac': 0.2853595001442058, 'train@eng.sdrt.stac_precision@eng.sdrt.stac': 0.34860730455853506, 'train@eng.sdrt.stac_recall@eng.sdrt.stac': 0.30855848400447694, 'train@eng.sdrt.stac_loss@eng.sdrt.stac': 1.5772827863693237, 'train@eng.sdrt.stac_runtime': 112.933, 'train@eng.sdrt.stac_samples_per_second': 84.829, 'train@eng.sdrt.stac_steps_per_second': 2.656, 'epoch': 3.0}
{'loss': 1.6693, 'learning_rate': 1.5000000000000002e-05, 'epoch': 3.0}
{'eval_loss': 1.555005669593811, 'eval_accuracy@eng.sdrt.stac': 0.4812227074235808, 'eval_f1@eng.sdrt.stac': 0.2590900613787962, 'eval_precision@eng.sdrt.stac': 0.3726661685674093, 'eval_recall@eng.sdrt.stac': 0.2757932529397695, 'eval_loss@eng.sdrt.stac': 1.555005669593811, 'eval_runtime': 13.8558, 'eval_samples_per_second': 82.637, 'eval_steps_per_second': 2.598, 'epoch': 3.0}
{'train@eng.sdrt.stac_loss': 1.5289489030838013, 'train@eng.sdrt.stac_accuracy@eng.sdrt.stac': 0.5092901878914405, 'train@eng.sdrt.stac_f1@eng.sdrt.stac': 0.3172427809859647, 'train@eng.sdrt.stac_precision@eng.sdrt.stac': 0.38120764748495517, 'train@eng.sdrt.stac_recall@eng.sdrt.stac': 0.3311401796167436, 'train@eng.sdrt.stac_loss@eng.sdrt.stac': 1.5289489030838013, 'train@eng.sdrt.stac_runtime': 113.2724, 'train@eng.sdrt.stac_samples_per_second': 84.575, 'train@eng.sdrt.stac_steps_per_second': 2.648, 'epoch': 4.0}
{'loss': 1.6072, 'learning_rate': 1.3333333333333333e-05, 'epoch': 4.0}
{'eval_loss': 1.500213861465454, 'eval_accuracy@eng.sdrt.stac': 0.5179039301310043, 'eval_f1@eng.sdrt.stac': 0.3215396351107307, 'eval_precision@eng.sdrt.stac': 0.3839620244997563, 'eval_recall@eng.sdrt.stac': 0.3266465470254939, 'eval_loss@eng.sdrt.stac': 1.500213861465454, 'eval_runtime': 13.8736, 'eval_samples_per_second': 82.531, 'eval_steps_per_second': 2.595, 'epoch': 4.0}
{'train@eng.sdrt.stac_loss': 1.501036524772644, 'train@eng.sdrt.stac_accuracy@eng.sdrt.stac': 0.5230688935281838, 'train@eng.sdrt.stac_f1@eng.sdrt.stac': 0.3396944381506863, 'train@eng.sdrt.stac_precision@eng.sdrt.stac': 0.39194046955360184, 'train@eng.sdrt.stac_recall@eng.sdrt.stac': 0.35329497412291944, 'train@eng.sdrt.stac_loss@eng.sdrt.stac': 1.5010366439819336, 'train@eng.sdrt.stac_runtime': 112.8484, 'train@eng.sdrt.stac_samples_per_second': 84.893, 'train@eng.sdrt.stac_steps_per_second': 2.658, 'epoch': 5.0}
{'loss': 1.5666, 'learning_rate': 1.1666666666666668e-05, 'epoch': 5.0}
{'eval_loss': 1.4796738624572754, 'eval_accuracy@eng.sdrt.stac': 0.5266375545851528, 'eval_f1@eng.sdrt.stac': 0.3431980388216175, 'eval_precision@eng.sdrt.stac': 0.3963042828061947, 'eval_recall@eng.sdrt.stac': 0.34576269725801456, 'eval_loss@eng.sdrt.stac': 1.4796737432479858, 'eval_runtime': 13.8279, 'eval_samples_per_second': 82.804, 'eval_steps_per_second': 2.603, 'epoch': 5.0}
{'train@eng.sdrt.stac_loss': 1.464819312095642, 'train@eng.sdrt.stac_accuracy@eng.sdrt.stac': 0.5313152400835073, 'train@eng.sdrt.stac_f1@eng.sdrt.stac': 0.35623565582991584, 'train@eng.sdrt.stac_precision@eng.sdrt.stac': 0.4059710798785783, 'train@eng.sdrt.stac_recall@eng.sdrt.stac': 0.364097783241465, 'train@eng.sdrt.stac_loss@eng.sdrt.stac': 1.464819312095642, 'train@eng.sdrt.stac_runtime': 113.0812, 'train@eng.sdrt.stac_samples_per_second': 84.718, 'train@eng.sdrt.stac_steps_per_second': 2.653, 'epoch': 6.0}
{'loss': 1.5303, 'learning_rate': 1e-05, 'epoch': 6.0}
{'eval_loss': 1.4608091115951538, 'eval_accuracy@eng.sdrt.stac': 0.5344978165938865, 'eval_f1@eng.sdrt.stac': 0.3461569896988797, 'eval_precision@eng.sdrt.stac': 0.40065100507577284, 'eval_recall@eng.sdrt.stac': 0.350120753161772, 'eval_loss@eng.sdrt.stac': 1.460809350013733, 'eval_runtime': 13.8385, 'eval_samples_per_second': 82.74, 'eval_steps_per_second': 2.601, 'epoch': 6.0}
{'train@eng.sdrt.stac_loss': 1.4574171304702759, 'train@eng.sdrt.stac_accuracy@eng.sdrt.stac': 0.5370563674321504, 'train@eng.sdrt.stac_f1@eng.sdrt.stac': 0.3631850355403339, 'train@eng.sdrt.stac_precision@eng.sdrt.stac': 0.4005649742935753, 'train@eng.sdrt.stac_recall@eng.sdrt.stac': 0.3748873515893204, 'train@eng.sdrt.stac_loss@eng.sdrt.stac': 1.4574171304702759, 'train@eng.sdrt.stac_runtime': 112.915, 'train@eng.sdrt.stac_samples_per_second': 84.843, 'train@eng.sdrt.stac_steps_per_second': 2.657, 'epoch': 7.0}
{'loss': 1.5137, 'learning_rate': 8.333333333333334e-06, 'epoch': 7.0}
{'eval_loss': 1.453599214553833, 'eval_accuracy@eng.sdrt.stac': 0.5397379912663756, 'eval_f1@eng.sdrt.stac': 0.35764599849849166, 'eval_precision@eng.sdrt.stac': 0.40779112670659684, 'eval_recall@eng.sdrt.stac': 0.3609451292444454, 'eval_loss@eng.sdrt.stac': 1.453599214553833, 'eval_runtime': 13.8354, 'eval_samples_per_second': 82.759, 'eval_steps_per_second': 2.602, 'epoch': 7.0}
{'train@eng.sdrt.stac_loss': 1.4287289381027222, 'train@eng.sdrt.stac_accuracy@eng.sdrt.stac': 0.5447807933194154, 'train@eng.sdrt.stac_f1@eng.sdrt.stac': 0.37118617196285253, 'train@eng.sdrt.stac_precision@eng.sdrt.stac': 0.40314885546471885, 'train@eng.sdrt.stac_recall@eng.sdrt.stac': 0.3796746709212501, 'train@eng.sdrt.stac_loss@eng.sdrt.stac': 1.4287290573120117, 'train@eng.sdrt.stac_runtime': 112.86, 'train@eng.sdrt.stac_samples_per_second': 84.884, 'train@eng.sdrt.stac_steps_per_second': 2.658, 'epoch': 8.0}
{'loss': 1.4894, 'learning_rate': 6.666666666666667e-06, 'epoch': 8.0}
{'eval_loss': 1.4280763864517212, 'eval_accuracy@eng.sdrt.stac': 0.5502183406113537, 'eval_f1@eng.sdrt.stac': 0.37118375134168247, 'eval_precision@eng.sdrt.stac': 0.41028195828541664, 'eval_recall@eng.sdrt.stac': 0.3685575607833915, 'eval_loss@eng.sdrt.stac': 1.4280765056610107, 'eval_runtime': 13.7957, 'eval_samples_per_second': 82.997, 'eval_steps_per_second': 2.61, 'epoch': 8.0}
{'train@eng.sdrt.stac_loss': 1.4175955057144165, 'train@eng.sdrt.stac_accuracy@eng.sdrt.stac': 0.5494780793319416, 'train@eng.sdrt.stac_f1@eng.sdrt.stac': 0.3804223882803046, 'train@eng.sdrt.stac_precision@eng.sdrt.stac': 0.4070400108174766, 'train@eng.sdrt.stac_recall@eng.sdrt.stac': 0.3914846692602635, 'train@eng.sdrt.stac_loss@eng.sdrt.stac': 1.4175955057144165, 'train@eng.sdrt.stac_runtime': 112.8082, 'train@eng.sdrt.stac_samples_per_second': 84.923, 'train@eng.sdrt.stac_steps_per_second': 2.659, 'epoch': 9.0}
{'loss': 1.4719, 'learning_rate': 5e-06, 'epoch': 9.0}
{'eval_loss': 1.4176009893417358, 'eval_accuracy@eng.sdrt.stac': 0.5510917030567686, 'eval_f1@eng.sdrt.stac': 0.3621726751716957, 'eval_precision@eng.sdrt.stac': 0.37809364544527174, 'eval_recall@eng.sdrt.stac': 0.36948016131645955, 'eval_loss@eng.sdrt.stac': 1.4176009893417358, 'eval_runtime': 13.8272, 'eval_samples_per_second': 82.808, 'eval_steps_per_second': 2.604, 'epoch': 9.0}
{'train@eng.sdrt.stac_loss': 1.4071823358535767, 'train@eng.sdrt.stac_accuracy@eng.sdrt.stac': 0.5530271398747391, 'train@eng.sdrt.stac_f1@eng.sdrt.stac': 0.3852534854198821, 'train@eng.sdrt.stac_precision@eng.sdrt.stac': 0.41133055366022875, 'train@eng.sdrt.stac_recall@eng.sdrt.stac': 0.3952488129528944, 'train@eng.sdrt.stac_loss@eng.sdrt.stac': 1.4071824550628662, 'train@eng.sdrt.stac_runtime': 112.8897, 'train@eng.sdrt.stac_samples_per_second': 84.862, 'train@eng.sdrt.stac_steps_per_second': 2.657, 'epoch': 10.0}
{'loss': 1.4639, 'learning_rate': 3.3333333333333333e-06, 'epoch': 10.0}
{'eval_loss': 1.4138588905334473, 'eval_accuracy@eng.sdrt.stac': 0.5537117903930131, 'eval_f1@eng.sdrt.stac': 0.3703763613687821, 'eval_precision@eng.sdrt.stac': 0.3907072089700119, 'eval_recall@eng.sdrt.stac': 0.37552427359544865, 'eval_loss@eng.sdrt.stac': 1.4138587713241577, 'eval_runtime': 13.8358, 'eval_samples_per_second': 82.756, 'eval_steps_per_second': 2.602, 'epoch': 10.0}
{'train@eng.sdrt.stac_loss': 1.4005417823791504, 'train@eng.sdrt.stac_accuracy@eng.sdrt.stac': 0.554384133611691, 'train@eng.sdrt.stac_f1@eng.sdrt.stac': 0.38766372284806144, 'train@eng.sdrt.stac_precision@eng.sdrt.stac': 0.4114398962255967, 'train@eng.sdrt.stac_recall@eng.sdrt.stac': 0.3982800960770815, 'train@eng.sdrt.stac_loss@eng.sdrt.stac': 1.4005415439605713, 'train@eng.sdrt.stac_runtime': 112.8498, 'train@eng.sdrt.stac_samples_per_second': 84.892, 'train@eng.sdrt.stac_steps_per_second': 2.658, 'epoch': 11.0}
{'loss': 1.4597, 'learning_rate': 1.6666666666666667e-06, 'epoch': 11.0}
{'eval_loss': 1.4103317260742188, 'eval_accuracy@eng.sdrt.stac': 0.5545851528384279, 'eval_f1@eng.sdrt.stac': 0.3708980758986383, 'eval_precision@eng.sdrt.stac': 0.38025272449266023, 'eval_recall@eng.sdrt.stac': 0.3786472420702741, 'eval_loss@eng.sdrt.stac': 1.4103317260742188, 'eval_runtime': 13.8303, 'eval_samples_per_second': 82.789, 'eval_steps_per_second': 2.603, 'epoch': 11.0}
{'train@eng.sdrt.stac_loss': 1.4005272388458252, 'train@eng.sdrt.stac_accuracy@eng.sdrt.stac': 0.5550104384133612, 'train@eng.sdrt.stac_f1@eng.sdrt.stac': 0.38795012469921625, 'train@eng.sdrt.stac_precision@eng.sdrt.stac': 0.4264672390230286, 'train@eng.sdrt.stac_recall@eng.sdrt.stac': 0.3983886715277303, 'train@eng.sdrt.stac_loss@eng.sdrt.stac': 1.4005272388458252, 'train@eng.sdrt.stac_runtime': 113.3207, 'train@eng.sdrt.stac_samples_per_second': 84.539, 'train@eng.sdrt.stac_steps_per_second': 2.647, 'epoch': 12.0}
{'loss': 1.4521, 'learning_rate': 0.0, 'epoch': 12.0}
{'eval_loss': 1.410142421722412, 'eval_accuracy@eng.sdrt.stac': 0.5528384279475983, 'eval_f1@eng.sdrt.stac': 0.3689831831849526, 'eval_precision@eng.sdrt.stac': 0.37917594241992736, 'eval_recall@eng.sdrt.stac': 0.3776055325896833, 'eval_loss@eng.sdrt.stac': 1.410142421722412, 'eval_runtime': 13.9124, 'eval_samples_per_second': 82.301, 'eval_steps_per_second': 2.588, 'epoch': 12.0}
{'train_runtime': 4381.3163, 'train_samples_per_second': 26.239, 'train_steps_per_second': 0.822, 'train_loss': 1.6049866231282552, 'epoch': 12.0}
***** train_logger metrics *****
  epoch                    =       12.0
  train_loss               =     1.3582
  train_runtime            = 2:01:09.85
  train_samples_per_second =     26.414
  train_steps_per_second   =      0.827
-------------------------------------------------------------------
Lang1:  fas.rst.prstc    Lang2:  eng.sdrt.stac
Saving run to:  runs/full_shot/FullShot=v4_fas.rst.prstc_eng.sdrt.stac_lrfunc=Adafactor_lr=2e-05_plm=bert-base-multilingual-cased
Running with params: BERT_MODEL=bert-base-multilingual-cased lr=2e-05
read 4100 examples
read 499 examples
read 592 examples
read 9580 examples
read 1145 examples
read 1510 examples
Total prediction labels:  33
BertAdapterModel(
  (shared_parameters): ModuleDict()
  (bert): BertModel(
    (shared_parameters): ModuleDict()
    (invertible_adapters): ModuleDict(
      (ar): NICECouplingBlock(
        (F): Sequential(
          (0): Linear(in_features=384, out_features=192, bias=True)
          (1): Activation_Function_Class(
            (f): ReLU()
          )
          (2): Linear(in_features=192, out_features=384, bias=True)
        )
        (G): Sequential(
          (0): Linear(in_features=384, out_features=192, bias=True)
          (1): Activation_Function_Class(
            (f): ReLU()
          )
          (2): Linear(in_features=192, out_features=384, bias=True)
        )
      )
      (en): NICECouplingBlock(
        (F): Sequential(
          (0): Linear(in_features=384, out_features=192, bias=True)
          (1): Activation_Function_Class(
            (f): ReLU()
          )
          (2): Linear(in_features=192, out_features=384, bias=True)
        )
        (G): Sequential(
          (0): Linear(in_features=384, out_features=192, bias=True)
          (1): Activation_Function_Class(
            (f): ReLU()
          )
          (2): Linear(in_features=192, out_features=384, bias=True)
        )
      )
    )
    (embeddings): BertEmbeddings(
      (word_embeddings): Embedding(119547, 768, padding_idx=0)
      (position_embeddings): Embedding(512, 768)
      (token_type_embeddings): Embedding(2, 768)
      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
      (dropout): Dropout(p=0.1, inplace=False)
    )
    (encoder): BertEncoder(
      (layer): ModuleList(
        (0): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (ar): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (en): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (1): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (ar): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (en): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (2): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (ar): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (en): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (3): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (ar): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (en): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (4): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (ar): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (en): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (5): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (ar): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (en): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (6): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (ar): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (en): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (7): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (ar): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (en): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (8): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (ar): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (en): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (9): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (ar): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (en): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (10): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (ar): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (en): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (11): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (ar): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (en): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
      )
    )
    (pooler): BertPooler(
      (dense): Linear(in_features=768, out_features=768, bias=True)
      (activation): Tanh()
    )
    (prefix_tuning): PrefixTuningPool(
      (prefix_tunings): ModuleDict()
    )
  )
  (heads): ModuleDict(
    (disrpt): ClassificationHead(
      (0): Dropout(p=0.1, inplace=False)
      (1): Linear(in_features=768, out_features=768, bias=True)
      (2): Activation_Function_Class(
        (f): Tanh()
      )
      (3): Dropout(p=0.1, inplace=False)
      (4): Linear(in_features=768, out_features=33, bias=True)
    )
  )
)
{'train@fas.rst.prstc_loss': 2.4095964431762695, 'train@fas.rst.prstc_accuracy@fas.rst.prstc': 0.23780487804878048, 'train@fas.rst.prstc_f1@fas.rst.prstc': 0.02260214430599826, 'train@fas.rst.prstc_precision@fas.rst.prstc': 0.013988522238163558, 'train@fas.rst.prstc_recall@fas.rst.prstc': 0.058823529411764705, 'train@fas.rst.prstc_loss@fas.rst.prstc': 2.4095964431762695, 'train@fas.rst.prstc_runtime': 48.3644, 'train@fas.rst.prstc_samples_per_second': 84.773, 'train@fas.rst.prstc_steps_per_second': 2.667, 'epoch': 1.0}
{'loss': 2.772, 'learning_rate': 1.8333333333333333e-05, 'epoch': 1.0}
{'eval_loss': 2.329552173614502, 'eval_accuracy@fas.rst.prstc': 0.24448897795591182, 'eval_f1@fas.rst.prstc': 0.026194310252281264, 'eval_precision@fas.rst.prstc': 0.016299265197060788, 'eval_recall@fas.rst.prstc': 0.06666666666666667, 'eval_loss@fas.rst.prstc': 2.329552173614502, 'eval_runtime': 6.1864, 'eval_samples_per_second': 80.661, 'eval_steps_per_second': 2.586, 'epoch': 1.0}
{'train@fas.rst.prstc_loss': 2.359341621398926, 'train@fas.rst.prstc_accuracy@fas.rst.prstc': 0.2648780487804878, 'train@fas.rst.prstc_f1@fas.rst.prstc': 0.04204946931260329, 'train@fas.rst.prstc_precision@fas.rst.prstc': 0.030706838026058145, 'train@fas.rst.prstc_recall@fas.rst.prstc': 0.06886899222318497, 'train@fas.rst.prstc_loss@fas.rst.prstc': 2.359341621398926, 'train@fas.rst.prstc_runtime': 48.5136, 'train@fas.rst.prstc_samples_per_second': 84.512, 'train@fas.rst.prstc_steps_per_second': 2.659, 'epoch': 2.0}
{'loss': 2.3996, 'learning_rate': 1.6666666666666667e-05, 'epoch': 2.0}
{'eval_loss': 2.272448778152466, 'eval_accuracy@fas.rst.prstc': 0.2785571142284569, 'eval_f1@fas.rst.prstc': 0.048765269426010664, 'eval_precision@fas.rst.prstc': 0.03730345315738915, 'eval_recall@fas.rst.prstc': 0.07738655262532669, 'eval_loss@fas.rst.prstc': 2.272448778152466, 'eval_runtime': 6.1838, 'eval_samples_per_second': 80.695, 'eval_steps_per_second': 2.587, 'epoch': 2.0}
{'train@fas.rst.prstc_loss': 2.3411824703216553, 'train@fas.rst.prstc_accuracy@fas.rst.prstc': 0.24682926829268292, 'train@fas.rst.prstc_f1@fas.rst.prstc': 0.02844878701927606, 'train@fas.rst.prstc_precision@fas.rst.prstc': 0.03789783215191794, 'train@fas.rst.prstc_recall@fas.rst.prstc': 0.061469785950386706, 'train@fas.rst.prstc_loss@fas.rst.prstc': 2.341182231903076, 'train@fas.rst.prstc_runtime': 48.5592, 'train@fas.rst.prstc_samples_per_second': 84.433, 'train@fas.rst.prstc_steps_per_second': 2.657, 'epoch': 3.0}
{'loss': 2.3624, 'learning_rate': 1.5000000000000002e-05, 'epoch': 3.0}
{'eval_loss': 2.255521297454834, 'eval_accuracy@fas.rst.prstc': 0.250501002004008, 'eval_f1@fas.rst.prstc': 0.03152192182873693, 'eval_precision@fas.rst.prstc': 0.044204882500570385, 'eval_recall@fas.rst.prstc': 0.06847232121644095, 'eval_loss@fas.rst.prstc': 2.255520820617676, 'eval_runtime': 6.1819, 'eval_samples_per_second': 80.72, 'eval_steps_per_second': 2.588, 'epoch': 3.0}
{'train@fas.rst.prstc_loss': 2.3273255825042725, 'train@fas.rst.prstc_accuracy@fas.rst.prstc': 0.23780487804878048, 'train@fas.rst.prstc_f1@fas.rst.prstc': 0.02260214430599826, 'train@fas.rst.prstc_precision@fas.rst.prstc': 0.013988522238163558, 'train@fas.rst.prstc_recall@fas.rst.prstc': 0.058823529411764705, 'train@fas.rst.prstc_loss@fas.rst.prstc': 2.3273255825042725, 'train@fas.rst.prstc_runtime': 48.474, 'train@fas.rst.prstc_samples_per_second': 84.581, 'train@fas.rst.prstc_steps_per_second': 2.661, 'epoch': 4.0}
{'loss': 2.3508, 'learning_rate': 1.3333333333333333e-05, 'epoch': 4.0}
{'eval_loss': 2.246941089630127, 'eval_accuracy@fas.rst.prstc': 0.24448897795591182, 'eval_f1@fas.rst.prstc': 0.026194310252281264, 'eval_precision@fas.rst.prstc': 0.016299265197060788, 'eval_recall@fas.rst.prstc': 0.06666666666666667, 'eval_loss@fas.rst.prstc': 2.246941089630127, 'eval_runtime': 6.2017, 'eval_samples_per_second': 80.462, 'eval_steps_per_second': 2.58, 'epoch': 4.0}
{'train@fas.rst.prstc_loss': 2.3155477046966553, 'train@fas.rst.prstc_accuracy@fas.rst.prstc': 0.25926829268292684, 'train@fas.rst.prstc_f1@fas.rst.prstc': 0.034957108163654216, 'train@fas.rst.prstc_precision@fas.rst.prstc': 0.03333681160771076, 'train@fas.rst.prstc_recall@fas.rst.prstc': 0.0652090754468727, 'train@fas.rst.prstc_loss@fas.rst.prstc': 2.315547466278076, 'train@fas.rst.prstc_runtime': 48.4686, 'train@fas.rst.prstc_samples_per_second': 84.591, 'train@fas.rst.prstc_steps_per_second': 2.662, 'epoch': 5.0}
{'loss': 2.3396, 'learning_rate': 1.1666666666666668e-05, 'epoch': 5.0}
{'eval_loss': 2.2323784828186035, 'eval_accuracy@fas.rst.prstc': 0.2565130260521042, 'eval_f1@fas.rst.prstc': 0.035799086757990865, 'eval_precision@fas.rst.prstc': 0.038314493153202824, 'eval_recall@fas.rst.prstc': 0.07027797576621525, 'eval_loss@fas.rst.prstc': 2.2323782444000244, 'eval_runtime': 6.2155, 'eval_samples_per_second': 80.284, 'eval_steps_per_second': 2.574, 'epoch': 5.0}
{'train@fas.rst.prstc_loss': 2.3014509677886963, 'train@fas.rst.prstc_accuracy@fas.rst.prstc': 0.24292682926829268, 'train@fas.rst.prstc_f1@fas.rst.prstc': 0.025912188416493863, 'train@fas.rst.prstc_precision@fas.rst.prstc': 0.040275395048446135, 'train@fas.rst.prstc_recall@fas.rst.prstc': 0.060311286544077525, 'train@fas.rst.prstc_loss@fas.rst.prstc': 2.3014509677886963, 'train@fas.rst.prstc_runtime': 48.4888, 'train@fas.rst.prstc_samples_per_second': 84.556, 'train@fas.rst.prstc_steps_per_second': 2.66, 'epoch': 6.0}
{'loss': 2.3218, 'learning_rate': 1e-05, 'epoch': 6.0}
{'eval_loss': 2.2267906665802, 'eval_accuracy@fas.rst.prstc': 0.24849699398797595, 'eval_f1@fas.rst.prstc': 0.029538847454590247, 'eval_precision@fas.rst.prstc': 0.04969574036511156, 'eval_recall@fas.rst.prstc': 0.06785934901401758, 'eval_loss@fas.rst.prstc': 2.2267906665802, 'eval_runtime': 6.1945, 'eval_samples_per_second': 80.555, 'eval_steps_per_second': 2.583, 'epoch': 6.0}
{'train@fas.rst.prstc_loss': 2.2772061824798584, 'train@fas.rst.prstc_accuracy@fas.rst.prstc': 0.2846341463414634, 'train@fas.rst.prstc_f1@fas.rst.prstc': 0.04352675198932313, 'train@fas.rst.prstc_precision@fas.rst.prstc': 0.0335417593848372, 'train@fas.rst.prstc_recall@fas.rst.prstc': 0.07298310922841586, 'train@fas.rst.prstc_loss@fas.rst.prstc': 2.2772057056427, 'train@fas.rst.prstc_runtime': 48.4968, 'train@fas.rst.prstc_samples_per_second': 84.542, 'train@fas.rst.prstc_steps_per_second': 2.66, 'epoch': 7.0}
{'loss': 2.3078, 'learning_rate': 8.333333333333334e-06, 'epoch': 7.0}
{'eval_loss': 2.1954102516174316, 'eval_accuracy@fas.rst.prstc': 0.280561122244489, 'eval_f1@fas.rst.prstc': 0.047172619047619054, 'eval_precision@fas.rst.prstc': 0.03803340390496354, 'eval_recall@fas.rst.prstc': 0.07756711808030411, 'eval_loss@fas.rst.prstc': 2.1954102516174316, 'eval_runtime': 6.21, 'eval_samples_per_second': 80.354, 'eval_steps_per_second': 2.576, 'epoch': 7.0}
{'train@fas.rst.prstc_loss': 2.2353718280792236, 'train@fas.rst.prstc_accuracy@fas.rst.prstc': 0.2992682926829268, 'train@fas.rst.prstc_f1@fas.rst.prstc': 0.04828193498893623, 'train@fas.rst.prstc_precision@fas.rst.prstc': 0.034872404844290654, 'train@fas.rst.prstc_recall@fas.rst.prstc': 0.07845212498529144, 'train@fas.rst.prstc_loss@fas.rst.prstc': 2.2353720664978027, 'train@fas.rst.prstc_runtime': 48.4723, 'train@fas.rst.prstc_samples_per_second': 84.584, 'train@fas.rst.prstc_steps_per_second': 2.661, 'epoch': 8.0}
{'loss': 2.2808, 'learning_rate': 6.666666666666667e-06, 'epoch': 8.0}
{'eval_loss': 2.1565043926239014, 'eval_accuracy@fas.rst.prstc': 0.3246492985971944, 'eval_f1@fas.rst.prstc': 0.05832486393321415, 'eval_precision@fas.rst.prstc': 0.04300727217798494, 'eval_recall@fas.rst.prstc': 0.0907864100736517, 'eval_loss@fas.rst.prstc': 2.1565043926239014, 'eval_runtime': 6.1779, 'eval_samples_per_second': 80.772, 'eval_steps_per_second': 2.59, 'epoch': 8.0}
{'train@fas.rst.prstc_loss': 2.207019805908203, 'train@fas.rst.prstc_accuracy@fas.rst.prstc': 0.3026829268292683, 'train@fas.rst.prstc_f1@fas.rst.prstc': 0.04940837626996705, 'train@fas.rst.prstc_precision@fas.rst.prstc': 0.03594995799631132, 'train@fas.rst.prstc_recall@fas.rst.prstc': 0.07972914861525625, 'train@fas.rst.prstc_loss@fas.rst.prstc': 2.2070200443267822, 'train@fas.rst.prstc_runtime': 48.4855, 'train@fas.rst.prstc_samples_per_second': 84.561, 'train@fas.rst.prstc_steps_per_second': 2.661, 'epoch': 9.0}
{'loss': 2.2465, 'learning_rate': 5e-06, 'epoch': 9.0}
{'eval_loss': 2.1224615573883057, 'eval_accuracy@fas.rst.prstc': 0.3306613226452906, 'eval_f1@fas.rst.prstc': 0.05994483845547675, 'eval_precision@fas.rst.prstc': 0.044391856359401995, 'eval_recall@fas.rst.prstc': 0.09262532668092183, 'eval_loss@fas.rst.prstc': 2.1224615573883057, 'eval_runtime': 6.208, 'eval_samples_per_second': 80.38, 'eval_steps_per_second': 2.577, 'epoch': 9.0}
{'train@fas.rst.prstc_loss': 2.1898090839385986, 'train@fas.rst.prstc_accuracy@fas.rst.prstc': 0.3068292682926829, 'train@fas.rst.prstc_f1@fas.rst.prstc': 0.050538652676384875, 'train@fas.rst.prstc_precision@fas.rst.prstc': 0.037147299235899316, 'train@fas.rst.prstc_recall@fas.rst.prstc': 0.08110437191788882, 'train@fas.rst.prstc_loss@fas.rst.prstc': 2.189809799194336, 'train@fas.rst.prstc_runtime': 48.4942, 'train@fas.rst.prstc_samples_per_second': 84.546, 'train@fas.rst.prstc_steps_per_second': 2.66, 'epoch': 10.0}
{'loss': 2.2308, 'learning_rate': 3.3333333333333333e-06, 'epoch': 10.0}
{'eval_loss': 2.1004016399383545, 'eval_accuracy@fas.rst.prstc': 0.3346693386773547, 'eval_f1@fas.rst.prstc': 0.06105980014688536, 'eval_precision@fas.rst.prstc': 0.045552698113002804, 'eval_recall@fas.rst.prstc': 0.09385127108576859, 'eval_loss@fas.rst.prstc': 2.1004014015197754, 'eval_runtime': 6.1799, 'eval_samples_per_second': 80.746, 'eval_steps_per_second': 2.589, 'epoch': 10.0}
{'train@fas.rst.prstc_loss': 2.1803090572357178, 'train@fas.rst.prstc_accuracy@fas.rst.prstc': 0.3129268292682927, 'train@fas.rst.prstc_f1@fas.rst.prstc': 0.051895743878552274, 'train@fas.rst.prstc_precision@fas.rst.prstc': 0.03854072115862524, 'train@fas.rst.prstc_recall@fas.rst.prstc': 0.08295305028721799, 'train@fas.rst.prstc_loss@fas.rst.prstc': 2.1803090572357178, 'train@fas.rst.prstc_runtime': 48.4318, 'train@fas.rst.prstc_samples_per_second': 84.655, 'train@fas.rst.prstc_steps_per_second': 2.664, 'epoch': 11.0}
{'loss': 2.2133, 'learning_rate': 1.6666666666666667e-06, 'epoch': 11.0}
{'eval_loss': 2.088536024093628, 'eval_accuracy@fas.rst.prstc': 0.33867735470941884, 'eval_f1@fas.rst.prstc': 0.06229409195832553, 'eval_precision@fas.rst.prstc': 0.04703989703989704, 'eval_recall@fas.rst.prstc': 0.09507721549061535, 'eval_loss@fas.rst.prstc': 2.088536024093628, 'eval_runtime': 6.1938, 'eval_samples_per_second': 80.565, 'eval_steps_per_second': 2.583, 'epoch': 11.0}
{'train@fas.rst.prstc_loss': 2.1806018352508545, 'train@fas.rst.prstc_accuracy@fas.rst.prstc': 0.3129268292682927, 'train@fas.rst.prstc_f1@fas.rst.prstc': 0.05184993727423452, 'train@fas.rst.prstc_precision@fas.rst.prstc': 0.03844104627128561, 'train@fas.rst.prstc_recall@fas.rst.prstc': 0.08289785308558775, 'train@fas.rst.prstc_loss@fas.rst.prstc': 2.1806020736694336, 'train@fas.rst.prstc_runtime': 48.4475, 'train@fas.rst.prstc_samples_per_second': 84.628, 'train@fas.rst.prstc_steps_per_second': 2.663, 'epoch': 12.0}
{'loss': 2.206, 'learning_rate': 0.0, 'epoch': 12.0}
{'eval_loss': 2.088489532470703, 'eval_accuracy@fas.rst.prstc': 0.342685370741483, 'eval_f1@fas.rst.prstc': 0.06293961802362243, 'eval_precision@fas.rst.prstc': 0.04743874785686632, 'eval_recall@fas.rst.prstc': 0.09620337372297459, 'eval_loss@fas.rst.prstc': 2.088489532470703, 'eval_runtime': 6.2063, 'eval_samples_per_second': 80.403, 'eval_steps_per_second': 2.578, 'epoch': 12.0}
{'train_runtime': 1881.9501, 'train_samples_per_second': 26.143, 'train_steps_per_second': 0.823, 'train_loss': 2.335950393085332, 'epoch': 12.0}
***** train_log metrics *****
  epoch                    =       12.0
  train_loss               =      2.336
  train_runtime            = 0:31:21.95
  train_samples_per_second =     26.143
  train_steps_per_second   =      0.823
{'train@eng.sdrt.stac_loss': 1.9985508918762207, 'train@eng.sdrt.stac_accuracy@eng.sdrt.stac': 0.36784968684759917, 'train@eng.sdrt.stac_f1@eng.sdrt.stac': 0.0860794457246101, 'train@eng.sdrt.stac_precision@eng.sdrt.stac': 0.09309267266460916, 'train@eng.sdrt.stac_recall@eng.sdrt.stac': 0.11975127078868893, 'train@eng.sdrt.stac_loss@eng.sdrt.stac': 1.9985510110855103, 'train@eng.sdrt.stac_runtime': 112.8901, 'train@eng.sdrt.stac_samples_per_second': 84.861, 'train@eng.sdrt.stac_steps_per_second': 2.657, 'epoch': 1.0}
{'loss': 2.3133, 'learning_rate': 1.8333333333333333e-05, 'epoch': 1.0}
{'eval_loss': 1.9625684022903442, 'eval_accuracy@eng.sdrt.stac': 0.37554585152838427, 'eval_f1@eng.sdrt.stac': 0.08577310731452056, 'eval_precision@eng.sdrt.stac': 0.09139576473884312, 'eval_recall@eng.sdrt.stac': 0.12050834637777098, 'eval_loss@eng.sdrt.stac': 1.9625684022903442, 'eval_runtime': 14.915, 'eval_samples_per_second': 76.769, 'eval_steps_per_second': 2.414, 'epoch': 1.0}
{'train@eng.sdrt.stac_loss': 1.804266333580017, 'train@eng.sdrt.stac_accuracy@eng.sdrt.stac': 0.4431106471816284, 'train@eng.sdrt.stac_f1@eng.sdrt.stac': 0.15161487977014643, 'train@eng.sdrt.stac_precision@eng.sdrt.stac': 0.13760792241604108, 'train@eng.sdrt.stac_recall@eng.sdrt.stac': 0.19095149085176655, 'train@eng.sdrt.stac_loss@eng.sdrt.stac': 1.804266333580017, 'train@eng.sdrt.stac_runtime': 112.9109, 'train@eng.sdrt.stac_samples_per_second': 84.846, 'train@eng.sdrt.stac_steps_per_second': 2.657, 'epoch': 2.0}
{'loss': 1.9387, 'learning_rate': 1.6666666666666667e-05, 'epoch': 2.0}
{'eval_loss': 1.7729191780090332, 'eval_accuracy@eng.sdrt.stac': 0.44192139737991265, 'eval_f1@eng.sdrt.stac': 0.14614102032872003, 'eval_precision@eng.sdrt.stac': 0.13214867765698968, 'eval_recall@eng.sdrt.stac': 0.18812555625400013, 'eval_loss@eng.sdrt.stac': 1.772918939590454, 'eval_runtime': 13.8497, 'eval_samples_per_second': 82.673, 'eval_steps_per_second': 2.599, 'epoch': 2.0}
{'train@eng.sdrt.stac_loss': 1.7229481935501099, 'train@eng.sdrt.stac_accuracy@eng.sdrt.stac': 0.45668058455114824, 'train@eng.sdrt.stac_f1@eng.sdrt.stac': 0.16825516791431183, 'train@eng.sdrt.stac_precision@eng.sdrt.stac': 0.22317186757715715, 'train@eng.sdrt.stac_recall@eng.sdrt.stac': 0.20450195089771972, 'train@eng.sdrt.stac_loss@eng.sdrt.stac': 1.7229481935501099, 'train@eng.sdrt.stac_runtime': 112.8897, 'train@eng.sdrt.stac_samples_per_second': 84.862, 'train@eng.sdrt.stac_steps_per_second': 2.657, 'epoch': 3.0}
{'loss': 1.8091, 'learning_rate': 1.5000000000000002e-05, 'epoch': 3.0}
{'eval_loss': 1.6936454772949219, 'eval_accuracy@eng.sdrt.stac': 0.4550218340611354, 'eval_f1@eng.sdrt.stac': 0.15514511051984742, 'eval_precision@eng.sdrt.stac': 0.13572751954849485, 'eval_recall@eng.sdrt.stac': 0.19518888878541746, 'eval_loss@eng.sdrt.stac': 1.6936454772949219, 'eval_runtime': 13.829, 'eval_samples_per_second': 82.797, 'eval_steps_per_second': 2.603, 'epoch': 3.0}
{'train@eng.sdrt.stac_loss': 1.6665229797363281, 'train@eng.sdrt.stac_accuracy@eng.sdrt.stac': 0.4810020876826722, 'train@eng.sdrt.stac_f1@eng.sdrt.stac': 0.20344705166416674, 'train@eng.sdrt.stac_precision@eng.sdrt.stac': 0.24036462926831695, 'train@eng.sdrt.stac_recall@eng.sdrt.stac': 0.22812912725552353, 'train@eng.sdrt.stac_loss@eng.sdrt.stac': 1.6665228605270386, 'train@eng.sdrt.stac_runtime': 112.818, 'train@eng.sdrt.stac_samples_per_second': 84.916, 'train@eng.sdrt.stac_steps_per_second': 2.659, 'epoch': 4.0}
{'loss': 1.7376, 'learning_rate': 1.3333333333333333e-05, 'epoch': 4.0}
{'eval_loss': 1.6353005170822144, 'eval_accuracy@eng.sdrt.stac': 0.485589519650655, 'eval_f1@eng.sdrt.stac': 0.18982108632044675, 'eval_precision@eng.sdrt.stac': 0.21339006177748387, 'eval_recall@eng.sdrt.stac': 0.2176892929784679, 'eval_loss@eng.sdrt.stac': 1.6353005170822144, 'eval_runtime': 13.8321, 'eval_samples_per_second': 82.778, 'eval_steps_per_second': 2.603, 'epoch': 4.0}
{'train@eng.sdrt.stac_loss': 1.6257144212722778, 'train@eng.sdrt.stac_accuracy@eng.sdrt.stac': 0.4948851774530271, 'train@eng.sdrt.stac_f1@eng.sdrt.stac': 0.22245412651011998, 'train@eng.sdrt.stac_precision@eng.sdrt.stac': 0.22727109284929714, 'train@eng.sdrt.stac_recall@eng.sdrt.stac': 0.2465238274018717, 'train@eng.sdrt.stac_loss@eng.sdrt.stac': 1.6257144212722778, 'train@eng.sdrt.stac_runtime': 112.9161, 'train@eng.sdrt.stac_samples_per_second': 84.842, 'train@eng.sdrt.stac_steps_per_second': 2.657, 'epoch': 5.0}
{'loss': 1.697, 'learning_rate': 1.1666666666666668e-05, 'epoch': 5.0}
{'eval_loss': 1.5990642309188843, 'eval_accuracy@eng.sdrt.stac': 0.5004366812227075, 'eval_f1@eng.sdrt.stac': 0.21114252272316544, 'eval_precision@eng.sdrt.stac': 0.21817647331573325, 'eval_recall@eng.sdrt.stac': 0.23368121856451063, 'eval_loss@eng.sdrt.stac': 1.5990641117095947, 'eval_runtime': 13.7947, 'eval_samples_per_second': 83.003, 'eval_steps_per_second': 2.61, 'epoch': 5.0}
{'train@eng.sdrt.stac_loss': 1.5918282270431519, 'train@eng.sdrt.stac_accuracy@eng.sdrt.stac': 0.49467640918580374, 'train@eng.sdrt.stac_f1@eng.sdrt.stac': 0.22217576463906555, 'train@eng.sdrt.stac_precision@eng.sdrt.stac': 0.35831982844757126, 'train@eng.sdrt.stac_recall@eng.sdrt.stac': 0.24122223078010702, 'train@eng.sdrt.stac_loss@eng.sdrt.stac': 1.5918282270431519, 'train@eng.sdrt.stac_runtime': 112.9072, 'train@eng.sdrt.stac_samples_per_second': 84.848, 'train@eng.sdrt.stac_steps_per_second': 2.657, 'epoch': 6.0}
{'loss': 1.654, 'learning_rate': 1e-05, 'epoch': 6.0}
{'eval_loss': 1.5804522037506104, 'eval_accuracy@eng.sdrt.stac': 0.485589519650655, 'eval_f1@eng.sdrt.stac': 0.19968719711220748, 'eval_precision@eng.sdrt.stac': 0.21736751344460964, 'eval_recall@eng.sdrt.stac': 0.223364086321361, 'eval_loss@eng.sdrt.stac': 1.5804522037506104, 'eval_runtime': 13.8716, 'eval_samples_per_second': 82.543, 'eval_steps_per_second': 2.595, 'epoch': 6.0}
{'train@eng.sdrt.stac_loss': 1.5726556777954102, 'train@eng.sdrt.stac_accuracy@eng.sdrt.stac': 0.5080375782881003, 'train@eng.sdrt.stac_f1@eng.sdrt.stac': 0.2631656549213654, 'train@eng.sdrt.stac_precision@eng.sdrt.stac': 0.34195532524171113, 'train@eng.sdrt.stac_recall@eng.sdrt.stac': 0.27698742888679473, 'train@eng.sdrt.stac_loss@eng.sdrt.stac': 1.572655439376831, 'train@eng.sdrt.stac_runtime': 112.8925, 'train@eng.sdrt.stac_samples_per_second': 84.859, 'train@eng.sdrt.stac_steps_per_second': 2.657, 'epoch': 7.0}
{'loss': 1.6318, 'learning_rate': 8.333333333333334e-06, 'epoch': 7.0}
{'eval_loss': 1.5606025457382202, 'eval_accuracy@eng.sdrt.stac': 0.5021834061135371, 'eval_f1@eng.sdrt.stac': 0.23952730249560994, 'eval_precision@eng.sdrt.stac': 0.2564258400879042, 'eval_recall@eng.sdrt.stac': 0.25470539372065415, 'eval_loss@eng.sdrt.stac': 1.5606025457382202, 'eval_runtime': 13.845, 'eval_samples_per_second': 82.701, 'eval_steps_per_second': 2.6, 'epoch': 7.0}
{'train@eng.sdrt.stac_loss': 1.543278694152832, 'train@eng.sdrt.stac_accuracy@eng.sdrt.stac': 0.5160751565762004, 'train@eng.sdrt.stac_f1@eng.sdrt.stac': 0.2758179461632818, 'train@eng.sdrt.stac_precision@eng.sdrt.stac': 0.3240077119079231, 'train@eng.sdrt.stac_recall@eng.sdrt.stac': 0.29425262618731257, 'train@eng.sdrt.stac_loss@eng.sdrt.stac': 1.543278694152832, 'train@eng.sdrt.stac_runtime': 112.7956, 'train@eng.sdrt.stac_samples_per_second': 84.932, 'train@eng.sdrt.stac_steps_per_second': 2.66, 'epoch': 8.0}
{'loss': 1.6082, 'learning_rate': 6.666666666666667e-06, 'epoch': 8.0}
{'eval_loss': 1.5319448709487915, 'eval_accuracy@eng.sdrt.stac': 0.514410480349345, 'eval_f1@eng.sdrt.stac': 0.2491454134743922, 'eval_precision@eng.sdrt.stac': 0.25884150837848946, 'eval_recall@eng.sdrt.stac': 0.2665217514418983, 'eval_loss@eng.sdrt.stac': 1.5319448709487915, 'eval_runtime': 13.8357, 'eval_samples_per_second': 82.757, 'eval_steps_per_second': 2.602, 'epoch': 8.0}
{'train@eng.sdrt.stac_loss': 1.530075192451477, 'train@eng.sdrt.stac_accuracy@eng.sdrt.stac': 0.5220250521920669, 'train@eng.sdrt.stac_f1@eng.sdrt.stac': 0.2834241452697698, 'train@eng.sdrt.stac_precision@eng.sdrt.stac': 0.3830504572716679, 'train@eng.sdrt.stac_recall@eng.sdrt.stac': 0.3037495823485086, 'train@eng.sdrt.stac_loss@eng.sdrt.stac': 1.530075192451477, 'train@eng.sdrt.stac_runtime': 112.8347, 'train@eng.sdrt.stac_samples_per_second': 84.903, 'train@eng.sdrt.stac_steps_per_second': 2.659, 'epoch': 9.0}
{'loss': 1.5874, 'learning_rate': 5e-06, 'epoch': 9.0}
{'eval_loss': 1.5235631465911865, 'eval_accuracy@eng.sdrt.stac': 0.5161572052401747, 'eval_f1@eng.sdrt.stac': 0.25607988976876, 'eval_precision@eng.sdrt.stac': 0.26174969554816274, 'eval_recall@eng.sdrt.stac': 0.2735183663102002, 'eval_loss@eng.sdrt.stac': 1.5235631465911865, 'eval_runtime': 13.8046, 'eval_samples_per_second': 82.943, 'eval_steps_per_second': 2.608, 'epoch': 9.0}
{'train@eng.sdrt.stac_loss': 1.5157108306884766, 'train@eng.sdrt.stac_accuracy@eng.sdrt.stac': 0.5254697286012526, 'train@eng.sdrt.stac_f1@eng.sdrt.stac': 0.2860051466031115, 'train@eng.sdrt.stac_precision@eng.sdrt.stac': 0.38264463392794196, 'train@eng.sdrt.stac_recall@eng.sdrt.stac': 0.3072094252387697, 'train@eng.sdrt.stac_loss@eng.sdrt.stac': 1.5157108306884766, 'train@eng.sdrt.stac_runtime': 112.981, 'train@eng.sdrt.stac_samples_per_second': 84.793, 'train@eng.sdrt.stac_steps_per_second': 2.655, 'epoch': 10.0}
{'loss': 1.573, 'learning_rate': 3.3333333333333333e-06, 'epoch': 10.0}
{'eval_loss': 1.510839819908142, 'eval_accuracy@eng.sdrt.stac': 0.5152838427947598, 'eval_f1@eng.sdrt.stac': 0.2530117268861902, 'eval_precision@eng.sdrt.stac': 0.2549877669393932, 'eval_recall@eng.sdrt.stac': 0.2718228298074914, 'eval_loss@eng.sdrt.stac': 1.510839819908142, 'eval_runtime': 13.8398, 'eval_samples_per_second': 82.732, 'eval_steps_per_second': 2.601, 'epoch': 10.0}
{'train@eng.sdrt.stac_loss': 1.5078951120376587, 'train@eng.sdrt.stac_accuracy@eng.sdrt.stac': 0.5276617954070981, 'train@eng.sdrt.stac_f1@eng.sdrt.stac': 0.291861569852231, 'train@eng.sdrt.stac_precision@eng.sdrt.stac': 0.38645326900148685, 'train@eng.sdrt.stac_recall@eng.sdrt.stac': 0.3133061794571168, 'train@eng.sdrt.stac_loss@eng.sdrt.stac': 1.5078949928283691, 'train@eng.sdrt.stac_runtime': 113.0929, 'train@eng.sdrt.stac_samples_per_second': 84.709, 'train@eng.sdrt.stac_steps_per_second': 2.653, 'epoch': 11.0}
{'loss': 1.5649, 'learning_rate': 1.6666666666666667e-06, 'epoch': 11.0}
{'eval_loss': 1.5070406198501587, 'eval_accuracy@eng.sdrt.stac': 0.5135371179039301, 'eval_f1@eng.sdrt.stac': 0.25705443381561344, 'eval_precision@eng.sdrt.stac': 0.26215585780310313, 'eval_recall@eng.sdrt.stac': 0.27447192049856084, 'eval_loss@eng.sdrt.stac': 1.5070407390594482, 'eval_runtime': 13.8525, 'eval_samples_per_second': 82.657, 'eval_steps_per_second': 2.599, 'epoch': 11.0}
{'train@eng.sdrt.stac_loss': 1.5064408779144287, 'train@eng.sdrt.stac_accuracy@eng.sdrt.stac': 0.5276617954070981, 'train@eng.sdrt.stac_f1@eng.sdrt.stac': 0.2907926699238068, 'train@eng.sdrt.stac_precision@eng.sdrt.stac': 0.3855325358590195, 'train@eng.sdrt.stac_recall@eng.sdrt.stac': 0.3126247409262616, 'train@eng.sdrt.stac_loss@eng.sdrt.stac': 1.5064408779144287, 'train@eng.sdrt.stac_runtime': 112.9982, 'train@eng.sdrt.stac_samples_per_second': 84.78, 'train@eng.sdrt.stac_steps_per_second': 2.655, 'epoch': 12.0}
{'loss': 1.5612, 'learning_rate': 0.0, 'epoch': 12.0}
{'eval_loss': 1.504991054534912, 'eval_accuracy@eng.sdrt.stac': 0.5179039301310043, 'eval_f1@eng.sdrt.stac': 0.2595012007569035, 'eval_precision@eng.sdrt.stac': 0.26681730708484186, 'eval_recall@eng.sdrt.stac': 0.2766369531782994, 'eval_loss@eng.sdrt.stac': 1.504990816116333, 'eval_runtime': 13.8331, 'eval_samples_per_second': 82.772, 'eval_steps_per_second': 2.602, 'epoch': 12.0}
{'train_runtime': 4386.0172, 'train_samples_per_second': 26.211, 'train_steps_per_second': 0.821, 'train_loss': 1.7230159335666233, 'epoch': 12.0}
***** train_logger metrics *****
  epoch                    =       12.0
  train_loss               =      2.336
  train_runtime            = 0:31:21.95
  train_samples_per_second =     26.143
  train_steps_per_second   =      0.823
-------------------------------------------------------------------
Lang1:  fra.sdrt.annodis    Lang2:  eng.sdrt.stac
Saving run to:  runs/full_shot/FullShot=v4_fra.sdrt.annodis_eng.sdrt.stac_lrfunc=Adafactor_lr=2e-05_plm=bert-base-multilingual-cased
Running with params: BERT_MODEL=bert-base-multilingual-cased lr=2e-05
read 2185 examples
read 528 examples
read 625 examples
read 9580 examples
read 1145 examples
read 1510 examples
Total prediction labels:  34
BertAdapterModel(
  (shared_parameters): ModuleDict()
  (bert): BertModel(
    (shared_parameters): ModuleDict()
    (invertible_adapters): ModuleDict(
      (fr): NICECouplingBlock(
        (F): Sequential(
          (0): Linear(in_features=384, out_features=192, bias=True)
          (1): Activation_Function_Class(
            (f): NewGELUActivation()
          )
          (2): Linear(in_features=192, out_features=384, bias=True)
        )
        (G): Sequential(
          (0): Linear(in_features=384, out_features=192, bias=True)
          (1): Activation_Function_Class(
            (f): NewGELUActivation()
          )
          (2): Linear(in_features=192, out_features=384, bias=True)
        )
      )
      (en): NICECouplingBlock(
        (F): Sequential(
          (0): Linear(in_features=384, out_features=192, bias=True)
          (1): Activation_Function_Class(
            (f): ReLU()
          )
          (2): Linear(in_features=192, out_features=384, bias=True)
        )
        (G): Sequential(
          (0): Linear(in_features=384, out_features=192, bias=True)
          (1): Activation_Function_Class(
            (f): ReLU()
          )
          (2): Linear(in_features=192, out_features=384, bias=True)
        )
      )
    )
    (embeddings): BertEmbeddings(
      (word_embeddings): Embedding(119547, 768, padding_idx=0)
      (position_embeddings): Embedding(512, 768)
      (token_type_embeddings): Embedding(2, 768)
      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
      (dropout): Dropout(p=0.1, inplace=False)
    )
    (encoder): BertEncoder(
      (layer): ModuleList(
        (0): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (fr): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (en): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (1): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (fr): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (en): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (2): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (fr): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (en): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (3): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (fr): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (en): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (4): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (fr): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (en): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (5): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (fr): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (en): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (6): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (fr): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (en): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (7): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (fr): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (en): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (8): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (fr): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (en): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (9): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (fr): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (en): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (10): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (fr): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (en): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (11): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (fr): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (en): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
      )
    )
    (pooler): BertPooler(
      (dense): Linear(in_features=768, out_features=768, bias=True)
      (activation): Tanh()
    )
    (prefix_tuning): PrefixTuningPool(
      (prefix_tunings): ModuleDict()
    )
  )
  (heads): ModuleDict(
    (disrpt): ClassificationHead(
      (0): Dropout(p=0.1, inplace=False)
      (1): Linear(in_features=768, out_features=768, bias=True)
      (2): Activation_Function_Class(
        (f): Tanh()
      )
      (3): Dropout(p=0.1, inplace=False)
      (4): Linear(in_features=768, out_features=34, bias=True)
    )
  )
)
{'train@fra.sdrt.annodis_loss': 2.6311113834381104, 'train@fra.sdrt.annodis_accuracy@fra.sdrt.annodis': 0.23524027459954233, 'train@fra.sdrt.annodis_f1@fra.sdrt.annodis': 0.04281700120404196, 'train@fra.sdrt.annodis_precision@fra.sdrt.annodis': 0.042155843712877186, 'train@fra.sdrt.annodis_recall@fra.sdrt.annodis': 0.06656268374128779, 'train@fra.sdrt.annodis_loss@fra.sdrt.annodis': 2.6311113834381104, 'train@fra.sdrt.annodis_runtime': 26.334, 'train@fra.sdrt.annodis_samples_per_second': 82.972, 'train@fra.sdrt.annodis_steps_per_second': 2.62, 'epoch': 1.0}
{'loss': 3.084, 'learning_rate': 1.8333333333333333e-05, 'epoch': 1.0}
{'eval_loss': 2.653789520263672, 'eval_accuracy@fra.sdrt.annodis': 0.19128787878787878, 'eval_f1@fra.sdrt.annodis': 0.03354773059845311, 'eval_precision@fra.sdrt.annodis': 0.032496712809487655, 'eval_recall@fra.sdrt.annodis': 0.05323270657296272, 'eval_loss@fra.sdrt.annodis': 2.6537892818450928, 'eval_runtime': 6.6677, 'eval_samples_per_second': 79.187, 'eval_steps_per_second': 2.55, 'epoch': 1.0}
{'train@fra.sdrt.annodis_loss': 2.3803231716156006, 'train@fra.sdrt.annodis_accuracy@fra.sdrt.annodis': 0.2938215102974828, 'train@fra.sdrt.annodis_f1@fra.sdrt.annodis': 0.06291019339162923, 'train@fra.sdrt.annodis_precision@fra.sdrt.annodis': 0.04883227630305141, 'train@fra.sdrt.annodis_recall@fra.sdrt.annodis': 0.0888122910858071, 'train@fra.sdrt.annodis_loss@fra.sdrt.annodis': 2.3803231716156006, 'train@fra.sdrt.annodis_runtime': 26.4699, 'train@fra.sdrt.annodis_samples_per_second': 82.547, 'train@fra.sdrt.annodis_steps_per_second': 2.607, 'epoch': 2.0}
{'loss': 2.5035, 'learning_rate': 1.6666666666666667e-05, 'epoch': 2.0}
{'eval_loss': 2.4000284671783447, 'eval_accuracy@fra.sdrt.annodis': 0.24431818181818182, 'eval_f1@fra.sdrt.annodis': 0.05249282880861828, 'eval_precision@fra.sdrt.annodis': 0.041012539286887625, 'eval_recall@fra.sdrt.annodis': 0.0732295119626376, 'eval_loss@fra.sdrt.annodis': 2.400028944015503, 'eval_runtime': 6.6296, 'eval_samples_per_second': 79.643, 'eval_steps_per_second': 2.564, 'epoch': 2.0}
{'train@fra.sdrt.annodis_loss': 2.302490472793579, 'train@fra.sdrt.annodis_accuracy@fra.sdrt.annodis': 0.294279176201373, 'train@fra.sdrt.annodis_f1@fra.sdrt.annodis': 0.0625241632922782, 'train@fra.sdrt.annodis_precision@fra.sdrt.annodis': 0.052631779244682465, 'train@fra.sdrt.annodis_recall@fra.sdrt.annodis': 0.08661940506035361, 'train@fra.sdrt.annodis_loss@fra.sdrt.annodis': 2.302490234375, 'train@fra.sdrt.annodis_runtime': 26.4692, 'train@fra.sdrt.annodis_samples_per_second': 82.549, 'train@fra.sdrt.annodis_steps_per_second': 2.607, 'epoch': 3.0}
{'loss': 2.3658, 'learning_rate': 1.5000000000000002e-05, 'epoch': 3.0}
{'eval_loss': 2.3291492462158203, 'eval_accuracy@fra.sdrt.annodis': 0.2746212121212121, 'eval_f1@fra.sdrt.annodis': 0.05843432852913371, 'eval_precision@fra.sdrt.annodis': 0.048154446963970775, 'eval_recall@fra.sdrt.annodis': 0.08118658637869437, 'eval_loss@fra.sdrt.annodis': 2.3291494846343994, 'eval_runtime': 6.6808, 'eval_samples_per_second': 79.033, 'eval_steps_per_second': 2.545, 'epoch': 3.0}
{'train@fra.sdrt.annodis_loss': 2.247042655944824, 'train@fra.sdrt.annodis_accuracy@fra.sdrt.annodis': 0.31212814645308923, 'train@fra.sdrt.annodis_f1@fra.sdrt.annodis': 0.06928739480721616, 'train@fra.sdrt.annodis_precision@fra.sdrt.annodis': 0.08068504680427, 'train@fra.sdrt.annodis_recall@fra.sdrt.annodis': 0.09294254061169521, 'train@fra.sdrt.annodis_loss@fra.sdrt.annodis': 2.2470428943634033, 'train@fra.sdrt.annodis_runtime': 26.5668, 'train@fra.sdrt.annodis_samples_per_second': 82.246, 'train@fra.sdrt.annodis_steps_per_second': 2.597, 'epoch': 4.0}
{'loss': 2.3138, 'learning_rate': 1.3333333333333333e-05, 'epoch': 4.0}
{'eval_loss': 2.2806365489959717, 'eval_accuracy@fra.sdrt.annodis': 0.2727272727272727, 'eval_f1@fra.sdrt.annodis': 0.05856400099617287, 'eval_precision@fra.sdrt.annodis': 0.04774500967479161, 'eval_recall@fra.sdrt.annodis': 0.0800177124164663, 'eval_loss@fra.sdrt.annodis': 2.2806365489959717, 'eval_runtime': 6.6862, 'eval_samples_per_second': 78.969, 'eval_steps_per_second': 2.543, 'epoch': 4.0}
{'train@fra.sdrt.annodis_loss': 2.1993167400360107, 'train@fra.sdrt.annodis_accuracy@fra.sdrt.annodis': 0.3391304347826087, 'train@fra.sdrt.annodis_f1@fra.sdrt.annodis': 0.07566256174985253, 'train@fra.sdrt.annodis_precision@fra.sdrt.annodis': 0.1405977100059245, 'train@fra.sdrt.annodis_recall@fra.sdrt.annodis': 0.10438988214012618, 'train@fra.sdrt.annodis_loss@fra.sdrt.annodis': 2.1993165016174316, 'train@fra.sdrt.annodis_runtime': 26.5932, 'train@fra.sdrt.annodis_samples_per_second': 82.164, 'train@fra.sdrt.annodis_steps_per_second': 2.595, 'epoch': 5.0}
{'loss': 2.2568, 'learning_rate': 1.1666666666666668e-05, 'epoch': 5.0}
{'eval_loss': 2.2439498901367188, 'eval_accuracy@fra.sdrt.annodis': 0.29734848484848486, 'eval_f1@fra.sdrt.annodis': 0.062460418091615365, 'eval_precision@fra.sdrt.annodis': 0.04938338769157474, 'eval_recall@fra.sdrt.annodis': 0.09002754918296627, 'eval_loss@fra.sdrt.annodis': 2.243950128555298, 'eval_runtime': 6.6593, 'eval_samples_per_second': 79.287, 'eval_steps_per_second': 2.553, 'epoch': 5.0}
{'train@fra.sdrt.annodis_loss': 2.1551733016967773, 'train@fra.sdrt.annodis_accuracy@fra.sdrt.annodis': 0.35697940503432496, 'train@fra.sdrt.annodis_f1@fra.sdrt.annodis': 0.09783992480841307, 'train@fra.sdrt.annodis_precision@fra.sdrt.annodis': 0.12186297763061901, 'train@fra.sdrt.annodis_recall@fra.sdrt.annodis': 0.11822924857848444, 'train@fra.sdrt.annodis_loss@fra.sdrt.annodis': 2.1551735401153564, 'train@fra.sdrt.annodis_runtime': 26.5345, 'train@fra.sdrt.annodis_samples_per_second': 82.346, 'train@fra.sdrt.annodis_steps_per_second': 2.6, 'epoch': 6.0}
{'loss': 2.2137, 'learning_rate': 1e-05, 'epoch': 6.0}
{'eval_loss': 2.2069268226623535, 'eval_accuracy@fra.sdrt.annodis': 0.3068181818181818, 'eval_f1@fra.sdrt.annodis': 0.06756193910425541, 'eval_precision@fra.sdrt.annodis': 0.1065732524657149, 'eval_recall@fra.sdrt.annodis': 0.09418222918212736, 'eval_loss@fra.sdrt.annodis': 2.2069270610809326, 'eval_runtime': 6.6716, 'eval_samples_per_second': 79.142, 'eval_steps_per_second': 2.548, 'epoch': 6.0}
{'train@fra.sdrt.annodis_loss': 2.1148226261138916, 'train@fra.sdrt.annodis_accuracy@fra.sdrt.annodis': 0.3839816933638444, 'train@fra.sdrt.annodis_f1@fra.sdrt.annodis': 0.11474856581932978, 'train@fra.sdrt.annodis_precision@fra.sdrt.annodis': 0.12362657901747963, 'train@fra.sdrt.annodis_recall@fra.sdrt.annodis': 0.13387217023938902, 'train@fra.sdrt.annodis_loss@fra.sdrt.annodis': 2.1148226261138916, 'train@fra.sdrt.annodis_runtime': 26.625, 'train@fra.sdrt.annodis_samples_per_second': 82.066, 'train@fra.sdrt.annodis_steps_per_second': 2.592, 'epoch': 7.0}
{'loss': 2.1769, 'learning_rate': 8.333333333333334e-06, 'epoch': 7.0}
{'eval_loss': 2.174694299697876, 'eval_accuracy@fra.sdrt.annodis': 0.3181818181818182, 'eval_f1@fra.sdrt.annodis': 0.07757758186242514, 'eval_precision@fra.sdrt.annodis': 0.08432909076425348, 'eval_recall@fra.sdrt.annodis': 0.10054827086977434, 'eval_loss@fra.sdrt.annodis': 2.174694061279297, 'eval_runtime': 7.7527, 'eval_samples_per_second': 68.105, 'eval_steps_per_second': 2.193, 'epoch': 7.0}
{'train@fra.sdrt.annodis_loss': 2.079793930053711, 'train@fra.sdrt.annodis_accuracy@fra.sdrt.annodis': 0.40045766590389015, 'train@fra.sdrt.annodis_f1@fra.sdrt.annodis': 0.125832294717375, 'train@fra.sdrt.annodis_precision@fra.sdrt.annodis': 0.12663995838284275, 'train@fra.sdrt.annodis_recall@fra.sdrt.annodis': 0.1481456577310142, 'train@fra.sdrt.annodis_loss@fra.sdrt.annodis': 2.07979416847229, 'train@fra.sdrt.annodis_runtime': 26.5816, 'train@fra.sdrt.annodis_samples_per_second': 82.2, 'train@fra.sdrt.annodis_steps_per_second': 2.596, 'epoch': 8.0}
{'loss': 2.1467, 'learning_rate': 6.666666666666667e-06, 'epoch': 8.0}
{'eval_loss': 2.1477129459381104, 'eval_accuracy@fra.sdrt.annodis': 0.32575757575757575, 'eval_f1@fra.sdrt.annodis': 0.08887640129661865, 'eval_precision@fra.sdrt.annodis': 0.09818159993370575, 'eval_recall@fra.sdrt.annodis': 0.10911030453363618, 'eval_loss@fra.sdrt.annodis': 2.1477131843566895, 'eval_runtime': 6.7095, 'eval_samples_per_second': 78.695, 'eval_steps_per_second': 2.534, 'epoch': 8.0}
{'train@fra.sdrt.annodis_loss': 2.0535054206848145, 'train@fra.sdrt.annodis_accuracy@fra.sdrt.annodis': 0.40503432494279173, 'train@fra.sdrt.annodis_f1@fra.sdrt.annodis': 0.12797693190984505, 'train@fra.sdrt.annodis_precision@fra.sdrt.annodis': 0.12608933974432135, 'train@fra.sdrt.annodis_recall@fra.sdrt.annodis': 0.1549292167864065, 'train@fra.sdrt.annodis_loss@fra.sdrt.annodis': 2.0535054206848145, 'train@fra.sdrt.annodis_runtime': 26.5747, 'train@fra.sdrt.annodis_samples_per_second': 82.221, 'train@fra.sdrt.annodis_steps_per_second': 2.596, 'epoch': 9.0}
{'loss': 2.1138, 'learning_rate': 5e-06, 'epoch': 9.0}
{'eval_loss': 2.1270055770874023, 'eval_accuracy@fra.sdrt.annodis': 0.3314393939393939, 'eval_f1@fra.sdrt.annodis': 0.08940128041577317, 'eval_precision@fra.sdrt.annodis': 0.09613158650286352, 'eval_recall@fra.sdrt.annodis': 0.11103867804387194, 'eval_loss@fra.sdrt.annodis': 2.1270055770874023, 'eval_runtime': 6.6965, 'eval_samples_per_second': 78.847, 'eval_steps_per_second': 2.539, 'epoch': 9.0}
{'train@fra.sdrt.annodis_loss': 2.033846616744995, 'train@fra.sdrt.annodis_accuracy@fra.sdrt.annodis': 0.41189931350114417, 'train@fra.sdrt.annodis_f1@fra.sdrt.annodis': 0.13105239119091483, 'train@fra.sdrt.annodis_precision@fra.sdrt.annodis': 0.12600810610200633, 'train@fra.sdrt.annodis_recall@fra.sdrt.annodis': 0.1582061467077527, 'train@fra.sdrt.annodis_loss@fra.sdrt.annodis': 2.033846378326416, 'train@fra.sdrt.annodis_runtime': 26.5882, 'train@fra.sdrt.annodis_samples_per_second': 82.179, 'train@fra.sdrt.annodis_steps_per_second': 2.595, 'epoch': 10.0}
{'loss': 2.0908, 'learning_rate': 3.3333333333333333e-06, 'epoch': 10.0}
{'eval_loss': 2.111945629119873, 'eval_accuracy@fra.sdrt.annodis': 0.3371212121212121, 'eval_f1@fra.sdrt.annodis': 0.08999482480377445, 'eval_precision@fra.sdrt.annodis': 0.08987089803001906, 'eval_recall@fra.sdrt.annodis': 0.11252146924628928, 'eval_loss@fra.sdrt.annodis': 2.111945629119873, 'eval_runtime': 6.6777, 'eval_samples_per_second': 79.069, 'eval_steps_per_second': 2.546, 'epoch': 10.0}
{'train@fra.sdrt.annodis_loss': 2.021942377090454, 'train@fra.sdrt.annodis_accuracy@fra.sdrt.annodis': 0.41418764302059496, 'train@fra.sdrt.annodis_f1@fra.sdrt.annodis': 0.13325995610748098, 'train@fra.sdrt.annodis_precision@fra.sdrt.annodis': 0.12620835578431466, 'train@fra.sdrt.annodis_recall@fra.sdrt.annodis': 0.16115915050240925, 'train@fra.sdrt.annodis_loss@fra.sdrt.annodis': 2.021942138671875, 'train@fra.sdrt.annodis_runtime': 26.512, 'train@fra.sdrt.annodis_samples_per_second': 82.415, 'train@fra.sdrt.annodis_steps_per_second': 2.603, 'epoch': 11.0}
{'loss': 2.0653, 'learning_rate': 1.6666666666666667e-06, 'epoch': 11.0}
{'eval_loss': 2.102203845977783, 'eval_accuracy@fra.sdrt.annodis': 0.3371212121212121, 'eval_f1@fra.sdrt.annodis': 0.09199967819700919, 'eval_precision@fra.sdrt.annodis': 0.09060854817144719, 'eval_recall@fra.sdrt.annodis': 0.11478844210149743, 'eval_loss@fra.sdrt.annodis': 2.1022040843963623, 'eval_runtime': 6.6713, 'eval_samples_per_second': 79.145, 'eval_steps_per_second': 2.548, 'epoch': 11.0}
{'train@fra.sdrt.annodis_loss': 2.0178165435791016, 'train@fra.sdrt.annodis_accuracy@fra.sdrt.annodis': 0.41601830663615563, 'train@fra.sdrt.annodis_f1@fra.sdrt.annodis': 0.13391199700370113, 'train@fra.sdrt.annodis_precision@fra.sdrt.annodis': 0.12627997894016896, 'train@fra.sdrt.annodis_recall@fra.sdrt.annodis': 0.16175862301499155, 'train@fra.sdrt.annodis_loss@fra.sdrt.annodis': 2.0178167819976807, 'train@fra.sdrt.annodis_runtime': 26.478, 'train@fra.sdrt.annodis_samples_per_second': 82.521, 'train@fra.sdrt.annodis_steps_per_second': 2.606, 'epoch': 12.0}
{'loss': 2.0659, 'learning_rate': 0.0, 'epoch': 12.0}
{'eval_loss': 2.099106788635254, 'eval_accuracy@fra.sdrt.annodis': 0.3409090909090909, 'eval_f1@fra.sdrt.annodis': 0.09473140590503935, 'eval_precision@fra.sdrt.annodis': 0.09833057059941823, 'eval_recall@fra.sdrt.annodis': 0.11647796177151284, 'eval_loss@fra.sdrt.annodis': 2.099107027053833, 'eval_runtime': 6.6961, 'eval_samples_per_second': 78.851, 'eval_steps_per_second': 2.539, 'epoch': 12.0}
{'train_runtime': 1065.7951, 'train_samples_per_second': 24.601, 'train_steps_per_second': 0.777, 'train_loss': 2.2830795066944067, 'epoch': 12.0}
***** train_log metrics *****
  epoch                    =       12.0
  train_loss               =     2.2831
  train_runtime            = 0:17:45.79
  train_samples_per_second =     24.601
  train_steps_per_second   =      0.777
{'train@eng.sdrt.stac_loss': 2.0530359745025635, 'train@eng.sdrt.stac_accuracy@eng.sdrt.stac': 0.35720250521920666, 'train@eng.sdrt.stac_f1@eng.sdrt.stac': 0.07714466854459097, 'train@eng.sdrt.stac_precision@eng.sdrt.stac': 0.07070777474694347, 'train@eng.sdrt.stac_recall@eng.sdrt.stac': 0.11515301984287096, 'train@eng.sdrt.stac_loss@eng.sdrt.stac': 2.0530359745025635, 'train@eng.sdrt.stac_runtime': 112.9147, 'train@eng.sdrt.stac_samples_per_second': 84.843, 'train@eng.sdrt.stac_steps_per_second': 2.657, 'epoch': 1.0}
{'loss': 2.3761, 'learning_rate': 1.8333333333333333e-05, 'epoch': 1.0}
{'eval_loss': 2.006725549697876, 'eval_accuracy@eng.sdrt.stac': 0.3589519650655022, 'eval_f1@eng.sdrt.stac': 0.07695543591343071, 'eval_precision@eng.sdrt.stac': 0.07211818454877648, 'eval_recall@eng.sdrt.stac': 0.1130573300531974, 'eval_loss@eng.sdrt.stac': 2.006725788116455, 'eval_runtime': 13.8595, 'eval_samples_per_second': 82.615, 'eval_steps_per_second': 2.597, 'epoch': 1.0}
{'train@eng.sdrt.stac_loss': 1.8073745965957642, 'train@eng.sdrt.stac_accuracy@eng.sdrt.stac': 0.4407098121085595, 'train@eng.sdrt.stac_f1@eng.sdrt.stac': 0.14860652834876126, 'train@eng.sdrt.stac_precision@eng.sdrt.stac': 0.21343560090214528, 'train@eng.sdrt.stac_recall@eng.sdrt.stac': 0.18941119825525915, 'train@eng.sdrt.stac_loss@eng.sdrt.stac': 1.807374358177185, 'train@eng.sdrt.stac_runtime': 113.0172, 'train@eng.sdrt.stac_samples_per_second': 84.766, 'train@eng.sdrt.stac_steps_per_second': 2.654, 'epoch': 2.0}
{'loss': 1.966, 'learning_rate': 1.6666666666666667e-05, 'epoch': 2.0}
{'eval_loss': 1.767924427986145, 'eval_accuracy@eng.sdrt.stac': 0.44454148471615723, 'eval_f1@eng.sdrt.stac': 0.1442765156690572, 'eval_precision@eng.sdrt.stac': 0.18918570345447355, 'eval_recall@eng.sdrt.stac': 0.1861710851368613, 'eval_loss@eng.sdrt.stac': 1.7679246664047241, 'eval_runtime': 13.8474, 'eval_samples_per_second': 82.687, 'eval_steps_per_second': 2.6, 'epoch': 2.0}
{'train@eng.sdrt.stac_loss': 1.6954964399337769, 'train@eng.sdrt.stac_accuracy@eng.sdrt.stac': 0.4637787056367432, 'train@eng.sdrt.stac_f1@eng.sdrt.stac': 0.17928780387612372, 'train@eng.sdrt.stac_precision@eng.sdrt.stac': 0.21658295814238493, 'train@eng.sdrt.stac_recall@eng.sdrt.stac': 0.21361956784759203, 'train@eng.sdrt.stac_loss@eng.sdrt.stac': 1.6954964399337769, 'train@eng.sdrt.stac_runtime': 112.9078, 'train@eng.sdrt.stac_samples_per_second': 84.848, 'train@eng.sdrt.stac_steps_per_second': 2.657, 'epoch': 3.0}
{'loss': 1.793, 'learning_rate': 1.5000000000000002e-05, 'epoch': 3.0}
{'eval_loss': 1.659574031829834, 'eval_accuracy@eng.sdrt.stac': 0.4611353711790393, 'eval_f1@eng.sdrt.stac': 0.1687406465341084, 'eval_precision@eng.sdrt.stac': 0.168026542324138, 'eval_recall@eng.sdrt.stac': 0.20388446910921734, 'eval_loss@eng.sdrt.stac': 1.6595741510391235, 'eval_runtime': 13.8369, 'eval_samples_per_second': 82.75, 'eval_steps_per_second': 2.602, 'epoch': 3.0}
{'train@eng.sdrt.stac_loss': 1.6230820417404175, 'train@eng.sdrt.stac_accuracy@eng.sdrt.stac': 0.4892484342379958, 'train@eng.sdrt.stac_f1@eng.sdrt.stac': 0.2291866437874281, 'train@eng.sdrt.stac_precision@eng.sdrt.stac': 0.2851313131030604, 'train@eng.sdrt.stac_recall@eng.sdrt.stac': 0.24842773973821275, 'train@eng.sdrt.stac_loss@eng.sdrt.stac': 1.623081922531128, 'train@eng.sdrt.stac_runtime': 112.74, 'train@eng.sdrt.stac_samples_per_second': 84.974, 'train@eng.sdrt.stac_steps_per_second': 2.661, 'epoch': 4.0}
{'loss': 1.7072, 'learning_rate': 1.3333333333333333e-05, 'epoch': 4.0}
{'eval_loss': 1.585740327835083, 'eval_accuracy@eng.sdrt.stac': 0.4943231441048035, 'eval_f1@eng.sdrt.stac': 0.2133343173038899, 'eval_precision@eng.sdrt.stac': 0.21295384873044787, 'eval_recall@eng.sdrt.stac': 0.2348893781675061, 'eval_loss@eng.sdrt.stac': 1.5857402086257935, 'eval_runtime': 13.8528, 'eval_samples_per_second': 82.655, 'eval_steps_per_second': 2.599, 'epoch': 4.0}
{'train@eng.sdrt.stac_loss': 1.574289321899414, 'train@eng.sdrt.stac_accuracy@eng.sdrt.stac': 0.5068893528183716, 'train@eng.sdrt.stac_f1@eng.sdrt.stac': 0.2664029079484291, 'train@eng.sdrt.stac_precision@eng.sdrt.stac': 0.29342894412244247, 'train@eng.sdrt.stac_recall@eng.sdrt.stac': 0.2884109597896757, 'train@eng.sdrt.stac_loss@eng.sdrt.stac': 1.574289321899414, 'train@eng.sdrt.stac_runtime': 112.8782, 'train@eng.sdrt.stac_samples_per_second': 84.87, 'train@eng.sdrt.stac_steps_per_second': 2.658, 'epoch': 5.0}
{'loss': 1.6469, 'learning_rate': 1.1666666666666668e-05, 'epoch': 5.0}
{'eval_loss': 1.5420949459075928, 'eval_accuracy@eng.sdrt.stac': 0.5117903930131005, 'eval_f1@eng.sdrt.stac': 0.2470169049644583, 'eval_precision@eng.sdrt.stac': 0.24087999759946752, 'eval_recall@eng.sdrt.stac': 0.2698018403057044, 'eval_loss@eng.sdrt.stac': 1.5420949459075928, 'eval_runtime': 13.8355, 'eval_samples_per_second': 82.758, 'eval_steps_per_second': 2.602, 'epoch': 5.0}
{'train@eng.sdrt.stac_loss': 1.532212734222412, 'train@eng.sdrt.stac_accuracy@eng.sdrt.stac': 0.5149269311064718, 'train@eng.sdrt.stac_f1@eng.sdrt.stac': 0.2905444920421344, 'train@eng.sdrt.stac_precision@eng.sdrt.stac': 0.35762672385977967, 'train@eng.sdrt.stac_recall@eng.sdrt.stac': 0.30432628827105146, 'train@eng.sdrt.stac_loss@eng.sdrt.stac': 1.5322126150131226, 'train@eng.sdrt.stac_runtime': 112.8741, 'train@eng.sdrt.stac_samples_per_second': 84.873, 'train@eng.sdrt.stac_steps_per_second': 2.658, 'epoch': 6.0}
{'loss': 1.6009, 'learning_rate': 1e-05, 'epoch': 6.0}
{'eval_loss': 1.521186113357544, 'eval_accuracy@eng.sdrt.stac': 0.5048034934497817, 'eval_f1@eng.sdrt.stac': 0.26660187815922837, 'eval_precision@eng.sdrt.stac': 0.318517869908847, 'eval_recall@eng.sdrt.stac': 0.28099984555229096, 'eval_loss@eng.sdrt.stac': 1.5211862325668335, 'eval_runtime': 13.835, 'eval_samples_per_second': 82.761, 'eval_steps_per_second': 2.602, 'epoch': 6.0}
{'train@eng.sdrt.stac_loss': 1.5148743391036987, 'train@eng.sdrt.stac_accuracy@eng.sdrt.stac': 0.5210855949895616, 'train@eng.sdrt.stac_f1@eng.sdrt.stac': 0.3078677186024518, 'train@eng.sdrt.stac_precision@eng.sdrt.stac': 0.36313867606604655, 'train@eng.sdrt.stac_recall@eng.sdrt.stac': 0.32177247043888724, 'train@eng.sdrt.stac_loss@eng.sdrt.stac': 1.5148742198944092, 'train@eng.sdrt.stac_runtime': 112.7868, 'train@eng.sdrt.stac_samples_per_second': 84.939, 'train@eng.sdrt.stac_steps_per_second': 2.66, 'epoch': 7.0}
{'loss': 1.568, 'learning_rate': 8.333333333333334e-06, 'epoch': 7.0}
{'eval_loss': 1.5061554908752441, 'eval_accuracy@eng.sdrt.stac': 0.5152838427947598, 'eval_f1@eng.sdrt.stac': 0.2766529459045286, 'eval_precision@eng.sdrt.stac': 0.31574159544923275, 'eval_recall@eng.sdrt.stac': 0.2915109655850723, 'eval_loss@eng.sdrt.stac': 1.5061554908752441, 'eval_runtime': 13.8337, 'eval_samples_per_second': 82.769, 'eval_steps_per_second': 2.602, 'epoch': 7.0}
{'train@eng.sdrt.stac_loss': 1.4798308610916138, 'train@eng.sdrt.stac_accuracy@eng.sdrt.stac': 0.5347599164926932, 'train@eng.sdrt.stac_f1@eng.sdrt.stac': 0.32524702523621424, 'train@eng.sdrt.stac_precision@eng.sdrt.stac': 0.41115760015097674, 'train@eng.sdrt.stac_recall@eng.sdrt.stac': 0.337958049276392, 'train@eng.sdrt.stac_loss@eng.sdrt.stac': 1.4798306226730347, 'train@eng.sdrt.stac_runtime': 114.7781, 'train@eng.sdrt.stac_samples_per_second': 83.465, 'train@eng.sdrt.stac_steps_per_second': 2.614, 'epoch': 8.0}
{'loss': 1.5441, 'learning_rate': 6.666666666666667e-06, 'epoch': 8.0}
{'eval_loss': 1.4720929861068726, 'eval_accuracy@eng.sdrt.stac': 0.5283842794759825, 'eval_f1@eng.sdrt.stac': 0.29434262621802654, 'eval_precision@eng.sdrt.stac': 0.38080423020335596, 'eval_recall@eng.sdrt.stac': 0.3099762103700774, 'eval_loss@eng.sdrt.stac': 1.4720929861068726, 'eval_runtime': 14.1625, 'eval_samples_per_second': 80.848, 'eval_steps_per_second': 2.542, 'epoch': 8.0}
{'train@eng.sdrt.stac_loss': 1.4662586450576782, 'train@eng.sdrt.stac_accuracy@eng.sdrt.stac': 0.5384133611691023, 'train@eng.sdrt.stac_f1@eng.sdrt.stac': 0.3335004304887984, 'train@eng.sdrt.stac_precision@eng.sdrt.stac': 0.4152447833198557, 'train@eng.sdrt.stac_recall@eng.sdrt.stac': 0.3452462398248505, 'train@eng.sdrt.stac_loss@eng.sdrt.stac': 1.4662586450576782, 'train@eng.sdrt.stac_runtime': 113.0894, 'train@eng.sdrt.stac_samples_per_second': 84.712, 'train@eng.sdrt.stac_steps_per_second': 2.653, 'epoch': 9.0}
{'loss': 1.5225, 'learning_rate': 5e-06, 'epoch': 9.0}
{'eval_loss': 1.4630690813064575, 'eval_accuracy@eng.sdrt.stac': 0.5283842794759825, 'eval_f1@eng.sdrt.stac': 0.30649175691903185, 'eval_precision@eng.sdrt.stac': 0.4370249208368715, 'eval_recall@eng.sdrt.stac': 0.3165920894930956, 'eval_loss@eng.sdrt.stac': 1.463068962097168, 'eval_runtime': 13.7999, 'eval_samples_per_second': 82.972, 'eval_steps_per_second': 2.609, 'epoch': 9.0}
{'train@eng.sdrt.stac_loss': 1.4531737565994263, 'train@eng.sdrt.stac_accuracy@eng.sdrt.stac': 0.5418580375782881, 'train@eng.sdrt.stac_f1@eng.sdrt.stac': 0.34185454170795715, 'train@eng.sdrt.stac_precision@eng.sdrt.stac': 0.42499150739526415, 'train@eng.sdrt.stac_recall@eng.sdrt.stac': 0.3530385226793426, 'train@eng.sdrt.stac_loss@eng.sdrt.stac': 1.4531738758087158, 'train@eng.sdrt.stac_runtime': 112.7673, 'train@eng.sdrt.stac_samples_per_second': 84.954, 'train@eng.sdrt.stac_steps_per_second': 2.66, 'epoch': 10.0}
{'loss': 1.5084, 'learning_rate': 3.3333333333333333e-06, 'epoch': 10.0}
{'eval_loss': 1.4547913074493408, 'eval_accuracy@eng.sdrt.stac': 0.5283842794759825, 'eval_f1@eng.sdrt.stac': 0.30588521588292433, 'eval_precision@eng.sdrt.stac': 0.43563667198320694, 'eval_recall@eng.sdrt.stac': 0.31724211959404647, 'eval_loss@eng.sdrt.stac': 1.4547913074493408, 'eval_runtime': 13.8362, 'eval_samples_per_second': 82.754, 'eval_steps_per_second': 2.602, 'epoch': 10.0}
{'train@eng.sdrt.stac_loss': 1.445368766784668, 'train@eng.sdrt.stac_accuracy@eng.sdrt.stac': 0.545929018789144, 'train@eng.sdrt.stac_f1@eng.sdrt.stac': 0.34679898752456895, 'train@eng.sdrt.stac_precision@eng.sdrt.stac': 0.4198387879772196, 'train@eng.sdrt.stac_recall@eng.sdrt.stac': 0.35843698244369804, 'train@eng.sdrt.stac_loss@eng.sdrt.stac': 1.4453686475753784, 'train@eng.sdrt.stac_runtime': 112.9348, 'train@eng.sdrt.stac_samples_per_second': 84.828, 'train@eng.sdrt.stac_steps_per_second': 2.656, 'epoch': 11.0}
{'loss': 1.4971, 'learning_rate': 1.6666666666666667e-06, 'epoch': 11.0}
{'eval_loss': 1.4489237070083618, 'eval_accuracy@eng.sdrt.stac': 0.5318777292576419, 'eval_f1@eng.sdrt.stac': 0.3105920101143044, 'eval_precision@eng.sdrt.stac': 0.43978479130690984, 'eval_recall@eng.sdrt.stac': 0.32079924544689165, 'eval_loss@eng.sdrt.stac': 1.448923945426941, 'eval_runtime': 13.8179, 'eval_samples_per_second': 82.863, 'eval_steps_per_second': 2.605, 'epoch': 11.0}
{'train@eng.sdrt.stac_loss': 1.4440678358078003, 'train@eng.sdrt.stac_accuracy@eng.sdrt.stac': 0.5465553235908142, 'train@eng.sdrt.stac_f1@eng.sdrt.stac': 0.34860199544194276, 'train@eng.sdrt.stac_precision@eng.sdrt.stac': 0.4207753392510765, 'train@eng.sdrt.stac_recall@eng.sdrt.stac': 0.35992900382318693, 'train@eng.sdrt.stac_loss@eng.sdrt.stac': 1.4440679550170898, 'train@eng.sdrt.stac_runtime': 112.9824, 'train@eng.sdrt.stac_samples_per_second': 84.792, 'train@eng.sdrt.stac_steps_per_second': 2.655, 'epoch': 12.0}
{'loss': 1.4982, 'learning_rate': 0.0, 'epoch': 12.0}
{'eval_loss': 1.4483749866485596, 'eval_accuracy@eng.sdrt.stac': 0.5327510917030568, 'eval_f1@eng.sdrt.stac': 0.31092576008612005, 'eval_precision@eng.sdrt.stac': 0.44000885976045756, 'eval_recall@eng.sdrt.stac': 0.32117326788823813, 'eval_loss@eng.sdrt.stac': 1.4483749866485596, 'eval_runtime': 13.8408, 'eval_samples_per_second': 82.727, 'eval_steps_per_second': 2.601, 'epoch': 12.0}
{'train_runtime': 4385.9329, 'train_samples_per_second': 26.211, 'train_steps_per_second': 0.821, 'train_loss': 1.6856842803955079, 'epoch': 12.0}
***** train_logger metrics *****
  epoch                    =       12.0
  train_loss               =     2.2831
  train_runtime            = 0:17:45.79
  train_samples_per_second =     24.601
  train_steps_per_second   =      0.777
-------------------------------------------------------------------
Lang1:  nld.rst.nldt    Lang2:  eng.sdrt.stac
Saving run to:  runs/full_shot/FullShot=v4_nld.rst.nldt_eng.sdrt.stac_lrfunc=Adafactor_lr=2e-05_plm=bert-base-multilingual-cased
Running with params: BERT_MODEL=bert-base-multilingual-cased lr=2e-05
read 1608 examples
read 331 examples
read 326 examples
read 9580 examples
read 1145 examples
read 1510 examples
Total prediction labels:  49
BertAdapterModel(
  (shared_parameters): ModuleDict()
  (bert): BertModel(
    (shared_parameters): ModuleDict()
    (invertible_adapters): ModuleDict(
      (de): NICECouplingBlock(
        (F): Sequential(
          (0): Linear(in_features=384, out_features=192, bias=True)
          (1): Activation_Function_Class(
            (f): NewGELUActivation()
          )
          (2): Linear(in_features=192, out_features=384, bias=True)
        )
        (G): Sequential(
          (0): Linear(in_features=384, out_features=192, bias=True)
          (1): Activation_Function_Class(
            (f): NewGELUActivation()
          )
          (2): Linear(in_features=192, out_features=384, bias=True)
        )
      )
      (en): NICECouplingBlock(
        (F): Sequential(
          (0): Linear(in_features=384, out_features=192, bias=True)
          (1): Activation_Function_Class(
            (f): ReLU()
          )
          (2): Linear(in_features=192, out_features=384, bias=True)
        )
        (G): Sequential(
          (0): Linear(in_features=384, out_features=192, bias=True)
          (1): Activation_Function_Class(
            (f): ReLU()
          )
          (2): Linear(in_features=192, out_features=384, bias=True)
        )
      )
    )
    (embeddings): BertEmbeddings(
      (word_embeddings): Embedding(119547, 768, padding_idx=0)
      (position_embeddings): Embedding(512, 768)
      (token_type_embeddings): Embedding(2, 768)
      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
      (dropout): Dropout(p=0.1, inplace=False)
    )
    (encoder): BertEncoder(
      (layer): ModuleList(
        (0): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (de): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (en): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (1): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (de): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (en): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (2): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (de): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (en): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (3): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (de): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (en): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (4): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (de): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (en): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (5): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (de): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (en): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (6): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (de): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (en): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (7): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (de): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (en): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (8): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (de): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (en): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (9): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (de): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (en): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (10): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (de): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (en): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (11): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (de): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (en): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
      )
    )
    (pooler): BertPooler(
      (dense): Linear(in_features=768, out_features=768, bias=True)
      (activation): Tanh()
    )
    (prefix_tuning): PrefixTuningPool(
      (prefix_tunings): ModuleDict()
    )
  )
  (heads): ModuleDict(
    (disrpt): ClassificationHead(
      (0): Dropout(p=0.1, inplace=False)
      (1): Linear(in_features=768, out_features=768, bias=True)
      (2): Activation_Function_Class(
        (f): Tanh()
      )
      (3): Dropout(p=0.1, inplace=False)
      (4): Linear(in_features=768, out_features=49, bias=True)
    )
  )
)
{'train@nld.rst.nldt_loss': 3.30922532081604, 'train@nld.rst.nldt_accuracy@nld.rst.nldt': 0.26430348258706465, 'train@nld.rst.nldt_f1@nld.rst.nldt': 0.01306566650270536, 'train@nld.rst.nldt_precision@nld.rst.nldt': 0.00825948383084577, 'train@nld.rst.nldt_recall@nld.rst.nldt': 0.03125, 'train@nld.rst.nldt_loss@nld.rst.nldt': 3.30922532081604, 'train@nld.rst.nldt_runtime': 19.5601, 'train@nld.rst.nldt_samples_per_second': 82.208, 'train@nld.rst.nldt_steps_per_second': 2.607, 'epoch': 1.0}
{'loss': 3.6598, 'learning_rate': 1.8333333333333333e-05, 'epoch': 1.0}
{'eval_loss': 3.268282175064087, 'eval_accuracy@nld.rst.nldt': 0.27794561933534745, 'eval_f1@nld.rst.nldt': 0.016110673321075214, 'eval_precision@nld.rst.nldt': 0.010294282197605462, 'eval_recall@nld.rst.nldt': 0.037037037037037035, 'eval_loss@nld.rst.nldt': 3.2682816982269287, 'eval_runtime': 5.6319, 'eval_samples_per_second': 58.773, 'eval_steps_per_second': 1.953, 'epoch': 1.0}
{'train@nld.rst.nldt_loss': 2.9509029388427734, 'train@nld.rst.nldt_accuracy@nld.rst.nldt': 0.26430348258706465, 'train@nld.rst.nldt_f1@nld.rst.nldt': 0.01306566650270536, 'train@nld.rst.nldt_precision@nld.rst.nldt': 0.00825948383084577, 'train@nld.rst.nldt_recall@nld.rst.nldt': 0.03125, 'train@nld.rst.nldt_loss@nld.rst.nldt': 2.9509031772613525, 'train@nld.rst.nldt_runtime': 19.6225, 'train@nld.rst.nldt_samples_per_second': 81.947, 'train@nld.rst.nldt_steps_per_second': 2.599, 'epoch': 2.0}
{'loss': 3.1206, 'learning_rate': 1.6666666666666667e-05, 'epoch': 2.0}
{'eval_loss': 2.8872265815734863, 'eval_accuracy@nld.rst.nldt': 0.27794561933534745, 'eval_f1@nld.rst.nldt': 0.016110673321075214, 'eval_precision@nld.rst.nldt': 0.010294282197605462, 'eval_recall@nld.rst.nldt': 0.037037037037037035, 'eval_loss@nld.rst.nldt': 2.8872263431549072, 'eval_runtime': 4.4164, 'eval_samples_per_second': 74.948, 'eval_steps_per_second': 2.491, 'epoch': 2.0}
{'train@nld.rst.nldt_loss': 2.8364880084991455, 'train@nld.rst.nldt_accuracy@nld.rst.nldt': 0.26430348258706465, 'train@nld.rst.nldt_f1@nld.rst.nldt': 0.013072096456692911, 'train@nld.rst.nldt_precision@nld.rst.nldt': 0.008264623522090852, 'train@nld.rst.nldt_recall@nld.rst.nldt': 0.03125, 'train@nld.rst.nldt_loss@nld.rst.nldt': 2.8364880084991455, 'train@nld.rst.nldt_runtime': 19.6939, 'train@nld.rst.nldt_samples_per_second': 81.65, 'train@nld.rst.nldt_steps_per_second': 2.59, 'epoch': 3.0}
{'loss': 2.9185, 'learning_rate': 1.5000000000000002e-05, 'epoch': 3.0}
{'eval_loss': 2.778923749923706, 'eval_accuracy@nld.rst.nldt': 0.27794561933534745, 'eval_f1@nld.rst.nldt': 0.016110673321075214, 'eval_precision@nld.rst.nldt': 0.010294282197605462, 'eval_recall@nld.rst.nldt': 0.037037037037037035, 'eval_loss@nld.rst.nldt': 2.778923749923706, 'eval_runtime': 4.3975, 'eval_samples_per_second': 75.27, 'eval_steps_per_second': 2.501, 'epoch': 3.0}
{'train@nld.rst.nldt_loss': 2.780492067337036, 'train@nld.rst.nldt_accuracy@nld.rst.nldt': 0.27114427860696516, 'train@nld.rst.nldt_f1@nld.rst.nldt': 0.019291695921822324, 'train@nld.rst.nldt_precision@nld.rst.nldt': 0.026251169605042843, 'train@nld.rst.nldt_recall@nld.rst.nldt': 0.03473564425770308, 'train@nld.rst.nldt_loss@nld.rst.nldt': 2.780492067337036, 'train@nld.rst.nldt_runtime': 21.7954, 'train@nld.rst.nldt_samples_per_second': 73.777, 'train@nld.rst.nldt_steps_per_second': 2.34, 'epoch': 4.0}
{'loss': 2.8162, 'learning_rate': 1.3333333333333333e-05, 'epoch': 4.0}
{'eval_loss': 2.7295033931732178, 'eval_accuracy@nld.rst.nldt': 0.29607250755287007, 'eval_f1@nld.rst.nldt': 0.03145363032103403, 'eval_precision@nld.rst.nldt': 0.05183379129739282, 'eval_recall@nld.rst.nldt': 0.04738389182833628, 'eval_loss@nld.rst.nldt': 2.7295031547546387, 'eval_runtime': 4.3884, 'eval_samples_per_second': 75.427, 'eval_steps_per_second': 2.507, 'epoch': 4.0}
{'train@nld.rst.nldt_loss': 2.7417283058166504, 'train@nld.rst.nldt_accuracy@nld.rst.nldt': 0.2860696517412935, 'train@nld.rst.nldt_f1@nld.rst.nldt': 0.028092547604043127, 'train@nld.rst.nldt_precision@nld.rst.nldt': 0.02796762430508282, 'train@nld.rst.nldt_recall@nld.rst.nldt': 0.04231909430438842, 'train@nld.rst.nldt_loss@nld.rst.nldt': 2.7417283058166504, 'train@nld.rst.nldt_runtime': 19.6262, 'train@nld.rst.nldt_samples_per_second': 81.931, 'train@nld.rst.nldt_steps_per_second': 2.599, 'epoch': 5.0}
{'loss': 2.7782, 'learning_rate': 1.1666666666666668e-05, 'epoch': 5.0}
{'eval_loss': 2.6944313049316406, 'eval_accuracy@nld.rst.nldt': 0.30513595166163143, 'eval_f1@nld.rst.nldt': 0.03653519183499292, 'eval_precision@nld.rst.nldt': 0.038443646996278585, 'eval_recall@nld.rst.nldt': 0.053212943792653934, 'eval_loss@nld.rst.nldt': 2.6944313049316406, 'eval_runtime': 4.42, 'eval_samples_per_second': 74.887, 'eval_steps_per_second': 2.489, 'epoch': 5.0}
{'train@nld.rst.nldt_loss': 2.7069098949432373, 'train@nld.rst.nldt_accuracy@nld.rst.nldt': 0.2947761194029851, 'train@nld.rst.nldt_f1@nld.rst.nldt': 0.03192112971686795, 'train@nld.rst.nldt_precision@nld.rst.nldt': 0.027993612576372444, 'train@nld.rst.nldt_recall@nld.rst.nldt': 0.047888071895424836, 'train@nld.rst.nldt_loss@nld.rst.nldt': 2.706909418106079, 'train@nld.rst.nldt_runtime': 19.7096, 'train@nld.rst.nldt_samples_per_second': 81.585, 'train@nld.rst.nldt_steps_per_second': 2.588, 'epoch': 6.0}
{'loss': 2.7461, 'learning_rate': 1e-05, 'epoch': 6.0}
{'eval_loss': 2.6645915508270264, 'eval_accuracy@nld.rst.nldt': 0.311178247734139, 'eval_f1@nld.rst.nldt': 0.03893260089725699, 'eval_precision@nld.rst.nldt': 0.038621819098009574, 'eval_recall@nld.rst.nldt': 0.05798379469877055, 'eval_loss@nld.rst.nldt': 2.664591073989868, 'eval_runtime': 4.426, 'eval_samples_per_second': 74.786, 'eval_steps_per_second': 2.485, 'epoch': 6.0}
{'train@nld.rst.nldt_loss': 2.678699493408203, 'train@nld.rst.nldt_accuracy@nld.rst.nldt': 0.3034825870646766, 'train@nld.rst.nldt_f1@nld.rst.nldt': 0.034729611384783796, 'train@nld.rst.nldt_precision@nld.rst.nldt': 0.027917364069791034, 'train@nld.rst.nldt_recall@nld.rst.nldt': 0.053049136321195144, 'train@nld.rst.nldt_loss@nld.rst.nldt': 2.678699016571045, 'train@nld.rst.nldt_runtime': 19.6986, 'train@nld.rst.nldt_samples_per_second': 81.63, 'train@nld.rst.nldt_steps_per_second': 2.589, 'epoch': 7.0}
{'loss': 2.7169, 'learning_rate': 8.333333333333334e-06, 'epoch': 7.0}
{'eval_loss': 2.64074969291687, 'eval_accuracy@nld.rst.nldt': 0.31722054380664655, 'eval_f1@nld.rst.nldt': 0.039492004954865705, 'eval_precision@nld.rst.nldt': 0.035035301729879925, 'eval_recall@nld.rst.nldt': 0.06109960892569588, 'eval_loss@nld.rst.nldt': 2.640749454498291, 'eval_runtime': 4.4177, 'eval_samples_per_second': 74.926, 'eval_steps_per_second': 2.49, 'epoch': 7.0}
{'train@nld.rst.nldt_loss': 2.656614303588867, 'train@nld.rst.nldt_accuracy@nld.rst.nldt': 0.3041044776119403, 'train@nld.rst.nldt_f1@nld.rst.nldt': 0.034879134053398445, 'train@nld.rst.nldt_precision@nld.rst.nldt': 0.02680244442770606, 'train@nld.rst.nldt_recall@nld.rst.nldt': 0.05521767040149393, 'train@nld.rst.nldt_loss@nld.rst.nldt': 2.6566147804260254, 'train@nld.rst.nldt_runtime': 19.6999, 'train@nld.rst.nldt_samples_per_second': 81.625, 'train@nld.rst.nldt_steps_per_second': 2.589, 'epoch': 8.0}
{'loss': 2.6998, 'learning_rate': 6.666666666666667e-06, 'epoch': 8.0}
{'eval_loss': 2.62243390083313, 'eval_accuracy@nld.rst.nldt': 0.32628398791540786, 'eval_f1@nld.rst.nldt': 0.04158444841496541, 'eval_precision@nld.rst.nldt': 0.03659938006741299, 'eval_recall@nld.rst.nldt': 0.06527362421082229, 'eval_loss@nld.rst.nldt': 2.62243390083313, 'eval_runtime': 4.397, 'eval_samples_per_second': 75.279, 'eval_steps_per_second': 2.502, 'epoch': 8.0}
{'train@nld.rst.nldt_loss': 2.6431140899658203, 'train@nld.rst.nldt_accuracy@nld.rst.nldt': 0.30534825870646765, 'train@nld.rst.nldt_f1@nld.rst.nldt': 0.03530252335985426, 'train@nld.rst.nldt_precision@nld.rst.nldt': 0.026803562587426603, 'train@nld.rst.nldt_recall@nld.rst.nldt': 0.05588818860877684, 'train@nld.rst.nldt_loss@nld.rst.nldt': 2.6431140899658203, 'train@nld.rst.nldt_runtime': 19.6384, 'train@nld.rst.nldt_samples_per_second': 81.88, 'train@nld.rst.nldt_steps_per_second': 2.597, 'epoch': 9.0}
{'loss': 2.6823, 'learning_rate': 5e-06, 'epoch': 9.0}
{'eval_loss': 2.610543727874756, 'eval_accuracy@nld.rst.nldt': 0.32326283987915405, 'eval_f1@nld.rst.nldt': 0.040490052064169495, 'eval_precision@nld.rst.nldt': 0.035290575127973504, 'eval_recall@nld.rst.nldt': 0.063216011042098, 'eval_loss@nld.rst.nldt': 2.610543966293335, 'eval_runtime': 4.4086, 'eval_samples_per_second': 75.081, 'eval_steps_per_second': 2.495, 'epoch': 9.0}
{'train@nld.rst.nldt_loss': 2.6332356929779053, 'train@nld.rst.nldt_accuracy@nld.rst.nldt': 0.30597014925373134, 'train@nld.rst.nldt_f1@nld.rst.nldt': 0.035549013812842215, 'train@nld.rst.nldt_precision@nld.rst.nldt': 0.02703367108687403, 'train@nld.rst.nldt_recall@nld.rst.nldt': 0.05613620448179272, 'train@nld.rst.nldt_loss@nld.rst.nldt': 2.6332361698150635, 'train@nld.rst.nldt_runtime': 19.6917, 'train@nld.rst.nldt_samples_per_second': 81.659, 'train@nld.rst.nldt_steps_per_second': 2.59, 'epoch': 10.0}
{'loss': 2.6657, 'learning_rate': 3.3333333333333333e-06, 'epoch': 10.0}
{'eval_loss': 2.601877450942993, 'eval_accuracy@nld.rst.nldt': 0.32326283987915405, 'eval_f1@nld.rst.nldt': 0.040490052064169495, 'eval_precision@nld.rst.nldt': 0.035290575127973504, 'eval_recall@nld.rst.nldt': 0.063216011042098, 'eval_loss@nld.rst.nldt': 2.6018776893615723, 'eval_runtime': 4.4165, 'eval_samples_per_second': 74.945, 'eval_steps_per_second': 2.491, 'epoch': 10.0}
{'train@nld.rst.nldt_loss': 2.6259734630584717, 'train@nld.rst.nldt_accuracy@nld.rst.nldt': 0.30845771144278605, 'train@nld.rst.nldt_f1@nld.rst.nldt': 0.036246630896034276, 'train@nld.rst.nldt_precision@nld.rst.nldt': 0.027447334487033466, 'train@nld.rst.nldt_recall@nld.rst.nldt': 0.05741946778711485, 'train@nld.rst.nldt_loss@nld.rst.nldt': 2.6259732246398926, 'train@nld.rst.nldt_runtime': 19.6764, 'train@nld.rst.nldt_samples_per_second': 81.722, 'train@nld.rst.nldt_steps_per_second': 2.592, 'epoch': 11.0}
{'loss': 2.6507, 'learning_rate': 1.6666666666666667e-06, 'epoch': 11.0}
{'eval_loss': 2.595898389816284, 'eval_accuracy@nld.rst.nldt': 0.32628398791540786, 'eval_f1@nld.rst.nldt': 0.04143622012474472, 'eval_precision@nld.rst.nldt': 0.03586589018730636, 'eval_recall@nld.rst.nldt': 0.06527362421082229, 'eval_loss@nld.rst.nldt': 2.595898389816284, 'eval_runtime': 4.4035, 'eval_samples_per_second': 75.168, 'eval_steps_per_second': 2.498, 'epoch': 11.0}
{'train@nld.rst.nldt_loss': 2.623175859451294, 'train@nld.rst.nldt_accuracy@nld.rst.nldt': 0.30970149253731344, 'train@nld.rst.nldt_f1@nld.rst.nldt': 0.03655718338652421, 'train@nld.rst.nldt_precision@nld.rst.nldt': 0.027628595914927976, 'train@nld.rst.nldt_recall@nld.rst.nldt': 0.057973856209150326, 'train@nld.rst.nldt_loss@nld.rst.nldt': 2.623175621032715, 'train@nld.rst.nldt_runtime': 19.7228, 'train@nld.rst.nldt_samples_per_second': 81.53, 'train@nld.rst.nldt_steps_per_second': 2.586, 'epoch': 12.0}
{'loss': 2.6545, 'learning_rate': 0.0, 'epoch': 12.0}
{'eval_loss': 2.593932628631592, 'eval_accuracy@nld.rst.nldt': 0.3293051359516616, 'eval_f1@nld.rst.nldt': 0.042329953192771316, 'eval_precision@nld.rst.nldt': 0.03639964107800365, 'eval_recall@nld.rst.nldt': 0.06733123737954656, 'eval_loss@nld.rst.nldt': 2.593932628631592, 'eval_runtime': 4.418, 'eval_samples_per_second': 74.921, 'eval_steps_per_second': 2.49, 'epoch': 12.0}
{'train_runtime': 781.8755, 'train_samples_per_second': 24.679, 'train_steps_per_second': 0.783, 'train_loss': 2.842431236715878, 'epoch': 12.0}
***** train_log metrics *****
  epoch                    =       12.0
  train_loss               =     2.8424
  train_runtime            = 0:13:01.87
  train_samples_per_second =     24.679
  train_steps_per_second   =      0.783
{'train@eng.sdrt.stac_loss': 2.0814685821533203, 'train@eng.sdrt.stac_accuracy@eng.sdrt.stac': 0.3568893528183716, 'train@eng.sdrt.stac_f1@eng.sdrt.stac': 0.07130318390157922, 'train@eng.sdrt.stac_precision@eng.sdrt.stac': 0.058331636974424464, 'train@eng.sdrt.stac_recall@eng.sdrt.stac': 0.11274189624635471, 'train@eng.sdrt.stac_loss@eng.sdrt.stac': 2.0814688205718994, 'train@eng.sdrt.stac_runtime': 113.0485, 'train@eng.sdrt.stac_samples_per_second': 84.742, 'train@eng.sdrt.stac_steps_per_second': 2.654, 'epoch': 1.0}
{'loss': 2.5184, 'learning_rate': 1.8333333333333333e-05, 'epoch': 1.0}
{'eval_loss': 2.0453269481658936, 'eval_accuracy@eng.sdrt.stac': 0.36943231441048036, 'eval_f1@eng.sdrt.stac': 0.07436324689583003, 'eval_precision@eng.sdrt.stac': 0.0626579954506911, 'eval_recall@eng.sdrt.stac': 0.116105904710447, 'eval_loss@eng.sdrt.stac': 2.0453271865844727, 'eval_runtime': 13.9282, 'eval_samples_per_second': 82.207, 'eval_steps_per_second': 2.585, 'epoch': 1.0}
{'train@eng.sdrt.stac_loss': 1.8388054370880127, 'train@eng.sdrt.stac_accuracy@eng.sdrt.stac': 0.438517745302714, 'train@eng.sdrt.stac_f1@eng.sdrt.stac': 0.1377029758650484, 'train@eng.sdrt.stac_precision@eng.sdrt.stac': 0.1283802551225095, 'train@eng.sdrt.stac_recall@eng.sdrt.stac': 0.1823964879848608, 'train@eng.sdrt.stac_loss@eng.sdrt.stac': 1.8388054370880127, 'train@eng.sdrt.stac_runtime': 113.0112, 'train@eng.sdrt.stac_samples_per_second': 84.77, 'train@eng.sdrt.stac_steps_per_second': 2.655, 'epoch': 2.0}
{'loss': 2.0033, 'learning_rate': 1.6666666666666667e-05, 'epoch': 2.0}
{'eval_loss': 1.8014768362045288, 'eval_accuracy@eng.sdrt.stac': 0.4410480349344978, 'eval_f1@eng.sdrt.stac': 0.13313050295048906, 'eval_precision@eng.sdrt.stac': 0.12373100977962748, 'eval_recall@eng.sdrt.stac': 0.17954978213502884, 'eval_loss@eng.sdrt.stac': 1.8014768362045288, 'eval_runtime': 13.9322, 'eval_samples_per_second': 82.183, 'eval_steps_per_second': 2.584, 'epoch': 2.0}
{'train@eng.sdrt.stac_loss': 1.7142527103424072, 'train@eng.sdrt.stac_accuracy@eng.sdrt.stac': 0.4615866388308977, 'train@eng.sdrt.stac_f1@eng.sdrt.stac': 0.16867426707595792, 'train@eng.sdrt.stac_precision@eng.sdrt.stac': 0.25676843611574457, 'train@eng.sdrt.stac_recall@eng.sdrt.stac': 0.20533413829651573, 'train@eng.sdrt.stac_loss@eng.sdrt.stac': 1.7142527103424072, 'train@eng.sdrt.stac_runtime': 113.1791, 'train@eng.sdrt.stac_samples_per_second': 84.645, 'train@eng.sdrt.stac_steps_per_second': 2.651, 'epoch': 3.0}
{'loss': 1.8085, 'learning_rate': 1.5000000000000002e-05, 'epoch': 3.0}
{'eval_loss': 1.6739989519119263, 'eval_accuracy@eng.sdrt.stac': 0.4724890829694323, 'eval_f1@eng.sdrt.stac': 0.16598086710713614, 'eval_precision@eng.sdrt.stac': 0.20337490364641356, 'eval_recall@eng.sdrt.stac': 0.20390631469630027, 'eval_loss@eng.sdrt.stac': 1.6739989519119263, 'eval_runtime': 13.9747, 'eval_samples_per_second': 81.934, 'eval_steps_per_second': 2.576, 'epoch': 3.0}
{'train@eng.sdrt.stac_loss': 1.6345546245574951, 'train@eng.sdrt.stac_accuracy@eng.sdrt.stac': 0.4910229645093946, 'train@eng.sdrt.stac_f1@eng.sdrt.stac': 0.22188901904870803, 'train@eng.sdrt.stac_precision@eng.sdrt.stac': 0.2549731272258328, 'train@eng.sdrt.stac_recall@eng.sdrt.stac': 0.24476918574927573, 'train@eng.sdrt.stac_loss@eng.sdrt.stac': 1.6345547437667847, 'train@eng.sdrt.stac_runtime': 113.1274, 'train@eng.sdrt.stac_samples_per_second': 84.683, 'train@eng.sdrt.stac_steps_per_second': 2.652, 'epoch': 4.0}
{'loss': 1.7117, 'learning_rate': 1.3333333333333333e-05, 'epoch': 4.0}
{'eval_loss': 1.5899927616119385, 'eval_accuracy@eng.sdrt.stac': 0.4943231441048035, 'eval_f1@eng.sdrt.stac': 0.2102787079752078, 'eval_precision@eng.sdrt.stac': 0.2179725802409327, 'eval_recall@eng.sdrt.stac': 0.23176637315060633, 'eval_loss@eng.sdrt.stac': 1.589992880821228, 'eval_runtime': 13.9886, 'eval_samples_per_second': 81.852, 'eval_steps_per_second': 2.574, 'epoch': 4.0}
{'train@eng.sdrt.stac_loss': 1.5809038877487183, 'train@eng.sdrt.stac_accuracy@eng.sdrt.stac': 0.5069937369519834, 'train@eng.sdrt.stac_f1@eng.sdrt.stac': 0.26460594452617114, 'train@eng.sdrt.stac_precision@eng.sdrt.stac': 0.2725785423277256, 'train@eng.sdrt.stac_recall@eng.sdrt.stac': 0.284808382264755, 'train@eng.sdrt.stac_loss@eng.sdrt.stac': 1.5809037685394287, 'train@eng.sdrt.stac_runtime': 112.8371, 'train@eng.sdrt.stac_samples_per_second': 84.901, 'train@eng.sdrt.stac_steps_per_second': 2.659, 'epoch': 5.0}
{'loss': 1.6501, 'learning_rate': 1.1666666666666668e-05, 'epoch': 5.0}
{'eval_loss': 1.5476287603378296, 'eval_accuracy@eng.sdrt.stac': 0.503056768558952, 'eval_f1@eng.sdrt.stac': 0.24332641447905873, 'eval_precision@eng.sdrt.stac': 0.26130250247676645, 'eval_recall@eng.sdrt.stac': 0.26141182709416766, 'eval_loss@eng.sdrt.stac': 1.5476288795471191, 'eval_runtime': 13.936, 'eval_samples_per_second': 82.161, 'eval_steps_per_second': 2.583, 'epoch': 5.0}
{'train@eng.sdrt.stac_loss': 1.5357542037963867, 'train@eng.sdrt.stac_accuracy@eng.sdrt.stac': 0.5151356993736952, 'train@eng.sdrt.stac_f1@eng.sdrt.stac': 0.2790820475764388, 'train@eng.sdrt.stac_precision@eng.sdrt.stac': 0.34635600820691415, 'train@eng.sdrt.stac_recall@eng.sdrt.stac': 0.2973232641054826, 'train@eng.sdrt.stac_loss@eng.sdrt.stac': 1.5357542037963867, 'train@eng.sdrt.stac_runtime': 113.4345, 'train@eng.sdrt.stac_samples_per_second': 84.454, 'train@eng.sdrt.stac_steps_per_second': 2.645, 'epoch': 6.0}
{'loss': 1.603, 'learning_rate': 1e-05, 'epoch': 6.0}
{'eval_loss': 1.525216817855835, 'eval_accuracy@eng.sdrt.stac': 0.5021834061135371, 'eval_f1@eng.sdrt.stac': 0.24197947335580344, 'eval_precision@eng.sdrt.stac': 0.2631717330716877, 'eval_recall@eng.sdrt.stac': 0.26179778128393716, 'eval_loss@eng.sdrt.stac': 1.525216817855835, 'eval_runtime': 13.9982, 'eval_samples_per_second': 81.796, 'eval_steps_per_second': 2.572, 'epoch': 6.0}
{'train@eng.sdrt.stac_loss': 1.5200198888778687, 'train@eng.sdrt.stac_accuracy@eng.sdrt.stac': 0.5245302713987474, 'train@eng.sdrt.stac_f1@eng.sdrt.stac': 0.29811678237249606, 'train@eng.sdrt.stac_precision@eng.sdrt.stac': 0.36352532528714865, 'train@eng.sdrt.stac_recall@eng.sdrt.stac': 0.31636544842041886, 'train@eng.sdrt.stac_loss@eng.sdrt.stac': 1.5200198888778687, 'train@eng.sdrt.stac_runtime': 113.0422, 'train@eng.sdrt.stac_samples_per_second': 84.747, 'train@eng.sdrt.stac_steps_per_second': 2.654, 'epoch': 7.0}
{'loss': 1.5659, 'learning_rate': 8.333333333333334e-06, 'epoch': 7.0}
{'eval_loss': 1.513139009475708, 'eval_accuracy@eng.sdrt.stac': 0.519650655021834, 'eval_f1@eng.sdrt.stac': 0.2742429112903921, 'eval_precision@eng.sdrt.stac': 0.32895167684440635, 'eval_recall@eng.sdrt.stac': 0.2888775212582404, 'eval_loss@eng.sdrt.stac': 1.513139009475708, 'eval_runtime': 13.9343, 'eval_samples_per_second': 82.171, 'eval_steps_per_second': 2.584, 'epoch': 7.0}
{'train@eng.sdrt.stac_loss': 1.485044240951538, 'train@eng.sdrt.stac_accuracy@eng.sdrt.stac': 0.5322546972860125, 'train@eng.sdrt.stac_f1@eng.sdrt.stac': 0.31839737544802504, 'train@eng.sdrt.stac_precision@eng.sdrt.stac': 0.3689637694372304, 'train@eng.sdrt.stac_recall@eng.sdrt.stac': 0.33142684349240953, 'train@eng.sdrt.stac_loss@eng.sdrt.stac': 1.485044240951538, 'train@eng.sdrt.stac_runtime': 112.9114, 'train@eng.sdrt.stac_samples_per_second': 84.845, 'train@eng.sdrt.stac_steps_per_second': 2.657, 'epoch': 8.0}
{'loss': 1.5391, 'learning_rate': 6.666666666666667e-06, 'epoch': 8.0}
{'eval_loss': 1.4823215007781982, 'eval_accuracy@eng.sdrt.stac': 0.5231441048034935, 'eval_f1@eng.sdrt.stac': 0.2804291146639212, 'eval_precision@eng.sdrt.stac': 0.3819916736137382, 'eval_recall@eng.sdrt.stac': 0.2924392754577091, 'eval_loss@eng.sdrt.stac': 1.4823215007781982, 'eval_runtime': 13.904, 'eval_samples_per_second': 82.35, 'eval_steps_per_second': 2.589, 'epoch': 8.0}
{'train@eng.sdrt.stac_loss': 1.4712775945663452, 'train@eng.sdrt.stac_accuracy@eng.sdrt.stac': 0.5424843423799582, 'train@eng.sdrt.stac_f1@eng.sdrt.stac': 0.33645450725973114, 'train@eng.sdrt.stac_precision@eng.sdrt.stac': 0.43788347823473234, 'train@eng.sdrt.stac_recall@eng.sdrt.stac': 0.34927480635223385, 'train@eng.sdrt.stac_loss@eng.sdrt.stac': 1.4712775945663452, 'train@eng.sdrt.stac_runtime': 113.112, 'train@eng.sdrt.stac_samples_per_second': 84.695, 'train@eng.sdrt.stac_steps_per_second': 2.652, 'epoch': 9.0}
{'loss': 1.5182, 'learning_rate': 5e-06, 'epoch': 9.0}
{'eval_loss': 1.4741185903549194, 'eval_accuracy@eng.sdrt.stac': 0.5266375545851528, 'eval_f1@eng.sdrt.stac': 0.2899475784615321, 'eval_precision@eng.sdrt.stac': 0.3647989671331259, 'eval_recall@eng.sdrt.stac': 0.30117242701502056, 'eval_loss@eng.sdrt.stac': 1.4741184711456299, 'eval_runtime': 13.974, 'eval_samples_per_second': 81.938, 'eval_steps_per_second': 2.576, 'epoch': 9.0}
{'train@eng.sdrt.stac_loss': 1.4580860137939453, 'train@eng.sdrt.stac_accuracy@eng.sdrt.stac': 0.5434237995824635, 'train@eng.sdrt.stac_f1@eng.sdrt.stac': 0.3398077957424962, 'train@eng.sdrt.stac_precision@eng.sdrt.stac': 0.425555539558716, 'train@eng.sdrt.stac_recall@eng.sdrt.stac': 0.35250296429563843, 'train@eng.sdrt.stac_loss@eng.sdrt.stac': 1.4580858945846558, 'train@eng.sdrt.stac_runtime': 113.1015, 'train@eng.sdrt.stac_samples_per_second': 84.703, 'train@eng.sdrt.stac_steps_per_second': 2.652, 'epoch': 10.0}
{'loss': 1.5042, 'learning_rate': 3.3333333333333333e-06, 'epoch': 10.0}
{'eval_loss': 1.4654083251953125, 'eval_accuracy@eng.sdrt.stac': 0.537117903930131, 'eval_f1@eng.sdrt.stac': 0.303829912043674, 'eval_precision@eng.sdrt.stac': 0.4367836556295248, 'eval_recall@eng.sdrt.stac': 0.3124379890488112, 'eval_loss@eng.sdrt.stac': 1.465408205986023, 'eval_runtime': 13.9526, 'eval_samples_per_second': 82.064, 'eval_steps_per_second': 2.58, 'epoch': 10.0}
{'train@eng.sdrt.stac_loss': 1.4511812925338745, 'train@eng.sdrt.stac_accuracy@eng.sdrt.stac': 0.5470772442588726, 'train@eng.sdrt.stac_f1@eng.sdrt.stac': 0.34699805877548884, 'train@eng.sdrt.stac_precision@eng.sdrt.stac': 0.4240666187675838, 'train@eng.sdrt.stac_recall@eng.sdrt.stac': 0.3591933259805105, 'train@eng.sdrt.stac_loss@eng.sdrt.stac': 1.4511812925338745, 'train@eng.sdrt.stac_runtime': 112.8682, 'train@eng.sdrt.stac_samples_per_second': 84.878, 'train@eng.sdrt.stac_steps_per_second': 2.658, 'epoch': 11.0}
{'loss': 1.4955, 'learning_rate': 1.6666666666666667e-06, 'epoch': 11.0}
{'eval_loss': 1.460322618484497, 'eval_accuracy@eng.sdrt.stac': 0.5353711790393013, 'eval_f1@eng.sdrt.stac': 0.30163795397198856, 'eval_precision@eng.sdrt.stac': 0.4322022808246698, 'eval_recall@eng.sdrt.stac': 0.31100127110587783, 'eval_loss@eng.sdrt.stac': 1.460322618484497, 'eval_runtime': 13.9739, 'eval_samples_per_second': 81.938, 'eval_steps_per_second': 2.576, 'epoch': 11.0}
{'train@eng.sdrt.stac_loss': 1.4498227834701538, 'train@eng.sdrt.stac_accuracy@eng.sdrt.stac': 0.547286012526096, 'train@eng.sdrt.stac_f1@eng.sdrt.stac': 0.34601654687752026, 'train@eng.sdrt.stac_precision@eng.sdrt.stac': 0.429166491924748, 'train@eng.sdrt.stac_recall@eng.sdrt.stac': 0.3583420559628211, 'train@eng.sdrt.stac_loss@eng.sdrt.stac': 1.4498227834701538, 'train@eng.sdrt.stac_runtime': 113.1246, 'train@eng.sdrt.stac_samples_per_second': 84.685, 'train@eng.sdrt.stac_steps_per_second': 2.652, 'epoch': 12.0}
{'loss': 1.4907, 'learning_rate': 0.0, 'epoch': 12.0}
{'eval_loss': 1.4596682786941528, 'eval_accuracy@eng.sdrt.stac': 0.5353711790393013, 'eval_f1@eng.sdrt.stac': 0.3014697905898157, 'eval_precision@eng.sdrt.stac': 0.4327747931475831, 'eval_recall@eng.sdrt.stac': 0.31030533859795906, 'eval_loss@eng.sdrt.stac': 1.4596683979034424, 'eval_runtime': 13.9339, 'eval_samples_per_second': 82.174, 'eval_steps_per_second': 2.584, 'epoch': 12.0}
{'train_runtime': 4386.6217, 'train_samples_per_second': 26.207, 'train_steps_per_second': 0.821, 'train_loss': 1.7007034640842014, 'epoch': 12.0}
***** train_logger metrics *****
  epoch                    =       12.0
  train_loss               =     2.8424
  train_runtime            = 0:13:01.87
  train_samples_per_second =     24.679
  train_steps_per_second   =      0.783
-------------------------------------------------------------------
Lang1:  por.rst.cstn    Lang2:  eng.sdrt.stac
Saving run to:  runs/full_shot/FullShot=v4_por.rst.cstn_eng.sdrt.stac_lrfunc=Adafactor_lr=2e-05_plm=bert-base-multilingual-cased
Running with params: BERT_MODEL=bert-base-multilingual-cased lr=2e-05
read 4148 examples
read 573 examples
read 272 examples
read 9580 examples
read 1145 examples
read 1510 examples
Total prediction labels:  48
BertAdapterModel(
  (shared_parameters): ModuleDict()
  (bert): BertModel(
    (shared_parameters): ModuleDict()
    (invertible_adapters): ModuleDict(
      (pt): NICECouplingBlock(
        (F): Sequential(
          (0): Linear(in_features=384, out_features=192, bias=True)
          (1): Activation_Function_Class(
            (f): NewGELUActivation()
          )
          (2): Linear(in_features=192, out_features=384, bias=True)
        )
        (G): Sequential(
          (0): Linear(in_features=384, out_features=192, bias=True)
          (1): Activation_Function_Class(
            (f): NewGELUActivation()
          )
          (2): Linear(in_features=192, out_features=384, bias=True)
        )
      )
      (en): NICECouplingBlock(
        (F): Sequential(
          (0): Linear(in_features=384, out_features=192, bias=True)
          (1): Activation_Function_Class(
            (f): ReLU()
          )
          (2): Linear(in_features=192, out_features=384, bias=True)
        )
        (G): Sequential(
          (0): Linear(in_features=384, out_features=192, bias=True)
          (1): Activation_Function_Class(
            (f): ReLU()
          )
          (2): Linear(in_features=192, out_features=384, bias=True)
        )
      )
    )
    (embeddings): BertEmbeddings(
      (word_embeddings): Embedding(119547, 768, padding_idx=0)
      (position_embeddings): Embedding(512, 768)
      (token_type_embeddings): Embedding(2, 768)
      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
      (dropout): Dropout(p=0.1, inplace=False)
    )
    (encoder): BertEncoder(
      (layer): ModuleList(
        (0): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (pt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (en): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (1): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (pt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (en): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (2): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (pt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (en): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (3): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (pt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (en): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (4): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (pt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (en): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (5): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (pt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (en): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (6): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (pt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (en): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (7): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (pt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (en): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (8): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (pt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (en): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (9): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (pt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (en): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (10): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (pt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (en): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (11): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (pt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (en): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
      )
    )
    (pooler): BertPooler(
      (dense): Linear(in_features=768, out_features=768, bias=True)
      (activation): Tanh()
    )
    (prefix_tuning): PrefixTuningPool(
      (prefix_tunings): ModuleDict()
    )
  )
  (heads): ModuleDict(
    (pt): BertStyleMaskedLMHead(
      (0): Linear(in_features=768, out_features=768, bias=True)
      (1): Activation_Function_Class(
        (f): GELUActivation()
      )
      (2): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
      (3): Linear(in_features=768, out_features=119547, bias=True)
    )
    (disrpt): ClassificationHead(
      (0): Dropout(p=0.1, inplace=False)
      (1): Linear(in_features=768, out_features=768, bias=True)
      (2): Activation_Function_Class(
        (f): Tanh()
      )
      (3): Dropout(p=0.1, inplace=False)
      (4): Linear(in_features=768, out_features=48, bias=True)
    )
  )
)
{'train@por.rst.cstn_loss': 2.5304811000823975, 'train@por.rst.cstn_accuracy@por.rst.cstn': 0.2774831243972999, 'train@por.rst.cstn_f1@por.rst.cstn': 0.013575674655595393, 'train@por.rst.cstn_precision@por.rst.cstn': 0.008671347637415621, 'train@por.rst.cstn_recall@por.rst.cstn': 0.03125, 'train@por.rst.cstn_loss@por.rst.cstn': 2.5304808616638184, 'train@por.rst.cstn_runtime': 49.9368, 'train@por.rst.cstn_samples_per_second': 83.065, 'train@por.rst.cstn_steps_per_second': 2.603, 'epoch': 1.0}
{'loss': 3.14, 'learning_rate': 1.8333333333333333e-05, 'epoch': 1.0}
{'eval_loss': 2.6180546283721924, 'eval_accuracy@por.rst.cstn': 0.28097731239092494, 'eval_f1@por.rst.cstn': 0.019940549913301957, 'eval_precision@por.rst.cstn': 0.012771696017769315, 'eval_recall@por.rst.cstn': 0.045454545454545456, 'eval_loss@por.rst.cstn': 2.6180543899536133, 'eval_runtime': 7.2953, 'eval_samples_per_second': 78.543, 'eval_steps_per_second': 2.467, 'epoch': 1.0}
{'train@por.rst.cstn_loss': 2.2654175758361816, 'train@por.rst.cstn_accuracy@por.rst.cstn': 0.3599324975891996, 'train@por.rst.cstn_f1@por.rst.cstn': 0.05172759328505027, 'train@por.rst.cstn_precision@por.rst.cstn': 0.08455441799495964, 'train@por.rst.cstn_recall@por.rst.cstn': 0.055796811747218816, 'train@por.rst.cstn_loss@por.rst.cstn': 2.2654173374176025, 'train@por.rst.cstn_runtime': 50.0222, 'train@por.rst.cstn_samples_per_second': 82.923, 'train@por.rst.cstn_steps_per_second': 2.599, 'epoch': 2.0}
{'loss': 2.4238, 'learning_rate': 1.6666666666666667e-05, 'epoch': 2.0}
{'eval_loss': 2.3655970096588135, 'eval_accuracy@por.rst.cstn': 0.3054101221640489, 'eval_f1@por.rst.cstn': 0.05196210336344149, 'eval_precision@por.rst.cstn': 0.11353019343525673, 'eval_recall@por.rst.cstn': 0.06436638217894719, 'eval_loss@por.rst.cstn': 2.3655970096588135, 'eval_runtime': 7.2805, 'eval_samples_per_second': 78.703, 'eval_steps_per_second': 2.472, 'epoch': 2.0}
{'train@por.rst.cstn_loss': 2.023723840713501, 'train@por.rst.cstn_accuracy@por.rst.cstn': 0.4901157184185149, 'train@por.rst.cstn_f1@por.rst.cstn': 0.0828022609428318, 'train@por.rst.cstn_precision@por.rst.cstn': 0.09474447470556259, 'train@por.rst.cstn_recall@por.rst.cstn': 0.09301263313315225, 'train@por.rst.cstn_loss@por.rst.cstn': 2.02372407913208, 'train@por.rst.cstn_runtime': 49.9178, 'train@por.rst.cstn_samples_per_second': 83.097, 'train@por.rst.cstn_steps_per_second': 2.604, 'epoch': 3.0}
{'loss': 2.2027, 'learning_rate': 1.5000000000000002e-05, 'epoch': 3.0}
{'eval_loss': 2.1439766883850098, 'eval_accuracy@por.rst.cstn': 0.3961605584642234, 'eval_f1@por.rst.cstn': 0.10346834169580689, 'eval_precision@por.rst.cstn': 0.09646490980133296, 'eval_recall@por.rst.cstn': 0.12536520064084034, 'eval_loss@por.rst.cstn': 2.1439766883850098, 'eval_runtime': 7.2646, 'eval_samples_per_second': 78.875, 'eval_steps_per_second': 2.478, 'epoch': 3.0}
{'train@por.rst.cstn_loss': 1.8365156650543213, 'train@por.rst.cstn_accuracy@por.rst.cstn': 0.5315814850530376, 'train@por.rst.cstn_f1@por.rst.cstn': 0.0998409771333959, 'train@por.rst.cstn_precision@por.rst.cstn': 0.1274998172595331, 'train@por.rst.cstn_recall@por.rst.cstn': 0.11035243883418588, 'train@por.rst.cstn_loss@por.rst.cstn': 1.8365156650543213, 'train@por.rst.cstn_runtime': 49.9392, 'train@por.rst.cstn_samples_per_second': 83.061, 'train@por.rst.cstn_steps_per_second': 2.603, 'epoch': 4.0}
{'loss': 1.9898, 'learning_rate': 1.3333333333333333e-05, 'epoch': 4.0}
{'eval_loss': 1.9802603721618652, 'eval_accuracy@por.rst.cstn': 0.4397905759162304, 'eval_f1@por.rst.cstn': 0.11760459415125712, 'eval_precision@por.rst.cstn': 0.1317684517812395, 'eval_recall@por.rst.cstn': 0.1489275612975426, 'eval_loss@por.rst.cstn': 1.9802603721618652, 'eval_runtime': 7.2778, 'eval_samples_per_second': 78.732, 'eval_steps_per_second': 2.473, 'epoch': 4.0}
{'train@por.rst.cstn_loss': 1.716396689414978, 'train@por.rst.cstn_accuracy@por.rst.cstn': 0.5583413693346191, 'train@por.rst.cstn_f1@por.rst.cstn': 0.12116629257537684, 'train@por.rst.cstn_precision@por.rst.cstn': 0.1261355330639093, 'train@por.rst.cstn_recall@por.rst.cstn': 0.1322613570935074, 'train@por.rst.cstn_loss@por.rst.cstn': 1.7163968086242676, 'train@por.rst.cstn_runtime': 50.0312, 'train@por.rst.cstn_samples_per_second': 82.908, 'train@por.rst.cstn_steps_per_second': 2.598, 'epoch': 5.0}
{'loss': 1.8366, 'learning_rate': 1.1666666666666668e-05, 'epoch': 5.0}
{'eval_loss': 1.8686165809631348, 'eval_accuracy@por.rst.cstn': 0.4642233856893543, 'eval_f1@por.rst.cstn': 0.15200813143523056, 'eval_precision@por.rst.cstn': 0.1976337677060821, 'eval_recall@por.rst.cstn': 0.176128945252099, 'eval_loss@por.rst.cstn': 1.8686165809631348, 'eval_runtime': 7.3143, 'eval_samples_per_second': 78.339, 'eval_steps_per_second': 2.461, 'epoch': 5.0}
{'train@por.rst.cstn_loss': 1.6474535465240479, 'train@por.rst.cstn_accuracy@por.rst.cstn': 0.5730472516875603, 'train@por.rst.cstn_f1@por.rst.cstn': 0.12571601334979393, 'train@por.rst.cstn_precision@por.rst.cstn': 0.1320604179146661, 'train@por.rst.cstn_recall@por.rst.cstn': 0.13853869359913373, 'train@por.rst.cstn_loss@por.rst.cstn': 1.6474534273147583, 'train@por.rst.cstn_runtime': 50.1101, 'train@por.rst.cstn_samples_per_second': 82.778, 'train@por.rst.cstn_steps_per_second': 2.594, 'epoch': 6.0}
{'loss': 1.7417, 'learning_rate': 1e-05, 'epoch': 6.0}
{'eval_loss': 1.800373911857605, 'eval_accuracy@por.rst.cstn': 0.4799301919720768, 'eval_f1@por.rst.cstn': 0.16636290519105376, 'eval_precision@por.rst.cstn': 0.1780669484140153, 'eval_recall@por.rst.cstn': 0.1843126838836221, 'eval_loss@por.rst.cstn': 1.8003736734390259, 'eval_runtime': 7.2887, 'eval_samples_per_second': 78.615, 'eval_steps_per_second': 2.47, 'epoch': 6.0}
{'train@por.rst.cstn_loss': 1.6032228469848633, 'train@por.rst.cstn_accuracy@por.rst.cstn': 0.5797974927675988, 'train@por.rst.cstn_f1@por.rst.cstn': 0.1325805416419094, 'train@por.rst.cstn_precision@por.rst.cstn': 0.1312844942980609, 'train@por.rst.cstn_recall@por.rst.cstn': 0.1461945345079983, 'train@por.rst.cstn_loss@por.rst.cstn': 1.6032229661941528, 'train@por.rst.cstn_runtime': 50.1238, 'train@por.rst.cstn_samples_per_second': 82.755, 'train@por.rst.cstn_steps_per_second': 2.594, 'epoch': 7.0}
{'loss': 1.6845, 'learning_rate': 8.333333333333334e-06, 'epoch': 7.0}
{'eval_loss': 1.758800983428955, 'eval_accuracy@por.rst.cstn': 0.49040139616055844, 'eval_f1@por.rst.cstn': 0.17323113977525742, 'eval_precision@por.rst.cstn': 0.1748206673083394, 'eval_recall@por.rst.cstn': 0.1923614807170039, 'eval_loss@por.rst.cstn': 1.758800745010376, 'eval_runtime': 7.305, 'eval_samples_per_second': 78.44, 'eval_steps_per_second': 2.464, 'epoch': 7.0}
{'train@por.rst.cstn_loss': 1.5747227668762207, 'train@por.rst.cstn_accuracy@por.rst.cstn': 0.587029893924783, 'train@por.rst.cstn_f1@por.rst.cstn': 0.1350318161447722, 'train@por.rst.cstn_precision@por.rst.cstn': 0.133936671388098, 'train@por.rst.cstn_recall@por.rst.cstn': 0.14728715886245147, 'train@por.rst.cstn_loss@por.rst.cstn': 1.5747227668762207, 'train@por.rst.cstn_runtime': 50.0846, 'train@por.rst.cstn_samples_per_second': 82.82, 'train@por.rst.cstn_steps_per_second': 2.596, 'epoch': 8.0}
{'loss': 1.6518, 'learning_rate': 6.666666666666667e-06, 'epoch': 8.0}
{'eval_loss': 1.7372708320617676, 'eval_accuracy@por.rst.cstn': 0.5008726003490401, 'eval_f1@por.rst.cstn': 0.1888940940479647, 'eval_precision@por.rst.cstn': 0.23144649517116236, 'eval_recall@por.rst.cstn': 0.2006599735354676, 'eval_loss@por.rst.cstn': 1.737270712852478, 'eval_runtime': 7.3206, 'eval_samples_per_second': 78.272, 'eval_steps_per_second': 2.459, 'epoch': 8.0}
{'train@por.rst.cstn_loss': 1.5563042163848877, 'train@por.rst.cstn_accuracy@por.rst.cstn': 0.5904050144648023, 'train@por.rst.cstn_f1@por.rst.cstn': 0.13811166988915904, 'train@por.rst.cstn_precision@por.rst.cstn': 0.1666700229762311, 'train@por.rst.cstn_recall@por.rst.cstn': 0.14959404867305928, 'train@por.rst.cstn_loss@por.rst.cstn': 1.5563043355941772, 'train@por.rst.cstn_runtime': 50.0164, 'train@por.rst.cstn_samples_per_second': 82.933, 'train@por.rst.cstn_steps_per_second': 2.599, 'epoch': 9.0}
{'loss': 1.6251, 'learning_rate': 5e-06, 'epoch': 9.0}
{'eval_loss': 1.7218507528305054, 'eval_accuracy@por.rst.cstn': 0.5026178010471204, 'eval_f1@por.rst.cstn': 0.19266764012431678, 'eval_precision@por.rst.cstn': 0.21328261629947953, 'eval_recall@por.rst.cstn': 0.2026686971351003, 'eval_loss@por.rst.cstn': 1.721850872039795, 'eval_runtime': 7.2793, 'eval_samples_per_second': 78.716, 'eval_steps_per_second': 2.473, 'epoch': 9.0}
{'train@por.rst.cstn_loss': 1.5433063507080078, 'train@por.rst.cstn_accuracy@por.rst.cstn': 0.5925747348119575, 'train@por.rst.cstn_f1@por.rst.cstn': 0.14159181341518312, 'train@por.rst.cstn_precision@por.rst.cstn': 0.14952869069422056, 'train@por.rst.cstn_recall@por.rst.cstn': 0.153970829836938, 'train@por.rst.cstn_loss@por.rst.cstn': 1.5433062314987183, 'train@por.rst.cstn_runtime': 49.998, 'train@por.rst.cstn_samples_per_second': 82.963, 'train@por.rst.cstn_steps_per_second': 2.6, 'epoch': 10.0}
{'loss': 1.5967, 'learning_rate': 3.3333333333333333e-06, 'epoch': 10.0}
{'eval_loss': 1.7096554040908813, 'eval_accuracy@por.rst.cstn': 0.5095986038394416, 'eval_f1@por.rst.cstn': 0.19608624381182904, 'eval_precision@por.rst.cstn': 0.20185783877225238, 'eval_recall@por.rst.cstn': 0.20972249614334074, 'eval_loss@por.rst.cstn': 1.7096554040908813, 'eval_runtime': 7.3151, 'eval_samples_per_second': 78.331, 'eval_steps_per_second': 2.461, 'epoch': 10.0}
{'train@por.rst.cstn_loss': 1.533966302871704, 'train@por.rst.cstn_accuracy@por.rst.cstn': 0.5973963355834137, 'train@por.rst.cstn_f1@por.rst.cstn': 0.14385937404132304, 'train@por.rst.cstn_precision@por.rst.cstn': 0.14841301717966943, 'train@por.rst.cstn_recall@por.rst.cstn': 0.1556951136735704, 'train@por.rst.cstn_loss@por.rst.cstn': 1.533966302871704, 'train@por.rst.cstn_runtime': 50.1492, 'train@por.rst.cstn_samples_per_second': 82.713, 'train@por.rst.cstn_steps_per_second': 2.592, 'epoch': 11.0}
{'loss': 1.5952, 'learning_rate': 1.6666666666666667e-06, 'epoch': 11.0}
{'eval_loss': 1.7039319276809692, 'eval_accuracy@por.rst.cstn': 0.5095986038394416, 'eval_f1@por.rst.cstn': 0.19676632959121737, 'eval_precision@por.rst.cstn': 0.20305197730711175, 'eval_recall@por.rst.cstn': 0.20984210586353294, 'eval_loss@por.rst.cstn': 1.7039321660995483, 'eval_runtime': 8.3114, 'eval_samples_per_second': 68.941, 'eval_steps_per_second': 2.166, 'epoch': 11.0}
{'train@por.rst.cstn_loss': 1.5311838388442993, 'train@por.rst.cstn_accuracy@por.rst.cstn': 0.5978784956605593, 'train@por.rst.cstn_f1@por.rst.cstn': 0.1441363252829872, 'train@por.rst.cstn_precision@por.rst.cstn': 0.14877541210824802, 'train@por.rst.cstn_recall@por.rst.cstn': 0.1557648389095339, 'train@por.rst.cstn_loss@por.rst.cstn': 1.5311839580535889, 'train@por.rst.cstn_runtime': 50.0004, 'train@por.rst.cstn_samples_per_second': 82.959, 'train@por.rst.cstn_steps_per_second': 2.6, 'epoch': 12.0}
{'loss': 1.5893, 'learning_rate': 0.0, 'epoch': 12.0}
{'eval_loss': 1.7004454135894775, 'eval_accuracy@por.rst.cstn': 0.5095986038394416, 'eval_f1@por.rst.cstn': 0.19497984409956243, 'eval_precision@por.rst.cstn': 0.20136161769563785, 'eval_recall@por.rst.cstn': 0.20783338226390027, 'eval_loss@por.rst.cstn': 1.700445294380188, 'eval_runtime': 7.325, 'eval_samples_per_second': 78.226, 'eval_steps_per_second': 2.457, 'epoch': 12.0}
{'train_runtime': 1957.0804, 'train_samples_per_second': 25.434, 'train_steps_per_second': 0.797, 'train_loss': 1.9231034107697316, 'epoch': 12.0}
***** train_log metrics *****
  epoch                    =       12.0
  train_loss               =     1.9231
  train_runtime            = 0:32:37.08
  train_samples_per_second =     25.434
  train_steps_per_second   =      0.797
{'train@eng.sdrt.stac_loss': 2.0498392581939697, 'train@eng.sdrt.stac_accuracy@eng.sdrt.stac': 0.3554279749478079, 'train@eng.sdrt.stac_f1@eng.sdrt.stac': 0.08363761095995514, 'train@eng.sdrt.stac_precision@eng.sdrt.stac': 0.08637336771370287, 'train@eng.sdrt.stac_recall@eng.sdrt.stac': 0.11594597130224651, 'train@eng.sdrt.stac_loss@eng.sdrt.stac': 2.0498392581939697, 'train@eng.sdrt.stac_runtime': 113.066, 'train@eng.sdrt.stac_samples_per_second': 84.729, 'train@eng.sdrt.stac_steps_per_second': 2.653, 'epoch': 1.0}
{'loss': 2.5325, 'learning_rate': 1.8333333333333333e-05, 'epoch': 1.0}
{'eval_loss': 2.021486520767212, 'eval_accuracy@eng.sdrt.stac': 0.3764192139737991, 'eval_f1@eng.sdrt.stac': 0.08788688680579715, 'eval_precision@eng.sdrt.stac': 0.10234035410727674, 'eval_recall@eng.sdrt.stac': 0.12090308809253128, 'eval_loss@eng.sdrt.stac': 2.021486282348633, 'eval_runtime': 13.9726, 'eval_samples_per_second': 81.946, 'eval_steps_per_second': 2.576, 'epoch': 1.0}
{'train@eng.sdrt.stac_loss': 1.7801951169967651, 'train@eng.sdrt.stac_accuracy@eng.sdrt.stac': 0.4551148225469729, 'train@eng.sdrt.stac_f1@eng.sdrt.stac': 0.1716885310170057, 'train@eng.sdrt.stac_precision@eng.sdrt.stac': 0.19295279471898152, 'train@eng.sdrt.stac_recall@eng.sdrt.stac': 0.20296066008383784, 'train@eng.sdrt.stac_loss@eng.sdrt.stac': 1.7801949977874756, 'train@eng.sdrt.stac_runtime': 112.9249, 'train@eng.sdrt.stac_samples_per_second': 84.835, 'train@eng.sdrt.stac_steps_per_second': 2.657, 'epoch': 2.0}
{'loss': 1.9608, 'learning_rate': 1.6666666666666667e-05, 'epoch': 2.0}
{'eval_loss': 1.7562997341156006, 'eval_accuracy@eng.sdrt.stac': 0.44366812227074237, 'eval_f1@eng.sdrt.stac': 0.15700249687891624, 'eval_precision@eng.sdrt.stac': 0.18404540148666135, 'eval_recall@eng.sdrt.stac': 0.19275627652199084, 'eval_loss@eng.sdrt.stac': 1.7562997341156006, 'eval_runtime': 13.9186, 'eval_samples_per_second': 82.264, 'eval_steps_per_second': 2.586, 'epoch': 2.0}
{'train@eng.sdrt.stac_loss': 1.6725820302963257, 'train@eng.sdrt.stac_accuracy@eng.sdrt.stac': 0.47286012526096033, 'train@eng.sdrt.stac_f1@eng.sdrt.stac': 0.21406894841715168, 'train@eng.sdrt.stac_precision@eng.sdrt.stac': 0.3014829315640701, 'train@eng.sdrt.stac_recall@eng.sdrt.stac': 0.24247771948794583, 'train@eng.sdrt.stac_loss@eng.sdrt.stac': 1.6725819110870361, 'train@eng.sdrt.stac_runtime': 112.9463, 'train@eng.sdrt.stac_samples_per_second': 84.819, 'train@eng.sdrt.stac_steps_per_second': 2.656, 'epoch': 3.0}
{'loss': 1.7714, 'learning_rate': 1.5000000000000002e-05, 'epoch': 3.0}
{'eval_loss': 1.6426914930343628, 'eval_accuracy@eng.sdrt.stac': 0.46026200873362444, 'eval_f1@eng.sdrt.stac': 0.19845778522973878, 'eval_precision@eng.sdrt.stac': 0.2645948121834354, 'eval_recall@eng.sdrt.stac': 0.22468217997593676, 'eval_loss@eng.sdrt.stac': 1.6426913738250732, 'eval_runtime': 13.8912, 'eval_samples_per_second': 82.426, 'eval_steps_per_second': 2.592, 'epoch': 3.0}
{'train@eng.sdrt.stac_loss': 1.6081323623657227, 'train@eng.sdrt.stac_accuracy@eng.sdrt.stac': 0.4909185803757829, 'train@eng.sdrt.stac_f1@eng.sdrt.stac': 0.2598690232117897, 'train@eng.sdrt.stac_precision@eng.sdrt.stac': 0.332725433002258, 'train@eng.sdrt.stac_recall@eng.sdrt.stac': 0.27750730555278014, 'train@eng.sdrt.stac_loss@eng.sdrt.stac': 1.6081323623657227, 'train@eng.sdrt.stac_runtime': 113.0727, 'train@eng.sdrt.stac_samples_per_second': 84.724, 'train@eng.sdrt.stac_steps_per_second': 2.653, 'epoch': 4.0}
{'loss': 1.6879, 'learning_rate': 1.3333333333333333e-05, 'epoch': 4.0}
{'eval_loss': 1.5719282627105713, 'eval_accuracy@eng.sdrt.stac': 0.48646288209606986, 'eval_f1@eng.sdrt.stac': 0.2479399224238565, 'eval_precision@eng.sdrt.stac': 0.4177591759697888, 'eval_recall@eng.sdrt.stac': 0.2646858965553661, 'eval_loss@eng.sdrt.stac': 1.5719282627105713, 'eval_runtime': 13.9402, 'eval_samples_per_second': 82.136, 'eval_steps_per_second': 2.582, 'epoch': 4.0}
{'train@eng.sdrt.stac_loss': 1.5631077289581299, 'train@eng.sdrt.stac_accuracy@eng.sdrt.stac': 0.5078288100208769, 'train@eng.sdrt.stac_f1@eng.sdrt.stac': 0.3044242223464256, 'train@eng.sdrt.stac_precision@eng.sdrt.stac': 0.42028223402209486, 'train@eng.sdrt.stac_recall@eng.sdrt.stac': 0.3145471369906721, 'train@eng.sdrt.stac_loss@eng.sdrt.stac': 1.5631076097488403, 'train@eng.sdrt.stac_runtime': 113.0061, 'train@eng.sdrt.stac_samples_per_second': 84.774, 'train@eng.sdrt.stac_steps_per_second': 2.655, 'epoch': 5.0}
{'loss': 1.6332, 'learning_rate': 1.1666666666666668e-05, 'epoch': 5.0}
{'eval_loss': 1.5331015586853027, 'eval_accuracy@eng.sdrt.stac': 0.5048034934497817, 'eval_f1@eng.sdrt.stac': 0.28231001210360307, 'eval_precision@eng.sdrt.stac': 0.4462366178188607, 'eval_recall@eng.sdrt.stac': 0.2948553935521021, 'eval_loss@eng.sdrt.stac': 1.5331015586853027, 'eval_runtime': 13.9629, 'eval_samples_per_second': 82.003, 'eval_steps_per_second': 2.578, 'epoch': 5.0}
{'train@eng.sdrt.stac_loss': 1.5254437923431396, 'train@eng.sdrt.stac_accuracy@eng.sdrt.stac': 0.5140918580375783, 'train@eng.sdrt.stac_f1@eng.sdrt.stac': 0.309327690991281, 'train@eng.sdrt.stac_precision@eng.sdrt.stac': 0.41813786702942174, 'train@eng.sdrt.stac_recall@eng.sdrt.stac': 0.3223964964866265, 'train@eng.sdrt.stac_loss@eng.sdrt.stac': 1.52544367313385, 'train@eng.sdrt.stac_runtime': 112.9477, 'train@eng.sdrt.stac_samples_per_second': 84.818, 'train@eng.sdrt.stac_steps_per_second': 2.656, 'epoch': 6.0}
{'loss': 1.5934, 'learning_rate': 1e-05, 'epoch': 6.0}
{'eval_loss': 1.5183757543563843, 'eval_accuracy@eng.sdrt.stac': 0.4978165938864629, 'eval_f1@eng.sdrt.stac': 0.3005226224095029, 'eval_precision@eng.sdrt.stac': 0.4758826788456809, 'eval_recall@eng.sdrt.stac': 0.3161051494764541, 'eval_loss@eng.sdrt.stac': 1.5183757543563843, 'eval_runtime': 13.9445, 'eval_samples_per_second': 82.111, 'eval_steps_per_second': 2.582, 'epoch': 6.0}
{'train@eng.sdrt.stac_loss': 1.5069396495819092, 'train@eng.sdrt.stac_accuracy@eng.sdrt.stac': 0.5279749478079332, 'train@eng.sdrt.stac_f1@eng.sdrt.stac': 0.3425705286272792, 'train@eng.sdrt.stac_precision@eng.sdrt.stac': 0.4122465341711409, 'train@eng.sdrt.stac_recall@eng.sdrt.stac': 0.35200277724489915, 'train@eng.sdrt.stac_loss@eng.sdrt.stac': 1.5069396495819092, 'train@eng.sdrt.stac_runtime': 112.9816, 'train@eng.sdrt.stac_samples_per_second': 84.793, 'train@eng.sdrt.stac_steps_per_second': 2.655, 'epoch': 7.0}
{'loss': 1.5649, 'learning_rate': 8.333333333333334e-06, 'epoch': 7.0}
{'eval_loss': 1.4996296167373657, 'eval_accuracy@eng.sdrt.stac': 0.5213973799126638, 'eval_f1@eng.sdrt.stac': 0.338688804399157, 'eval_precision@eng.sdrt.stac': 0.45586722362685, 'eval_recall@eng.sdrt.stac': 0.3447913845623966, 'eval_loss@eng.sdrt.stac': 1.4996294975280762, 'eval_runtime': 13.9538, 'eval_samples_per_second': 82.056, 'eval_steps_per_second': 2.58, 'epoch': 7.0}
{'train@eng.sdrt.stac_loss': 1.479895830154419, 'train@eng.sdrt.stac_accuracy@eng.sdrt.stac': 0.5384133611691023, 'train@eng.sdrt.stac_f1@eng.sdrt.stac': 0.3532884466075275, 'train@eng.sdrt.stac_precision@eng.sdrt.stac': 0.41709062551664133, 'train@eng.sdrt.stac_recall@eng.sdrt.stac': 0.3608415769295775, 'train@eng.sdrt.stac_loss@eng.sdrt.stac': 1.479895830154419, 'train@eng.sdrt.stac_runtime': 112.97, 'train@eng.sdrt.stac_samples_per_second': 84.801, 'train@eng.sdrt.stac_steps_per_second': 2.656, 'epoch': 8.0}
{'loss': 1.5411, 'learning_rate': 6.666666666666667e-06, 'epoch': 8.0}
{'eval_loss': 1.4697463512420654, 'eval_accuracy@eng.sdrt.stac': 0.5266375545851528, 'eval_f1@eng.sdrt.stac': 0.34391030363443376, 'eval_precision@eng.sdrt.stac': 0.42906456854580133, 'eval_recall@eng.sdrt.stac': 0.3486048314185436, 'eval_loss@eng.sdrt.stac': 1.469746470451355, 'eval_runtime': 13.9457, 'eval_samples_per_second': 82.104, 'eval_steps_per_second': 2.581, 'epoch': 8.0}
{'train@eng.sdrt.stac_loss': 1.4670135974884033, 'train@eng.sdrt.stac_accuracy@eng.sdrt.stac': 0.5416492693110647, 'train@eng.sdrt.stac_f1@eng.sdrt.stac': 0.3570163134563122, 'train@eng.sdrt.stac_precision@eng.sdrt.stac': 0.41309634201804757, 'train@eng.sdrt.stac_recall@eng.sdrt.stac': 0.3673136199355514, 'train@eng.sdrt.stac_loss@eng.sdrt.stac': 1.4670135974884033, 'train@eng.sdrt.stac_runtime': 113.1942, 'train@eng.sdrt.stac_samples_per_second': 84.633, 'train@eng.sdrt.stac_steps_per_second': 2.65, 'epoch': 9.0}
{'loss': 1.519, 'learning_rate': 5e-06, 'epoch': 9.0}
{'eval_loss': 1.458507776260376, 'eval_accuracy@eng.sdrt.stac': 0.537117903930131, 'eval_f1@eng.sdrt.stac': 0.3509100322922288, 'eval_precision@eng.sdrt.stac': 0.440694925639337, 'eval_recall@eng.sdrt.stac': 0.354951698371181, 'eval_loss@eng.sdrt.stac': 1.4585076570510864, 'eval_runtime': 13.968, 'eval_samples_per_second': 81.973, 'eval_steps_per_second': 2.577, 'epoch': 9.0}
{'train@eng.sdrt.stac_loss': 1.4539268016815186, 'train@eng.sdrt.stac_accuracy@eng.sdrt.stac': 0.5460334029227557, 'train@eng.sdrt.stac_f1@eng.sdrt.stac': 0.36199721877191454, 'train@eng.sdrt.stac_precision@eng.sdrt.stac': 0.41500376282965734, 'train@eng.sdrt.stac_recall@eng.sdrt.stac': 0.3731137067771306, 'train@eng.sdrt.stac_loss@eng.sdrt.stac': 1.453926682472229, 'train@eng.sdrt.stac_runtime': 112.8314, 'train@eng.sdrt.stac_samples_per_second': 84.905, 'train@eng.sdrt.stac_steps_per_second': 2.659, 'epoch': 10.0}
{'loss': 1.5076, 'learning_rate': 3.3333333333333333e-06, 'epoch': 10.0}
{'eval_loss': 1.4517706632614136, 'eval_accuracy@eng.sdrt.stac': 0.5388646288209606, 'eval_f1@eng.sdrt.stac': 0.34847082291229414, 'eval_precision@eng.sdrt.stac': 0.4150805767455872, 'eval_recall@eng.sdrt.stac': 0.35698909286720326, 'eval_loss@eng.sdrt.stac': 1.4517706632614136, 'eval_runtime': 13.9372, 'eval_samples_per_second': 82.154, 'eval_steps_per_second': 2.583, 'epoch': 10.0}
{'train@eng.sdrt.stac_loss': 1.4470795392990112, 'train@eng.sdrt.stac_accuracy@eng.sdrt.stac': 0.5490605427974948, 'train@eng.sdrt.stac_f1@eng.sdrt.stac': 0.36609328970486027, 'train@eng.sdrt.stac_precision@eng.sdrt.stac': 0.41851484666132077, 'train@eng.sdrt.stac_recall@eng.sdrt.stac': 0.376255447359625, 'train@eng.sdrt.stac_loss@eng.sdrt.stac': 1.4470795392990112, 'train@eng.sdrt.stac_runtime': 112.9967, 'train@eng.sdrt.stac_samples_per_second': 84.781, 'train@eng.sdrt.stac_steps_per_second': 2.655, 'epoch': 11.0}
{'loss': 1.4991, 'learning_rate': 1.6666666666666667e-06, 'epoch': 11.0}
{'eval_loss': 1.4475114345550537, 'eval_accuracy@eng.sdrt.stac': 0.5449781659388646, 'eval_f1@eng.sdrt.stac': 0.3535739164433546, 'eval_precision@eng.sdrt.stac': 0.4185315209728582, 'eval_recall@eng.sdrt.stac': 0.3624539943450358, 'eval_loss@eng.sdrt.stac': 1.4475113153457642, 'eval_runtime': 13.9436, 'eval_samples_per_second': 82.116, 'eval_steps_per_second': 2.582, 'epoch': 11.0}
{'train@eng.sdrt.stac_loss': 1.4447462558746338, 'train@eng.sdrt.stac_accuracy@eng.sdrt.stac': 0.5497912317327767, 'train@eng.sdrt.stac_f1@eng.sdrt.stac': 0.366747039395939, 'train@eng.sdrt.stac_precision@eng.sdrt.stac': 0.4180319895640002, 'train@eng.sdrt.stac_recall@eng.sdrt.stac': 0.3766698697420493, 'train@eng.sdrt.stac_loss@eng.sdrt.stac': 1.4447462558746338, 'train@eng.sdrt.stac_runtime': 113.0487, 'train@eng.sdrt.stac_samples_per_second': 84.742, 'train@eng.sdrt.stac_steps_per_second': 2.654, 'epoch': 12.0}
{'loss': 1.4962, 'learning_rate': 0.0, 'epoch': 12.0}
{'eval_loss': 1.4467381238937378, 'eval_accuracy@eng.sdrt.stac': 0.5423580786026201, 'eval_f1@eng.sdrt.stac': 0.35186591102265563, 'eval_precision@eng.sdrt.stac': 0.40907759018694156, 'eval_recall@eng.sdrt.stac': 0.3606474894030151, 'eval_loss@eng.sdrt.stac': 1.4467380046844482, 'eval_runtime': 13.9269, 'eval_samples_per_second': 82.215, 'eval_steps_per_second': 2.585, 'epoch': 12.0}
{'train_runtime': 4398.1311, 'train_samples_per_second': 26.138, 'train_steps_per_second': 0.819, 'train_loss': 1.6922556559244792, 'epoch': 12.0}
***** train_logger metrics *****
  epoch                    =       12.0
  train_loss               =     1.9231
  train_runtime            = 0:32:37.08
  train_samples_per_second =     25.434
  train_steps_per_second   =      0.797
-------------------------------------------------------------------
Lang1:  rus.rst.rrt    Lang2:  eng.sdrt.stac
Saving run to:  runs/full_shot/FullShot=v4_rus.rst.rrt_eng.sdrt.stac_lrfunc=Adafactor_lr=2e-05_plm=bert-base-multilingual-cased
Running with params: BERT_MODEL=bert-base-multilingual-cased lr=2e-05
read 28822 examples
read 2855 examples
read 2843 examples
read 9580 examples
read 1145 examples
read 1510 examples
Total prediction labels:  38
BertAdapterModel(
  (shared_parameters): ModuleDict()
  (bert): BertModel(
    (shared_parameters): ModuleDict()
    (invertible_adapters): ModuleDict(
      (ru): NICECouplingBlock(
        (F): Sequential(
          (0): Linear(in_features=384, out_features=192, bias=True)
          (1): Activation_Function_Class(
            (f): NewGELUActivation()
          )
          (2): Linear(in_features=192, out_features=384, bias=True)
        )
        (G): Sequential(
          (0): Linear(in_features=384, out_features=192, bias=True)
          (1): Activation_Function_Class(
            (f): NewGELUActivation()
          )
          (2): Linear(in_features=192, out_features=384, bias=True)
        )
      )
      (en): NICECouplingBlock(
        (F): Sequential(
          (0): Linear(in_features=384, out_features=192, bias=True)
          (1): Activation_Function_Class(
            (f): ReLU()
          )
          (2): Linear(in_features=192, out_features=384, bias=True)
        )
        (G): Sequential(
          (0): Linear(in_features=384, out_features=192, bias=True)
          (1): Activation_Function_Class(
            (f): ReLU()
          )
          (2): Linear(in_features=192, out_features=384, bias=True)
        )
      )
    )
    (embeddings): BertEmbeddings(
      (word_embeddings): Embedding(119547, 768, padding_idx=0)
      (position_embeddings): Embedding(512, 768)
      (token_type_embeddings): Embedding(2, 768)
      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
      (dropout): Dropout(p=0.1, inplace=False)
    )
    (encoder): BertEncoder(
      (layer): ModuleList(
        (0): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (ru): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (en): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (1): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (ru): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (en): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (2): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (ru): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (en): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (3): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (ru): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (en): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (4): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (ru): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (en): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (5): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (ru): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (en): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (6): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (ru): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (en): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (7): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (ru): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (en): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (8): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (ru): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (en): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (9): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (ru): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (en): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (10): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (ru): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (en): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (11): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (ru): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (en): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
      )
    )
    (pooler): BertPooler(
      (dense): Linear(in_features=768, out_features=768, bias=True)
      (activation): Tanh()
    )
    (prefix_tuning): PrefixTuningPool(
      (prefix_tunings): ModuleDict()
    )
  )
  (heads): ModuleDict(
    (disrpt): ClassificationHead(
      (0): Dropout(p=0.1, inplace=False)
      (1): Linear(in_features=768, out_features=768, bias=True)
      (2): Activation_Function_Class(
        (f): Tanh()
      )
      (3): Dropout(p=0.1, inplace=False)
      (4): Linear(in_features=768, out_features=38, bias=True)
    )
  )
)
{'train@rus.rst.rrt_loss': 1.7362993955612183, 'train@rus.rst.rrt_accuracy@rus.rst.rrt': 0.49437929359517035, 'train@rus.rst.rrt_f1@rus.rst.rrt': 0.17752312557867156, 'train@rus.rst.rrt_precision@rus.rst.rrt': 0.2275831552415963, 'train@rus.rst.rrt_recall@rus.rst.rrt': 0.19426083275845246, 'train@rus.rst.rrt_loss@rus.rst.rrt': 1.7362996339797974, 'train@rus.rst.rrt_runtime': 346.1776, 'train@rus.rst.rrt_samples_per_second': 83.258, 'train@rus.rst.rrt_steps_per_second': 2.603, 'epoch': 1.0}
{'loss': 2.1793, 'learning_rate': 1.8333333333333333e-05, 'epoch': 1.0}
{'eval_loss': 1.776627779006958, 'eval_accuracy@rus.rst.rrt': 0.46830122591943957, 'eval_f1@rus.rst.rrt': 0.19172436125375075, 'eval_precision@rus.rst.rrt': 0.19700240277898834, 'eval_recall@rus.rst.rrt': 0.21021794674346012, 'eval_loss@rus.rst.rrt': 1.776627779006958, 'eval_runtime': 34.6424, 'eval_samples_per_second': 82.414, 'eval_steps_per_second': 2.598, 'epoch': 1.0}
{'train@rus.rst.rrt_loss': 1.5090720653533936, 'train@rus.rst.rrt_accuracy@rus.rst.rrt': 0.5406633821386441, 'train@rus.rst.rrt_f1@rus.rst.rrt': 0.22143456395005792, 'train@rus.rst.rrt_precision@rus.rst.rrt': 0.2711571213825379, 'train@rus.rst.rrt_recall@rus.rst.rrt': 0.22868379795079938, 'train@rus.rst.rrt_loss@rus.rst.rrt': 1.5090720653533936, 'train@rus.rst.rrt_runtime': 345.7223, 'train@rus.rst.rrt_samples_per_second': 83.367, 'train@rus.rst.rrt_steps_per_second': 2.606, 'epoch': 2.0}
{'loss': 1.6547, 'learning_rate': 1.6666666666666667e-05, 'epoch': 2.0}
{'eval_loss': 1.5768781900405884, 'eval_accuracy@rus.rst.rrt': 0.5134851138353765, 'eval_f1@rus.rst.rrt': 0.2446309918310211, 'eval_precision@rus.rst.rrt': 0.3432332460628416, 'eval_recall@rus.rst.rrt': 0.25213053644934114, 'eval_loss@rus.rst.rrt': 1.5768784284591675, 'eval_runtime': 34.6074, 'eval_samples_per_second': 82.497, 'eval_steps_per_second': 2.601, 'epoch': 2.0}
{'train@rus.rst.rrt_loss': 1.4252945184707642, 'train@rus.rst.rrt_accuracy@rus.rst.rrt': 0.5648809936853793, 'train@rus.rst.rrt_f1@rus.rst.rrt': 0.27629662817550327, 'train@rus.rst.rrt_precision@rus.rst.rrt': 0.4601735542969394, 'train@rus.rst.rrt_recall@rus.rst.rrt': 0.27065331633769524, 'train@rus.rst.rrt_loss@rus.rst.rrt': 1.4252946376800537, 'train@rus.rst.rrt_runtime': 345.8456, 'train@rus.rst.rrt_samples_per_second': 83.338, 'train@rus.rst.rrt_steps_per_second': 2.605, 'epoch': 3.0}
{'loss': 1.5156, 'learning_rate': 1.5000000000000002e-05, 'epoch': 3.0}
{'eval_loss': 1.4978201389312744, 'eval_accuracy@rus.rst.rrt': 0.5320490367775832, 'eval_f1@rus.rst.rrt': 0.29313278203032256, 'eval_precision@rus.rst.rrt': 0.33335413607236325, 'eval_recall@rus.rst.rrt': 0.29342432296352966, 'eval_loss@rus.rst.rrt': 1.4978201389312744, 'eval_runtime': 34.6036, 'eval_samples_per_second': 82.506, 'eval_steps_per_second': 2.601, 'epoch': 3.0}
{'train@rus.rst.rrt_loss': 1.3683912754058838, 'train@rus.rst.rrt_accuracy@rus.rst.rrt': 0.583512594545833, 'train@rus.rst.rrt_f1@rus.rst.rrt': 0.3151190426987914, 'train@rus.rst.rrt_precision@rus.rst.rrt': 0.4661573563693855, 'train@rus.rst.rrt_recall@rus.rst.rrt': 0.29679526827877667, 'train@rus.rst.rrt_loss@rus.rst.rrt': 1.368391513824463, 'train@rus.rst.rrt_runtime': 345.9634, 'train@rus.rst.rrt_samples_per_second': 83.309, 'train@rus.rst.rrt_steps_per_second': 2.604, 'epoch': 4.0}
{'loss': 1.447, 'learning_rate': 1.3333333333333333e-05, 'epoch': 4.0}
{'eval_loss': 1.446928858757019, 'eval_accuracy@rus.rst.rrt': 0.559369527145359, 'eval_f1@rus.rst.rrt': 0.3580281007069745, 'eval_precision@rus.rst.rrt': 0.5033185737613574, 'eval_recall@rus.rst.rrt': 0.338866555840095, 'eval_loss@rus.rst.rrt': 1.446928858757019, 'eval_runtime': 34.619, 'eval_samples_per_second': 82.469, 'eval_steps_per_second': 2.6, 'epoch': 4.0}
{'train@rus.rst.rrt_loss': 1.3346960544586182, 'train@rus.rst.rrt_accuracy@rus.rst.rrt': 0.5920477413087225, 'train@rus.rst.rrt_f1@rus.rst.rrt': 0.3317800229996333, 'train@rus.rst.rrt_precision@rus.rst.rrt': 0.4456369174638631, 'train@rus.rst.rrt_recall@rus.rst.rrt': 0.31178172115749403, 'train@rus.rst.rrt_loss@rus.rst.rrt': 1.3346961736679077, 'train@rus.rst.rrt_runtime': 345.6918, 'train@rus.rst.rrt_samples_per_second': 83.375, 'train@rus.rst.rrt_steps_per_second': 2.606, 'epoch': 5.0}
{'loss': 1.4067, 'learning_rate': 1.1666666666666668e-05, 'epoch': 5.0}
{'eval_loss': 1.4206898212432861, 'eval_accuracy@rus.rst.rrt': 0.5656742556917689, 'eval_f1@rus.rst.rrt': 0.37915082262134714, 'eval_precision@rus.rst.rrt': 0.5027057812865096, 'eval_recall@rus.rst.rrt': 0.35907473337813145, 'eval_loss@rus.rst.rrt': 1.4206898212432861, 'eval_runtime': 34.5802, 'eval_samples_per_second': 82.562, 'eval_steps_per_second': 2.603, 'epoch': 5.0}
{'train@rus.rst.rrt_loss': 1.311347484588623, 'train@rus.rst.rrt_accuracy@rus.rst.rrt': 0.5981194920546804, 'train@rus.rst.rrt_f1@rus.rst.rrt': 0.34304602207489454, 'train@rus.rst.rrt_precision@rus.rst.rrt': 0.4410569761202305, 'train@rus.rst.rrt_recall@rus.rst.rrt': 0.3232761392906603, 'train@rus.rst.rrt_loss@rus.rst.rrt': 1.311347484588623, 'train@rus.rst.rrt_runtime': 345.8787, 'train@rus.rst.rrt_samples_per_second': 83.33, 'train@rus.rst.rrt_steps_per_second': 2.605, 'epoch': 6.0}
{'loss': 1.3784, 'learning_rate': 1e-05, 'epoch': 6.0}
{'eval_loss': 1.4003421068191528, 'eval_accuracy@rus.rst.rrt': 0.5695271453590193, 'eval_f1@rus.rst.rrt': 0.3875295571909681, 'eval_precision@rus.rst.rrt': 0.48544120261208756, 'eval_recall@rus.rst.rrt': 0.3698656705505598, 'eval_loss@rus.rst.rrt': 1.400342345237732, 'eval_runtime': 34.6083, 'eval_samples_per_second': 82.495, 'eval_steps_per_second': 2.601, 'epoch': 6.0}
{'train@rus.rst.rrt_loss': 1.294674038887024, 'train@rus.rst.rrt_accuracy@rus.rst.rrt': 0.6019013253764486, 'train@rus.rst.rrt_f1@rus.rst.rrt': 0.352102037371413, 'train@rus.rst.rrt_precision@rus.rst.rrt': 0.48299628493171737, 'train@rus.rst.rrt_recall@rus.rst.rrt': 0.3323071136011295, 'train@rus.rst.rrt_loss@rus.rst.rrt': 1.294674277305603, 'train@rus.rst.rrt_runtime': 345.9041, 'train@rus.rst.rrt_samples_per_second': 83.324, 'train@rus.rst.rrt_steps_per_second': 2.605, 'epoch': 7.0}
{'loss': 1.3563, 'learning_rate': 8.333333333333334e-06, 'epoch': 7.0}
{'eval_loss': 1.3910777568817139, 'eval_accuracy@rus.rst.rrt': 0.5712784588441331, 'eval_f1@rus.rst.rrt': 0.39564780123604154, 'eval_precision@rus.rst.rrt': 0.4826527328049463, 'eval_recall@rus.rst.rrt': 0.3797882845197158, 'eval_loss@rus.rst.rrt': 1.3910777568817139, 'eval_runtime': 34.6054, 'eval_samples_per_second': 82.501, 'eval_steps_per_second': 2.601, 'epoch': 7.0}
{'train@rus.rst.rrt_loss': 1.2823179960250854, 'train@rus.rst.rrt_accuracy@rus.rst.rrt': 0.60478107001596, 'train@rus.rst.rrt_f1@rus.rst.rrt': 0.3615041248819351, 'train@rus.rst.rrt_precision@rus.rst.rrt': 0.4517676682564105, 'train@rus.rst.rrt_recall@rus.rst.rrt': 0.34405670748348094, 'train@rus.rst.rrt_loss@rus.rst.rrt': 1.282317876815796, 'train@rus.rst.rrt_runtime': 345.5416, 'train@rus.rst.rrt_samples_per_second': 83.411, 'train@rus.rst.rrt_steps_per_second': 2.608, 'epoch': 8.0}
{'loss': 1.3398, 'learning_rate': 6.666666666666667e-06, 'epoch': 8.0}
{'eval_loss': 1.3775498867034912, 'eval_accuracy@rus.rst.rrt': 0.578984238178634, 'eval_f1@rus.rst.rrt': 0.4049506309863308, 'eval_precision@rus.rst.rrt': 0.47999028960706697, 'eval_recall@rus.rst.rrt': 0.39314882282273345, 'eval_loss@rus.rst.rrt': 1.3775498867034912, 'eval_runtime': 34.5991, 'eval_samples_per_second': 82.517, 'eval_steps_per_second': 2.601, 'epoch': 8.0}
{'train@rus.rst.rrt_loss': 1.269708275794983, 'train@rus.rst.rrt_accuracy@rus.rst.rrt': 0.6071056831586982, 'train@rus.rst.rrt_f1@rus.rst.rrt': 0.36395824605043664, 'train@rus.rst.rrt_precision@rus.rst.rrt': 0.45614965142588504, 'train@rus.rst.rrt_recall@rus.rst.rrt': 0.3440377943907782, 'train@rus.rst.rrt_loss@rus.rst.rrt': 1.269708275794983, 'train@rus.rst.rrt_runtime': 347.3391, 'train@rus.rst.rrt_samples_per_second': 82.979, 'train@rus.rst.rrt_steps_per_second': 2.594, 'epoch': 9.0}
{'loss': 1.3299, 'learning_rate': 5e-06, 'epoch': 9.0}
{'eval_loss': 1.3707753419876099, 'eval_accuracy@rus.rst.rrt': 0.5754816112084064, 'eval_f1@rus.rst.rrt': 0.41035513560130554, 'eval_precision@rus.rst.rrt': 0.4949316767218689, 'eval_recall@rus.rst.rrt': 0.3941015975284834, 'eval_loss@rus.rst.rrt': 1.3707753419876099, 'eval_runtime': 34.6154, 'eval_samples_per_second': 82.478, 'eval_steps_per_second': 2.6, 'epoch': 9.0}
{'train@rus.rst.rrt_loss': 1.2634788751602173, 'train@rus.rst.rrt_accuracy@rus.rst.rrt': 0.6102629935465963, 'train@rus.rst.rrt_f1@rus.rst.rrt': 0.36723360924056353, 'train@rus.rst.rrt_precision@rus.rst.rrt': 0.461164340560479, 'train@rus.rst.rrt_recall@rus.rst.rrt': 0.3446579407795254, 'train@rus.rst.rrt_loss@rus.rst.rrt': 1.2634788751602173, 'train@rus.rst.rrt_runtime': 347.4704, 'train@rus.rst.rrt_samples_per_second': 82.948, 'train@rus.rst.rrt_steps_per_second': 2.593, 'epoch': 10.0}
{'loss': 1.3208, 'learning_rate': 3.3333333333333333e-06, 'epoch': 10.0}
{'eval_loss': 1.367979645729065, 'eval_accuracy@rus.rst.rrt': 0.5800350262697023, 'eval_f1@rus.rst.rrt': 0.41258610038442795, 'eval_precision@rus.rst.rrt': 0.5019240421860552, 'eval_recall@rus.rst.rrt': 0.3945941575210858, 'eval_loss@rus.rst.rrt': 1.3679797649383545, 'eval_runtime': 34.7205, 'eval_samples_per_second': 82.228, 'eval_steps_per_second': 2.592, 'epoch': 10.0}
{'train@rus.rst.rrt_loss': 1.259871244430542, 'train@rus.rst.rrt_accuracy@rus.rst.rrt': 0.611060995073208, 'train@rus.rst.rrt_f1@rus.rst.rrt': 0.37045092259339363, 'train@rus.rst.rrt_precision@rus.rst.rrt': 0.46204027177886536, 'train@rus.rst.rrt_recall@rus.rst.rrt': 0.34997262630624043, 'train@rus.rst.rrt_loss@rus.rst.rrt': 1.2598713636398315, 'train@rus.rst.rrt_runtime': 347.5381, 'train@rus.rst.rrt_samples_per_second': 82.932, 'train@rus.rst.rrt_steps_per_second': 2.593, 'epoch': 11.0}
{'loss': 1.3134, 'learning_rate': 1.6666666666666667e-06, 'epoch': 11.0}
{'eval_loss': 1.3675639629364014, 'eval_accuracy@rus.rst.rrt': 0.5768826619964974, 'eval_f1@rus.rst.rrt': 0.41838500258089406, 'eval_precision@rus.rst.rrt': 0.5216172414818039, 'eval_recall@rus.rst.rrt': 0.3998502998848112, 'eval_loss@rus.rst.rrt': 1.3675639629364014, 'eval_runtime': 34.7767, 'eval_samples_per_second': 82.095, 'eval_steps_per_second': 2.588, 'epoch': 11.0}
{'train@rus.rst.rrt_loss': 1.258286952972412, 'train@rus.rst.rrt_accuracy@rus.rst.rrt': 0.6114426479772396, 'train@rus.rst.rrt_f1@rus.rst.rrt': 0.3702287810900728, 'train@rus.rst.rrt_precision@rus.rst.rrt': 0.4664957519721917, 'train@rus.rst.rrt_recall@rus.rst.rrt': 0.3498702991499889, 'train@rus.rst.rrt_loss@rus.rst.rrt': 1.258286952972412, 'train@rus.rst.rrt_runtime': 347.2292, 'train@rus.rst.rrt_samples_per_second': 83.006, 'train@rus.rst.rrt_steps_per_second': 2.595, 'epoch': 12.0}
{'loss': 1.3104, 'learning_rate': 0.0, 'epoch': 12.0}
{'eval_loss': 1.36573326587677, 'eval_accuracy@rus.rst.rrt': 0.5810858143607706, 'eval_f1@rus.rst.rrt': 0.4196723257418085, 'eval_precision@rus.rst.rrt': 0.5249356057903154, 'eval_recall@rus.rst.rrt': 0.40129905011358347, 'eval_loss@rus.rst.rrt': 1.3657331466674805, 'eval_runtime': 34.669, 'eval_samples_per_second': 82.35, 'eval_steps_per_second': 2.596, 'epoch': 12.0}
{'train_runtime': 13340.9444, 'train_samples_per_second': 25.925, 'train_steps_per_second': 0.81, 'train_loss': 1.4626812809624143, 'epoch': 12.0}
***** train_log metrics *****
  epoch                    =       12.0
  train_loss               =     1.4627
  train_runtime            = 3:42:20.94
  train_samples_per_second =     25.925
  train_steps_per_second   =       0.81
{'train@eng.sdrt.stac_loss': 1.886257529258728, 'train@eng.sdrt.stac_accuracy@eng.sdrt.stac': 0.39258872651356996, 'train@eng.sdrt.stac_f1@eng.sdrt.stac': 0.1282764022724316, 'train@eng.sdrt.stac_precision@eng.sdrt.stac': 0.16674977487100567, 'train@eng.sdrt.stac_recall@eng.sdrt.stac': 0.14859437831413802, 'train@eng.sdrt.stac_loss@eng.sdrt.stac': 1.8862577676773071, 'train@eng.sdrt.stac_runtime': 113.341, 'train@eng.sdrt.stac_samples_per_second': 84.524, 'train@eng.sdrt.stac_steps_per_second': 2.647, 'epoch': 1.0}
{'loss': 2.2777, 'learning_rate': 1.8333333333333333e-05, 'epoch': 1.0}
{'eval_loss': 1.819505214691162, 'eval_accuracy@eng.sdrt.stac': 0.41397379912663756, 'eval_f1@eng.sdrt.stac': 0.13628505706099778, 'eval_precision@eng.sdrt.stac': 0.1657111338106066, 'eval_recall@eng.sdrt.stac': 0.15423195538197418, 'eval_loss@eng.sdrt.stac': 1.819505214691162, 'eval_runtime': 13.8688, 'eval_samples_per_second': 82.56, 'eval_steps_per_second': 2.596, 'epoch': 1.0}
{'train@eng.sdrt.stac_loss': 1.6827210187911987, 'train@eng.sdrt.stac_accuracy@eng.sdrt.stac': 0.46962421711899793, 'train@eng.sdrt.stac_f1@eng.sdrt.stac': 0.22713075947168898, 'train@eng.sdrt.stac_precision@eng.sdrt.stac': 0.25238596228616816, 'train@eng.sdrt.stac_recall@eng.sdrt.stac': 0.2573037262712188, 'train@eng.sdrt.stac_loss@eng.sdrt.stac': 1.6827210187911987, 'train@eng.sdrt.stac_runtime': 113.2414, 'train@eng.sdrt.stac_samples_per_second': 84.598, 'train@eng.sdrt.stac_steps_per_second': 2.649, 'epoch': 2.0}
{'loss': 1.8172, 'learning_rate': 1.6666666666666667e-05, 'epoch': 2.0}
{'eval_loss': 1.6299787759780884, 'eval_accuracy@eng.sdrt.stac': 0.46899563318777293, 'eval_f1@eng.sdrt.stac': 0.20810769203983293, 'eval_precision@eng.sdrt.stac': 0.2162057060043762, 'eval_recall@eng.sdrt.stac': 0.23810031778362672, 'eval_loss@eng.sdrt.stac': 1.629978895187378, 'eval_runtime': 13.8705, 'eval_samples_per_second': 82.549, 'eval_steps_per_second': 2.595, 'epoch': 2.0}
{'train@eng.sdrt.stac_loss': 1.594761610031128, 'train@eng.sdrt.stac_accuracy@eng.sdrt.stac': 0.48434237995824636, 'train@eng.sdrt.stac_f1@eng.sdrt.stac': 0.24948048843215354, 'train@eng.sdrt.stac_precision@eng.sdrt.stac': 0.3537360064556746, 'train@eng.sdrt.stac_recall@eng.sdrt.stac': 0.2777893500860294, 'train@eng.sdrt.stac_loss@eng.sdrt.stac': 1.594761610031128, 'train@eng.sdrt.stac_runtime': 113.2167, 'train@eng.sdrt.stac_samples_per_second': 84.617, 'train@eng.sdrt.stac_steps_per_second': 2.65, 'epoch': 3.0}
{'loss': 1.6818, 'learning_rate': 1.5000000000000002e-05, 'epoch': 3.0}
{'eval_loss': 1.5644004344940186, 'eval_accuracy@eng.sdrt.stac': 0.47685589519650656, 'eval_f1@eng.sdrt.stac': 0.2170131746146434, 'eval_precision@eng.sdrt.stac': 0.23763089173100385, 'eval_recall@eng.sdrt.stac': 0.25301415472809574, 'eval_loss@eng.sdrt.stac': 1.564400553703308, 'eval_runtime': 13.86, 'eval_samples_per_second': 82.612, 'eval_steps_per_second': 2.597, 'epoch': 3.0}
{'train@eng.sdrt.stac_loss': 1.5319586992263794, 'train@eng.sdrt.stac_accuracy@eng.sdrt.stac': 0.5144050104384134, 'train@eng.sdrt.stac_f1@eng.sdrt.stac': 0.32147752424746484, 'train@eng.sdrt.stac_precision@eng.sdrt.stac': 0.4166569653041038, 'train@eng.sdrt.stac_recall@eng.sdrt.stac': 0.3313183777912381, 'train@eng.sdrt.stac_loss@eng.sdrt.stac': 1.531958818435669, 'train@eng.sdrt.stac_runtime': 113.1484, 'train@eng.sdrt.stac_samples_per_second': 84.668, 'train@eng.sdrt.stac_steps_per_second': 2.651, 'epoch': 4.0}
{'loss': 1.613, 'learning_rate': 1.3333333333333333e-05, 'epoch': 4.0}
{'eval_loss': 1.5060237646102905, 'eval_accuracy@eng.sdrt.stac': 0.5179039301310043, 'eval_f1@eng.sdrt.stac': 0.3148459394224453, 'eval_precision@eng.sdrt.stac': 0.36346174916556095, 'eval_recall@eng.sdrt.stac': 0.32201589261369223, 'eval_loss@eng.sdrt.stac': 1.5060240030288696, 'eval_runtime': 13.8806, 'eval_samples_per_second': 82.489, 'eval_steps_per_second': 2.594, 'epoch': 4.0}
{'train@eng.sdrt.stac_loss': 1.4987772703170776, 'train@eng.sdrt.stac_accuracy@eng.sdrt.stac': 0.5313152400835073, 'train@eng.sdrt.stac_f1@eng.sdrt.stac': 0.3444418925124961, 'train@eng.sdrt.stac_precision@eng.sdrt.stac': 0.42189139285292154, 'train@eng.sdrt.stac_recall@eng.sdrt.stac': 0.3538064397489333, 'train@eng.sdrt.stac_loss@eng.sdrt.stac': 1.498777151107788, 'train@eng.sdrt.stac_runtime': 113.1431, 'train@eng.sdrt.stac_samples_per_second': 84.672, 'train@eng.sdrt.stac_steps_per_second': 2.652, 'epoch': 5.0}
{'loss': 1.5657, 'learning_rate': 1.1666666666666668e-05, 'epoch': 5.0}
{'eval_loss': 1.4787346124649048, 'eval_accuracy@eng.sdrt.stac': 0.5222707423580786, 'eval_f1@eng.sdrt.stac': 0.33216206691860484, 'eval_precision@eng.sdrt.stac': 0.41387091064301884, 'eval_recall@eng.sdrt.stac': 0.34010456102834274, 'eval_loss@eng.sdrt.stac': 1.4787346124649048, 'eval_runtime': 13.8498, 'eval_samples_per_second': 82.672, 'eval_steps_per_second': 2.599, 'epoch': 5.0}
{'train@eng.sdrt.stac_loss': 1.4558746814727783, 'train@eng.sdrt.stac_accuracy@eng.sdrt.stac': 0.5416492693110647, 'train@eng.sdrt.stac_f1@eng.sdrt.stac': 0.35908355833222305, 'train@eng.sdrt.stac_precision@eng.sdrt.stac': 0.42831822115731744, 'train@eng.sdrt.stac_recall@eng.sdrt.stac': 0.3657926799046848, 'train@eng.sdrt.stac_loss@eng.sdrt.stac': 1.4558746814727783, 'train@eng.sdrt.stac_runtime': 113.3742, 'train@eng.sdrt.stac_samples_per_second': 84.499, 'train@eng.sdrt.stac_steps_per_second': 2.646, 'epoch': 6.0}
{'loss': 1.5266, 'learning_rate': 1e-05, 'epoch': 6.0}
{'eval_loss': 1.459214448928833, 'eval_accuracy@eng.sdrt.stac': 0.5327510917030568, 'eval_f1@eng.sdrt.stac': 0.3467685787021548, 'eval_precision@eng.sdrt.stac': 0.437221445384553, 'eval_recall@eng.sdrt.stac': 0.3533892737283696, 'eval_loss@eng.sdrt.stac': 1.459214448928833, 'eval_runtime': 13.8915, 'eval_samples_per_second': 82.424, 'eval_steps_per_second': 2.592, 'epoch': 6.0}
{'train@eng.sdrt.stac_loss': 1.4512524604797363, 'train@eng.sdrt.stac_accuracy@eng.sdrt.stac': 0.5481210855949895, 'train@eng.sdrt.stac_f1@eng.sdrt.stac': 0.36919107930921313, 'train@eng.sdrt.stac_precision@eng.sdrt.stac': 0.4211527427296057, 'train@eng.sdrt.stac_recall@eng.sdrt.stac': 0.37700547494127756, 'train@eng.sdrt.stac_loss@eng.sdrt.stac': 1.4512524604797363, 'train@eng.sdrt.stac_runtime': 113.0281, 'train@eng.sdrt.stac_samples_per_second': 84.758, 'train@eng.sdrt.stac_steps_per_second': 2.654, 'epoch': 7.0}
{'loss': 1.5017, 'learning_rate': 8.333333333333334e-06, 'epoch': 7.0}
{'eval_loss': 1.4524790048599243, 'eval_accuracy@eng.sdrt.stac': 0.5388646288209606, 'eval_f1@eng.sdrt.stac': 0.36568417108893514, 'eval_precision@eng.sdrt.stac': 0.4273537585916165, 'eval_recall@eng.sdrt.stac': 0.3673061945991676, 'eval_loss@eng.sdrt.stac': 1.4524790048599243, 'eval_runtime': 15.4146, 'eval_samples_per_second': 74.28, 'eval_steps_per_second': 2.335, 'epoch': 7.0}
{'train@eng.sdrt.stac_loss': 1.4233362674713135, 'train@eng.sdrt.stac_accuracy@eng.sdrt.stac': 0.5576200417536534, 'train@eng.sdrt.stac_f1@eng.sdrt.stac': 0.3848206603225812, 'train@eng.sdrt.stac_precision@eng.sdrt.stac': 0.42293898877700487, 'train@eng.sdrt.stac_recall@eng.sdrt.stac': 0.3907864845010094, 'train@eng.sdrt.stac_loss@eng.sdrt.stac': 1.4233362674713135, 'train@eng.sdrt.stac_runtime': 112.9205, 'train@eng.sdrt.stac_samples_per_second': 84.838, 'train@eng.sdrt.stac_steps_per_second': 2.657, 'epoch': 8.0}
{'loss': 1.4769, 'learning_rate': 6.666666666666667e-06, 'epoch': 8.0}
{'eval_loss': 1.4280798435211182, 'eval_accuracy@eng.sdrt.stac': 0.5467248908296943, 'eval_f1@eng.sdrt.stac': 0.37602894495062655, 'eval_precision@eng.sdrt.stac': 0.4186932384320914, 'eval_recall@eng.sdrt.stac': 0.3773793294180994, 'eval_loss@eng.sdrt.stac': 1.4280799627304077, 'eval_runtime': 13.8869, 'eval_samples_per_second': 82.452, 'eval_steps_per_second': 2.592, 'epoch': 8.0}
{'train@eng.sdrt.stac_loss': 1.4104493856430054, 'train@eng.sdrt.stac_accuracy@eng.sdrt.stac': 0.561482254697286, 'train@eng.sdrt.stac_f1@eng.sdrt.stac': 0.39227184350037914, 'train@eng.sdrt.stac_precision@eng.sdrt.stac': 0.4253103155851217, 'train@eng.sdrt.stac_recall@eng.sdrt.stac': 0.40002067665809515, 'train@eng.sdrt.stac_loss@eng.sdrt.stac': 1.4104493856430054, 'train@eng.sdrt.stac_runtime': 112.795, 'train@eng.sdrt.stac_samples_per_second': 84.933, 'train@eng.sdrt.stac_steps_per_second': 2.66, 'epoch': 9.0}
{'loss': 1.4611, 'learning_rate': 5e-06, 'epoch': 9.0}
{'eval_loss': 1.4157501459121704, 'eval_accuracy@eng.sdrt.stac': 0.5502183406113537, 'eval_f1@eng.sdrt.stac': 0.3812774565637458, 'eval_precision@eng.sdrt.stac': 0.4118913075458283, 'eval_recall@eng.sdrt.stac': 0.3830450778879208, 'eval_loss@eng.sdrt.stac': 1.4157500267028809, 'eval_runtime': 13.8397, 'eval_samples_per_second': 82.733, 'eval_steps_per_second': 2.601, 'epoch': 9.0}
{'train@eng.sdrt.stac_loss': 1.3974508047103882, 'train@eng.sdrt.stac_accuracy@eng.sdrt.stac': 0.5644050104384134, 'train@eng.sdrt.stac_f1@eng.sdrt.stac': 0.3987302694227698, 'train@eng.sdrt.stac_precision@eng.sdrt.stac': 0.42655001159731853, 'train@eng.sdrt.stac_recall@eng.sdrt.stac': 0.4052868113684026, 'train@eng.sdrt.stac_loss@eng.sdrt.stac': 1.3974506855010986, 'train@eng.sdrt.stac_runtime': 112.958, 'train@eng.sdrt.stac_samples_per_second': 84.81, 'train@eng.sdrt.stac_steps_per_second': 2.656, 'epoch': 10.0}
{'loss': 1.4496, 'learning_rate': 3.3333333333333333e-06, 'epoch': 10.0}
{'eval_loss': 1.4119495153427124, 'eval_accuracy@eng.sdrt.stac': 0.5475982532751091, 'eval_f1@eng.sdrt.stac': 0.3828929560754898, 'eval_precision@eng.sdrt.stac': 0.40598581943618667, 'eval_recall@eng.sdrt.stac': 0.38915071784044625, 'eval_loss@eng.sdrt.stac': 1.4119495153427124, 'eval_runtime': 13.8674, 'eval_samples_per_second': 82.568, 'eval_steps_per_second': 2.596, 'epoch': 10.0}
{'train@eng.sdrt.stac_loss': 1.3919943571090698, 'train@eng.sdrt.stac_accuracy@eng.sdrt.stac': 0.5648225469728602, 'train@eng.sdrt.stac_f1@eng.sdrt.stac': 0.39993457561874124, 'train@eng.sdrt.stac_precision@eng.sdrt.stac': 0.42603853564060545, 'train@eng.sdrt.stac_recall@eng.sdrt.stac': 0.40710633080747305, 'train@eng.sdrt.stac_loss@eng.sdrt.stac': 1.3919944763183594, 'train@eng.sdrt.stac_runtime': 113.0494, 'train@eng.sdrt.stac_samples_per_second': 84.742, 'train@eng.sdrt.stac_steps_per_second': 2.654, 'epoch': 11.0}
{'loss': 1.4514, 'learning_rate': 1.6666666666666667e-06, 'epoch': 11.0}
{'eval_loss': 1.4094431400299072, 'eval_accuracy@eng.sdrt.stac': 0.5537117903930131, 'eval_f1@eng.sdrt.stac': 0.3875522296399402, 'eval_precision@eng.sdrt.stac': 0.4069166412458499, 'eval_recall@eng.sdrt.stac': 0.3938755113696366, 'eval_loss@eng.sdrt.stac': 1.4094431400299072, 'eval_runtime': 13.8855, 'eval_samples_per_second': 82.46, 'eval_steps_per_second': 2.593, 'epoch': 11.0}
{'train@eng.sdrt.stac_loss': 1.3911387920379639, 'train@eng.sdrt.stac_accuracy@eng.sdrt.stac': 0.5659707724425888, 'train@eng.sdrt.stac_f1@eng.sdrt.stac': 0.4003125355754201, 'train@eng.sdrt.stac_precision@eng.sdrt.stac': 0.42438068852521904, 'train@eng.sdrt.stac_recall@eng.sdrt.stac': 0.4079494104530113, 'train@eng.sdrt.stac_loss@eng.sdrt.stac': 1.3911389112472534, 'train@eng.sdrt.stac_runtime': 113.1279, 'train@eng.sdrt.stac_samples_per_second': 84.683, 'train@eng.sdrt.stac_steps_per_second': 2.652, 'epoch': 12.0}
{'loss': 1.4433, 'learning_rate': 0.0, 'epoch': 12.0}
{'eval_loss': 1.4093413352966309, 'eval_accuracy@eng.sdrt.stac': 0.5493449781659389, 'eval_f1@eng.sdrt.stac': 0.3835807760619563, 'eval_precision@eng.sdrt.stac': 0.40232812444819077, 'eval_recall@eng.sdrt.stac': 0.390371331859028, 'eval_loss@eng.sdrt.stac': 1.4093414545059204, 'eval_runtime': 13.8784, 'eval_samples_per_second': 82.502, 'eval_steps_per_second': 2.594, 'epoch': 12.0}
{'train_runtime': 4390.2565, 'train_samples_per_second': 26.185, 'train_steps_per_second': 0.82, 'train_loss': 1.6054736243353949, 'epoch': 12.0}
***** train_logger metrics *****
  epoch                    =       12.0
  train_loss               =     1.4627
  train_runtime            = 3:42:20.94
  train_samples_per_second =     25.925
  train_steps_per_second   =       0.81
-------------------------------------------------------------------
Lang1:  spa.rst.rststb    Lang2:  eng.sdrt.stac
Saving run to:  runs/full_shot/FullShot=v4_spa.rst.rststb_eng.sdrt.stac_lrfunc=Adafactor_lr=2e-05_plm=bert-base-multilingual-cased
Running with params: BERT_MODEL=bert-base-multilingual-cased lr=2e-05
read 2240 examples
read 383 examples
read 426 examples
read 9580 examples
read 1145 examples
read 1510 examples
Total prediction labels:  45
BertAdapterModel(
  (shared_parameters): ModuleDict()
  (bert): BertModel(
    (shared_parameters): ModuleDict()
    (invertible_adapters): ModuleDict(
      (es): NICECouplingBlock(
        (F): Sequential(
          (0): Linear(in_features=384, out_features=192, bias=True)
          (1): Activation_Function_Class(
            (f): NewGELUActivation()
          )
          (2): Linear(in_features=192, out_features=384, bias=True)
        )
        (G): Sequential(
          (0): Linear(in_features=384, out_features=192, bias=True)
          (1): Activation_Function_Class(
            (f): NewGELUActivation()
          )
          (2): Linear(in_features=192, out_features=384, bias=True)
        )
      )
      (en): NICECouplingBlock(
        (F): Sequential(
          (0): Linear(in_features=384, out_features=192, bias=True)
          (1): Activation_Function_Class(
            (f): ReLU()
          )
          (2): Linear(in_features=192, out_features=384, bias=True)
        )
        (G): Sequential(
          (0): Linear(in_features=384, out_features=192, bias=True)
          (1): Activation_Function_Class(
            (f): ReLU()
          )
          (2): Linear(in_features=192, out_features=384, bias=True)
        )
      )
    )
    (embeddings): BertEmbeddings(
      (word_embeddings): Embedding(119547, 768, padding_idx=0)
      (position_embeddings): Embedding(512, 768)
      (token_type_embeddings): Embedding(2, 768)
      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
      (dropout): Dropout(p=0.1, inplace=False)
    )
    (encoder): BertEncoder(
      (layer): ModuleList(
        (0): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (es): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (en): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (1): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (es): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (en): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (2): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (es): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (en): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (3): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (es): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (en): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (4): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (es): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (en): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (5): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (es): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (en): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (6): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (es): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (en): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (7): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (es): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (en): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (8): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (es): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (en): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (9): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (es): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (en): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (10): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (es): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (en): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (11): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (es): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (en): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
      )
    )
    (pooler): BertPooler(
      (dense): Linear(in_features=768, out_features=768, bias=True)
      (activation): Tanh()
    )
    (prefix_tuning): PrefixTuningPool(
      (prefix_tunings): ModuleDict()
    )
  )
  (heads): ModuleDict(
    (disrpt): ClassificationHead(
      (0): Dropout(p=0.1, inplace=False)
      (1): Linear(in_features=768, out_features=768, bias=True)
      (2): Activation_Function_Class(
        (f): Tanh()
      )
      (3): Dropout(p=0.1, inplace=False)
      (4): Linear(in_features=768, out_features=45, bias=True)
    )
  )
)
{'train@spa.rst.rststb_loss': 2.891707181930542, 'train@spa.rst.rststb_accuracy@spa.rst.rststb': 0.23482142857142857, 'train@spa.rst.rststb_f1@spa.rst.rststb': 0.031082275059012805, 'train@spa.rst.rststb_precision@spa.rst.rststb': 0.04524471009443574, 'train@spa.rst.rststb_recall@spa.rst.rststb': 0.04585705519591222, 'train@spa.rst.rststb_loss@spa.rst.rststb': 2.891707181930542, 'train@spa.rst.rststb_runtime': 27.0878, 'train@spa.rst.rststb_samples_per_second': 82.694, 'train@spa.rst.rststb_steps_per_second': 2.584, 'epoch': 1.0}
{'loss': 3.3908, 'learning_rate': 1.8333333333333333e-05, 'epoch': 1.0}
{'eval_loss': 2.945507526397705, 'eval_accuracy@spa.rst.rststb': 0.20365535248041775, 'eval_f1@spa.rst.rststb': 0.020150801356972465, 'eval_precision@spa.rst.rststb': 0.014686081315902, 'eval_recall@spa.rst.rststb': 0.04361790387632558, 'eval_loss@spa.rst.rststb': 2.9455082416534424, 'eval_runtime': 4.9762, 'eval_samples_per_second': 76.967, 'eval_steps_per_second': 2.411, 'epoch': 1.0}
{'train@spa.rst.rststb_loss': 2.6047139167785645, 'train@spa.rst.rststb_accuracy@spa.rst.rststb': 0.2638392857142857, 'train@spa.rst.rststb_f1@spa.rst.rststb': 0.03759988131678186, 'train@spa.rst.rststb_precision@spa.rst.rststb': 0.04764592198446254, 'train@spa.rst.rststb_recall@spa.rst.rststb': 0.05289908151733884, 'train@spa.rst.rststb_loss@spa.rst.rststb': 2.6047136783599854, 'train@spa.rst.rststb_runtime': 27.1514, 'train@spa.rst.rststb_samples_per_second': 82.5, 'train@spa.rst.rststb_steps_per_second': 2.578, 'epoch': 2.0}
{'loss': 2.7499, 'learning_rate': 1.6666666666666667e-05, 'epoch': 2.0}
{'eval_loss': 2.7296717166900635, 'eval_accuracy@spa.rst.rststb': 0.22715404699738903, 'eval_f1@spa.rst.rststb': 0.02884295453311416, 'eval_precision@spa.rst.rststb': 0.05745341614906832, 'eval_recall@spa.rst.rststb': 0.05080354469210957, 'eval_loss@spa.rst.rststb': 2.7296717166900635, 'eval_runtime': 5.0118, 'eval_samples_per_second': 76.42, 'eval_steps_per_second': 2.394, 'epoch': 2.0}
{'train@spa.rst.rststb_loss': 2.485764741897583, 'train@spa.rst.rststb_accuracy@spa.rst.rststb': 0.31160714285714286, 'train@spa.rst.rststb_f1@spa.rst.rststb': 0.04670958027008617, 'train@spa.rst.rststb_precision@spa.rst.rststb': 0.039368643319040234, 'train@spa.rst.rststb_recall@spa.rst.rststb': 0.06661020298010471, 'train@spa.rst.rststb_loss@spa.rst.rststb': 2.485764741897583, 'train@spa.rst.rststb_runtime': 27.1772, 'train@spa.rst.rststb_samples_per_second': 82.422, 'train@spa.rst.rststb_steps_per_second': 2.576, 'epoch': 3.0}
{'loss': 2.5829, 'learning_rate': 1.5000000000000002e-05, 'epoch': 3.0}
{'eval_loss': 2.638126850128174, 'eval_accuracy@spa.rst.rststb': 0.26631853785900783, 'eval_f1@spa.rst.rststb': 0.04527384948068793, 'eval_precision@spa.rst.rststb': 0.04124860646599777, 'eval_recall@spa.rst.rststb': 0.06645774977616048, 'eval_loss@spa.rst.rststb': 2.6381266117095947, 'eval_runtime': 4.9922, 'eval_samples_per_second': 76.719, 'eval_steps_per_second': 2.404, 'epoch': 3.0}
{'train@spa.rst.rststb_loss': 2.389850616455078, 'train@spa.rst.rststb_accuracy@spa.rst.rststb': 0.31651785714285713, 'train@spa.rst.rststb_f1@spa.rst.rststb': 0.049981081258218575, 'train@spa.rst.rststb_precision@spa.rst.rststb': 0.06491059420552965, 'train@spa.rst.rststb_recall@spa.rst.rststb': 0.06862420683354695, 'train@spa.rst.rststb_loss@spa.rst.rststb': 2.389850616455078, 'train@spa.rst.rststb_runtime': 27.1872, 'train@spa.rst.rststb_samples_per_second': 82.392, 'train@spa.rst.rststb_steps_per_second': 2.575, 'epoch': 4.0}
{'loss': 2.484, 'learning_rate': 1.3333333333333333e-05, 'epoch': 4.0}
{'eval_loss': 2.5672783851623535, 'eval_accuracy@spa.rst.rststb': 0.2793733681462141, 'eval_f1@spa.rst.rststb': 0.05139700565955331, 'eval_precision@spa.rst.rststb': 0.08838690172514785, 'eval_recall@spa.rst.rststb': 0.07118611639143267, 'eval_loss@spa.rst.rststb': 2.5672783851623535, 'eval_runtime': 4.9907, 'eval_samples_per_second': 76.743, 'eval_steps_per_second': 2.404, 'epoch': 4.0}
{'train@spa.rst.rststb_loss': 2.3042469024658203, 'train@spa.rst.rststb_accuracy@spa.rst.rststb': 0.35, 'train@spa.rst.rststb_f1@spa.rst.rststb': 0.07412552984423924, 'train@spa.rst.rststb_precision@spa.rst.rststb': 0.08659558088168777, 'train@spa.rst.rststb_recall@spa.rst.rststb': 0.08818717238090215, 'train@spa.rst.rststb_loss@spa.rst.rststb': 2.3042471408843994, 'train@spa.rst.rststb_runtime': 27.1506, 'train@spa.rst.rststb_samples_per_second': 82.503, 'train@spa.rst.rststb_steps_per_second': 2.578, 'epoch': 5.0}
{'loss': 2.395, 'learning_rate': 1.1666666666666668e-05, 'epoch': 5.0}
{'eval_loss': 2.5060980319976807, 'eval_accuracy@spa.rst.rststb': 0.3185378590078329, 'eval_f1@spa.rst.rststb': 0.07710059761756648, 'eval_precision@spa.rst.rststb': 0.10929787373144023, 'eval_recall@spa.rst.rststb': 0.09223221906606031, 'eval_loss@spa.rst.rststb': 2.506098508834839, 'eval_runtime': 4.9986, 'eval_samples_per_second': 76.621, 'eval_steps_per_second': 2.401, 'epoch': 5.0}
{'train@spa.rst.rststb_loss': 2.2343738079071045, 'train@spa.rst.rststb_accuracy@spa.rst.rststb': 0.3808035714285714, 'train@spa.rst.rststb_f1@spa.rst.rststb': 0.09006817510751633, 'train@spa.rst.rststb_precision@spa.rst.rststb': 0.10551674382641552, 'train@spa.rst.rststb_recall@spa.rst.rststb': 0.10951401716984235, 'train@spa.rst.rststb_loss@spa.rst.rststb': 2.2343738079071045, 'train@spa.rst.rststb_runtime': 27.1846, 'train@spa.rst.rststb_samples_per_second': 82.4, 'train@spa.rst.rststb_steps_per_second': 2.575, 'epoch': 6.0}
{'loss': 2.3205, 'learning_rate': 1e-05, 'epoch': 6.0}
{'eval_loss': 2.4537389278411865, 'eval_accuracy@spa.rst.rststb': 0.3577023498694517, 'eval_f1@spa.rst.rststb': 0.10302347771659699, 'eval_precision@spa.rst.rststb': 0.11277124619396359, 'eval_recall@spa.rst.rststb': 0.12136872175869538, 'eval_loss@spa.rst.rststb': 2.4537389278411865, 'eval_runtime': 4.9706, 'eval_samples_per_second': 77.054, 'eval_steps_per_second': 2.414, 'epoch': 6.0}
{'train@spa.rst.rststb_loss': 2.1770036220550537, 'train@spa.rst.rststb_accuracy@spa.rst.rststb': 0.396875, 'train@spa.rst.rststb_f1@spa.rst.rststb': 0.09586304860178363, 'train@spa.rst.rststb_precision@spa.rst.rststb': 0.11085418337933173, 'train@spa.rst.rststb_recall@spa.rst.rststb': 0.11817116342251273, 'train@spa.rst.rststb_loss@spa.rst.rststb': 2.177003860473633, 'train@spa.rst.rststb_runtime': 27.1698, 'train@spa.rst.rststb_samples_per_second': 82.444, 'train@spa.rst.rststb_steps_per_second': 2.576, 'epoch': 7.0}
{'loss': 2.2689, 'learning_rate': 8.333333333333334e-06, 'epoch': 7.0}
{'eval_loss': 2.412186861038208, 'eval_accuracy@spa.rst.rststb': 0.34986945169712796, 'eval_f1@spa.rst.rststb': 0.0992172742235976, 'eval_precision@spa.rst.rststb': 0.08976772742080011, 'eval_recall@spa.rst.rststb': 0.12530879613326484, 'eval_loss@spa.rst.rststb': 2.412186622619629, 'eval_runtime': 4.9987, 'eval_samples_per_second': 76.62, 'eval_steps_per_second': 2.401, 'epoch': 7.0}
{'train@spa.rst.rststb_loss': 2.1334424018859863, 'train@spa.rst.rststb_accuracy@spa.rst.rststb': 0.4044642857142857, 'train@spa.rst.rststb_f1@spa.rst.rststb': 0.097092856171742, 'train@spa.rst.rststb_precision@spa.rst.rststb': 0.09911190060824711, 'train@spa.rst.rststb_recall@spa.rst.rststb': 0.12115870106040862, 'train@spa.rst.rststb_loss@spa.rst.rststb': 2.1334424018859863, 'train@spa.rst.rststb_runtime': 27.2289, 'train@spa.rst.rststb_samples_per_second': 82.265, 'train@spa.rst.rststb_steps_per_second': 2.571, 'epoch': 8.0}
{'loss': 2.2123, 'learning_rate': 6.666666666666667e-06, 'epoch': 8.0}
{'eval_loss': 2.3788719177246094, 'eval_accuracy@spa.rst.rststb': 0.3629242819843342, 'eval_f1@spa.rst.rststb': 0.10410339869626963, 'eval_precision@spa.rst.rststb': 0.09661398205617563, 'eval_recall@spa.rst.rststb': 0.12926016516279293, 'eval_loss@spa.rst.rststb': 2.3788719177246094, 'eval_runtime': 7.3148, 'eval_samples_per_second': 52.359, 'eval_steps_per_second': 1.641, 'epoch': 8.0}
{'train@spa.rst.rststb_loss': 2.101602554321289, 'train@spa.rst.rststb_accuracy@spa.rst.rststb': 0.4080357142857143, 'train@spa.rst.rststb_f1@spa.rst.rststb': 0.09854430678270441, 'train@spa.rst.rststb_precision@spa.rst.rststb': 0.09587288045064021, 'train@spa.rst.rststb_recall@spa.rst.rststb': 0.12510166672815237, 'train@spa.rst.rststb_loss@spa.rst.rststb': 2.101602792739868, 'train@spa.rst.rststb_runtime': 27.1941, 'train@spa.rst.rststb_samples_per_second': 82.371, 'train@spa.rst.rststb_steps_per_second': 2.574, 'epoch': 9.0}
{'loss': 2.1713, 'learning_rate': 5e-06, 'epoch': 9.0}
{'eval_loss': 2.3525807857513428, 'eval_accuracy@spa.rst.rststb': 0.370757180156658, 'eval_f1@spa.rst.rststb': 0.10089819179809069, 'eval_precision@spa.rst.rststb': 0.0871219885521945, 'eval_recall@spa.rst.rststb': 0.13499126244425616, 'eval_loss@spa.rst.rststb': 2.352581024169922, 'eval_runtime': 5.0087, 'eval_samples_per_second': 76.467, 'eval_steps_per_second': 2.396, 'epoch': 9.0}
{'train@spa.rst.rststb_loss': 2.079124689102173, 'train@spa.rst.rststb_accuracy@spa.rst.rststb': 0.4138392857142857, 'train@spa.rst.rststb_f1@spa.rst.rststb': 0.10155350279873045, 'train@spa.rst.rststb_precision@spa.rst.rststb': 0.11431380520099652, 'train@spa.rst.rststb_recall@spa.rst.rststb': 0.12722878805848636, 'train@spa.rst.rststb_loss@spa.rst.rststb': 2.079124689102173, 'train@spa.rst.rststb_runtime': 27.202, 'train@spa.rst.rststb_samples_per_second': 82.347, 'train@spa.rst.rststb_steps_per_second': 2.573, 'epoch': 10.0}
{'loss': 2.1432, 'learning_rate': 3.3333333333333333e-06, 'epoch': 10.0}
{'eval_loss': 2.336625576019287, 'eval_accuracy@spa.rst.rststb': 0.381201044386423, 'eval_f1@spa.rst.rststb': 0.10359646407994288, 'eval_precision@spa.rst.rststb': 0.09037841093824164, 'eval_recall@spa.rst.rststb': 0.13729126490854449, 'eval_loss@spa.rst.rststb': 2.336625576019287, 'eval_runtime': 4.9922, 'eval_samples_per_second': 76.72, 'eval_steps_per_second': 2.404, 'epoch': 10.0}
{'train@spa.rst.rststb_loss': 2.0659773349761963, 'train@spa.rst.rststb_accuracy@spa.rst.rststb': 0.41696428571428573, 'train@spa.rst.rststb_f1@spa.rst.rststb': 0.10366159766031194, 'train@spa.rst.rststb_precision@spa.rst.rststb': 0.1201550569203439, 'train@spa.rst.rststb_recall@spa.rst.rststb': 0.12884610323935022, 'train@spa.rst.rststb_loss@spa.rst.rststb': 2.0659775733947754, 'train@spa.rst.rststb_runtime': 27.1884, 'train@spa.rst.rststb_samples_per_second': 82.388, 'train@spa.rst.rststb_steps_per_second': 2.575, 'epoch': 11.0}
{'loss': 2.1247, 'learning_rate': 1.6666666666666667e-06, 'epoch': 11.0}
{'eval_loss': 2.3254189491271973, 'eval_accuracy@spa.rst.rststb': 0.38903394255874674, 'eval_f1@spa.rst.rststb': 0.10775478481512883, 'eval_precision@spa.rst.rststb': 0.09584389658331141, 'eval_recall@spa.rst.rststb': 0.13923805270867426, 'eval_loss@spa.rst.rststb': 2.3254189491271973, 'eval_runtime': 4.9934, 'eval_samples_per_second': 76.701, 'eval_steps_per_second': 2.403, 'epoch': 11.0}
{'train@spa.rst.rststb_loss': 2.061756134033203, 'train@spa.rst.rststb_accuracy@spa.rst.rststb': 0.41875, 'train@spa.rst.rststb_f1@spa.rst.rststb': 0.10330902143726049, 'train@spa.rst.rststb_precision@spa.rst.rststb': 0.11484434132174183, 'train@spa.rst.rststb_recall@spa.rst.rststb': 0.12875827203818416, 'train@spa.rst.rststb_loss@spa.rst.rststb': 2.061756134033203, 'train@spa.rst.rststb_runtime': 27.2048, 'train@spa.rst.rststb_samples_per_second': 82.338, 'train@spa.rst.rststb_steps_per_second': 2.573, 'epoch': 12.0}
{'loss': 2.1145, 'learning_rate': 0.0, 'epoch': 12.0}
{'eval_loss': 2.322152853012085, 'eval_accuracy@spa.rst.rststb': 0.38642297650130547, 'eval_f1@spa.rst.rststb': 0.1062326760117209, 'eval_precision@spa.rst.rststb': 0.09412416885866161, 'eval_recall@spa.rst.rststb': 0.13768525767761836, 'eval_loss@spa.rst.rststb': 2.322152853012085, 'eval_runtime': 4.9777, 'eval_samples_per_second': 76.943, 'eval_steps_per_second': 2.411, 'epoch': 12.0}
{'train_runtime': 1070.6475, 'train_samples_per_second': 25.106, 'train_steps_per_second': 0.785, 'train_loss': 2.413160178774879, 'epoch': 12.0}
***** train_log metrics *****
  epoch                    =       12.0
  train_loss               =     2.4132
  train_runtime            = 0:17:50.64
  train_samples_per_second =     25.106
  train_steps_per_second   =      0.785
{'train@eng.sdrt.stac_loss': 2.10398006439209, 'train@eng.sdrt.stac_accuracy@eng.sdrt.stac': 0.3544885177453027, 'train@eng.sdrt.stac_f1@eng.sdrt.stac': 0.06872866883511648, 'train@eng.sdrt.stac_precision@eng.sdrt.stac': 0.06473347018877115, 'train@eng.sdrt.stac_recall@eng.sdrt.stac': 0.11119334816073068, 'train@eng.sdrt.stac_loss@eng.sdrt.stac': 2.103980302810669, 'train@eng.sdrt.stac_runtime': 113.4447, 'train@eng.sdrt.stac_samples_per_second': 84.446, 'train@eng.sdrt.stac_steps_per_second': 2.644, 'epoch': 1.0}
{'loss': 2.5919, 'learning_rate': 1.8333333333333333e-05, 'epoch': 1.0}
{'eval_loss': 2.059905767440796, 'eval_accuracy@eng.sdrt.stac': 0.36419213973799125, 'eval_f1@eng.sdrt.stac': 0.06792292355559128, 'eval_precision@eng.sdrt.stac': 0.06461530901168397, 'eval_recall@eng.sdrt.stac': 0.11269667618413268, 'eval_loss@eng.sdrt.stac': 2.059905767440796, 'eval_runtime': 13.9611, 'eval_samples_per_second': 82.014, 'eval_steps_per_second': 2.579, 'epoch': 1.0}
{'train@eng.sdrt.stac_loss': 1.8757270574569702, 'train@eng.sdrt.stac_accuracy@eng.sdrt.stac': 0.4164926931106472, 'train@eng.sdrt.stac_f1@eng.sdrt.stac': 0.1198640273011259, 'train@eng.sdrt.stac_precision@eng.sdrt.stac': 0.12835195342538008, 'train@eng.sdrt.stac_recall@eng.sdrt.stac': 0.16533936626098875, 'train@eng.sdrt.stac_loss@eng.sdrt.stac': 1.8757271766662598, 'train@eng.sdrt.stac_runtime': 113.4045, 'train@eng.sdrt.stac_samples_per_second': 84.476, 'train@eng.sdrt.stac_steps_per_second': 2.645, 'epoch': 2.0}
{'loss': 2.0319, 'learning_rate': 1.6666666666666667e-05, 'epoch': 2.0}
{'eval_loss': 1.834254503250122, 'eval_accuracy@eng.sdrt.stac': 0.4200873362445415, 'eval_f1@eng.sdrt.stac': 0.11608315453720712, 'eval_precision@eng.sdrt.stac': 0.11224678037422345, 'eval_recall@eng.sdrt.stac': 0.16419243588775345, 'eval_loss@eng.sdrt.stac': 1.834254503250122, 'eval_runtime': 13.9614, 'eval_samples_per_second': 82.012, 'eval_steps_per_second': 2.579, 'epoch': 2.0}
{'train@eng.sdrt.stac_loss': 1.7428680658340454, 'train@eng.sdrt.stac_accuracy@eng.sdrt.stac': 0.455741127348643, 'train@eng.sdrt.stac_f1@eng.sdrt.stac': 0.1614785923015261, 'train@eng.sdrt.stac_precision@eng.sdrt.stac': 0.2333215055139483, 'train@eng.sdrt.stac_recall@eng.sdrt.stac': 0.1991973033122833, 'train@eng.sdrt.stac_loss@eng.sdrt.stac': 1.7428680658340454, 'train@eng.sdrt.stac_runtime': 112.8496, 'train@eng.sdrt.stac_samples_per_second': 84.892, 'train@eng.sdrt.stac_steps_per_second': 2.658, 'epoch': 3.0}
{'loss': 1.841, 'learning_rate': 1.5000000000000002e-05, 'epoch': 3.0}
{'eval_loss': 1.705034852027893, 'eval_accuracy@eng.sdrt.stac': 0.46899563318777293, 'eval_f1@eng.sdrt.stac': 0.15748922372031432, 'eval_precision@eng.sdrt.stac': 0.182235462400658, 'eval_recall@eng.sdrt.stac': 0.19870181785668375, 'eval_loss@eng.sdrt.stac': 1.7050350904464722, 'eval_runtime': 13.8514, 'eval_samples_per_second': 82.663, 'eval_steps_per_second': 2.599, 'epoch': 3.0}
{'train@eng.sdrt.stac_loss': 1.6535296440124512, 'train@eng.sdrt.stac_accuracy@eng.sdrt.stac': 0.48862212943632566, 'train@eng.sdrt.stac_f1@eng.sdrt.stac': 0.2441995748760143, 'train@eng.sdrt.stac_precision@eng.sdrt.stac': 0.25094570194158594, 'train@eng.sdrt.stac_recall@eng.sdrt.stac': 0.26616503590007556, 'train@eng.sdrt.stac_loss@eng.sdrt.stac': 1.653529405593872, 'train@eng.sdrt.stac_runtime': 112.7568, 'train@eng.sdrt.stac_samples_per_second': 84.962, 'train@eng.sdrt.stac_steps_per_second': 2.661, 'epoch': 4.0}
{'loss': 1.7411, 'learning_rate': 1.3333333333333333e-05, 'epoch': 4.0}
{'eval_loss': 1.6120680570602417, 'eval_accuracy@eng.sdrt.stac': 0.5056768558951965, 'eval_f1@eng.sdrt.stac': 0.2338370367197692, 'eval_precision@eng.sdrt.stac': 0.2483470087142977, 'eval_recall@eng.sdrt.stac': 0.25323198495792465, 'eval_loss@eng.sdrt.stac': 1.6120679378509521, 'eval_runtime': 13.8725, 'eval_samples_per_second': 82.537, 'eval_steps_per_second': 2.595, 'epoch': 4.0}
{'train@eng.sdrt.stac_loss': 1.5982121229171753, 'train@eng.sdrt.stac_accuracy@eng.sdrt.stac': 0.5029227557411273, 'train@eng.sdrt.stac_f1@eng.sdrt.stac': 0.2601331948488221, 'train@eng.sdrt.stac_precision@eng.sdrt.stac': 0.3216442234348498, 'train@eng.sdrt.stac_recall@eng.sdrt.stac': 0.2870127179803068, 'train@eng.sdrt.stac_loss@eng.sdrt.stac': 1.5982121229171753, 'train@eng.sdrt.stac_runtime': 112.9767, 'train@eng.sdrt.stac_samples_per_second': 84.796, 'train@eng.sdrt.stac_steps_per_second': 2.655, 'epoch': 5.0}
{'loss': 1.6725, 'learning_rate': 1.1666666666666668e-05, 'epoch': 5.0}
{'eval_loss': 1.5636793375015259, 'eval_accuracy@eng.sdrt.stac': 0.5109170305676856, 'eval_f1@eng.sdrt.stac': 0.2460118683698364, 'eval_precision@eng.sdrt.stac': 0.2428811462467408, 'eval_recall@eng.sdrt.stac': 0.26915322487822957, 'eval_loss@eng.sdrt.stac': 1.563679575920105, 'eval_runtime': 13.926, 'eval_samples_per_second': 82.22, 'eval_steps_per_second': 2.585, 'epoch': 5.0}
{'train@eng.sdrt.stac_loss': 1.5536524057388306, 'train@eng.sdrt.stac_accuracy@eng.sdrt.stac': 0.5103340292275574, 'train@eng.sdrt.stac_f1@eng.sdrt.stac': 0.27768825124123664, 'train@eng.sdrt.stac_precision@eng.sdrt.stac': 0.331246272169473, 'train@eng.sdrt.stac_recall@eng.sdrt.stac': 0.29818718052662463, 'train@eng.sdrt.stac_loss@eng.sdrt.stac': 1.553652286529541, 'train@eng.sdrt.stac_runtime': 113.0039, 'train@eng.sdrt.stac_samples_per_second': 84.776, 'train@eng.sdrt.stac_steps_per_second': 2.655, 'epoch': 6.0}
{'loss': 1.6238, 'learning_rate': 1e-05, 'epoch': 6.0}
{'eval_loss': 1.5383230447769165, 'eval_accuracy@eng.sdrt.stac': 0.5109170305676856, 'eval_f1@eng.sdrt.stac': 0.26776844021934393, 'eval_precision@eng.sdrt.stac': 0.31645432082779257, 'eval_recall@eng.sdrt.stac': 0.2817701099444944, 'eval_loss@eng.sdrt.stac': 1.5383230447769165, 'eval_runtime': 13.9567, 'eval_samples_per_second': 82.039, 'eval_steps_per_second': 2.579, 'epoch': 6.0}
{'train@eng.sdrt.stac_loss': 1.5371366739273071, 'train@eng.sdrt.stac_accuracy@eng.sdrt.stac': 0.5177453027139874, 'train@eng.sdrt.stac_f1@eng.sdrt.stac': 0.2977287742323369, 'train@eng.sdrt.stac_precision@eng.sdrt.stac': 0.3586586810639754, 'train@eng.sdrt.stac_recall@eng.sdrt.stac': 0.31954461846557636, 'train@eng.sdrt.stac_loss@eng.sdrt.stac': 1.5371366739273071, 'train@eng.sdrt.stac_runtime': 113.1201, 'train@eng.sdrt.stac_samples_per_second': 84.689, 'train@eng.sdrt.stac_steps_per_second': 2.652, 'epoch': 7.0}
{'loss': 1.5896, 'learning_rate': 8.333333333333334e-06, 'epoch': 7.0}
{'eval_loss': 1.5240541696548462, 'eval_accuracy@eng.sdrt.stac': 0.508296943231441, 'eval_f1@eng.sdrt.stac': 0.26961553575965413, 'eval_precision@eng.sdrt.stac': 0.3040126480616751, 'eval_recall@eng.sdrt.stac': 0.2887732616219106, 'eval_loss@eng.sdrt.stac': 1.5240542888641357, 'eval_runtime': 13.9397, 'eval_samples_per_second': 82.139, 'eval_steps_per_second': 2.583, 'epoch': 7.0}
{'train@eng.sdrt.stac_loss': 1.503965139389038, 'train@eng.sdrt.stac_accuracy@eng.sdrt.stac': 0.5244258872651357, 'train@eng.sdrt.stac_f1@eng.sdrt.stac': 0.3068965354026112, 'train@eng.sdrt.stac_precision@eng.sdrt.stac': 0.3630829664546013, 'train@eng.sdrt.stac_recall@eng.sdrt.stac': 0.32597082663451493, 'train@eng.sdrt.stac_loss@eng.sdrt.stac': 1.5039652585983276, 'train@eng.sdrt.stac_runtime': 112.8073, 'train@eng.sdrt.stac_samples_per_second': 84.924, 'train@eng.sdrt.stac_steps_per_second': 2.659, 'epoch': 8.0}
{'loss': 1.5622, 'learning_rate': 6.666666666666667e-06, 'epoch': 8.0}
{'eval_loss': 1.4922198057174683, 'eval_accuracy@eng.sdrt.stac': 0.5187772925764192, 'eval_f1@eng.sdrt.stac': 0.2834304811964495, 'eval_precision@eng.sdrt.stac': 0.3968437616116762, 'eval_recall@eng.sdrt.stac': 0.29838223060717445, 'eval_loss@eng.sdrt.stac': 1.4922198057174683, 'eval_runtime': 13.9129, 'eval_samples_per_second': 82.298, 'eval_steps_per_second': 2.588, 'epoch': 8.0}
{'train@eng.sdrt.stac_loss': 1.4902020692825317, 'train@eng.sdrt.stac_accuracy@eng.sdrt.stac': 0.5327766179540709, 'train@eng.sdrt.stac_f1@eng.sdrt.stac': 0.322176225346432, 'train@eng.sdrt.stac_precision@eng.sdrt.stac': 0.42904692162862595, 'train@eng.sdrt.stac_recall@eng.sdrt.stac': 0.33951418859672555, 'train@eng.sdrt.stac_loss@eng.sdrt.stac': 1.4902019500732422, 'train@eng.sdrt.stac_runtime': 113.0673, 'train@eng.sdrt.stac_samples_per_second': 84.728, 'train@eng.sdrt.stac_steps_per_second': 2.653, 'epoch': 9.0}
{'loss': 1.542, 'learning_rate': 5e-06, 'epoch': 9.0}
{'eval_loss': 1.4820408821105957, 'eval_accuracy@eng.sdrt.stac': 0.519650655021834, 'eval_f1@eng.sdrt.stac': 0.29148875466414204, 'eval_precision@eng.sdrt.stac': 0.4420564264169841, 'eval_recall@eng.sdrt.stac': 0.3021770525975967, 'eval_loss@eng.sdrt.stac': 1.4820408821105957, 'eval_runtime': 13.9529, 'eval_samples_per_second': 82.062, 'eval_steps_per_second': 2.58, 'epoch': 9.0}
{'train@eng.sdrt.stac_loss': 1.4750325679779053, 'train@eng.sdrt.stac_accuracy@eng.sdrt.stac': 0.5356993736951984, 'train@eng.sdrt.stac_f1@eng.sdrt.stac': 0.3280926393254414, 'train@eng.sdrt.stac_precision@eng.sdrt.stac': 0.4291013559663047, 'train@eng.sdrt.stac_recall@eng.sdrt.stac': 0.3446782107024199, 'train@eng.sdrt.stac_loss@eng.sdrt.stac': 1.4750325679779053, 'train@eng.sdrt.stac_runtime': 113.0768, 'train@eng.sdrt.stac_samples_per_second': 84.721, 'train@eng.sdrt.stac_steps_per_second': 2.653, 'epoch': 10.0}
{'loss': 1.5302, 'learning_rate': 3.3333333333333333e-06, 'epoch': 10.0}
{'eval_loss': 1.4707242250442505, 'eval_accuracy@eng.sdrt.stac': 0.5222707423580786, 'eval_f1@eng.sdrt.stac': 0.2994287607646776, 'eval_precision@eng.sdrt.stac': 0.4474444924022114, 'eval_recall@eng.sdrt.stac': 0.3081813907996929, 'eval_loss@eng.sdrt.stac': 1.470724105834961, 'eval_runtime': 13.9343, 'eval_samples_per_second': 82.171, 'eval_steps_per_second': 2.584, 'epoch': 10.0}
{'train@eng.sdrt.stac_loss': 1.4675499200820923, 'train@eng.sdrt.stac_accuracy@eng.sdrt.stac': 0.5373695198329854, 'train@eng.sdrt.stac_f1@eng.sdrt.stac': 0.33189733771512175, 'train@eng.sdrt.stac_precision@eng.sdrt.stac': 0.4283725722320558, 'train@eng.sdrt.stac_recall@eng.sdrt.stac': 0.34779537848589787, 'train@eng.sdrt.stac_loss@eng.sdrt.stac': 1.4675500392913818, 'train@eng.sdrt.stac_runtime': 113.1825, 'train@eng.sdrt.stac_samples_per_second': 84.642, 'train@eng.sdrt.stac_steps_per_second': 2.651, 'epoch': 11.0}
{'loss': 1.5139, 'learning_rate': 1.6666666666666667e-06, 'epoch': 11.0}
{'eval_loss': 1.466089129447937, 'eval_accuracy@eng.sdrt.stac': 0.5275109170305677, 'eval_f1@eng.sdrt.stac': 0.3038649738186622, 'eval_precision@eng.sdrt.stac': 0.45118125047749513, 'eval_recall@eng.sdrt.stac': 0.31306343393806163, 'eval_loss@eng.sdrt.stac': 1.4660890102386475, 'eval_runtime': 13.9074, 'eval_samples_per_second': 82.33, 'eval_steps_per_second': 2.589, 'epoch': 11.0}
{'train@eng.sdrt.stac_loss': 1.4662634134292603, 'train@eng.sdrt.stac_accuracy@eng.sdrt.stac': 0.5376826722338205, 'train@eng.sdrt.stac_f1@eng.sdrt.stac': 0.3325097586038967, 'train@eng.sdrt.stac_precision@eng.sdrt.stac': 0.42823220839680287, 'train@eng.sdrt.stac_recall@eng.sdrt.stac': 0.34856751619965887, 'train@eng.sdrt.stac_loss@eng.sdrt.stac': 1.4662632942199707, 'train@eng.sdrt.stac_runtime': 113.4152, 'train@eng.sdrt.stac_samples_per_second': 84.468, 'train@eng.sdrt.stac_steps_per_second': 2.645, 'epoch': 12.0}
{'loss': 1.5159, 'learning_rate': 0.0, 'epoch': 12.0}
{'eval_loss': 1.4652326107025146, 'eval_accuracy@eng.sdrt.stac': 0.5275109170305677, 'eval_f1@eng.sdrt.stac': 0.30636096594284284, 'eval_precision@eng.sdrt.stac': 0.4501977855032847, 'eval_recall@eng.sdrt.stac': 0.3143660252514352, 'eval_loss@eng.sdrt.stac': 1.4652328491210938, 'eval_runtime': 13.8579, 'eval_samples_per_second': 82.624, 'eval_steps_per_second': 2.598, 'epoch': 12.0}
{'train_runtime': 4387.6595, 'train_samples_per_second': 26.201, 'train_steps_per_second': 0.82, 'train_loss': 1.729655999077691, 'epoch': 12.0}
***** train_logger metrics *****
  epoch                    =       12.0
  train_loss               =     2.4132
  train_runtime            = 0:17:50.64
  train_samples_per_second =     25.106
  train_steps_per_second   =      0.785
-------------------------------------------------------------------
Lang1:  spa.rst.sctb    Lang2:  eng.sdrt.stac
Saving run to:  runs/full_shot/FullShot=v4_spa.rst.sctb_eng.sdrt.stac_lrfunc=Adafactor_lr=2e-05_plm=bert-base-multilingual-cased
Running with params: BERT_MODEL=bert-base-multilingual-cased lr=2e-05
read 439 examples
read 94 examples
read 159 examples
read 9580 examples
read 1145 examples
read 1510 examples
Total prediction labels:  41
BertAdapterModel(
  (shared_parameters): ModuleDict()
  (bert): BertModel(
    (shared_parameters): ModuleDict()
    (invertible_adapters): ModuleDict(
      (es): NICECouplingBlock(
        (F): Sequential(
          (0): Linear(in_features=384, out_features=192, bias=True)
          (1): Activation_Function_Class(
            (f): NewGELUActivation()
          )
          (2): Linear(in_features=192, out_features=384, bias=True)
        )
        (G): Sequential(
          (0): Linear(in_features=384, out_features=192, bias=True)
          (1): Activation_Function_Class(
            (f): NewGELUActivation()
          )
          (2): Linear(in_features=192, out_features=384, bias=True)
        )
      )
      (en): NICECouplingBlock(
        (F): Sequential(
          (0): Linear(in_features=384, out_features=192, bias=True)
          (1): Activation_Function_Class(
            (f): ReLU()
          )
          (2): Linear(in_features=192, out_features=384, bias=True)
        )
        (G): Sequential(
          (0): Linear(in_features=384, out_features=192, bias=True)
          (1): Activation_Function_Class(
            (f): ReLU()
          )
          (2): Linear(in_features=192, out_features=384, bias=True)
        )
      )
    )
    (embeddings): BertEmbeddings(
      (word_embeddings): Embedding(119547, 768, padding_idx=0)
      (position_embeddings): Embedding(512, 768)
      (token_type_embeddings): Embedding(2, 768)
      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
      (dropout): Dropout(p=0.1, inplace=False)
    )
    (encoder): BertEncoder(
      (layer): ModuleList(
        (0): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (es): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (en): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (1): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (es): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (en): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (2): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (es): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (en): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (3): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (es): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (en): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (4): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (es): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (en): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (5): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (es): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (en): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (6): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (es): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (en): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (7): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (es): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (en): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (8): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (es): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (en): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (9): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (es): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (en): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (10): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (es): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (en): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (11): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (es): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (en): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
      )
    )
    (pooler): BertPooler(
      (dense): Linear(in_features=768, out_features=768, bias=True)
      (activation): Tanh()
    )
    (prefix_tuning): PrefixTuningPool(
      (prefix_tunings): ModuleDict()
    )
  )
  (heads): ModuleDict(
    (disrpt): ClassificationHead(
      (0): Dropout(p=0.1, inplace=False)
      (1): Linear(in_features=768, out_features=768, bias=True)
      (2): Activation_Function_Class(
        (f): Tanh()
      )
      (3): Dropout(p=0.1, inplace=False)
      (4): Linear(in_features=768, out_features=41, bias=True)
    )
  )
)
{'train@spa.rst.sctb_loss': 3.436274290084839, 'train@spa.rst.sctb_accuracy@spa.rst.sctb': 0.3712984054669704, 'train@spa.rst.sctb_f1@spa.rst.sctb': 0.03987101321474514, 'train@spa.rst.sctb_precision@spa.rst.sctb': 0.031408679009452385, 'train@spa.rst.sctb_recall@spa.rst.sctb': 0.0546415770609319, 'train@spa.rst.sctb_loss@spa.rst.sctb': 3.436274290084839, 'train@spa.rst.sctb_runtime': 5.5502, 'train@spa.rst.sctb_samples_per_second': 79.096, 'train@spa.rst.sctb_steps_per_second': 2.522, 'epoch': 1.0}
{'loss': 3.5965, 'learning_rate': 1.8333333333333333e-05, 'epoch': 1.0}
{'eval_loss': 3.4192378520965576, 'eval_accuracy@spa.rst.sctb': 0.3723404255319149, 'eval_f1@spa.rst.sctb': 0.054052637097619795, 'eval_precision@spa.rst.sctb': 0.04334185848252344, 'eval_recall@spa.rst.sctb': 0.07420958814147668, 'eval_loss@spa.rst.sctb': 3.4192376136779785, 'eval_runtime': 1.4903, 'eval_samples_per_second': 63.073, 'eval_steps_per_second': 2.013, 'epoch': 1.0}
{'train@spa.rst.sctb_loss': 3.1734414100646973, 'train@spa.rst.sctb_accuracy@spa.rst.sctb': 0.35990888382687924, 'train@spa.rst.sctb_f1@spa.rst.sctb': 0.029982206405693948, 'train@spa.rst.sctb_precision@spa.rst.sctb': 0.03328389068680331, 'train@spa.rst.sctb_recall@spa.rst.sctb': 0.04593189964157707, 'train@spa.rst.sctb_loss@spa.rst.sctb': 3.1734416484832764, 'train@spa.rst.sctb_runtime': 5.5897, 'train@spa.rst.sctb_samples_per_second': 78.538, 'train@spa.rst.sctb_steps_per_second': 2.505, 'epoch': 2.0}
{'loss': 3.3233, 'learning_rate': 1.6666666666666667e-05, 'epoch': 2.0}
{'eval_loss': 3.157223701477051, 'eval_accuracy@spa.rst.sctb': 0.39361702127659576, 'eval_f1@spa.rst.sctb': 0.05399698340874811, 'eval_precision@spa.rst.sctb': 0.0636530474258669, 'eval_recall@spa.rst.sctb': 0.07252087437845951, 'eval_loss@spa.rst.sctb': 3.1572229862213135, 'eval_runtime': 1.4869, 'eval_samples_per_second': 63.221, 'eval_steps_per_second': 2.018, 'epoch': 2.0}
{'train@spa.rst.sctb_loss': 2.9112486839294434, 'train@spa.rst.sctb_accuracy@spa.rst.sctb': 0.3553530751708428, 'train@spa.rst.sctb_f1@spa.rst.sctb': 0.02642568292481638, 'train@spa.rst.sctb_precision@spa.rst.sctb': 0.035470335675253706, 'train@spa.rst.sctb_recall@spa.rst.sctb': 0.044354838709677415, 'train@spa.rst.sctb_loss@spa.rst.sctb': 2.9112486839294434, 'train@spa.rst.sctb_runtime': 5.5871, 'train@spa.rst.sctb_samples_per_second': 78.574, 'train@spa.rst.sctb_steps_per_second': 2.506, 'epoch': 3.0}
{'loss': 3.0809, 'learning_rate': 1.5000000000000002e-05, 'epoch': 3.0}
{'eval_loss': 2.900343894958496, 'eval_accuracy@spa.rst.sctb': 0.3723404255319149, 'eval_f1@spa.rst.sctb': 0.042004485078488876, 'eval_precision@spa.rst.sctb': 0.06054729584141348, 'eval_recall@spa.rst.sctb': 0.06501547987616098, 'eval_loss@spa.rst.sctb': 2.900344133377075, 'eval_runtime': 1.4903, 'eval_samples_per_second': 63.075, 'eval_steps_per_second': 2.013, 'epoch': 3.0}
{'train@spa.rst.sctb_loss': 2.683849573135376, 'train@spa.rst.sctb_accuracy@spa.rst.sctb': 0.3439635535307517, 'train@spa.rst.sctb_f1@spa.rst.sctb': 0.02219911357603337, 'train@spa.rst.sctb_precision@spa.rst.sctb': 0.02822375127420999, 'train@spa.rst.sctb_recall@spa.rst.sctb': 0.042114695340501794, 'train@spa.rst.sctb_loss@spa.rst.sctb': 2.683849811553955, 'train@spa.rst.sctb_runtime': 5.6082, 'train@spa.rst.sctb_samples_per_second': 78.278, 'train@spa.rst.sctb_steps_per_second': 2.496, 'epoch': 4.0}
{'loss': 2.8324, 'learning_rate': 1.3333333333333333e-05, 'epoch': 4.0}
{'eval_loss': 2.682910919189453, 'eval_accuracy@spa.rst.sctb': 0.3617021276595745, 'eval_f1@spa.rst.sctb': 0.03669467787114846, 'eval_precision@spa.rst.sctb': 0.07969639468690703, 'eval_recall@spa.rst.sctb': 0.06191950464396284, 'eval_loss@spa.rst.sctb': 2.6829113960266113, 'eval_runtime': 1.4855, 'eval_samples_per_second': 63.278, 'eval_steps_per_second': 2.019, 'epoch': 4.0}
{'train@spa.rst.sctb_loss': 2.518561363220215, 'train@spa.rst.sctb_accuracy@spa.rst.sctb': 0.3416856492027335, 'train@spa.rst.sctb_f1@spa.rst.sctb': 0.021222410865874366, 'train@spa.rst.sctb_precision@spa.rst.sctb': 0.014236902050113895, 'train@spa.rst.sctb_recall@spa.rst.sctb': 0.041666666666666664, 'train@spa.rst.sctb_loss@spa.rst.sctb': 2.518561601638794, 'train@spa.rst.sctb_runtime': 5.5983, 'train@spa.rst.sctb_samples_per_second': 78.417, 'train@spa.rst.sctb_steps_per_second': 2.501, 'epoch': 5.0}
{'loss': 2.6347, 'learning_rate': 1.1666666666666668e-05, 'epoch': 5.0}
{'eval_loss': 2.5261733531951904, 'eval_accuracy@spa.rst.sctb': 0.3617021276595745, 'eval_f1@spa.rst.sctb': 0.03669467787114846, 'eval_precision@spa.rst.sctb': 0.07969639468690703, 'eval_recall@spa.rst.sctb': 0.06191950464396284, 'eval_loss@spa.rst.sctb': 2.5261731147766113, 'eval_runtime': 1.4867, 'eval_samples_per_second': 63.228, 'eval_steps_per_second': 2.018, 'epoch': 5.0}
{'train@spa.rst.sctb_loss': 2.415670156478882, 'train@spa.rst.sctb_accuracy@spa.rst.sctb': 0.3416856492027335, 'train@spa.rst.sctb_f1@spa.rst.sctb': 0.021222410865874366, 'train@spa.rst.sctb_precision@spa.rst.sctb': 0.014236902050113895, 'train@spa.rst.sctb_recall@spa.rst.sctb': 0.041666666666666664, 'train@spa.rst.sctb_loss@spa.rst.sctb': 2.41567063331604, 'train@spa.rst.sctb_runtime': 5.6199, 'train@spa.rst.sctb_samples_per_second': 78.115, 'train@spa.rst.sctb_steps_per_second': 2.491, 'epoch': 6.0}
{'loss': 2.502, 'learning_rate': 1e-05, 'epoch': 6.0}
{'eval_loss': 2.4319913387298584, 'eval_accuracy@spa.rst.sctb': 0.35106382978723405, 'eval_f1@spa.rst.sctb': 0.03056970819823993, 'eval_precision@spa.rst.sctb': 0.02065081351689612, 'eval_recall@spa.rst.sctb': 0.058823529411764705, 'eval_loss@spa.rst.sctb': 2.4319913387298584, 'eval_runtime': 1.5167, 'eval_samples_per_second': 61.979, 'eval_steps_per_second': 1.978, 'epoch': 6.0}
{'train@spa.rst.sctb_loss': 2.3533594608306885, 'train@spa.rst.sctb_accuracy@spa.rst.sctb': 0.3462414578587699, 'train@spa.rst.sctb_f1@spa.rst.sctb': 0.023104789861149944, 'train@spa.rst.sctb_precision@spa.rst.sctb': 0.031067588325652845, 'train@spa.rst.sctb_recall@spa.rst.sctb': 0.04256272401433692, 'train@spa.rst.sctb_loss@spa.rst.sctb': 2.3533589839935303, 'train@spa.rst.sctb_runtime': 5.62, 'train@spa.rst.sctb_samples_per_second': 78.114, 'train@spa.rst.sctb_steps_per_second': 2.491, 'epoch': 7.0}
{'loss': 2.4226, 'learning_rate': 8.333333333333334e-06, 'epoch': 7.0}
{'eval_loss': 2.376582622528076, 'eval_accuracy@spa.rst.sctb': 0.3617021276595745, 'eval_f1@spa.rst.sctb': 0.0366610644257703, 'eval_precision@spa.rst.sctb': 0.05051150895140665, 'eval_recall@spa.rst.sctb': 0.06191950464396284, 'eval_loss@spa.rst.sctb': 2.376581907272339, 'eval_runtime': 1.4887, 'eval_samples_per_second': 63.143, 'eval_steps_per_second': 2.015, 'epoch': 7.0}
{'train@spa.rst.sctb_loss': 2.3169515132904053, 'train@spa.rst.sctb_accuracy@spa.rst.sctb': 0.34851936218678814, 'train@spa.rst.sctb_f1@spa.rst.sctb': 0.0240027045300879, 'train@spa.rst.sctb_precision@spa.rst.sctb': 0.028423772609819122, 'train@spa.rst.sctb_recall@spa.rst.sctb': 0.04301075268817204, 'train@spa.rst.sctb_loss@spa.rst.sctb': 2.3169517517089844, 'train@spa.rst.sctb_runtime': 5.6154, 'train@spa.rst.sctb_samples_per_second': 78.178, 'train@spa.rst.sctb_steps_per_second': 2.493, 'epoch': 8.0}
{'loss': 2.3602, 'learning_rate': 6.666666666666667e-06, 'epoch': 8.0}
{'eval_loss': 2.3457443714141846, 'eval_accuracy@spa.rst.sctb': 0.3723404255319149, 'eval_f1@spa.rst.sctb': 0.042004485078488876, 'eval_precision@spa.rst.sctb': 0.06054729584141348, 'eval_recall@spa.rst.sctb': 0.06501547987616098, 'eval_loss@spa.rst.sctb': 2.3457443714141846, 'eval_runtime': 1.4748, 'eval_samples_per_second': 63.737, 'eval_steps_per_second': 2.034, 'epoch': 8.0}
{'train@spa.rst.sctb_loss': 2.292527675628662, 'train@spa.rst.sctb_accuracy@spa.rst.sctb': 0.35990888382687924, 'train@spa.rst.sctb_f1@spa.rst.sctb': 0.02848005430242272, 'train@spa.rst.sctb_precision@spa.rst.sctb': 0.03451858813700919, 'train@spa.rst.sctb_recall@spa.rst.sctb': 0.04542114695340502, 'train@spa.rst.sctb_loss@spa.rst.sctb': 2.292527675628662, 'train@spa.rst.sctb_runtime': 5.6114, 'train@spa.rst.sctb_samples_per_second': 78.234, 'train@spa.rst.sctb_steps_per_second': 2.495, 'epoch': 9.0}
{'loss': 2.3465, 'learning_rate': 5e-06, 'epoch': 9.0}
{'eval_loss': 2.324647903442383, 'eval_accuracy@spa.rst.sctb': 0.3829787234042553, 'eval_f1@spa.rst.sctb': 0.04690911359241469, 'eval_precision@spa.rst.sctb': 0.06568627450980392, 'eval_recall@spa.rst.sctb': 0.06811145510835914, 'eval_loss@spa.rst.sctb': 2.324648380279541, 'eval_runtime': 1.5094, 'eval_samples_per_second': 62.277, 'eval_steps_per_second': 1.988, 'epoch': 9.0}
{'train@spa.rst.sctb_loss': 2.276148557662964, 'train@spa.rst.sctb_accuracy@spa.rst.sctb': 0.3735763097949886, 'train@spa.rst.sctb_f1@spa.rst.sctb': 0.03317962169073405, 'train@spa.rst.sctb_precision@spa.rst.sctb': 0.035398986327204146, 'train@spa.rst.sctb_recall@spa.rst.sctb': 0.04844982078853047, 'train@spa.rst.sctb_loss@spa.rst.sctb': 2.276148796081543, 'train@spa.rst.sctb_runtime': 5.6312, 'train@spa.rst.sctb_samples_per_second': 77.958, 'train@spa.rst.sctb_steps_per_second': 2.486, 'epoch': 10.0}
{'loss': 2.3193, 'learning_rate': 3.3333333333333333e-06, 'epoch': 10.0}
{'eval_loss': 2.311326026916504, 'eval_accuracy@spa.rst.sctb': 0.3723404255319149, 'eval_f1@spa.rst.sctb': 0.04556412729026036, 'eval_precision@spa.rst.sctb': 0.05644415069398546, 'eval_recall@spa.rst.sctb': 0.06632892391406324, 'eval_loss@spa.rst.sctb': 2.3113255500793457, 'eval_runtime': 1.4753, 'eval_samples_per_second': 63.715, 'eval_steps_per_second': 2.033, 'epoch': 10.0}
{'train@spa.rst.sctb_loss': 2.2672901153564453, 'train@spa.rst.sctb_accuracy@spa.rst.sctb': 0.3735763097949886, 'train@spa.rst.sctb_f1@spa.rst.sctb': 0.033484848484848485, 'train@spa.rst.sctb_precision@spa.rst.sctb': 0.03443910256410256, 'train@spa.rst.sctb_recall@spa.rst.sctb': 0.04862007168458782, 'train@spa.rst.sctb_loss@spa.rst.sctb': 2.2672901153564453, 'train@spa.rst.sctb_runtime': 5.6281, 'train@spa.rst.sctb_samples_per_second': 78.001, 'train@spa.rst.sctb_steps_per_second': 2.487, 'epoch': 11.0}
{'loss': 2.3122, 'learning_rate': 1.6666666666666667e-06, 'epoch': 11.0}
{'eval_loss': 2.303877115249634, 'eval_accuracy@spa.rst.sctb': 0.3829787234042553, 'eval_f1@spa.rst.sctb': 0.04993680116674768, 'eval_precision@spa.rst.sctb': 0.06060606060606061, 'eval_recall@spa.rst.sctb': 0.06942489914626138, 'eval_loss@spa.rst.sctb': 2.3038768768310547, 'eval_runtime': 1.4841, 'eval_samples_per_second': 63.338, 'eval_steps_per_second': 2.021, 'epoch': 11.0}
{'train@spa.rst.sctb_loss': 2.264371156692505, 'train@spa.rst.sctb_accuracy@spa.rst.sctb': 0.3804100227790433, 'train@spa.rst.sctb_f1@spa.rst.sctb': 0.03520549800257296, 'train@spa.rst.sctb_precision@spa.rst.sctb': 0.036156591099916036, 'train@spa.rst.sctb_recall@spa.rst.sctb': 0.04996415770609319, 'train@spa.rst.sctb_loss@spa.rst.sctb': 2.264371156692505, 'train@spa.rst.sctb_runtime': 5.6239, 'train@spa.rst.sctb_samples_per_second': 78.06, 'train@spa.rst.sctb_steps_per_second': 2.489, 'epoch': 12.0}
{'loss': 2.305, 'learning_rate': 0.0, 'epoch': 12.0}
{'eval_loss': 2.3016765117645264, 'eval_accuracy@spa.rst.sctb': 0.39361702127659576, 'eval_f1@spa.rst.sctb': 0.05399698340874811, 'eval_precision@spa.rst.sctb': 0.0636530474258669, 'eval_recall@spa.rst.sctb': 0.07252087437845951, 'eval_loss@spa.rst.sctb': 2.3016769886016846, 'eval_runtime': 1.4823, 'eval_samples_per_second': 63.414, 'eval_steps_per_second': 2.024, 'epoch': 12.0}
{'train_runtime': 218.7261, 'train_samples_per_second': 24.085, 'train_steps_per_second': 0.768, 'train_loss': 2.66962852932158, 'epoch': 12.0}
***** train_log metrics *****
  epoch                    =       12.0
  train_loss               =     2.6696
  train_runtime            = 0:03:38.72
  train_samples_per_second =     24.085
  train_steps_per_second   =      0.768
{'train@eng.sdrt.stac_loss': 2.0852837562561035, 'train@eng.sdrt.stac_accuracy@eng.sdrt.stac': 0.35302713987473905, 'train@eng.sdrt.stac_f1@eng.sdrt.stac': 0.07533988132796937, 'train@eng.sdrt.stac_precision@eng.sdrt.stac': 0.06842070980746878, 'train@eng.sdrt.stac_recall@eng.sdrt.stac': 0.11340269218528079, 'train@eng.sdrt.stac_loss@eng.sdrt.stac': 2.0852837562561035, 'train@eng.sdrt.stac_runtime': 112.9242, 'train@eng.sdrt.stac_samples_per_second': 84.836, 'train@eng.sdrt.stac_steps_per_second': 2.657, 'epoch': 1.0}
{'loss': 2.4893, 'learning_rate': 1.8333333333333333e-05, 'epoch': 1.0}
{'eval_loss': 2.0400655269622803, 'eval_accuracy@eng.sdrt.stac': 0.36593886462882097, 'eval_f1@eng.sdrt.stac': 0.07840125629436281, 'eval_precision@eng.sdrt.stac': 0.07800068154133555, 'eval_recall@eng.sdrt.stac': 0.11663856937580638, 'eval_loss@eng.sdrt.stac': 2.0400657653808594, 'eval_runtime': 13.8981, 'eval_samples_per_second': 82.385, 'eval_steps_per_second': 2.59, 'epoch': 1.0}
{'train@eng.sdrt.stac_loss': 1.8512779474258423, 'train@eng.sdrt.stac_accuracy@eng.sdrt.stac': 0.4370563674321503, 'train@eng.sdrt.stac_f1@eng.sdrt.stac': 0.12880035451429647, 'train@eng.sdrt.stac_precision@eng.sdrt.stac': 0.14253271393411648, 'train@eng.sdrt.stac_recall@eng.sdrt.stac': 0.17924174974964033, 'train@eng.sdrt.stac_loss@eng.sdrt.stac': 1.8512777090072632, 'train@eng.sdrt.stac_runtime': 112.8922, 'train@eng.sdrt.stac_samples_per_second': 84.86, 'train@eng.sdrt.stac_steps_per_second': 2.657, 'epoch': 2.0}
{'loss': 2.0043, 'learning_rate': 1.6666666666666667e-05, 'epoch': 2.0}
{'eval_loss': 1.8002196550369263, 'eval_accuracy@eng.sdrt.stac': 0.4427947598253275, 'eval_f1@eng.sdrt.stac': 0.12503490337286188, 'eval_precision@eng.sdrt.stac': 0.11179715866968047, 'eval_recall@eng.sdrt.stac': 0.17807404060681997, 'eval_loss@eng.sdrt.stac': 1.8002196550369263, 'eval_runtime': 13.8997, 'eval_samples_per_second': 82.376, 'eval_steps_per_second': 2.59, 'epoch': 2.0}
{'train@eng.sdrt.stac_loss': 1.7493133544921875, 'train@eng.sdrt.stac_accuracy@eng.sdrt.stac': 0.45292275574112734, 'train@eng.sdrt.stac_f1@eng.sdrt.stac': 0.1511013637044581, 'train@eng.sdrt.stac_precision@eng.sdrt.stac': 0.1784982991432503, 'train@eng.sdrt.stac_recall@eng.sdrt.stac': 0.19315054904268447, 'train@eng.sdrt.stac_loss@eng.sdrt.stac': 1.7493133544921875, 'train@eng.sdrt.stac_runtime': 113.4736, 'train@eng.sdrt.stac_samples_per_second': 84.425, 'train@eng.sdrt.stac_steps_per_second': 2.644, 'epoch': 3.0}
{'loss': 1.8318, 'learning_rate': 1.5000000000000002e-05, 'epoch': 3.0}
{'eval_loss': 1.696420669555664, 'eval_accuracy@eng.sdrt.stac': 0.4585152838427948, 'eval_f1@eng.sdrt.stac': 0.14683198209626447, 'eval_precision@eng.sdrt.stac': 0.13916781203098882, 'eval_recall@eng.sdrt.stac': 0.1902642908762694, 'eval_loss@eng.sdrt.stac': 1.696420669555664, 'eval_runtime': 13.9139, 'eval_samples_per_second': 82.292, 'eval_steps_per_second': 2.587, 'epoch': 3.0}
{'train@eng.sdrt.stac_loss': 1.6660606861114502, 'train@eng.sdrt.stac_accuracy@eng.sdrt.stac': 0.48058455114822546, 'train@eng.sdrt.stac_f1@eng.sdrt.stac': 0.2034103085653735, 'train@eng.sdrt.stac_precision@eng.sdrt.stac': 0.21562745748405698, 'train@eng.sdrt.stac_recall@eng.sdrt.stac': 0.2297666583925324, 'train@eng.sdrt.stac_loss@eng.sdrt.stac': 1.666060447692871, 'train@eng.sdrt.stac_runtime': 112.7564, 'train@eng.sdrt.stac_samples_per_second': 84.962, 'train@eng.sdrt.stac_steps_per_second': 2.661, 'epoch': 4.0}
{'loss': 1.7474, 'learning_rate': 1.3333333333333333e-05, 'epoch': 4.0}
{'eval_loss': 1.610992193222046, 'eval_accuracy@eng.sdrt.stac': 0.4943231441048035, 'eval_f1@eng.sdrt.stac': 0.1985594589613457, 'eval_precision@eng.sdrt.stac': 0.2147849873971514, 'eval_recall@eng.sdrt.stac': 0.22519442660247804, 'eval_loss@eng.sdrt.stac': 1.610992193222046, 'eval_runtime': 13.829, 'eval_samples_per_second': 82.797, 'eval_steps_per_second': 2.603, 'epoch': 4.0}
{'train@eng.sdrt.stac_loss': 1.609460473060608, 'train@eng.sdrt.stac_accuracy@eng.sdrt.stac': 0.49478079331941544, 'train@eng.sdrt.stac_f1@eng.sdrt.stac': 0.2216752313814393, 'train@eng.sdrt.stac_precision@eng.sdrt.stac': 0.263039024314738, 'train@eng.sdrt.stac_recall@eng.sdrt.stac': 0.24681182034790788, 'train@eng.sdrt.stac_loss@eng.sdrt.stac': 1.6094605922698975, 'train@eng.sdrt.stac_runtime': 112.835, 'train@eng.sdrt.stac_samples_per_second': 84.903, 'train@eng.sdrt.stac_steps_per_second': 2.659, 'epoch': 5.0}
{'loss': 1.6838, 'learning_rate': 1.1666666666666668e-05, 'epoch': 5.0}
{'eval_loss': 1.5631259679794312, 'eval_accuracy@eng.sdrt.stac': 0.5109170305676856, 'eval_f1@eng.sdrt.stac': 0.2187011865780849, 'eval_precision@eng.sdrt.stac': 0.2333540314588718, 'eval_recall@eng.sdrt.stac': 0.24504022068265918, 'eval_loss@eng.sdrt.stac': 1.5631259679794312, 'eval_runtime': 13.8613, 'eval_samples_per_second': 82.604, 'eval_steps_per_second': 2.597, 'epoch': 5.0}
{'train@eng.sdrt.stac_loss': 1.5583618879318237, 'train@eng.sdrt.stac_accuracy@eng.sdrt.stac': 0.5063674321503131, 'train@eng.sdrt.stac_f1@eng.sdrt.stac': 0.2587298526761123, 'train@eng.sdrt.stac_precision@eng.sdrt.stac': 0.28159235384070386, 'train@eng.sdrt.stac_recall@eng.sdrt.stac': 0.2768564632627115, 'train@eng.sdrt.stac_loss@eng.sdrt.stac': 1.5583617687225342, 'train@eng.sdrt.stac_runtime': 113.386, 'train@eng.sdrt.stac_samples_per_second': 84.49, 'train@eng.sdrt.stac_steps_per_second': 2.646, 'epoch': 6.0}
{'loss': 1.6248, 'learning_rate': 1e-05, 'epoch': 6.0}
{'eval_loss': 1.5354888439178467, 'eval_accuracy@eng.sdrt.stac': 0.5021834061135371, 'eval_f1@eng.sdrt.stac': 0.24117690562653643, 'eval_precision@eng.sdrt.stac': 0.2828240369235203, 'eval_recall@eng.sdrt.stac': 0.25795898378918775, 'eval_loss@eng.sdrt.stac': 1.5354889631271362, 'eval_runtime': 13.9967, 'eval_samples_per_second': 81.805, 'eval_steps_per_second': 2.572, 'epoch': 6.0}
{'train@eng.sdrt.stac_loss': 1.5428708791732788, 'train@eng.sdrt.stac_accuracy@eng.sdrt.stac': 0.5156576200417536, 'train@eng.sdrt.stac_f1@eng.sdrt.stac': 0.2762340575252385, 'train@eng.sdrt.stac_precision@eng.sdrt.stac': 0.42481186976461216, 'train@eng.sdrt.stac_recall@eng.sdrt.stac': 0.29877548965153294, 'train@eng.sdrt.stac_loss@eng.sdrt.stac': 1.5428708791732788, 'train@eng.sdrt.stac_runtime': 113.4273, 'train@eng.sdrt.stac_samples_per_second': 84.459, 'train@eng.sdrt.stac_steps_per_second': 2.645, 'epoch': 7.0}
{'loss': 1.5887, 'learning_rate': 8.333333333333334e-06, 'epoch': 7.0}
{'eval_loss': 1.5270709991455078, 'eval_accuracy@eng.sdrt.stac': 0.5135371179039301, 'eval_f1@eng.sdrt.stac': 0.2553773102818702, 'eval_precision@eng.sdrt.stac': 0.33104314284792846, 'eval_recall@eng.sdrt.stac': 0.2766630540414723, 'eval_loss@eng.sdrt.stac': 1.5270709991455078, 'eval_runtime': 14.0139, 'eval_samples_per_second': 81.705, 'eval_steps_per_second': 2.569, 'epoch': 7.0}
{'train@eng.sdrt.stac_loss': 1.5005614757537842, 'train@eng.sdrt.stac_accuracy@eng.sdrt.stac': 0.5304801670146138, 'train@eng.sdrt.stac_f1@eng.sdrt.stac': 0.3038717584803067, 'train@eng.sdrt.stac_precision@eng.sdrt.stac': 0.4304328213226679, 'train@eng.sdrt.stac_recall@eng.sdrt.stac': 0.3204469315124443, 'train@eng.sdrt.stac_loss@eng.sdrt.stac': 1.5005614757537842, 'train@eng.sdrt.stac_runtime': 113.0777, 'train@eng.sdrt.stac_samples_per_second': 84.721, 'train@eng.sdrt.stac_steps_per_second': 2.653, 'epoch': 8.0}
{'loss': 1.563, 'learning_rate': 6.666666666666667e-06, 'epoch': 8.0}
{'eval_loss': 1.4866963624954224, 'eval_accuracy@eng.sdrt.stac': 0.5292576419213973, 'eval_f1@eng.sdrt.stac': 0.2672126890225349, 'eval_precision@eng.sdrt.stac': 0.33452046693663495, 'eval_recall@eng.sdrt.stac': 0.2882462708657453, 'eval_loss@eng.sdrt.stac': 1.4866963624954224, 'eval_runtime': 13.8516, 'eval_samples_per_second': 82.662, 'eval_steps_per_second': 2.599, 'epoch': 8.0}
{'train@eng.sdrt.stac_loss': 1.4835072755813599, 'train@eng.sdrt.stac_accuracy@eng.sdrt.stac': 0.5388308977035491, 'train@eng.sdrt.stac_f1@eng.sdrt.stac': 0.32707163510469073, 'train@eng.sdrt.stac_precision@eng.sdrt.stac': 0.4275527365029167, 'train@eng.sdrt.stac_recall@eng.sdrt.stac': 0.3389043699152609, 'train@eng.sdrt.stac_loss@eng.sdrt.stac': 1.4835072755813599, 'train@eng.sdrt.stac_runtime': 112.5893, 'train@eng.sdrt.stac_samples_per_second': 85.088, 'train@eng.sdrt.stac_steps_per_second': 2.665, 'epoch': 9.0}
{'loss': 1.5372, 'learning_rate': 5e-06, 'epoch': 9.0}
{'eval_loss': 1.4768439531326294, 'eval_accuracy@eng.sdrt.stac': 0.5283842794759825, 'eval_f1@eng.sdrt.stac': 0.28171455562473596, 'eval_precision@eng.sdrt.stac': 0.38914907097846685, 'eval_recall@eng.sdrt.stac': 0.2975617813739867, 'eval_loss@eng.sdrt.stac': 1.4768439531326294, 'eval_runtime': 13.8276, 'eval_samples_per_second': 82.805, 'eval_steps_per_second': 2.603, 'epoch': 9.0}
{'train@eng.sdrt.stac_loss': 1.466941237449646, 'train@eng.sdrt.stac_accuracy@eng.sdrt.stac': 0.544572025052192, 'train@eng.sdrt.stac_f1@eng.sdrt.stac': 0.3379144428142865, 'train@eng.sdrt.stac_precision@eng.sdrt.stac': 0.4302505124835742, 'train@eng.sdrt.stac_recall@eng.sdrt.stac': 0.3491685000660987, 'train@eng.sdrt.stac_loss@eng.sdrt.stac': 1.466941475868225, 'train@eng.sdrt.stac_runtime': 113.2856, 'train@eng.sdrt.stac_samples_per_second': 84.565, 'train@eng.sdrt.stac_steps_per_second': 2.648, 'epoch': 10.0}
{'loss': 1.5199, 'learning_rate': 3.3333333333333333e-06, 'epoch': 10.0}
{'eval_loss': 1.4630331993103027, 'eval_accuracy@eng.sdrt.stac': 0.5344978165938865, 'eval_f1@eng.sdrt.stac': 0.30733784666143726, 'eval_precision@eng.sdrt.stac': 0.4583556884143127, 'eval_recall@eng.sdrt.stac': 0.3159947078853962, 'eval_loss@eng.sdrt.stac': 1.4630331993103027, 'eval_runtime': 13.9955, 'eval_samples_per_second': 81.812, 'eval_steps_per_second': 2.572, 'epoch': 10.0}
{'train@eng.sdrt.stac_loss': 1.4590067863464355, 'train@eng.sdrt.stac_accuracy@eng.sdrt.stac': 0.547286012526096, 'train@eng.sdrt.stac_f1@eng.sdrt.stac': 0.34403943660972097, 'train@eng.sdrt.stac_precision@eng.sdrt.stac': 0.42809742427921327, 'train@eng.sdrt.stac_recall@eng.sdrt.stac': 0.3541585377864993, 'train@eng.sdrt.stac_loss@eng.sdrt.stac': 1.459006667137146, 'train@eng.sdrt.stac_runtime': 113.3785, 'train@eng.sdrt.stac_samples_per_second': 84.496, 'train@eng.sdrt.stac_steps_per_second': 2.646, 'epoch': 11.0}
{'loss': 1.5117, 'learning_rate': 1.6666666666666667e-06, 'epoch': 11.0}
{'eval_loss': 1.4574438333511353, 'eval_accuracy@eng.sdrt.stac': 0.5406113537117904, 'eval_f1@eng.sdrt.stac': 0.31932706819115014, 'eval_precision@eng.sdrt.stac': 0.45956326610077647, 'eval_recall@eng.sdrt.stac': 0.3245050322586403, 'eval_loss@eng.sdrt.stac': 1.4574438333511353, 'eval_runtime': 13.9849, 'eval_samples_per_second': 81.874, 'eval_steps_per_second': 2.574, 'epoch': 11.0}
{'train@eng.sdrt.stac_loss': 1.4575448036193848, 'train@eng.sdrt.stac_accuracy@eng.sdrt.stac': 0.5471816283924843, 'train@eng.sdrt.stac_f1@eng.sdrt.stac': 0.3449844312969611, 'train@eng.sdrt.stac_precision@eng.sdrt.stac': 0.4296560004168562, 'train@eng.sdrt.stac_recall@eng.sdrt.stac': 0.35451215691501675, 'train@eng.sdrt.stac_loss@eng.sdrt.stac': 1.4575446844100952, 'train@eng.sdrt.stac_runtime': 113.4764, 'train@eng.sdrt.stac_samples_per_second': 84.423, 'train@eng.sdrt.stac_steps_per_second': 2.644, 'epoch': 12.0}
{'loss': 1.5061, 'learning_rate': 0.0, 'epoch': 12.0}
{'eval_loss': 1.4577332735061646, 'eval_accuracy@eng.sdrt.stac': 0.5406113537117904, 'eval_f1@eng.sdrt.stac': 0.3202049143292613, 'eval_precision@eng.sdrt.stac': 0.4613370609068903, 'eval_recall@eng.sdrt.stac': 0.3259461539803983, 'eval_loss@eng.sdrt.stac': 1.457733154296875, 'eval_runtime': 13.9454, 'eval_samples_per_second': 82.106, 'eval_steps_per_second': 2.581, 'epoch': 12.0}
{'train_runtime': 4390.0196, 'train_samples_per_second': 26.187, 'train_steps_per_second': 0.82, 'train_loss': 1.7173303816053602, 'epoch': 12.0}
***** train_logger metrics *****
  epoch                    =       12.0
  train_loss               =     2.6696
  train_runtime            = 0:03:38.72
  train_samples_per_second =     24.085
  train_steps_per_second   =      0.768
-------------------------------------------------------------------
Lang1:  tur.pdtb.tdb    Lang2:  eng.sdrt.stac
Saving run to:  runs/full_shot/FullShot=v4_tur.pdtb.tdb_eng.sdrt.stac_lrfunc=Adafactor_lr=2e-05_plm=bert-base-multilingual-cased
Running with params: BERT_MODEL=bert-base-multilingual-cased lr=2e-05
read 2451 examples
read 312 examples
read 422 examples
read 9580 examples
read 1145 examples
read 1510 examples
Total prediction labels:  39
BertAdapterModel(
  (shared_parameters): ModuleDict()
  (bert): BertModel(
    (shared_parameters): ModuleDict()
    (invertible_adapters): ModuleDict(
      (tr): NICECouplingBlock(
        (F): Sequential(
          (0): Linear(in_features=384, out_features=192, bias=True)
          (1): Activation_Function_Class(
            (f): NewGELUActivation()
          )
          (2): Linear(in_features=192, out_features=384, bias=True)
        )
        (G): Sequential(
          (0): Linear(in_features=384, out_features=192, bias=True)
          (1): Activation_Function_Class(
            (f): NewGELUActivation()
          )
          (2): Linear(in_features=192, out_features=384, bias=True)
        )
      )
      (en): NICECouplingBlock(
        (F): Sequential(
          (0): Linear(in_features=384, out_features=192, bias=True)
          (1): Activation_Function_Class(
            (f): ReLU()
          )
          (2): Linear(in_features=192, out_features=384, bias=True)
        )
        (G): Sequential(
          (0): Linear(in_features=384, out_features=192, bias=True)
          (1): Activation_Function_Class(
            (f): ReLU()
          )
          (2): Linear(in_features=192, out_features=384, bias=True)
        )
      )
    )
    (embeddings): BertEmbeddings(
      (word_embeddings): Embedding(119547, 768, padding_idx=0)
      (position_embeddings): Embedding(512, 768)
      (token_type_embeddings): Embedding(2, 768)
      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
      (dropout): Dropout(p=0.1, inplace=False)
    )
    (encoder): BertEncoder(
      (layer): ModuleList(
        (0): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (tr): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (en): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (1): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (tr): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (en): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (2): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (tr): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (en): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (3): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (tr): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (en): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (4): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (tr): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (en): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (5): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (tr): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (en): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (6): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (tr): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (en): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (7): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (tr): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (en): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (8): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (tr): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (en): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (9): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (tr): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (en): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (10): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (tr): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (en): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (11): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (tr): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (en): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
      )
    )
    (pooler): BertPooler(
      (dense): Linear(in_features=768, out_features=768, bias=True)
      (activation): Tanh()
    )
    (prefix_tuning): PrefixTuningPool(
      (prefix_tunings): ModuleDict()
    )
  )
  (heads): ModuleDict(
    (tr): BertStyleMaskedLMHead(
      (0): Linear(in_features=768, out_features=768, bias=True)
      (1): Activation_Function_Class(
        (f): GELUActivation()
      )
      (2): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
      (3): Linear(in_features=768, out_features=119547, bias=True)
    )
    (disrpt): ClassificationHead(
      (0): Dropout(p=0.1, inplace=False)
      (1): Linear(in_features=768, out_features=768, bias=True)
      (2): Activation_Function_Class(
        (f): Tanh()
      )
      (3): Dropout(p=0.1, inplace=False)
      (4): Linear(in_features=768, out_features=39, bias=True)
    )
  )
)
{'train@tur.pdtb.tdb_loss': 2.8658671379089355, 'train@tur.pdtb.tdb_accuracy@tur.pdtb.tdb': 0.25050999592003265, 'train@tur.pdtb.tdb_f1@tur.pdtb.tdb': 0.01741967515426626, 'train@tur.pdtb.tdb_precision@tur.pdtb.tdb': 0.010891738953044898, 'train@tur.pdtb.tdb_recall@tur.pdtb.tdb': 0.043478260869565216, 'train@tur.pdtb.tdb_loss@tur.pdtb.tdb': 2.8658671379089355, 'train@tur.pdtb.tdb_runtime': 29.5293, 'train@tur.pdtb.tdb_samples_per_second': 83.002, 'train@tur.pdtb.tdb_steps_per_second': 2.608, 'epoch': 1.0}
{'loss': 3.2649, 'learning_rate': 1.8333333333333333e-05, 'epoch': 1.0}
{'eval_loss': 2.799389123916626, 'eval_accuracy@tur.pdtb.tdb': 0.266025641025641, 'eval_f1@tur.pdtb.tdb': 0.019102416570771, 'eval_precision@tur.pdtb.tdb': 0.012092074592074592, 'eval_recall@tur.pdtb.tdb': 0.045454545454545456, 'eval_loss@tur.pdtb.tdb': 2.799389123916626, 'eval_runtime': 4.1017, 'eval_samples_per_second': 76.066, 'eval_steps_per_second': 2.438, 'epoch': 1.0}
{'train@tur.pdtb.tdb_loss': 2.4454188346862793, 'train@tur.pdtb.tdb_accuracy@tur.pdtb.tdb': 0.25050999592003265, 'train@tur.pdtb.tdb_f1@tur.pdtb.tdb': 0.01741967515426626, 'train@tur.pdtb.tdb_precision@tur.pdtb.tdb': 0.010891738953044898, 'train@tur.pdtb.tdb_recall@tur.pdtb.tdb': 0.043478260869565216, 'train@tur.pdtb.tdb_loss@tur.pdtb.tdb': 2.4454188346862793, 'train@tur.pdtb.tdb_runtime': 29.6573, 'train@tur.pdtb.tdb_samples_per_second': 82.644, 'train@tur.pdtb.tdb_steps_per_second': 2.596, 'epoch': 2.0}
{'loss': 2.6407, 'learning_rate': 1.6666666666666667e-05, 'epoch': 2.0}
{'eval_loss': 2.341886043548584, 'eval_accuracy@tur.pdtb.tdb': 0.266025641025641, 'eval_f1@tur.pdtb.tdb': 0.019102416570771, 'eval_precision@tur.pdtb.tdb': 0.012092074592074592, 'eval_recall@tur.pdtb.tdb': 0.045454545454545456, 'eval_loss@tur.pdtb.tdb': 2.341886043548584, 'eval_runtime': 4.0929, 'eval_samples_per_second': 76.23, 'eval_steps_per_second': 2.443, 'epoch': 2.0}
{'train@tur.pdtb.tdb_loss': 2.3582940101623535, 'train@tur.pdtb.tdb_accuracy@tur.pdtb.tdb': 0.25010199918400655, 'train@tur.pdtb.tdb_f1@tur.pdtb.tdb': 0.01740834350949933, 'train@tur.pdtb.tdb_precision@tur.pdtb.tdb': 0.010887325944870702, 'train@tur.pdtb.tdb_recall@tur.pdtb.tdb': 0.043407449369777654, 'train@tur.pdtb.tdb_loss@tur.pdtb.tdb': 2.3582940101623535, 'train@tur.pdtb.tdb_runtime': 29.5659, 'train@tur.pdtb.tdb_samples_per_second': 82.9, 'train@tur.pdtb.tdb_steps_per_second': 2.604, 'epoch': 3.0}
{'loss': 2.425, 'learning_rate': 1.5000000000000002e-05, 'epoch': 3.0}
{'eval_loss': 2.2831764221191406, 'eval_accuracy@tur.pdtb.tdb': 0.266025641025641, 'eval_f1@tur.pdtb.tdb': 0.019102416570771, 'eval_precision@tur.pdtb.tdb': 0.012092074592074592, 'eval_recall@tur.pdtb.tdb': 0.045454545454545456, 'eval_loss@tur.pdtb.tdb': 2.2831764221191406, 'eval_runtime': 4.1708, 'eval_samples_per_second': 74.807, 'eval_steps_per_second': 2.398, 'epoch': 3.0}
{'train@tur.pdtb.tdb_loss': 2.2866106033325195, 'train@tur.pdtb.tdb_accuracy@tur.pdtb.tdb': 0.28192574459404324, 'train@tur.pdtb.tdb_f1@tur.pdtb.tdb': 0.04317418176626384, 'train@tur.pdtb.tdb_precision@tur.pdtb.tdb': 0.08492265296101327, 'train@tur.pdtb.tdb_recall@tur.pdtb.tdb': 0.061210905549020346, 'train@tur.pdtb.tdb_loss@tur.pdtb.tdb': 2.2866108417510986, 'train@tur.pdtb.tdb_runtime': 29.6269, 'train@tur.pdtb.tdb_samples_per_second': 82.729, 'train@tur.pdtb.tdb_steps_per_second': 2.599, 'epoch': 4.0}
{'loss': 2.3525, 'learning_rate': 1.3333333333333333e-05, 'epoch': 4.0}
{'eval_loss': 2.2455554008483887, 'eval_accuracy@tur.pdtb.tdb': 0.28205128205128205, 'eval_f1@tur.pdtb.tdb': 0.04457751368128726, 'eval_precision@tur.pdtb.tdb': 0.08313041125541125, 'eval_recall@tur.pdtb.tdb': 0.05973358675343084, 'eval_loss@tur.pdtb.tdb': 2.2455554008483887, 'eval_runtime': 4.1148, 'eval_samples_per_second': 75.825, 'eval_steps_per_second': 2.43, 'epoch': 4.0}
{'train@tur.pdtb.tdb_loss': 2.2391200065612793, 'train@tur.pdtb.tdb_accuracy@tur.pdtb.tdb': 0.31334149326805383, 'train@tur.pdtb.tdb_f1@tur.pdtb.tdb': 0.0786635931128929, 'train@tur.pdtb.tdb_precision@tur.pdtb.tdb': 0.090524568753394, 'train@tur.pdtb.tdb_recall@tur.pdtb.tdb': 0.09742461004325373, 'train@tur.pdtb.tdb_loss@tur.pdtb.tdb': 2.2391202449798584, 'train@tur.pdtb.tdb_runtime': 29.6519, 'train@tur.pdtb.tdb_samples_per_second': 82.659, 'train@tur.pdtb.tdb_steps_per_second': 2.597, 'epoch': 5.0}
{'loss': 2.3042, 'learning_rate': 1.1666666666666668e-05, 'epoch': 5.0}
{'eval_loss': 2.2111995220184326, 'eval_accuracy@tur.pdtb.tdb': 0.2980769230769231, 'eval_f1@tur.pdtb.tdb': 0.07109597142384028, 'eval_precision@tur.pdtb.tdb': 0.06457362181570557, 'eval_recall@tur.pdtb.tdb': 0.09968304590912669, 'eval_loss@tur.pdtb.tdb': 2.2111995220184326, 'eval_runtime': 4.0906, 'eval_samples_per_second': 76.273, 'eval_steps_per_second': 2.445, 'epoch': 5.0}
{'train@tur.pdtb.tdb_loss': 2.1932685375213623, 'train@tur.pdtb.tdb_accuracy@tur.pdtb.tdb': 0.3215014279885761, 'train@tur.pdtb.tdb_f1@tur.pdtb.tdb': 0.0810514456893088, 'train@tur.pdtb.tdb_precision@tur.pdtb.tdb': 0.10805679834069847, 'train@tur.pdtb.tdb_recall@tur.pdtb.tdb': 0.10360029533773475, 'train@tur.pdtb.tdb_loss@tur.pdtb.tdb': 2.1932690143585205, 'train@tur.pdtb.tdb_runtime': 29.5753, 'train@tur.pdtb.tdb_samples_per_second': 82.873, 'train@tur.pdtb.tdb_steps_per_second': 2.604, 'epoch': 6.0}
{'loss': 2.2544, 'learning_rate': 1e-05, 'epoch': 6.0}
{'eval_loss': 2.182478904724121, 'eval_accuracy@tur.pdtb.tdb': 0.2980769230769231, 'eval_f1@tur.pdtb.tdb': 0.07397388261847254, 'eval_precision@tur.pdtb.tdb': 0.06641669733774996, 'eval_recall@tur.pdtb.tdb': 0.1012615524486538, 'eval_loss@tur.pdtb.tdb': 2.182478666305542, 'eval_runtime': 4.1084, 'eval_samples_per_second': 75.941, 'eval_steps_per_second': 2.434, 'epoch': 6.0}
{'train@tur.pdtb.tdb_loss': 2.1636335849761963, 'train@tur.pdtb.tdb_accuracy@tur.pdtb.tdb': 0.3288453692370461, 'train@tur.pdtb.tdb_f1@tur.pdtb.tdb': 0.08491467637412202, 'train@tur.pdtb.tdb_precision@tur.pdtb.tdb': 0.10285314819659577, 'train@tur.pdtb.tdb_recall@tur.pdtb.tdb': 0.10851916809480207, 'train@tur.pdtb.tdb_loss@tur.pdtb.tdb': 2.1636335849761963, 'train@tur.pdtb.tdb_runtime': 29.6282, 'train@tur.pdtb.tdb_samples_per_second': 82.725, 'train@tur.pdtb.tdb_steps_per_second': 2.599, 'epoch': 7.0}
{'loss': 2.2219, 'learning_rate': 8.333333333333334e-06, 'epoch': 7.0}
{'eval_loss': 2.1651322841644287, 'eval_accuracy@tur.pdtb.tdb': 0.2948717948717949, 'eval_f1@tur.pdtb.tdb': 0.07094272541377865, 'eval_precision@tur.pdtb.tdb': 0.06193263022319133, 'eval_recall@tur.pdtb.tdb': 0.10150316059245895, 'eval_loss@tur.pdtb.tdb': 2.1651322841644287, 'eval_runtime': 4.1191, 'eval_samples_per_second': 75.744, 'eval_steps_per_second': 2.428, 'epoch': 7.0}
{'train@tur.pdtb.tdb_loss': 2.145259141921997, 'train@tur.pdtb.tdb_accuracy@tur.pdtb.tdb': 0.3349653202774378, 'train@tur.pdtb.tdb_f1@tur.pdtb.tdb': 0.08847644228200059, 'train@tur.pdtb.tdb_precision@tur.pdtb.tdb': 0.09823014694263638, 'train@tur.pdtb.tdb_recall@tur.pdtb.tdb': 0.1112528428420286, 'train@tur.pdtb.tdb_loss@tur.pdtb.tdb': 2.145259380340576, 'train@tur.pdtb.tdb_runtime': 29.7442, 'train@tur.pdtb.tdb_samples_per_second': 82.403, 'train@tur.pdtb.tdb_steps_per_second': 2.589, 'epoch': 8.0}
{'loss': 2.1975, 'learning_rate': 6.666666666666667e-06, 'epoch': 8.0}
{'eval_loss': 2.1518735885620117, 'eval_accuracy@tur.pdtb.tdb': 0.2948717948717949, 'eval_f1@tur.pdtb.tdb': 0.07120325737666838, 'eval_precision@tur.pdtb.tdb': 0.06238384961409044, 'eval_recall@tur.pdtb.tdb': 0.10150316059245895, 'eval_loss@tur.pdtb.tdb': 2.1518735885620117, 'eval_runtime': 4.1314, 'eval_samples_per_second': 75.52, 'eval_steps_per_second': 2.421, 'epoch': 8.0}
{'train@tur.pdtb.tdb_loss': 2.125016689300537, 'train@tur.pdtb.tdb_accuracy@tur.pdtb.tdb': 0.33659730722154224, 'train@tur.pdtb.tdb_f1@tur.pdtb.tdb': 0.0874347770613694, 'train@tur.pdtb.tdb_precision@tur.pdtb.tdb': 0.09983265172528126, 'train@tur.pdtb.tdb_recall@tur.pdtb.tdb': 0.11255241812858477, 'train@tur.pdtb.tdb_loss@tur.pdtb.tdb': 2.125016450881958, 'train@tur.pdtb.tdb_runtime': 29.761, 'train@tur.pdtb.tdb_samples_per_second': 82.356, 'train@tur.pdtb.tdb_steps_per_second': 2.587, 'epoch': 9.0}
{'loss': 2.1768, 'learning_rate': 5e-06, 'epoch': 9.0}
{'eval_loss': 2.136381149291992, 'eval_accuracy@tur.pdtb.tdb': 0.2916666666666667, 'eval_f1@tur.pdtb.tdb': 0.06591353851627824, 'eval_precision@tur.pdtb.tdb': 0.05592754946203222, 'eval_recall@tur.pdtb.tdb': 0.10102978759959624, 'eval_loss@tur.pdtb.tdb': 2.136381149291992, 'eval_runtime': 4.1498, 'eval_samples_per_second': 75.185, 'eval_steps_per_second': 2.41, 'epoch': 9.0}
{'train@tur.pdtb.tdb_loss': 2.113452196121216, 'train@tur.pdtb.tdb_accuracy@tur.pdtb.tdb': 0.3382292941656467, 'train@tur.pdtb.tdb_f1@tur.pdtb.tdb': 0.08955558397136419, 'train@tur.pdtb.tdb_precision@tur.pdtb.tdb': 0.09512708659627524, 'train@tur.pdtb.tdb_recall@tur.pdtb.tdb': 0.11361792692264085, 'train@tur.pdtb.tdb_loss@tur.pdtb.tdb': 2.113452196121216, 'train@tur.pdtb.tdb_runtime': 29.8047, 'train@tur.pdtb.tdb_samples_per_second': 82.235, 'train@tur.pdtb.tdb_steps_per_second': 2.583, 'epoch': 10.0}
{'loss': 2.1652, 'learning_rate': 3.3333333333333333e-06, 'epoch': 10.0}
{'eval_loss': 2.1281991004943848, 'eval_accuracy@tur.pdtb.tdb': 0.2916666666666667, 'eval_f1@tur.pdtb.tdb': 0.06599832915622389, 'eval_precision@tur.pdtb.tdb': 0.05524767378215654, 'eval_recall@tur.pdtb.tdb': 0.10095551546650058, 'eval_loss@tur.pdtb.tdb': 2.1281988620758057, 'eval_runtime': 4.1365, 'eval_samples_per_second': 75.426, 'eval_steps_per_second': 2.417, 'epoch': 10.0}
{'train@tur.pdtb.tdb_loss': 2.108269453048706, 'train@tur.pdtb.tdb_accuracy@tur.pdtb.tdb': 0.33700530395756834, 'train@tur.pdtb.tdb_f1@tur.pdtb.tdb': 0.08923285305650196, 'train@tur.pdtb.tdb_precision@tur.pdtb.tdb': 0.09486805735150942, 'train@tur.pdtb.tdb_recall@tur.pdtb.tdb': 0.11284051648778402, 'train@tur.pdtb.tdb_loss@tur.pdtb.tdb': 2.108269453048706, 'train@tur.pdtb.tdb_runtime': 29.7344, 'train@tur.pdtb.tdb_samples_per_second': 82.43, 'train@tur.pdtb.tdb_steps_per_second': 2.59, 'epoch': 11.0}
{'loss': 2.1592, 'learning_rate': 1.6666666666666667e-06, 'epoch': 11.0}
{'eval_loss': 2.125670909881592, 'eval_accuracy@tur.pdtb.tdb': 0.2916666666666667, 'eval_f1@tur.pdtb.tdb': 0.06733324935963082, 'eval_precision@tur.pdtb.tdb': 0.056448805868535075, 'eval_recall@tur.pdtb.tdb': 0.10095551546650058, 'eval_loss@tur.pdtb.tdb': 2.125670909881592, 'eval_runtime': 4.1472, 'eval_samples_per_second': 75.231, 'eval_steps_per_second': 2.411, 'epoch': 11.0}
{'train@tur.pdtb.tdb_loss': 2.105335235595703, 'train@tur.pdtb.tdb_accuracy@tur.pdtb.tdb': 0.33700530395756834, 'train@tur.pdtb.tdb_f1@tur.pdtb.tdb': 0.08970971560538167, 'train@tur.pdtb.tdb_precision@tur.pdtb.tdb': 0.09398172302433404, 'train@tur.pdtb.tdb_recall@tur.pdtb.tdb': 0.11309652930039159, 'train@tur.pdtb.tdb_loss@tur.pdtb.tdb': 2.105334997177124, 'train@tur.pdtb.tdb_runtime': 29.7773, 'train@tur.pdtb.tdb_samples_per_second': 82.311, 'train@tur.pdtb.tdb_steps_per_second': 2.586, 'epoch': 12.0}
{'loss': 2.1468, 'learning_rate': 0.0, 'epoch': 12.0}
{'eval_loss': 2.1231586933135986, 'eval_accuracy@tur.pdtb.tdb': 0.2916666666666667, 'eval_f1@tur.pdtb.tdb': 0.06733324935963082, 'eval_precision@tur.pdtb.tdb': 0.056448805868535075, 'eval_recall@tur.pdtb.tdb': 0.10095551546650058, 'eval_loss@tur.pdtb.tdb': 2.1231586933135986, 'eval_runtime': 4.1381, 'eval_samples_per_second': 75.397, 'eval_steps_per_second': 2.417, 'epoch': 12.0}
{'train_runtime': 1153.9097, 'train_samples_per_second': 25.489, 'train_steps_per_second': 0.801, 'train_loss': 2.359097237194771, 'epoch': 12.0}
***** train_log metrics *****
  epoch                    =       12.0
  train_loss               =     2.3591
  train_runtime            = 0:19:13.90
  train_samples_per_second =     25.489
  train_steps_per_second   =      0.801
{'train@eng.sdrt.stac_loss': 2.0375404357910156, 'train@eng.sdrt.stac_accuracy@eng.sdrt.stac': 0.35083507306889355, 'train@eng.sdrt.stac_f1@eng.sdrt.stac': 0.06872631533682487, 'train@eng.sdrt.stac_precision@eng.sdrt.stac': 0.10029234666621012, 'train@eng.sdrt.stac_recall@eng.sdrt.stac': 0.1105515319459777, 'train@eng.sdrt.stac_loss@eng.sdrt.stac': 2.0375404357910156, 'train@eng.sdrt.stac_runtime': 113.4086, 'train@eng.sdrt.stac_samples_per_second': 84.473, 'train@eng.sdrt.stac_steps_per_second': 2.645, 'epoch': 1.0}
{'loss': 2.3695, 'learning_rate': 1.8333333333333333e-05, 'epoch': 1.0}
{'eval_loss': 1.9928523302078247, 'eval_accuracy@eng.sdrt.stac': 0.3572052401746725, 'eval_f1@eng.sdrt.stac': 0.06762981995752648, 'eval_precision@eng.sdrt.stac': 0.06579465585429163, 'eval_recall@eng.sdrt.stac': 0.11186135551694745, 'eval_loss@eng.sdrt.stac': 1.9928520917892456, 'eval_runtime': 13.9164, 'eval_samples_per_second': 82.277, 'eval_steps_per_second': 2.587, 'epoch': 1.0}
{'train@eng.sdrt.stac_loss': 1.80568265914917, 'train@eng.sdrt.stac_accuracy@eng.sdrt.stac': 0.44457202505219207, 'train@eng.sdrt.stac_f1@eng.sdrt.stac': 0.15283800156849056, 'train@eng.sdrt.stac_precision@eng.sdrt.stac': 0.18532867795972413, 'train@eng.sdrt.stac_recall@eng.sdrt.stac': 0.19043038462735218, 'train@eng.sdrt.stac_loss@eng.sdrt.stac': 1.8056827783584595, 'train@eng.sdrt.stac_runtime': 112.7044, 'train@eng.sdrt.stac_samples_per_second': 85.001, 'train@eng.sdrt.stac_steps_per_second': 2.662, 'epoch': 2.0}
{'loss': 1.9609, 'learning_rate': 1.6666666666666667e-05, 'epoch': 2.0}
{'eval_loss': 1.7672812938690186, 'eval_accuracy@eng.sdrt.stac': 0.4462882096069869, 'eval_f1@eng.sdrt.stac': 0.1487315905233766, 'eval_precision@eng.sdrt.stac': 0.17460902674304518, 'eval_recall@eng.sdrt.stac': 0.18933988107984523, 'eval_loss@eng.sdrt.stac': 1.7672812938690186, 'eval_runtime': 13.8544, 'eval_samples_per_second': 82.645, 'eval_steps_per_second': 2.598, 'epoch': 2.0}
{'train@eng.sdrt.stac_loss': 1.7025928497314453, 'train@eng.sdrt.stac_accuracy@eng.sdrt.stac': 0.46795407098121083, 'train@eng.sdrt.stac_f1@eng.sdrt.stac': 0.18214943471152067, 'train@eng.sdrt.stac_precision@eng.sdrt.stac': 0.1949684861438406, 'train@eng.sdrt.stac_recall@eng.sdrt.stac': 0.21909673903698934, 'train@eng.sdrt.stac_loss@eng.sdrt.stac': 1.7025929689407349, 'train@eng.sdrt.stac_runtime': 112.8492, 'train@eng.sdrt.stac_samples_per_second': 84.892, 'train@eng.sdrt.stac_steps_per_second': 2.658, 'epoch': 3.0}
{'loss': 1.7933, 'learning_rate': 1.5000000000000002e-05, 'epoch': 3.0}
{'eval_loss': 1.6649690866470337, 'eval_accuracy@eng.sdrt.stac': 0.4698689956331878, 'eval_f1@eng.sdrt.stac': 0.17646789182467196, 'eval_precision@eng.sdrt.stac': 0.22175095569658257, 'eval_recall@eng.sdrt.stac': 0.2138592817804158, 'eval_loss@eng.sdrt.stac': 1.6649690866470337, 'eval_runtime': 13.8677, 'eval_samples_per_second': 82.566, 'eval_steps_per_second': 2.596, 'epoch': 3.0}
{'train@eng.sdrt.stac_loss': 1.6331099271774292, 'train@eng.sdrt.stac_accuracy@eng.sdrt.stac': 0.49050104384133614, 'train@eng.sdrt.stac_f1@eng.sdrt.stac': 0.23204775989137516, 'train@eng.sdrt.stac_precision@eng.sdrt.stac': 0.32979269262978084, 'train@eng.sdrt.stac_recall@eng.sdrt.stac': 0.25203698198749314, 'train@eng.sdrt.stac_loss@eng.sdrt.stac': 1.6331098079681396, 'train@eng.sdrt.stac_runtime': 113.3623, 'train@eng.sdrt.stac_samples_per_second': 84.508, 'train@eng.sdrt.stac_steps_per_second': 2.646, 'epoch': 4.0}
{'loss': 1.7093, 'learning_rate': 1.3333333333333333e-05, 'epoch': 4.0}
{'eval_loss': 1.5925806760787964, 'eval_accuracy@eng.sdrt.stac': 0.4882096069868996, 'eval_f1@eng.sdrt.stac': 0.21751224023120716, 'eval_precision@eng.sdrt.stac': 0.27113762161659444, 'eval_recall@eng.sdrt.stac': 0.2366164253045638, 'eval_loss@eng.sdrt.stac': 1.592580795288086, 'eval_runtime': 13.9082, 'eval_samples_per_second': 82.326, 'eval_steps_per_second': 2.588, 'epoch': 4.0}
{'train@eng.sdrt.stac_loss': 1.587292194366455, 'train@eng.sdrt.stac_accuracy@eng.sdrt.stac': 0.5078288100208769, 'train@eng.sdrt.stac_f1@eng.sdrt.stac': 0.2807804358418266, 'train@eng.sdrt.stac_precision@eng.sdrt.stac': 0.32294210282438984, 'train@eng.sdrt.stac_recall@eng.sdrt.stac': 0.2984074177612704, 'train@eng.sdrt.stac_loss@eng.sdrt.stac': 1.587292194366455, 'train@eng.sdrt.stac_runtime': 113.1235, 'train@eng.sdrt.stac_samples_per_second': 84.686, 'train@eng.sdrt.stac_steps_per_second': 2.652, 'epoch': 5.0}
{'loss': 1.6619, 'learning_rate': 1.1666666666666668e-05, 'epoch': 5.0}
{'eval_loss': 1.5514978170394897, 'eval_accuracy@eng.sdrt.stac': 0.5074235807860262, 'eval_f1@eng.sdrt.stac': 0.26573498012906893, 'eval_precision@eng.sdrt.stac': 0.3266435303328333, 'eval_recall@eng.sdrt.stac': 0.2793900232868768, 'eval_loss@eng.sdrt.stac': 1.5514976978302002, 'eval_runtime': 13.9087, 'eval_samples_per_second': 82.322, 'eval_steps_per_second': 2.588, 'epoch': 5.0}
{'train@eng.sdrt.stac_loss': 1.5485897064208984, 'train@eng.sdrt.stac_accuracy@eng.sdrt.stac': 0.515866388308977, 'train@eng.sdrt.stac_f1@eng.sdrt.stac': 0.30216683964247737, 'train@eng.sdrt.stac_precision@eng.sdrt.stac': 0.3715083128908403, 'train@eng.sdrt.stac_recall@eng.sdrt.stac': 0.31458187479012945, 'train@eng.sdrt.stac_loss@eng.sdrt.stac': 1.548589825630188, 'train@eng.sdrt.stac_runtime': 112.9895, 'train@eng.sdrt.stac_samples_per_second': 84.787, 'train@eng.sdrt.stac_steps_per_second': 2.655, 'epoch': 6.0}
{'loss': 1.6126, 'learning_rate': 1e-05, 'epoch': 6.0}
{'eval_loss': 1.5285577774047852, 'eval_accuracy@eng.sdrt.stac': 0.5065502183406113, 'eval_f1@eng.sdrt.stac': 0.2670810012176937, 'eval_precision@eng.sdrt.stac': 0.3001810131698469, 'eval_recall@eng.sdrt.stac': 0.2780867032479023, 'eval_loss@eng.sdrt.stac': 1.5285576581954956, 'eval_runtime': 13.8216, 'eval_samples_per_second': 82.841, 'eval_steps_per_second': 2.605, 'epoch': 6.0}
{'train@eng.sdrt.stac_loss': 1.5298042297363281, 'train@eng.sdrt.stac_accuracy@eng.sdrt.stac': 0.5248434237995825, 'train@eng.sdrt.stac_f1@eng.sdrt.stac': 0.3155513305175748, 'train@eng.sdrt.stac_precision@eng.sdrt.stac': 0.36416095595807907, 'train@eng.sdrt.stac_recall@eng.sdrt.stac': 0.329538493074409, 'train@eng.sdrt.stac_loss@eng.sdrt.stac': 1.5298044681549072, 'train@eng.sdrt.stac_runtime': 112.5967, 'train@eng.sdrt.stac_samples_per_second': 85.082, 'train@eng.sdrt.stac_steps_per_second': 2.664, 'epoch': 7.0}
{'loss': 1.5825, 'learning_rate': 8.333333333333334e-06, 'epoch': 7.0}
{'eval_loss': 1.5143471956253052, 'eval_accuracy@eng.sdrt.stac': 0.5275109170305677, 'eval_f1@eng.sdrt.stac': 0.29875289549182255, 'eval_precision@eng.sdrt.stac': 0.3837504420516288, 'eval_recall@eng.sdrt.stac': 0.30701001481963724, 'eval_loss@eng.sdrt.stac': 1.5143470764160156, 'eval_runtime': 13.8579, 'eval_samples_per_second': 82.624, 'eval_steps_per_second': 2.598, 'epoch': 7.0}
{'train@eng.sdrt.stac_loss': 1.5008065700531006, 'train@eng.sdrt.stac_accuracy@eng.sdrt.stac': 0.5340292275574112, 'train@eng.sdrt.stac_f1@eng.sdrt.stac': 0.32712413039549426, 'train@eng.sdrt.stac_precision@eng.sdrt.stac': 0.36625801328366414, 'train@eng.sdrt.stac_recall@eng.sdrt.stac': 0.34088434610698015, 'train@eng.sdrt.stac_loss@eng.sdrt.stac': 1.5008065700531006, 'train@eng.sdrt.stac_runtime': 115.9076, 'train@eng.sdrt.stac_samples_per_second': 82.652, 'train@eng.sdrt.stac_steps_per_second': 2.588, 'epoch': 8.0}
{'loss': 1.563, 'learning_rate': 6.666666666666667e-06, 'epoch': 8.0}
{'eval_loss': 1.4843906164169312, 'eval_accuracy@eng.sdrt.stac': 0.5292576419213973, 'eval_f1@eng.sdrt.stac': 0.3180517947988227, 'eval_precision@eng.sdrt.stac': 0.3854210129235148, 'eval_recall@eng.sdrt.stac': 0.3290757018132795, 'eval_loss@eng.sdrt.stac': 1.4843906164169312, 'eval_runtime': 13.8732, 'eval_samples_per_second': 82.533, 'eval_steps_per_second': 2.595, 'epoch': 8.0}
{'train@eng.sdrt.stac_loss': 1.4879870414733887, 'train@eng.sdrt.stac_accuracy@eng.sdrt.stac': 0.5381002087682673, 'train@eng.sdrt.stac_f1@eng.sdrt.stac': 0.3350911306010018, 'train@eng.sdrt.stac_precision@eng.sdrt.stac': 0.3638981387605761, 'train@eng.sdrt.stac_recall@eng.sdrt.stac': 0.3494940757229006, 'train@eng.sdrt.stac_loss@eng.sdrt.stac': 1.4879871606826782, 'train@eng.sdrt.stac_runtime': 113.2028, 'train@eng.sdrt.stac_samples_per_second': 84.627, 'train@eng.sdrt.stac_steps_per_second': 2.65, 'epoch': 9.0}
{'loss': 1.5373, 'learning_rate': 5e-06, 'epoch': 9.0}
{'eval_loss': 1.476090908050537, 'eval_accuracy@eng.sdrt.stac': 0.5275109170305677, 'eval_f1@eng.sdrt.stac': 0.31726146867769917, 'eval_precision@eng.sdrt.stac': 0.3490803415957266, 'eval_recall@eng.sdrt.stac': 0.3277781736753145, 'eval_loss@eng.sdrt.stac': 1.4760907888412476, 'eval_runtime': 13.932, 'eval_samples_per_second': 82.185, 'eval_steps_per_second': 2.584, 'epoch': 9.0}
{'train@eng.sdrt.stac_loss': 1.4731703996658325, 'train@eng.sdrt.stac_accuracy@eng.sdrt.stac': 0.5408141962421712, 'train@eng.sdrt.stac_f1@eng.sdrt.stac': 0.3365662253272484, 'train@eng.sdrt.stac_precision@eng.sdrt.stac': 0.36461901589558815, 'train@eng.sdrt.stac_recall@eng.sdrt.stac': 0.3528709468879505, 'train@eng.sdrt.stac_loss@eng.sdrt.stac': 1.4731703996658325, 'train@eng.sdrt.stac_runtime': 113.2772, 'train@eng.sdrt.stac_samples_per_second': 84.571, 'train@eng.sdrt.stac_steps_per_second': 2.648, 'epoch': 10.0}
{'loss': 1.5256, 'learning_rate': 3.3333333333333333e-06, 'epoch': 10.0}
{'eval_loss': 1.466838002204895, 'eval_accuracy@eng.sdrt.stac': 0.5327510917030568, 'eval_f1@eng.sdrt.stac': 0.3244601061131576, 'eval_precision@eng.sdrt.stac': 0.3841757996398961, 'eval_recall@eng.sdrt.stac': 0.33790395764056413, 'eval_loss@eng.sdrt.stac': 1.466838002204895, 'eval_runtime': 13.8978, 'eval_samples_per_second': 82.387, 'eval_steps_per_second': 2.59, 'epoch': 10.0}
{'train@eng.sdrt.stac_loss': 1.4669421911239624, 'train@eng.sdrt.stac_accuracy@eng.sdrt.stac': 0.5426931106471816, 'train@eng.sdrt.stac_f1@eng.sdrt.stac': 0.33971356837624633, 'train@eng.sdrt.stac_precision@eng.sdrt.stac': 0.3635372202047397, 'train@eng.sdrt.stac_recall@eng.sdrt.stac': 0.35538887347861126, 'train@eng.sdrt.stac_loss@eng.sdrt.stac': 1.4669421911239624, 'train@eng.sdrt.stac_runtime': 112.8524, 'train@eng.sdrt.stac_samples_per_second': 84.89, 'train@eng.sdrt.stac_steps_per_second': 2.658, 'epoch': 11.0}
{'loss': 1.5172, 'learning_rate': 1.6666666666666667e-06, 'epoch': 11.0}
{'eval_loss': 1.4620424509048462, 'eval_accuracy@eng.sdrt.stac': 0.537117903930131, 'eval_f1@eng.sdrt.stac': 0.32766749697142145, 'eval_precision@eng.sdrt.stac': 0.38539033083393665, 'eval_recall@eng.sdrt.stac': 0.3414732191185844, 'eval_loss@eng.sdrt.stac': 1.4620424509048462, 'eval_runtime': 13.8577, 'eval_samples_per_second': 82.626, 'eval_steps_per_second': 2.598, 'epoch': 11.0}
{'train@eng.sdrt.stac_loss': 1.4650336503982544, 'train@eng.sdrt.stac_accuracy@eng.sdrt.stac': 0.5425887265135699, 'train@eng.sdrt.stac_f1@eng.sdrt.stac': 0.33933764234858843, 'train@eng.sdrt.stac_precision@eng.sdrt.stac': 0.3620799062332266, 'train@eng.sdrt.stac_recall@eng.sdrt.stac': 0.35569308547970035, 'train@eng.sdrt.stac_loss@eng.sdrt.stac': 1.4650334119796753, 'train@eng.sdrt.stac_runtime': 113.0389, 'train@eng.sdrt.stac_samples_per_second': 84.75, 'train@eng.sdrt.stac_steps_per_second': 2.654, 'epoch': 12.0}
{'loss': 1.5131, 'learning_rate': 0.0, 'epoch': 12.0}
{'eval_loss': 1.4615575075149536, 'eval_accuracy@eng.sdrt.stac': 0.5353711790393013, 'eval_f1@eng.sdrt.stac': 0.32558917145699084, 'eval_precision@eng.sdrt.stac': 0.38362825917660925, 'eval_recall@eng.sdrt.stac': 0.3391771915069041, 'eval_loss@eng.sdrt.stac': 1.461557388305664, 'eval_runtime': 13.909, 'eval_samples_per_second': 82.321, 'eval_steps_per_second': 2.588, 'epoch': 12.0}
{'train_runtime': 4405.6997, 'train_samples_per_second': 26.093, 'train_steps_per_second': 0.817, 'train_loss': 1.6955247921413845, 'epoch': 12.0}
***** train_logger metrics *****
  epoch                    =       12.0
  train_loss               =     2.3591
  train_runtime            = 0:19:13.90
  train_samples_per_second =     25.489
  train_steps_per_second   =      0.801
-------------------------------------------------------------------
Lang1:  zho.rst.sctb    Lang2:  eng.sdrt.stac
Saving run to:  runs/full_shot/FullShot=v4_zho.rst.sctb_eng.sdrt.stac_lrfunc=Adafactor_lr=2e-05_plm=bert-base-multilingual-cased
Running with params: BERT_MODEL=bert-base-multilingual-cased lr=2e-05
read 439 examples
read 94 examples
read 159 examples
read 9580 examples
read 1145 examples
read 1510 examples
Total prediction labels:  42
BertAdapterModel(
  (shared_parameters): ModuleDict()
  (bert): BertModel(
    (shared_parameters): ModuleDict()
    (invertible_adapters): ModuleDict(
      (zh): NICECouplingBlock(
        (F): Sequential(
          (0): Linear(in_features=384, out_features=192, bias=True)
          (1): Activation_Function_Class(
            (f): ReLU()
          )
          (2): Linear(in_features=192, out_features=384, bias=True)
        )
        (G): Sequential(
          (0): Linear(in_features=384, out_features=192, bias=True)
          (1): Activation_Function_Class(
            (f): ReLU()
          )
          (2): Linear(in_features=192, out_features=384, bias=True)
        )
      )
      (en): NICECouplingBlock(
        (F): Sequential(
          (0): Linear(in_features=384, out_features=192, bias=True)
          (1): Activation_Function_Class(
            (f): ReLU()
          )
          (2): Linear(in_features=192, out_features=384, bias=True)
        )
        (G): Sequential(
          (0): Linear(in_features=384, out_features=192, bias=True)
          (1): Activation_Function_Class(
            (f): ReLU()
          )
          (2): Linear(in_features=192, out_features=384, bias=True)
        )
      )
    )
    (embeddings): BertEmbeddings(
      (word_embeddings): Embedding(119547, 768, padding_idx=0)
      (position_embeddings): Embedding(512, 768)
      (token_type_embeddings): Embedding(2, 768)
      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
      (dropout): Dropout(p=0.1, inplace=False)
    )
    (encoder): BertEncoder(
      (layer): ModuleList(
        (0): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (zh): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (en): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (1): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (zh): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (en): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (2): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (zh): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (en): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (3): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (zh): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (en): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (4): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (zh): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (en): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (5): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (zh): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (en): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (6): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (zh): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (en): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (7): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (zh): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (en): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (8): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (zh): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (en): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (9): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (zh): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (en): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (10): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (zh): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (en): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (11): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (zh): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (en): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
      )
    )
    (pooler): BertPooler(
      (dense): Linear(in_features=768, out_features=768, bias=True)
      (activation): Tanh()
    )
    (prefix_tuning): PrefixTuningPool(
      (prefix_tunings): ModuleDict()
    )
  )
  (heads): ModuleDict(
    (disrpt): ClassificationHead(
      (0): Dropout(p=0.1, inplace=False)
      (1): Linear(in_features=768, out_features=768, bias=True)
      (2): Activation_Function_Class(
        (f): Tanh()
      )
      (3): Dropout(p=0.1, inplace=False)
      (4): Linear(in_features=768, out_features=42, bias=True)
    )
  )
)
{'train@zho.rst.sctb_loss': 3.504791021347046, 'train@zho.rst.sctb_accuracy@zho.rst.sctb': 0.33712984054669703, 'train@zho.rst.sctb_f1@zho.rst.sctb': 0.02084979793596794, 'train@zho.rst.sctb_precision@zho.rst.sctb': 0.051311388839992964, 'train@zho.rst.sctb_recall@zho.rst.sctb': 0.039271255060728746, 'train@zho.rst.sctb_loss@zho.rst.sctb': 3.504791259765625, 'train@zho.rst.sctb_runtime': 5.4927, 'train@zho.rst.sctb_samples_per_second': 79.925, 'train@zho.rst.sctb_steps_per_second': 2.549, 'epoch': 1.0}
{'loss': 3.6482, 'learning_rate': 1.8333333333333333e-05, 'epoch': 1.0}
{'eval_loss': 3.5317864418029785, 'eval_accuracy@zho.rst.sctb': 0.32978723404255317, 'eval_f1@zho.rst.sctb': 0.026105263157894736, 'eval_precision@zho.rst.sctb': 0.017543859649122806, 'eval_recall@zho.rst.sctb': 0.05098684210526316, 'eval_loss@zho.rst.sctb': 3.5317864418029785, 'eval_runtime': 1.4597, 'eval_samples_per_second': 64.399, 'eval_steps_per_second': 2.055, 'epoch': 1.0}
{'train@zho.rst.sctb_loss': 3.290132999420166, 'train@zho.rst.sctb_accuracy@zho.rst.sctb': 0.33712984054669703, 'train@zho.rst.sctb_f1@zho.rst.sctb': 0.02084979793596794, 'train@zho.rst.sctb_precision@zho.rst.sctb': 0.051311388839992964, 'train@zho.rst.sctb_recall@zho.rst.sctb': 0.039271255060728746, 'train@zho.rst.sctb_loss@zho.rst.sctb': 3.290132761001587, 'train@zho.rst.sctb_runtime': 5.5219, 'train@zho.rst.sctb_samples_per_second': 79.501, 'train@zho.rst.sctb_steps_per_second': 2.535, 'epoch': 2.0}
{'loss': 3.4178, 'learning_rate': 1.6666666666666667e-05, 'epoch': 2.0}
{'eval_loss': 3.324038505554199, 'eval_accuracy@zho.rst.sctb': 0.32978723404255317, 'eval_f1@zho.rst.sctb': 0.026105263157894736, 'eval_precision@zho.rst.sctb': 0.017543859649122806, 'eval_recall@zho.rst.sctb': 0.05098684210526316, 'eval_loss@zho.rst.sctb': 3.324038505554199, 'eval_runtime': 1.5124, 'eval_samples_per_second': 62.155, 'eval_steps_per_second': 1.984, 'epoch': 2.0}
{'train@zho.rst.sctb_loss': 3.0908308029174805, 'train@zho.rst.sctb_accuracy@zho.rst.sctb': 0.33712984054669703, 'train@zho.rst.sctb_f1@zho.rst.sctb': 0.02084979793596794, 'train@zho.rst.sctb_precision@zho.rst.sctb': 0.051311388839992964, 'train@zho.rst.sctb_recall@zho.rst.sctb': 0.039271255060728746, 'train@zho.rst.sctb_loss@zho.rst.sctb': 3.0908310413360596, 'train@zho.rst.sctb_runtime': 5.5263, 'train@zho.rst.sctb_samples_per_second': 79.438, 'train@zho.rst.sctb_steps_per_second': 2.533, 'epoch': 3.0}
{'loss': 3.2298, 'learning_rate': 1.5000000000000002e-05, 'epoch': 3.0}
{'eval_loss': 3.1307928562164307, 'eval_accuracy@zho.rst.sctb': 0.32978723404255317, 'eval_f1@zho.rst.sctb': 0.026105263157894736, 'eval_precision@zho.rst.sctb': 0.017543859649122806, 'eval_recall@zho.rst.sctb': 0.05098684210526316, 'eval_loss@zho.rst.sctb': 3.1307921409606934, 'eval_runtime': 1.5097, 'eval_samples_per_second': 62.265, 'eval_steps_per_second': 1.987, 'epoch': 3.0}
{'train@zho.rst.sctb_loss': 2.914137601852417, 'train@zho.rst.sctb_accuracy@zho.rst.sctb': 0.33712984054669703, 'train@zho.rst.sctb_f1@zho.rst.sctb': 0.02084979793596794, 'train@zho.rst.sctb_precision@zho.rst.sctb': 0.051311388839992964, 'train@zho.rst.sctb_recall@zho.rst.sctb': 0.039271255060728746, 'train@zho.rst.sctb_loss@zho.rst.sctb': 2.914137840270996, 'train@zho.rst.sctb_runtime': 5.5373, 'train@zho.rst.sctb_samples_per_second': 79.281, 'train@zho.rst.sctb_steps_per_second': 2.528, 'epoch': 4.0}
{'loss': 3.0313, 'learning_rate': 1.3333333333333333e-05, 'epoch': 4.0}
{'eval_loss': 2.9607303142547607, 'eval_accuracy@zho.rst.sctb': 0.32978723404255317, 'eval_f1@zho.rst.sctb': 0.026105263157894736, 'eval_precision@zho.rst.sctb': 0.017543859649122806, 'eval_recall@zho.rst.sctb': 0.05098684210526316, 'eval_loss@zho.rst.sctb': 2.960731029510498, 'eval_runtime': 1.4803, 'eval_samples_per_second': 63.5, 'eval_steps_per_second': 2.027, 'epoch': 4.0}
{'train@zho.rst.sctb_loss': 2.763096809387207, 'train@zho.rst.sctb_accuracy@zho.rst.sctb': 0.33712984054669703, 'train@zho.rst.sctb_f1@zho.rst.sctb': 0.020866713061116596, 'train@zho.rst.sctb_precision@zho.rst.sctb': 0.038520348153375676, 'train@zho.rst.sctb_recall@zho.rst.sctb': 0.039271255060728746, 'train@zho.rst.sctb_loss@zho.rst.sctb': 2.763096809387207, 'train@zho.rst.sctb_runtime': 5.5521, 'train@zho.rst.sctb_samples_per_second': 79.068, 'train@zho.rst.sctb_steps_per_second': 2.522, 'epoch': 5.0}
{'loss': 2.8919, 'learning_rate': 1.1666666666666668e-05, 'epoch': 5.0}
{'eval_loss': 2.8172523975372314, 'eval_accuracy@zho.rst.sctb': 0.32978723404255317, 'eval_f1@zho.rst.sctb': 0.026105263157894736, 'eval_precision@zho.rst.sctb': 0.017543859649122806, 'eval_recall@zho.rst.sctb': 0.05098684210526316, 'eval_loss@zho.rst.sctb': 2.8172519207000732, 'eval_runtime': 1.4685, 'eval_samples_per_second': 64.01, 'eval_steps_per_second': 2.043, 'epoch': 5.0}
{'train@zho.rst.sctb_loss': 2.637089967727661, 'train@zho.rst.sctb_accuracy@zho.rst.sctb': 0.33712984054669703, 'train@zho.rst.sctb_f1@zho.rst.sctb': 0.020884069076840164, 'train@zho.rst.sctb_precision@zho.rst.sctb': 0.0321396993810787, 'train@zho.rst.sctb_recall@zho.rst.sctb': 0.039271255060728746, 'train@zho.rst.sctb_loss@zho.rst.sctb': 2.6370902061462402, 'train@zho.rst.sctb_runtime': 5.5118, 'train@zho.rst.sctb_samples_per_second': 79.647, 'train@zho.rst.sctb_steps_per_second': 2.54, 'epoch': 6.0}
{'loss': 2.74, 'learning_rate': 1e-05, 'epoch': 6.0}
{'eval_loss': 2.701320171356201, 'eval_accuracy@zho.rst.sctb': 0.32978723404255317, 'eval_f1@zho.rst.sctb': 0.026105263157894736, 'eval_precision@zho.rst.sctb': 0.017543859649122806, 'eval_recall@zho.rst.sctb': 0.05098684210526316, 'eval_loss@zho.rst.sctb': 2.7013206481933594, 'eval_runtime': 1.4883, 'eval_samples_per_second': 63.161, 'eval_steps_per_second': 2.016, 'epoch': 6.0}
{'train@zho.rst.sctb_loss': 2.54046893119812, 'train@zho.rst.sctb_accuracy@zho.rst.sctb': 0.3416856492027335, 'train@zho.rst.sctb_f1@zho.rst.sctb': 0.02244329654169084, 'train@zho.rst.sctb_precision@zho.rst.sctb': 0.03860958133475455, 'train@zho.rst.sctb_recall@zho.rst.sctb': 0.04008097165991902, 'train@zho.rst.sctb_loss@zho.rst.sctb': 2.540468692779541, 'train@zho.rst.sctb_runtime': 5.5156, 'train@zho.rst.sctb_samples_per_second': 79.592, 'train@zho.rst.sctb_steps_per_second': 2.538, 'epoch': 7.0}
{'loss': 2.6403, 'learning_rate': 8.333333333333334e-06, 'epoch': 7.0}
{'eval_loss': 2.6153359413146973, 'eval_accuracy@zho.rst.sctb': 0.32978723404255317, 'eval_f1@zho.rst.sctb': 0.026105263157894736, 'eval_precision@zho.rst.sctb': 0.017543859649122806, 'eval_recall@zho.rst.sctb': 0.05098684210526316, 'eval_loss@zho.rst.sctb': 2.6153359413146973, 'eval_runtime': 1.4959, 'eval_samples_per_second': 62.839, 'eval_steps_per_second': 2.005, 'epoch': 7.0}
{'train@zho.rst.sctb_loss': 2.4758200645446777, 'train@zho.rst.sctb_accuracy@zho.rst.sctb': 0.3439635535307517, 'train@zho.rst.sctb_f1@zho.rst.sctb': 0.02320113565788306, 'train@zho.rst.sctb_precision@zho.rst.sctb': 0.04047110297110297, 'train@zho.rst.sctb_recall@zho.rst.sctb': 0.04048582995951417, 'train@zho.rst.sctb_loss@zho.rst.sctb': 2.4758200645446777, 'train@zho.rst.sctb_runtime': 5.5677, 'train@zho.rst.sctb_samples_per_second': 78.848, 'train@zho.rst.sctb_steps_per_second': 2.514, 'epoch': 8.0}
{'loss': 2.5548, 'learning_rate': 6.666666666666667e-06, 'epoch': 8.0}
{'eval_loss': 2.560184955596924, 'eval_accuracy@zho.rst.sctb': 0.3404255319148936, 'eval_f1@zho.rst.sctb': 0.035909445745511324, 'eval_precision@zho.rst.sctb': 0.043859649122807015, 'eval_recall@zho.rst.sctb': 0.05553405572755418, 'eval_loss@zho.rst.sctb': 2.5601847171783447, 'eval_runtime': 1.4879, 'eval_samples_per_second': 63.175, 'eval_steps_per_second': 2.016, 'epoch': 8.0}
{'train@zho.rst.sctb_loss': 2.4321553707122803, 'train@zho.rst.sctb_accuracy@zho.rst.sctb': 0.3553530751708428, 'train@zho.rst.sctb_f1@zho.rst.sctb': 0.027260458839406206, 'train@zho.rst.sctb_precision@zho.rst.sctb': 0.04135824866956942, 'train@zho.rst.sctb_recall@zho.rst.sctb': 0.04265154456214297, 'train@zho.rst.sctb_loss@zho.rst.sctb': 2.4321558475494385, 'train@zho.rst.sctb_runtime': 5.5527, 'train@zho.rst.sctb_samples_per_second': 79.061, 'train@zho.rst.sctb_steps_per_second': 2.521, 'epoch': 9.0}
{'loss': 2.4897, 'learning_rate': 5e-06, 'epoch': 9.0}
{'eval_loss': 2.5232765674591064, 'eval_accuracy@zho.rst.sctb': 0.3404255319148936, 'eval_f1@zho.rst.sctb': 0.035909445745511324, 'eval_precision@zho.rst.sctb': 0.043859649122807015, 'eval_recall@zho.rst.sctb': 0.05553405572755418, 'eval_loss@zho.rst.sctb': 2.5232765674591064, 'eval_runtime': 1.5007, 'eval_samples_per_second': 62.636, 'eval_steps_per_second': 1.999, 'epoch': 9.0}
{'train@zho.rst.sctb_loss': 2.4044835567474365, 'train@zho.rst.sctb_accuracy@zho.rst.sctb': 0.3690205011389522, 'train@zho.rst.sctb_f1@zho.rst.sctb': 0.030928839542145533, 'train@zho.rst.sctb_precision@zho.rst.sctb': 0.041834158734242344, 'train@zho.rst.sctb_recall@zho.rst.sctb': 0.04508069435971382, 'train@zho.rst.sctb_loss@zho.rst.sctb': 2.4044837951660156, 'train@zho.rst.sctb_runtime': 5.5363, 'train@zho.rst.sctb_samples_per_second': 79.294, 'train@zho.rst.sctb_steps_per_second': 2.529, 'epoch': 10.0}
{'loss': 2.4587, 'learning_rate': 3.3333333333333333e-06, 'epoch': 10.0}
{'eval_loss': 2.500778913497925, 'eval_accuracy@zho.rst.sctb': 0.3404255319148936, 'eval_f1@zho.rst.sctb': 0.039168573607932876, 'eval_precision@zho.rst.sctb': 0.04366028708133971, 'eval_recall@zho.rst.sctb': 0.05698529411764706, 'eval_loss@zho.rst.sctb': 2.500778913497925, 'eval_runtime': 1.4786, 'eval_samples_per_second': 63.572, 'eval_steps_per_second': 2.029, 'epoch': 10.0}
{'train@zho.rst.sctb_loss': 2.390425682067871, 'train@zho.rst.sctb_accuracy@zho.rst.sctb': 0.3735763097949886, 'train@zho.rst.sctb_f1@zho.rst.sctb': 0.032097069597069594, 'train@zho.rst.sctb_precision@zho.rst.sctb': 0.042701597918989226, 'train@zho.rst.sctb_recall@zho.rst.sctb': 0.04589041095890411, 'train@zho.rst.sctb_loss@zho.rst.sctb': 2.39042592048645, 'train@zho.rst.sctb_runtime': 5.5324, 'train@zho.rst.sctb_samples_per_second': 79.351, 'train@zho.rst.sctb_steps_per_second': 2.531, 'epoch': 11.0}
{'loss': 2.4324, 'learning_rate': 1.6666666666666667e-06, 'epoch': 11.0}
{'eval_loss': 2.489454507827759, 'eval_accuracy@zho.rst.sctb': 0.3404255319148936, 'eval_f1@zho.rst.sctb': 0.039168573607932876, 'eval_precision@zho.rst.sctb': 0.04366028708133971, 'eval_recall@zho.rst.sctb': 0.05698529411764706, 'eval_loss@zho.rst.sctb': 2.489454507827759, 'eval_runtime': 1.4887, 'eval_samples_per_second': 63.141, 'eval_steps_per_second': 2.015, 'epoch': 11.0}
{'train@zho.rst.sctb_loss': 2.385836124420166, 'train@zho.rst.sctb_accuracy@zho.rst.sctb': 0.3735763097949886, 'train@zho.rst.sctb_f1@zho.rst.sctb': 0.032032043416008286, 'train@zho.rst.sctb_precision@zho.rst.sctb': 0.04160995458257518, 'train@zho.rst.sctb_recall@zho.rst.sctb': 0.04589041095890411, 'train@zho.rst.sctb_loss@zho.rst.sctb': 2.385836124420166, 'train@zho.rst.sctb_runtime': 5.5558, 'train@zho.rst.sctb_samples_per_second': 79.017, 'train@zho.rst.sctb_steps_per_second': 2.52, 'epoch': 12.0}
{'loss': 2.4193, 'learning_rate': 0.0, 'epoch': 12.0}
{'eval_loss': 2.4857447147369385, 'eval_accuracy@zho.rst.sctb': 0.3404255319148936, 'eval_f1@zho.rst.sctb': 0.039168573607932876, 'eval_precision@zho.rst.sctb': 0.04366028708133971, 'eval_recall@zho.rst.sctb': 0.05698529411764706, 'eval_loss@zho.rst.sctb': 2.4857451915740967, 'eval_runtime': 3.4668, 'eval_samples_per_second': 27.114, 'eval_steps_per_second': 0.865, 'epoch': 12.0}
{'train_runtime': 217.5224, 'train_samples_per_second': 24.218, 'train_steps_per_second': 0.772, 'train_loss': 2.8295264471144903, 'epoch': 12.0}
***** train_log metrics *****
  epoch                    =       12.0
  train_loss               =     2.8295
  train_runtime            = 0:03:37.52
  train_samples_per_second =     24.218
  train_steps_per_second   =      0.772
{'train@eng.sdrt.stac_loss': 2.0847055912017822, 'train@eng.sdrt.stac_accuracy@eng.sdrt.stac': 0.3548016701461378, 'train@eng.sdrt.stac_f1@eng.sdrt.stac': 0.07512975532212722, 'train@eng.sdrt.stac_precision@eng.sdrt.stac': 0.0606846501210857, 'train@eng.sdrt.stac_recall@eng.sdrt.stac': 0.11274612247100227, 'train@eng.sdrt.stac_loss@eng.sdrt.stac': 2.0847055912017822, 'train@eng.sdrt.stac_runtime': 113.5045, 'train@eng.sdrt.stac_samples_per_second': 84.402, 'train@eng.sdrt.stac_steps_per_second': 2.643, 'epoch': 1.0}
{'loss': 2.4985, 'learning_rate': 1.8333333333333333e-05, 'epoch': 1.0}
{'eval_loss': 2.0379152297973633, 'eval_accuracy@eng.sdrt.stac': 0.36069868995633186, 'eval_f1@eng.sdrt.stac': 0.07278824121523055, 'eval_precision@eng.sdrt.stac': 0.05944014489634113, 'eval_recall@eng.sdrt.stac': 0.11190426562580057, 'eval_loss@eng.sdrt.stac': 2.0379152297973633, 'eval_runtime': 13.972, 'eval_samples_per_second': 81.949, 'eval_steps_per_second': 2.577, 'epoch': 1.0}
{'train@eng.sdrt.stac_loss': 1.9223467111587524, 'train@eng.sdrt.stac_accuracy@eng.sdrt.stac': 0.384446764091858, 'train@eng.sdrt.stac_f1@eng.sdrt.stac': 0.08766222662900928, 'train@eng.sdrt.stac_precision@eng.sdrt.stac': 0.09633240340471352, 'train@eng.sdrt.stac_recall@eng.sdrt.stac': 0.12387050467334, 'train@eng.sdrt.stac_loss@eng.sdrt.stac': 1.922346591949463, 'train@eng.sdrt.stac_runtime': 113.1625, 'train@eng.sdrt.stac_samples_per_second': 84.657, 'train@eng.sdrt.stac_steps_per_second': 2.651, 'epoch': 2.0}
{'loss': 2.0376, 'learning_rate': 1.6666666666666667e-05, 'epoch': 2.0}
{'eval_loss': 1.8847618103027344, 'eval_accuracy@eng.sdrt.stac': 0.3956331877729258, 'eval_f1@eng.sdrt.stac': 0.08998470390484262, 'eval_precision@eng.sdrt.stac': 0.08886749144102085, 'eval_recall@eng.sdrt.stac': 0.1256988732774215, 'eval_loss@eng.sdrt.stac': 1.8847618103027344, 'eval_runtime': 13.9443, 'eval_samples_per_second': 82.112, 'eval_steps_per_second': 2.582, 'epoch': 2.0}
{'train@eng.sdrt.stac_loss': 1.7540141344070435, 'train@eng.sdrt.stac_accuracy@eng.sdrt.stac': 0.45375782881002086, 'train@eng.sdrt.stac_f1@eng.sdrt.stac': 0.15366396958191472, 'train@eng.sdrt.stac_precision@eng.sdrt.stac': 0.1982315333349106, 'train@eng.sdrt.stac_recall@eng.sdrt.stac': 0.19365002603255072, 'train@eng.sdrt.stac_loss@eng.sdrt.stac': 1.7540141344070435, 'train@eng.sdrt.stac_runtime': 112.9579, 'train@eng.sdrt.stac_samples_per_second': 84.81, 'train@eng.sdrt.stac_steps_per_second': 2.656, 'epoch': 3.0}
{'loss': 1.8827, 'learning_rate': 1.5000000000000002e-05, 'epoch': 3.0}
{'eval_loss': 1.7117100954055786, 'eval_accuracy@eng.sdrt.stac': 0.45676855895196505, 'eval_f1@eng.sdrt.stac': 0.14785873686215467, 'eval_precision@eng.sdrt.stac': 0.1336897472233539, 'eval_recall@eng.sdrt.stac': 0.19046053430031362, 'eval_loss@eng.sdrt.stac': 1.7117100954055786, 'eval_runtime': 13.8847, 'eval_samples_per_second': 82.465, 'eval_steps_per_second': 2.593, 'epoch': 3.0}
{'train@eng.sdrt.stac_loss': 1.671286702156067, 'train@eng.sdrt.stac_accuracy@eng.sdrt.stac': 0.4830897703549061, 'train@eng.sdrt.stac_f1@eng.sdrt.stac': 0.20636325981671352, 'train@eng.sdrt.stac_precision@eng.sdrt.stac': 0.22658660975484535, 'train@eng.sdrt.stac_recall@eng.sdrt.stac': 0.2284679787893342, 'train@eng.sdrt.stac_loss@eng.sdrt.stac': 1.671286702156067, 'train@eng.sdrt.stac_runtime': 112.9458, 'train@eng.sdrt.stac_samples_per_second': 84.819, 'train@eng.sdrt.stac_steps_per_second': 2.656, 'epoch': 4.0}
{'loss': 1.7593, 'learning_rate': 1.3333333333333333e-05, 'epoch': 4.0}
{'eval_loss': 1.6257647275924683, 'eval_accuracy@eng.sdrt.stac': 0.49170305676855897, 'eval_f1@eng.sdrt.stac': 0.20038897510636366, 'eval_precision@eng.sdrt.stac': 0.2390875882210677, 'eval_recall@eng.sdrt.stac': 0.22401122546392493, 'eval_loss@eng.sdrt.stac': 1.6257647275924683, 'eval_runtime': 13.9138, 'eval_samples_per_second': 82.292, 'eval_steps_per_second': 2.587, 'epoch': 4.0}
{'train@eng.sdrt.stac_loss': 1.6155346632003784, 'train@eng.sdrt.stac_accuracy@eng.sdrt.stac': 0.4987473903966597, 'train@eng.sdrt.stac_f1@eng.sdrt.stac': 0.22757586817969308, 'train@eng.sdrt.stac_precision@eng.sdrt.stac': 0.2397875264423505, 'train@eng.sdrt.stac_recall@eng.sdrt.stac': 0.24810842061369226, 'train@eng.sdrt.stac_loss@eng.sdrt.stac': 1.6155349016189575, 'train@eng.sdrt.stac_runtime': 113.2597, 'train@eng.sdrt.stac_samples_per_second': 84.584, 'train@eng.sdrt.stac_steps_per_second': 2.649, 'epoch': 5.0}
{'loss': 1.6889, 'learning_rate': 1.1666666666666668e-05, 'epoch': 5.0}
{'eval_loss': 1.5772671699523926, 'eval_accuracy@eng.sdrt.stac': 0.5100436681222708, 'eval_f1@eng.sdrt.stac': 0.2239969159772773, 'eval_precision@eng.sdrt.stac': 0.2581005669032043, 'eval_recall@eng.sdrt.stac': 0.2432974325546613, 'eval_loss@eng.sdrt.stac': 1.5772671699523926, 'eval_runtime': 13.9315, 'eval_samples_per_second': 82.188, 'eval_steps_per_second': 2.584, 'epoch': 5.0}
{'train@eng.sdrt.stac_loss': 1.568913459777832, 'train@eng.sdrt.stac_accuracy@eng.sdrt.stac': 0.506054279749478, 'train@eng.sdrt.stac_f1@eng.sdrt.stac': 0.23486168456193127, 'train@eng.sdrt.stac_precision@eng.sdrt.stac': 0.31317608192753743, 'train@eng.sdrt.stac_recall@eng.sdrt.stac': 0.2529231085295218, 'train@eng.sdrt.stac_loss@eng.sdrt.stac': 1.568913459777832, 'train@eng.sdrt.stac_runtime': 113.2676, 'train@eng.sdrt.stac_samples_per_second': 84.578, 'train@eng.sdrt.stac_steps_per_second': 2.649, 'epoch': 6.0}
{'loss': 1.6362, 'learning_rate': 1e-05, 'epoch': 6.0}
{'eval_loss': 1.5507310628890991, 'eval_accuracy@eng.sdrt.stac': 0.5056768558951965, 'eval_f1@eng.sdrt.stac': 0.22234274632586248, 'eval_precision@eng.sdrt.stac': 0.23616007144342455, 'eval_recall@eng.sdrt.stac': 0.2416165830378176, 'eval_loss@eng.sdrt.stac': 1.5507310628890991, 'eval_runtime': 13.9079, 'eval_samples_per_second': 82.327, 'eval_steps_per_second': 2.588, 'epoch': 6.0}
{'train@eng.sdrt.stac_loss': 1.5471516847610474, 'train@eng.sdrt.stac_accuracy@eng.sdrt.stac': 0.5165970772442589, 'train@eng.sdrt.stac_f1@eng.sdrt.stac': 0.2666507598884914, 'train@eng.sdrt.stac_precision@eng.sdrt.stac': 0.36995633859198335, 'train@eng.sdrt.stac_recall@eng.sdrt.stac': 0.27640219520244635, 'train@eng.sdrt.stac_loss@eng.sdrt.stac': 1.5471515655517578, 'train@eng.sdrt.stac_runtime': 113.0266, 'train@eng.sdrt.stac_samples_per_second': 84.759, 'train@eng.sdrt.stac_steps_per_second': 2.654, 'epoch': 7.0}
{'loss': 1.6021, 'learning_rate': 8.333333333333334e-06, 'epoch': 7.0}
{'eval_loss': 1.5338481664657593, 'eval_accuracy@eng.sdrt.stac': 0.5039301310043668, 'eval_f1@eng.sdrt.stac': 0.23883611393258086, 'eval_precision@eng.sdrt.stac': 0.3291609676461571, 'eval_recall@eng.sdrt.stac': 0.2521195854403703, 'eval_loss@eng.sdrt.stac': 1.5338481664657593, 'eval_runtime': 13.8897, 'eval_samples_per_second': 82.435, 'eval_steps_per_second': 2.592, 'epoch': 7.0}
{'train@eng.sdrt.stac_loss': 1.5129493474960327, 'train@eng.sdrt.stac_accuracy@eng.sdrt.stac': 0.5289144050104384, 'train@eng.sdrt.stac_f1@eng.sdrt.stac': 0.29832894106723273, 'train@eng.sdrt.stac_precision@eng.sdrt.stac': 0.42969104826949, 'train@eng.sdrt.stac_recall@eng.sdrt.stac': 0.30438162965824916, 'train@eng.sdrt.stac_loss@eng.sdrt.stac': 1.5129493474960327, 'train@eng.sdrt.stac_runtime': 112.979, 'train@eng.sdrt.stac_samples_per_second': 84.795, 'train@eng.sdrt.stac_steps_per_second': 2.655, 'epoch': 8.0}
{'loss': 1.5758, 'learning_rate': 6.666666666666667e-06, 'epoch': 8.0}
{'eval_loss': 1.501664161682129, 'eval_accuracy@eng.sdrt.stac': 0.5266375545851528, 'eval_f1@eng.sdrt.stac': 0.26517590450496087, 'eval_precision@eng.sdrt.stac': 0.3283470215353056, 'eval_recall@eng.sdrt.stac': 0.2772660025943712, 'eval_loss@eng.sdrt.stac': 1.501664161682129, 'eval_runtime': 13.9407, 'eval_samples_per_second': 82.134, 'eval_steps_per_second': 2.582, 'epoch': 8.0}
{'train@eng.sdrt.stac_loss': 1.4958091974258423, 'train@eng.sdrt.stac_accuracy@eng.sdrt.stac': 0.5369519832985387, 'train@eng.sdrt.stac_f1@eng.sdrt.stac': 0.32233341737467447, 'train@eng.sdrt.stac_precision@eng.sdrt.stac': 0.4351325306359053, 'train@eng.sdrt.stac_recall@eng.sdrt.stac': 0.32803122767469606, 'train@eng.sdrt.stac_loss@eng.sdrt.stac': 1.4958089590072632, 'train@eng.sdrt.stac_runtime': 113.1223, 'train@eng.sdrt.stac_samples_per_second': 84.687, 'train@eng.sdrt.stac_steps_per_second': 2.652, 'epoch': 9.0}
{'loss': 1.5467, 'learning_rate': 5e-06, 'epoch': 9.0}
{'eval_loss': 1.4916430711746216, 'eval_accuracy@eng.sdrt.stac': 0.5231441048034935, 'eval_f1@eng.sdrt.stac': 0.27396104000295696, 'eval_precision@eng.sdrt.stac': 0.29677606987712146, 'eval_recall@eng.sdrt.stac': 0.2854419899936721, 'eval_loss@eng.sdrt.stac': 1.4916430711746216, 'eval_runtime': 13.9245, 'eval_samples_per_second': 82.229, 'eval_steps_per_second': 2.585, 'epoch': 9.0}
{'train@eng.sdrt.stac_loss': 1.4799095392227173, 'train@eng.sdrt.stac_accuracy@eng.sdrt.stac': 0.5391440501043842, 'train@eng.sdrt.stac_f1@eng.sdrt.stac': 0.32732730375993124, 'train@eng.sdrt.stac_precision@eng.sdrt.stac': 0.41984097646338314, 'train@eng.sdrt.stac_recall@eng.sdrt.stac': 0.3342860081188957, 'train@eng.sdrt.stac_loss@eng.sdrt.stac': 1.4799095392227173, 'train@eng.sdrt.stac_runtime': 113.2922, 'train@eng.sdrt.stac_samples_per_second': 84.56, 'train@eng.sdrt.stac_steps_per_second': 2.648, 'epoch': 10.0}
{'loss': 1.5346, 'learning_rate': 3.3333333333333333e-06, 'epoch': 10.0}
{'eval_loss': 1.4791829586029053, 'eval_accuracy@eng.sdrt.stac': 0.525764192139738, 'eval_f1@eng.sdrt.stac': 0.27642631016748714, 'eval_precision@eng.sdrt.stac': 0.3033617847935017, 'eval_recall@eng.sdrt.stac': 0.28893208248835484, 'eval_loss@eng.sdrt.stac': 1.4791830778121948, 'eval_runtime': 13.9329, 'eval_samples_per_second': 82.18, 'eval_steps_per_second': 2.584, 'epoch': 10.0}
{'train@eng.sdrt.stac_loss': 1.4716676473617554, 'train@eng.sdrt.stac_accuracy@eng.sdrt.stac': 0.5423799582463465, 'train@eng.sdrt.stac_f1@eng.sdrt.stac': 0.3334737336699334, 'train@eng.sdrt.stac_precision@eng.sdrt.stac': 0.42713997197730075, 'train@eng.sdrt.stac_recall@eng.sdrt.stac': 0.3394123283637574, 'train@eng.sdrt.stac_loss@eng.sdrt.stac': 1.471667766571045, 'train@eng.sdrt.stac_runtime': 113.3142, 'train@eng.sdrt.stac_samples_per_second': 84.544, 'train@eng.sdrt.stac_steps_per_second': 2.648, 'epoch': 11.0}
{'loss': 1.5258, 'learning_rate': 1.6666666666666667e-06, 'epoch': 11.0}
{'eval_loss': 1.473575234413147, 'eval_accuracy@eng.sdrt.stac': 0.5248908296943231, 'eval_f1@eng.sdrt.stac': 0.2813571535309427, 'eval_precision@eng.sdrt.stac': 0.35442204120281773, 'eval_recall@eng.sdrt.stac': 0.2916457509378211, 'eval_loss@eng.sdrt.stac': 1.4735753536224365, 'eval_runtime': 13.942, 'eval_samples_per_second': 82.126, 'eval_steps_per_second': 2.582, 'epoch': 11.0}
{'train@eng.sdrt.stac_loss': 1.469326138496399, 'train@eng.sdrt.stac_accuracy@eng.sdrt.stac': 0.5427974947807933, 'train@eng.sdrt.stac_f1@eng.sdrt.stac': 0.33476872660364937, 'train@eng.sdrt.stac_precision@eng.sdrt.stac': 0.4271522300383964, 'train@eng.sdrt.stac_recall@eng.sdrt.stac': 0.34063593848065665, 'train@eng.sdrt.stac_loss@eng.sdrt.stac': 1.469326138496399, 'train@eng.sdrt.stac_runtime': 112.8369, 'train@eng.sdrt.stac_samples_per_second': 84.901, 'train@eng.sdrt.stac_steps_per_second': 2.659, 'epoch': 12.0}
{'loss': 1.5176, 'learning_rate': 0.0, 'epoch': 12.0}
{'eval_loss': 1.472266435623169, 'eval_accuracy@eng.sdrt.stac': 0.5292576419213973, 'eval_f1@eng.sdrt.stac': 0.284406173609568, 'eval_precision@eng.sdrt.stac': 0.3577456653225391, 'eval_recall@eng.sdrt.stac': 0.29509909256161493, 'eval_loss@eng.sdrt.stac': 1.4722663164138794, 'eval_runtime': 13.8474, 'eval_samples_per_second': 82.687, 'eval_steps_per_second': 2.6, 'epoch': 12.0}
{'train_runtime': 4389.894, 'train_samples_per_second': 26.187, 'train_steps_per_second': 0.82, 'train_loss': 1.7338090176052516, 'epoch': 12.0}
***** train_logger metrics *****
  epoch                    =       12.0
  train_loss               =     2.8295
  train_runtime            = 0:03:37.52
  train_samples_per_second =     24.218
  train_steps_per_second   =      0.772

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#seeding for comparing experiment in part 2\n",
    "import torch\n",
    "import json\n",
    "SEED = 2011\n",
    "torch.manual_seed(SEED)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "\n",
    "device = torch.device('cuda:0')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SNLI Bert\n",
    "## Second Tutorial\n",
    "https://towardsdatascience.com/fine-tuning-pre-trained-transformer-models-for-sentence-entailment-d87caf9ec9db\n",
    "Check his Github code for complete notebook. I never referred to it. Medium was enough.\n",
    "BERT in keras-tf: https://towardsdatascience.com/bert-in-keras-with-tensorflow-hub-76bcbc9417b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define macros\n",
    "BERT_MODEL = 'bert-base-german-cased'\n",
    "batch_size = 4\n",
    "batches_per_epoch = 110\n",
    "\n",
    "save_path_suffix = '20visualizeBertPooler_randtrue_deu_'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## load the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# custom reader needed to handle quotechars\n",
    "def read_df_custom(file):\n",
    "    header = 'doc     unit1_toks      unit2_toks      unit1_txt       unit2_txt       s1_toks s2_toks unit1_sent      unit2_sent      dir     nuc_children    sat_children    genre   u1_discontinuous        u2_discontinuous       u1_issent        u2_issent       u1_length       u2_length       length_ratio    u1_speaker      u2_speaker      same_speaker    u1_func u1_pos  u1_depdir       u2_func u2_pos  u2_depdir       doclen  u1_position      u2_position     percent_distance        distance        lex_overlap_words       lex_overlap_length      unit1_case      unit2_case      label'\n",
    "    extracted_columns = ['unit1_txt', 'unit1_sent', 'unit2_txt', 'unit2_sent', 'dir', 'label', 'distance', 'u1_depdir', 'u2_depdir', 'u2_func', 'u1_position', 'u2_position', 'sat_children', 'nuc_children', 'genre', 'unit1_case', 'unit2_case',\n",
    "                            'u1_discontinuous', 'u2_discontinuous', 'same_speaker', 'lex_overlap_length', 'u1_func']\n",
    "    header = header.split()\n",
    "    df = pd.DataFrame(columns=extracted_columns)\n",
    "    file = open(file, 'r')\n",
    "\n",
    "    rows = []\n",
    "    count = 0 \n",
    "    for line in file:\n",
    "        line = line[:-1].split('\\t')\n",
    "        count+=1\n",
    "        if count ==1: continue\n",
    "        row = {}\n",
    "        for column in extracted_columns:\n",
    "            index = header.index(column)\n",
    "            row[column] = line[index]\n",
    "        rows.append(row)\n",
    "\n",
    "    df = pd.concat([df, pd.DataFrame.from_records(rows)])\n",
    "    return df\n",
    "\n",
    "train_df = read_df_custom('../../processed/deu.rst.pcc_train_enriched.rels')\n",
    "test_df = read_df_custom('../../processed/deu.rst.pcc_test_enriched.rels')\n",
    "val_df = read_df_custom('../../processed/deu.rst.pcc_dev_enriched.rels')\n",
    "lang = 'deu'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clean the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dropping any empty values\n",
    "train_df.dropna(inplace=True)\n",
    "val_df.dropna(inplace=True)\n",
    "test_df.dropna(inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare a dataset handler class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>unit1_txt</th>\n",
       "      <th>unit1_sent</th>\n",
       "      <th>unit2_txt</th>\n",
       "      <th>unit2_sent</th>\n",
       "      <th>dir</th>\n",
       "      <th>label</th>\n",
       "      <th>distance</th>\n",
       "      <th>u1_depdir</th>\n",
       "      <th>u2_depdir</th>\n",
       "      <th>u2_func</th>\n",
       "      <th>...</th>\n",
       "      <th>sat_children</th>\n",
       "      <th>nuc_children</th>\n",
       "      <th>genre</th>\n",
       "      <th>unit1_case</th>\n",
       "      <th>unit2_case</th>\n",
       "      <th>u1_discontinuous</th>\n",
       "      <th>u2_discontinuous</th>\n",
       "      <th>same_speaker</th>\n",
       "      <th>lex_overlap_length</th>\n",
       "      <th>u1_func</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Dagmar Ziegler sitzt in der Schuldenfalle .</td>\n",
       "      <td>Dagmar Ziegler sitzt in der Schuldenfalle .</td>\n",
       "      <td>hat sie jetzt eine seit mehr als einem Jahr er...</td>\n",
       "      <td>Auf Grund der dramatischen Kassenlage in Brand...</td>\n",
       "      <td>1&gt;2</td>\n",
       "      <td>interpretation</td>\n",
       "      <td>2</td>\n",
       "      <td>ROOT</td>\n",
       "      <td>ROOT</td>\n",
       "      <td>root</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>news</td>\n",
       "      <td>cap_initial</td>\n",
       "      <td>other</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>0</td>\n",
       "      <td>root</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Auf Grund der dramatischen Kassenlage in Brand...</td>\n",
       "      <td>Auf Grund der dramatischen Kassenlage in Brand...</td>\n",
       "      <td>hat sie jetzt eine seit mehr als einem Jahr er...</td>\n",
       "      <td>Auf Grund der dramatischen Kassenlage in Brand...</td>\n",
       "      <td>1&gt;2</td>\n",
       "      <td>cause</td>\n",
       "      <td>1</td>\n",
       "      <td>RIGHT</td>\n",
       "      <td>ROOT</td>\n",
       "      <td>root</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>news</td>\n",
       "      <td>cap_initial</td>\n",
       "      <td>other</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>0</td>\n",
       "      <td>obl</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>hat sie jetzt eine seit mehr als einem Jahr er...</td>\n",
       "      <td>Auf Grund der dramatischen Kassenlage in Brand...</td>\n",
       "      <td>Der Rückzieher der Finanzministerin ist aber v...</td>\n",
       "      <td>Der Rückzieher der Finanzministerin ist aber v...</td>\n",
       "      <td>1&gt;2</td>\n",
       "      <td>evaluation-n</td>\n",
       "      <td>4</td>\n",
       "      <td>ROOT</td>\n",
       "      <td>ROOT</td>\n",
       "      <td>root</td>\n",
       "      <td>...</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>news</td>\n",
       "      <td>other</td>\n",
       "      <td>cap_initial</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>0</td>\n",
       "      <td>root</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>hat sie jetzt eine seit mehr als einem Jahr er...</td>\n",
       "      <td>Auf Grund der dramatischen Kassenlage in Brand...</td>\n",
       "      <td>und vorgeschlagen , erst 2003 darüber zu entsc...</td>\n",
       "      <td>Auf Grund der dramatischen Kassenlage in Brand...</td>\n",
       "      <td>1&lt;2</td>\n",
       "      <td>conjunction</td>\n",
       "      <td>1</td>\n",
       "      <td>ROOT</td>\n",
       "      <td>LEFT</td>\n",
       "      <td>conj</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>news</td>\n",
       "      <td>other</td>\n",
       "      <td>other</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>0</td>\n",
       "      <td>root</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>hat sie jetzt eine seit mehr als einem Jahr er...</td>\n",
       "      <td>Auf Grund der dramatischen Kassenlage in Brand...</td>\n",
       "      <td>Überraschend ,</td>\n",
       "      <td>Überraschend , weil das Finanz- und das Bildun...</td>\n",
       "      <td>1&lt;2</td>\n",
       "      <td>interpretation</td>\n",
       "      <td>2</td>\n",
       "      <td>ROOT</td>\n",
       "      <td>ROOT</td>\n",
       "      <td>root</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>news</td>\n",
       "      <td>other</td>\n",
       "      <td>title</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>0</td>\n",
       "      <td>root</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 22 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                           unit1_txt  \\\n",
       "0        Dagmar Ziegler sitzt in der Schuldenfalle .   \n",
       "1  Auf Grund der dramatischen Kassenlage in Brand...   \n",
       "2  hat sie jetzt eine seit mehr als einem Jahr er...   \n",
       "3  hat sie jetzt eine seit mehr als einem Jahr er...   \n",
       "4  hat sie jetzt eine seit mehr als einem Jahr er...   \n",
       "\n",
       "                                          unit1_sent  \\\n",
       "0        Dagmar Ziegler sitzt in der Schuldenfalle .   \n",
       "1  Auf Grund der dramatischen Kassenlage in Brand...   \n",
       "2  Auf Grund der dramatischen Kassenlage in Brand...   \n",
       "3  Auf Grund der dramatischen Kassenlage in Brand...   \n",
       "4  Auf Grund der dramatischen Kassenlage in Brand...   \n",
       "\n",
       "                                           unit2_txt  \\\n",
       "0  hat sie jetzt eine seit mehr als einem Jahr er...   \n",
       "1  hat sie jetzt eine seit mehr als einem Jahr er...   \n",
       "2  Der Rückzieher der Finanzministerin ist aber v...   \n",
       "3  und vorgeschlagen , erst 2003 darüber zu entsc...   \n",
       "4                                     Überraschend ,   \n",
       "\n",
       "                                          unit2_sent  dir           label  \\\n",
       "0  Auf Grund der dramatischen Kassenlage in Brand...  1>2  interpretation   \n",
       "1  Auf Grund der dramatischen Kassenlage in Brand...  1>2           cause   \n",
       "2  Der Rückzieher der Finanzministerin ist aber v...  1>2    evaluation-n   \n",
       "3  Auf Grund der dramatischen Kassenlage in Brand...  1<2     conjunction   \n",
       "4  Überraschend , weil das Finanz- und das Bildun...  1<2  interpretation   \n",
       "\n",
       "  distance u1_depdir u2_depdir u2_func  ... sat_children nuc_children genre  \\\n",
       "0        2      ROOT      ROOT    root  ...            0            4  news   \n",
       "1        1     RIGHT      ROOT    root  ...            0            4  news   \n",
       "2        4      ROOT      ROOT    root  ...            4            3  news   \n",
       "3        1      ROOT      LEFT    conj  ...            0            4  news   \n",
       "4        2      ROOT      ROOT    root  ...            1            4  news   \n",
       "\n",
       "    unit1_case   unit2_case u1_discontinuous u2_discontinuous same_speaker  \\\n",
       "0  cap_initial        other            False            False         True   \n",
       "1  cap_initial        other            False            False         True   \n",
       "2        other  cap_initial            False            False         True   \n",
       "3        other        other            False            False         True   \n",
       "4        other        title            False            False         True   \n",
       "\n",
       "  lex_overlap_length u1_func  \n",
       "0                  0    root  \n",
       "1                  0     obl  \n",
       "2                  0    root  \n",
       "3                  0    root  \n",
       "4                  0    root  \n",
       "\n",
       "[5 rows x 22 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['unit1_txt', 'unit1_sent', 'unit2_txt', 'unit2_sent', 'dir', 'label',\n",
       "       'distance', 'u1_depdir', 'u2_depdir', 'u2_func', 'u1_position',\n",
       "       'u2_position', 'sat_children', 'nuc_children', 'genre', 'unit1_case',\n",
       "       'unit2_case', 'u1_discontinuous', 'u2_discontinuous', 'same_speaker',\n",
       "       'lex_overlap_length', 'u1_func'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# feats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-12-12 13:15:26.889681: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-12-12 13:15:27.173139: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/cuda/lib64:/usr/local/cuda-10.1/lib64:/usr/local/cuda/extras/CUPTI/lib64:/usr/local/cuda-10.2/targets/x86_64-linux/lib/:/usr/local/cuda/extras/CUPTI/lib64:/usr/local/cuda/lib64:/usr/local/nvidia/lib:/usr/local/nvidia/lib64:/home/VD/kaveri/anaconda3/envs/py310/lib/\n",
      "2022-12-12 13:15:27.173187: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n",
      "2022-12-12 13:15:27.226018: E tensorflow/stream_executor/cuda/cuda_blas.cc:2981] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2022-12-12 13:15:28.127332: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/cuda/lib64:/usr/local/cuda-10.1/lib64:/usr/local/cuda/extras/CUPTI/lib64:/usr/local/cuda-10.2/targets/x86_64-linux/lib/:/usr/local/cuda/extras/CUPTI/lib64:/usr/local/cuda/lib64:/usr/local/nvidia/lib:/usr/local/nvidia/lib64:/home/VD/kaveri/anaconda3/envs/py310/lib/\n",
      "2022-12-12 13:15:28.127443: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/cuda/lib64:/usr/local/cuda-10.1/lib64:/usr/local/cuda/extras/CUPTI/lib64:/usr/local/cuda-10.2/targets/x86_64-linux/lib/:/usr/local/cuda/extras/CUPTI/lib64:/usr/local/cuda/lib64:/usr/local/nvidia/lib:/usr/local/nvidia/lib64:/home/VD/kaveri/anaconda3/envs/py310/lib/\n",
      "2022-12-12 13:15:28.127452: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n"
     ]
    }
   ],
   "source": [
    "from multiprocessing.sharedctypes import Value\n",
    "import torch\n",
    "from torch.utils.data import Dataset, TensorDataset, DataLoader, ConcatDataset\n",
    "from sys import path\n",
    "path.append('/home/VD/kaveri/anaconda3/envs/py310/lib/python3.10/site-packages/allennlp/data/data_loaders/')\n",
    "from allennlp.data import allennlp_collate, Vocabulary\n",
    "from features_custom2 import get_vocab_feature_name\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from transformers import BertTokenizer\n",
    "import pandas as pd\n",
    "\n",
    "class MNLIDataBert(Dataset):\n",
    "\n",
    "  def __init__(self, train_df, val_df, test_df):\n",
    "    self.lang = lang\n",
    "    self.num_labels = set()\n",
    "    self.train_df = train_df\n",
    "    self.val_df = val_df\n",
    "    self.test_df = test_df\n",
    "\n",
    "    self.tokenizer = BertTokenizer.from_pretrained(BERT_MODEL, do_lower_case=True) # Using a pre-trained BERT tokenizer to encode sentences\n",
    "    self.train_data = None\n",
    "    self.val_data = None\n",
    "    self.test_data = None\n",
    "    self.train_idx = None\n",
    "    self.val_idx = None\n",
    "    self.test_idx = None\n",
    "    self.vocab = Vocabulary(counter=None, max_vocab_size=100000)\n",
    "    self.init_data()\n",
    "\n",
    "  def init_data(self):\n",
    "    self.get_label_mapping()\n",
    "    self.init_feature_list()\n",
    "    self.init_feature_mappings_and_bins()\n",
    "    self.apply_bins()\n",
    "    self.calculate_unique_values()\n",
    "    self.train_data, self.train_idx = self.load_data(self.train_df)\n",
    "    self.val_data, self.val_idx = self.load_data(self.val_df)\n",
    "    self.test_data, self.test_idx = self.load_data(self.test_df)\n",
    "    \n",
    "\n",
    "  def combine_unique_column_values_to_dict(self, column_name):\n",
    "    ini_set = set([*self.train_df[column_name].unique(), *self.val_df[column_name].unique()])\n",
    "    res = dict.fromkeys(ini_set, 0)\n",
    "    return res\n",
    "\n",
    "  def get_label_mapping(self):\n",
    "    labels = {}\n",
    "    labels_list = list(set(list(self.train_df['label'].unique()) + list(self.test_df['label'].unique()) + list(self.val_df['label'].unique())))\n",
    "    for i in range(len(labels_list)):\n",
    "        labels[labels_list[i]] = i\n",
    "    self.label_dict = labels\n",
    "    # needed later for classification report object to generate precision and recall on test dataset\n",
    "    self.rev_label_dict = {self.label_dict[k]:k for k in self.label_dict.keys()} \n",
    "\n",
    "  def init_feature_mappings_and_bins(self):\n",
    "    self.feature_maps = { 'genre': self.combine_unique_column_values_to_dict('genre'),\n",
    "                          'unit1_case': self.combine_unique_column_values_to_dict('unit1_case'),\n",
    "                          'unit2_case': self.combine_unique_column_values_to_dict('unit2_case'),\n",
    "                          'u1_func': self.combine_unique_column_values_to_dict('u1_func'),\n",
    "                          'u2_func': self.combine_unique_column_values_to_dict('u2_func') }\n",
    "\n",
    "    self.bins = {\n",
    "      'distance': [[-1e9, -8], [-8, -2], [-2, 0], [0, 2], [2, 8], [8, 1e9]],\n",
    "      'u1_position': [[0.0, 0.1], [0.1, 0.2], [0.2, 0.3], [0.3, 0.4], [0.4, 0.5], [0.5, 0.6], [0.6, 0.7], [0.7, 0.8], [0.8, 0.9], [0.9, 1.0], [1.0, 1e9]],\n",
    "      'u2_position': [[0.0, 0.1], [0.1, 0.2], [0.2, 0.3], [0.3, 0.4], [0.4, 0.5], [0.5, 0.6], [0.6, 0.7], [0.7, 0.8], [0.8, 0.9], [0.9, 1.0], [1.0, 1e9]],\n",
    "      'lex_overlap_length': [[0, 2], [2, 7], [7, 1e9]]\n",
    "    }   \n",
    "\n",
    "  def add_directionality(self, premise, hypothesis, dir):\n",
    "    if dir == \"1<2\":\n",
    "        hypothesis = '< ' + hypothesis + ' {'\n",
    "    else:\n",
    "        premise = '} ' + premise + ' >'\n",
    "    return premise, hypothesis\n",
    "\n",
    "  def init_feature_list(self):\n",
    "    if self.lang=='nld':\n",
    "      self.feature_list = ['distance', 'u1_depdir', 'sat_children', 'genre', 'u1_position']\n",
    "    elif self.lang=='deu':\n",
    "      self.feature_list = ['distance', 'u1_depdir', 'u2_depdir', 'u2_func', 'u1_position', 'u2_position', 'sat_children', 'nuc_children']\n",
    "    elif self.lang=='eng.rst.gum':\n",
    "      self.feature_list = ['distance', 'same_speaker', 'u2_func', 'u2_depdir', 'unit1_case', 'unit2_case', 'nuc_children',\n",
    "                      'sat_children', 'genre', 'lex_overlap_length', 'u2_discontinuous', 'u1_discontinuous', 'u1_position', 'u2_position']\n",
    "    elif self.lang=='fas':\n",
    "      self.feature_list = ['distance', 'nuc_children', 'sat_children', 'u2_discontinuous', 'genre']\n",
    "    elif self.lang=='spa.rst.sctb':\n",
    "      self.feature_list = ['distance', 'u1_position', 'sat_children']\n",
    "    elif self.lang=='zho.rst.sctb':\n",
    "      self.feature_list = ['sat_children', 'nuc_children', 'genre', 'u2_discontinuous', 'u1_discontinuous', 'u1_depdir', 'u1_func']\n",
    "    else: \n",
    "      raise ValueError()\n",
    "\n",
    "  def get_mapping_from_dictionary(self, column_name, dict_val):\n",
    "    return self.feature_maps[column_name][dict_val]\n",
    "\n",
    "  def get_allen_features_list(self, features, feature_name):\n",
    "    if feature_name in ['distance', 'u1_depdir', 'u2_depdir', 'u1_func', 'u2_func', \n",
    "    'u1_position', 'u2_position', 'genre', 'same_speaker', 'unit1_case', 'unit2_case',\n",
    "    'lex_overlap_length', 'u2_discontinuous', 'u1_discontinuous', 'dir']: feature_value = self.apply_vocab(features[feature_name], feature_name) #for categorical values\n",
    "    elif feature_name in ['sat_children', 'nuc_children']: feature_value = float(features[feature_name]) #for identiy values\n",
    "    else: \n",
    "      print(feature_name)\n",
    "      raise ValueError()\n",
    "    return feature_value\n",
    "\n",
    "  def transform_feature(self, features):\n",
    "    assert len(features)==17\n",
    "    #after applying the vocab. we need to pass them as int\n",
    "    return {feature_name: torch.tensor(int(self.get_allen_features_list(features, feature_name))).to(device) for feature_name in self.feature_list+['dir']}\n",
    "\n",
    "  def calculate_unique_values(self):\n",
    "    for feature_name in self.feature_list+['dir']:\n",
    "      vocab_feature_name = get_vocab_feature_name(feature_name)\n",
    "      self.vocab.add_tokens_to_namespace(train_df[feature_name].apply(lambda x: str(x)), namespace=vocab_feature_name)\n",
    "      self.vocab.add_tokens_to_namespace(val_df[feature_name].apply(lambda x: str(x)), namespace=vocab_feature_name)\n",
    "\n",
    "  def apply_bins(self):\n",
    "    for df in [self.train_df, self.test_df, self.val_df]:\n",
    "      for feature_name in self.bins.keys():\n",
    "        df[feature_name] = df[feature_name].apply(lambda x: self.get_mapping_from_bin(feature_name, float(x)))\n",
    "\n",
    "  def get_mapping_from_bin(self, column_name, dict_val):\n",
    "    bins = self.bins[column_name]\n",
    "    for b,i in zip(bins, range(len(bins))):\n",
    "      left = b[0]\n",
    "      right = b[1]\n",
    "      if left<=dict_val and right>=dict_val: return i\n",
    "\n",
    "  def apply_vocab(self, feature_value, feature_name):\n",
    "    return self.vocab.get_token_index(str(feature_value), namespace=get_vocab_feature_name(feature_name))\n",
    "\n",
    "  def set_labels(self):\n",
    "    self.num_labels = len(self.num_labels)\n",
    "    \n",
    "  def load_data(self, df):\n",
    "    MAX_LEN = 512 \n",
    "    token_ids = []\n",
    "    mask_ids = []\n",
    "    seg_ids = []\n",
    "    y = []\n",
    "    feats = []\n",
    "    idx = []\n",
    "    idx_map = {}\n",
    "\n",
    "    self.num_labels.update(df['label'].unique())\n",
    "\n",
    "    count=0\n",
    "    for row in df.iterrows():\n",
    "      row = row[1]\n",
    "      premise = row['unit1_txt']\n",
    "      hypothesis = row['unit2_txt']\n",
    "      label = row['label']\n",
    "      dir = row['dir']\n",
    "\n",
    "      features = {'distance': row['distance'],\n",
    "                'u1_depdir': row['u1_depdir'],\n",
    "                'u2_depdir': row['u2_depdir'],\n",
    "                'u1_func': row['u1_func'],\n",
    "                'u2_func': row['u2_func'],\n",
    "                'u1_position': row['u1_position'],\n",
    "                'u2_position': row['u2_position'],\n",
    "                'sat_children': row['sat_children'],\n",
    "                'nuc_children': row['nuc_children'],\n",
    "                'genre': row['genre'],\n",
    "                'unit1_case': row['unit1_case'],\n",
    "                'unit2_case': row['unit2_case'],\n",
    "                'u1_discontinuous': row['u1_discontinuous'],\n",
    "                'u2_discontinuous': row['u2_discontinuous'],\n",
    "                'same_speaker': row['same_speaker'],\n",
    "                'lex_overlap_length': row['lex_overlap_length'],\n",
    "                'dir': row['dir']}\n",
    "\n",
    "      premise, hypothesis = self.add_directionality(premise, hypothesis, dir)\n",
    "      encoded = self.tokenizer.encode_plus(premise, hypothesis, add_special_tokens = True, max_length=MAX_LEN, truncation=True, padding=False) #padding='max_length'\n",
    "      pair_token_ids = torch.tensor(encoded['input_ids'])\n",
    "\n",
    "      segment_ids = torch.tensor(encoded['token_type_ids'])\n",
    "      attention_mask_ids = torch.tensor(encoded['attention_mask'])\n",
    "      assert len(pair_token_ids)==len(attention_mask_ids)\n",
    "\n",
    "      features = self.transform_feature(features)\n",
    "\n",
    "      token_ids.append(pair_token_ids)\n",
    "      seg_ids.append(segment_ids)\n",
    "      mask_ids.append(attention_mask_ids)\n",
    "      y.append(self.label_dict[label])\n",
    "      feats.append(features)\n",
    "      \n",
    "      idx_map[count] = [premise, hypothesis]\n",
    "      idx.append(count)\n",
    "      count+=1\n",
    "      \n",
    "    token_ids = pad_sequence(token_ids, batch_first=True)\n",
    "    mask_ids = pad_sequence(mask_ids, batch_first=True)\n",
    "    seg_ids = pad_sequence(seg_ids, batch_first=True)\n",
    "    y = torch.tensor(y)\n",
    "    idx = torch.tensor(idx)\n",
    "\n",
    "    class featureDataset(Dataset):\n",
    "      def __init__(self, token_ids, mask_ids, seg_ids, feats, y, idx):\n",
    "          self.token_ids = token_ids\n",
    "          self.mask_ids = mask_ids\n",
    "          self.seg_ids = seg_ids\n",
    "          self.feats = feats\n",
    "          self.y = y\n",
    "          self.idx = idx\n",
    "\n",
    "      def __len__(self):\n",
    "          return len(self.feats)\n",
    "\n",
    "      def __getitem__(self, idx):\n",
    "          return self.token_ids[idx], self.mask_ids[idx], self.seg_ids[idx], self.feats[idx], self.y[idx], self.idx[idx]\n",
    "\n",
    "    dataset = featureDataset(token_ids, mask_ids, seg_ids, feats, y, idx)\n",
    "    return dataset, idx_map\n",
    "\n",
    "  def get_data_loaders(self, batch_size=4, batches_per_epoch=402, shuffle=True): #1609 samples / 64:25=1600 / 402:4=1608\n",
    "    self.set_labels()\n",
    "    train_loader_torch = DataLoader(\n",
    "      self.train_data,\n",
    "      shuffle=shuffle,\n",
    "      batch_size=batch_size,\n",
    "    )\n",
    "\n",
    "    val_loader_torch = DataLoader(\n",
    "      self.val_data,\n",
    "      shuffle=shuffle,\n",
    "      batch_size=batch_size,\n",
    "    )\n",
    "\n",
    "    test_loader_torch = DataLoader(\n",
    "      self.test_data,\n",
    "      shuffle=False,\n",
    "      batch_size=batch_size,\n",
    "    )\n",
    "    \n",
    "    train_loader = LoaderWrapper(train_loader_torch, n_step=batches_per_epoch)\n",
    "    val_loader = LoaderWrapper(val_loader_torch, n_step=batches_per_epoch)\n",
    "    test_loader = LoaderWrapper(test_loader_torch, n_step=batches_per_epoch)\n",
    "\n",
    "    return train_loader, val_loader_torch, test_loader_torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LoaderWrapper:\n",
    "    def __init__(self, loader, n_step):\n",
    "        self.step = n_step\n",
    "        self.idx = 0\n",
    "        self.iter_loader = iter(loader)\n",
    "        self.loader = loader\n",
    "    \n",
    "    def __iter__(self):\n",
    "        return self\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.step\n",
    "\n",
    "    def __next__(self):\n",
    "        # if reached number of steps desired, stop\n",
    "        if self.idx == self.step:\n",
    "            self.idx = 0\n",
    "            raise StopIteration\n",
    "        else:\n",
    "            self.idx += 1\n",
    "        # while True\n",
    "        try:\n",
    "            return next(self.iter_loader)\n",
    "        except StopIteration:\n",
    "            # reinstate iter_loader, then continue\n",
    "            self.iter_loader = iter(self.loader)\n",
    "            return next(self.iter_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "mnli_dataset = MNLIDataBert(train_df, val_df, test_df)\n",
    "\n",
    "train_loader, val_loader, test_loader = mnli_dataset.get_data_loaders(batch_size=batch_size, batches_per_epoch=batches_per_epoch) #64X250\n",
    "label_dict = mnli_dataset.label_dict # required by custom func to calculate accuracy, bert model\n",
    "rev_label_dict = mnli_dataset.rev_label_dict # required by custom func to calculate accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "distance :  {'@@PADDING@@': 0, '@@UNKNOWN@@': 1, '3': 2, '4': 3, '5': 4}\n",
      "u1_depdir :  {'@@PADDING@@': 0, '@@UNKNOWN@@': 1, 'ROOT': 2, 'RIGHT': 3, 'LEFT': 4}\n",
      "u2_depdir :  {'@@PADDING@@': 0, '@@UNKNOWN@@': 1, 'ROOT': 2, 'RIGHT': 3, 'LEFT': 4}\n",
      "u2_func :  {'@@PADDING@@': 0, '@@UNKNOWN@@': 1, 'root': 2, 'conj': 3, 'advcl': 4, 'acl': 5, 'xcomp': 6, 'obl': 7, 'ccomp': 8, 'parataxis': 9, 'advmod': 10, 'dep': 11, 'csubj': 12, 'nmod': 13, 'punct': 14, 'cc': 15, 'appos': 16, 'aux': 17, 'obj': 18, 'iobj': 19, 'nsubj': 20, 'nsubj:pass': 21, 'csubj:pass': 22}\n",
      "u1_position :  {'@@PADDING@@': 0, '@@UNKNOWN@@': 1, '0': 2, '2': 3, '3': 4, '4': 5, '5': 6, '6': 7, '7': 8, '8': 9, '9': 10, '1': 11}\n",
      "u2_position :  {'@@PADDING@@': 0, '@@UNKNOWN@@': 1, '0': 2, '2': 3, '1': 4, '3': 5, '4': 6, '5': 7, '6': 8, '7': 9, '8': 10, '9': 11}\n",
      "sat_children :  {'@@PADDING@@': 0, '@@UNKNOWN@@': 1, '0': 2, '4': 3, '1': 4, '2': 5, '3': 6, '5': 7, '6': 8}\n",
      "nuc_children :  {'@@PADDING@@': 0, '@@UNKNOWN@@': 1, '4': 2, '3': 3, '1': 4, '2': 5, '5': 6, '6': 7, '7': 8, '8': 9}\n",
      "dir :  {'@@PADDING@@': 0, '@@UNKNOWN@@': 1, '1>2': 2, '1<2': 3}\n"
     ]
    }
   ],
   "source": [
    "for feature in mnli_dataset.feature_list:\n",
    "    vocab_feature_name = get_vocab_feature_name(feature)\n",
    "    print(feature, ': ', mnli_dataset.vocab.get_token_to_index_vocabulary(vocab_feature_name))\n",
    "print('dir', ': ', mnli_dataset.vocab.get_token_to_index_vocabulary('dir'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "for batch_idx, (pair_token_ids, mask_ids, seg_ids, feat, y, idx) in enumerate(train_loader):\n",
    "    # assert pair_token_ids.shape[-1]==512 #torch.Size([4, 512])\n",
    "    # assert mask_ids.shape[-1]==512\n",
    "    # assert seg_ids.shape[-1]==512\n",
    "    assert len(feat)==len(mnli_dataset.feature_list)+1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## load pretrained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.3333333333333333\n",
      "1.0\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertForSequenceClassification, AdamW\n",
    "from torch import optim\n",
    "import os\n",
    "path.append(os.path.join(os.getcwd(), '../utils/'))\n",
    "from CategoricalAccuracy import CategoricalAccuracy as CA\n",
    "import numpy as np\n",
    "\n",
    "ca = CA()\n",
    "\n",
    "x = torch.tensor(np.array([[[1,0,0], [1,0,0], [1,0,0]]]))\n",
    "y1 = torch.tensor(np.array([[0], [1], [1]]))\n",
    "y2 = torch.tensor(np.array([[0], [0], [0]]))\n",
    "\n",
    "ca(x,y1)\n",
    "print(ca.get_metric(reset=True))\n",
    "ca(x,y2)\n",
    "print(ca.get_metric(reset=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: '@@PADDING@@', 1: '@@UNKNOWN@@'}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mnli_dataset.vocab.get_index_to_token_vocabulary('u1_depdir')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## define evaulation metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to evaluate model for train and test. And also use classification report for testing\n",
    "import torch.nn as nn\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# helper function to calculate the batch accuracy\n",
    "def multi_acc(y_pred, y_test, allennlp=False):\n",
    "  if allennlp==False:\n",
    "    acc = (torch.log_softmax(y_pred, dim=1).argmax(dim=1) == y_test).sum().float() / float(y_test.size(0))\n",
    "    return acc\n",
    "\n",
    "# freeze model weights and measure validation / test \n",
    "def evaluate_accuracy(model, optimizer, data_loader, rev_label_dict, label_dict, is_training=True):\n",
    "  model.eval()\n",
    "  total_val_acc  = 0\n",
    "  total_val_loss = 0\n",
    "  \n",
    "  #for classification report\n",
    "  y_true = []\n",
    "  y_pred = []\n",
    "  idx_list = []\n",
    "  premise_list = []\n",
    "  hypo_list = []\n",
    "  idx_map = mnli_dataset.val_idx if is_training else mnli_dataset.test_idx\n",
    "\n",
    "  with torch.no_grad():\n",
    "    for batch_idx, (pair_token_ids, mask_ids, seg_ids, feat, y, idx) in enumerate(data_loader):      \n",
    "      optimizer.zero_grad()\n",
    "      pair_token_ids = pair_token_ids.to(device)\n",
    "      mask_ids = mask_ids.to(device)\n",
    "      seg_ids = seg_ids.to(device)\n",
    "      labels = y.to(device)\n",
    "      # feat = feat.to(device)\n",
    "      \n",
    "      outputs = model(pair_token_ids, \n",
    "                            token_type_ids=seg_ids, \n",
    "                            attention_mask=mask_ids, \n",
    "                            feat=feat)\n",
    "      criterion = nn.CrossEntropyLoss()\n",
    "      loss = criterion(outputs, labels)\n",
    "      acc = multi_acc(outputs, labels)\n",
    "\n",
    "      total_val_loss += loss.item()\n",
    "      total_val_acc  += acc.item()\n",
    "\n",
    "      # log predictions for classification report\n",
    "      argmax_predictions = torch.argmax(outputs,dim=1).tolist()\n",
    "      labels_list = labels.tolist()\n",
    "      assert(len(labels_list)==len(argmax_predictions))\n",
    "      for p in argmax_predictions: y_pred.append(rev_label_dict[int(p)])\n",
    "      for l in labels_list: y_true.append(rev_label_dict[l])\n",
    "      for i in idx.tolist():\n",
    "        idx_list.append(i)\n",
    "        if i not in idx_map.keys():\n",
    "          print(idx_map)\n",
    "        premise_list.append(idx_map[i][0])\n",
    "        hypo_list.append(idx_map[i][1])\n",
    "\n",
    "  val_acc  = total_val_acc/len(data_loader)\n",
    "  val_loss = total_val_loss/len(data_loader)\n",
    "  cr = classification_report(y_true, y_pred)\n",
    "\n",
    "  idx_json = {'idx': idx_list, 'gold_label': y_true, 'pred_label': y_pred, 'premise': premise_list, 'hypothesis': hypo_list}\n",
    "  \n",
    "  return val_acc, val_loss, cr, model, optimizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## define custom bert model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ASSIGN: 26\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-german-cased were not used when initializing FeaturefulBert: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.bias']\n",
      "- This IS expected if you are initializing FeaturefulBert from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing FeaturefulBert from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of the model checkpoint at bert-base-german-cased were not used when initializing BertModel: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([768, 768])\n",
      "torch.Size([768, 768])\n",
      "Parameter containing:\n",
      "tensor([[0.8675, 0.1791, 0.4914,  ..., 0.8033, 0.1800, 0.7565],\n",
      "        [0.2525, 0.7510, 0.4346,  ..., 0.7418, 0.5287, 0.5078],\n",
      "        [0.6666, 0.2743, 0.3049,  ..., 0.3149, 0.3701, 0.8864],\n",
      "        ...,\n",
      "        [0.4879, 0.1848, 0.8009,  ..., 0.6173, 0.2064, 0.7918],\n",
      "        [0.9213, 0.3528, 0.9720,  ..., 0.8329, 0.9012, 0.6019],\n",
      "        [0.9650, 0.1516, 0.0451,  ..., 0.4805, 0.9559, 0.3503]],\n",
      "       requires_grad=True)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/VD/kaveri/anaconda3/envs/py310/lib/python3.10/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import copy\n",
    "from typing import Any, Dict, Optional\n",
    "from transformers import BertModel, AutoTokenizer, BertConfig\n",
    "from transformers.models.bert.modeling_bert import BertPooler\n",
    "import torch.nn as nn\n",
    "from tensorflow.keras.layers import TimeDistributed\n",
    "from featurefulbertembedder_custom2 import FeaturefulBertEmbedder\n",
    "from featureful_bert_custom2 import get_combined_feature_tensor_2 as get_combined_feature_tensor_forward\n",
    "from featureful_bert_custom2 import get_feature_modules\n",
    "from numpy.linalg import norm\n",
    "\n",
    "\n",
    "class CustomPooler2(nn.Module):\n",
    "    def __init__(self, *,\n",
    "                        randomize_weights: bool = False,\n",
    "                        requires_grad: bool = True,\n",
    "                        dropout: float = 0.0,\n",
    "                        transformer_kwargs: Optional[Dict[str, Any]] = None, ) -> None:\n",
    "        super().__init__()\n",
    "        bert = BertModel.from_pretrained(BERT_MODEL) #only used to pass config. BertAttentionClass used in FeatureFulBert\n",
    "        self._dropout = torch.nn.Dropout(p=dropout)\n",
    "        self.pooler = copy.deepcopy(bert.pooler)\n",
    "        if randomize_weights:\n",
    "            print(self.pooler.dense.weight.shape)\n",
    "            self.pooler.dense.weight = nn.Parameter(torch.rand(self.pooler.dense.weight.shape))\n",
    "            self.pooler.dense.bias = nn.Parameter(torch.rand(self.pooler.dense.bias.shape))\n",
    "            print(self.pooler.dense.weight.shape)\n",
    "        for param in self.pooler.parameters():\n",
    "            param.requires_grad = requires_grad\n",
    "        self._embedding_dim = bert.config.hidden_size\n",
    "\n",
    "    def get_input_dim(self) -> int:\n",
    "        return self._embedding_dim\n",
    "\n",
    "    def get_output_dim(self) -> int:\n",
    "        return self._embedding_dim\n",
    "\n",
    "    def forward(self, tokens: torch.Tensor, mask: torch.BoolTensor = None, num_wrapping_dims: int = 0):\n",
    "        pooler = self.pooler\n",
    "        \n",
    "        for _ in range(num_wrapping_dims):\n",
    "            pooler = TimeDistributed(pooler)\n",
    "        pooled = pooler(tokens)\n",
    "        pooled = self._dropout(pooled)\n",
    "        return pooled\n",
    "\n",
    "class MyModule(nn.Module):    \n",
    "    def __init__(self, feature_list, vocab):\n",
    "        super(MyModule, self).__init__()\n",
    "        self.feature_list = feature_list\n",
    "        self.feature_modules, self._feature_module_size = get_feature_modules(feature_list, vocab)\n",
    "\n",
    "    def forward(self, features):\n",
    "        return get_combined_feature_tensor_forward(features, self.feature_list, self.feature_modules)\n",
    "\n",
    "class CustomBERTModel(nn.Module):\n",
    "    def __init__(self, num_labels, vocab):\n",
    "          super(CustomBERTModel, self).__init__()\n",
    "          self.num_classes = num_labels\n",
    "          self.feature_list = mnli_dataset.feature_list\n",
    "          print('ASSIGN:', self.num_classes)\n",
    "\n",
    "          self.embedder = self.create_featureful_bert()\n",
    "          self.encoder = CustomPooler2(randomize_weights=True)\n",
    "          self.module1 = MyModule(self.feature_list, vocab)\n",
    "          self.dropout1 = nn.Dropout(p=0.0)\n",
    "        #   self.dropout_decoder = nn.Dropout(p=0.5)\n",
    "          self._decoder_input_size = self.encoder._embedding_dim + self.module1._feature_module_size\n",
    "          self.relation_decoder = nn.Linear(self._decoder_input_size, self.num_classes)\n",
    "\n",
    "          self.history_w = {\n",
    "            'cos': [],\n",
    "            # 'l1_linear': [],\n",
    "            # 'l2': []\n",
    "          }\n",
    "          self.pooler_weight = copy.deepcopy(self.encoder.pooler.dense.weight)\n",
    "          print(self.pooler_weight)\n",
    "\n",
    "    def forward(self, pair_token_ids, token_type_ids, attention_mask, feat):\n",
    "        direction_tensor = feat['dir'].to(device)\n",
    "        embedded_sentence = self.embedder(token_ids=pair_token_ids, #featurefulmebedder\n",
    "                        mask=attention_mask, \n",
    "                        type_ids=token_type_ids,\n",
    "                        segment_concat_mask = None,\n",
    "                        direction_tensor = direction_tensor,\n",
    "                        feature_list = self.feature_list,\n",
    "                        features = feat)\n",
    "        mask = token_type_ids\n",
    "        bertpooler_output = self.encoder(tokens=embedded_sentence, mask=mask)\n",
    "        feat = self.convert_to_feature_list(feat)\n",
    "        feat = self.dropout1(feat)\n",
    "        feat = self.module1(feat)\n",
    "        # print(bertpooler_output.shape, self.module1._feature_module_size, feat.shape)\n",
    "        try:\n",
    "            feat_concat = torch.concat((bertpooler_output, feat),-1)\n",
    "        except:\n",
    "            print(bertpooler_output.shape, feat.shape)\n",
    "            raise ValueError()\n",
    "        assert feat_concat.shape[-1] == self._decoder_input_size\n",
    "        feat_concat = self.dropout1(feat_concat)\n",
    "        # feat_concat = self.dropout_decoder(feat_concat)\n",
    "        linear1_output = self.relation_decoder(feat_concat)\n",
    "        return linear1_output\n",
    "\n",
    "    def compute_pooler_similarity(self):\n",
    "        cur = self.encoder.pooler.dense.weight\n",
    "        pre = self.pooler_weight\n",
    "        print(cur)\n",
    "        print(pre)\n",
    "        assert not torch.all(cur.eq(pre))\n",
    "        for metric in self.history_w.keys():\n",
    "            self.history_w[metric].append(self.similarity(cur, pre, metric))\n",
    "        self.pooler_weight = copy.deepcopy(self.encoder.pooler.dense.weight)\n",
    "\n",
    "    def similarity(self, cur, pre, metric):\n",
    "        metric = 0\n",
    "        n = 0\n",
    "        for A, B in zip(cur.cpu().detach().numpy(), pre.cpu().detach().numpy()):\n",
    "            cosine = np.dot(A,B)/(norm(A)*norm(B))\n",
    "            metric+= cosine\n",
    "            n+=1\n",
    "        return float(metric)/float(n), metric\n",
    "\n",
    "    def create_bert_without_activations(self):\n",
    "        config = BertConfig.from_pretrained(BERT_MODEL, hidden_act='gelu')\n",
    "        bert = BertModel.from_pretrained(BERT_MODEL, config=config)\n",
    "        return bert\n",
    "\n",
    "    def create_featureful_bert(self):\n",
    "        featureful_bert = FeaturefulBertEmbedder(model_name = BERT_MODEL,\n",
    "                                hidden_activation_allen = 'gelu',\n",
    "                                feature_list = self.feature_list, \n",
    "                                vocab=mnli_dataset.vocab)\n",
    "        return featureful_bert\n",
    "\n",
    "    def convert_to_feature_list(self, feat):\n",
    "        feature_linear = [feat[feature_name] for feature_name in self.feature_list]\n",
    "        feature_linear = torch.stack(feature_linear, dim=-1)\n",
    "        return feature_linear\n",
    "        \n",
    "\n",
    "model = CustomBERTModel(mnli_dataset.num_labels, mnli_dataset.vocab)\n",
    "model.to(device)\n",
    "optimizer = AdamW(model.parameters(), lr=5e-6, correct_bias=False) # original 2e-5\n",
    "scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, factor=0.8, mode='max', patience=35, min_lr=5e-7, verbose=True) #original factor=0.6, min_lr=5e-7"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## define training regime"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### prinintg model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CustomBERTModel(\n",
      "  (embedder): FeaturefulBertEmbedder(\n",
      "    (transformer_model): FeaturefulBert(\n",
      "      (embeddings): BertEmbeddings(\n",
      "        (word_embeddings): Embedding(30000, 768, padding_idx=0)\n",
      "        (position_embeddings): Embedding(512, 768)\n",
      "        (token_type_embeddings): Embedding(2, 768)\n",
      "        (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "      (encoder): BertEncoder(\n",
      "        (layer): ModuleList(\n",
      "          (0): BertLayer(\n",
      "            (attention): BertAttention(\n",
      "              (self): BertSelfAttention(\n",
      "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "              (output): BertSelfOutput(\n",
      "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "            )\n",
      "            (intermediate): BertIntermediate(\n",
      "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "              (intermediate_act_fn): GELUActivation()\n",
      "            )\n",
      "            (output): BertOutput(\n",
      "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (1): BertLayer(\n",
      "            (attention): BertAttention(\n",
      "              (self): BertSelfAttention(\n",
      "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "              (output): BertSelfOutput(\n",
      "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "            )\n",
      "            (intermediate): BertIntermediate(\n",
      "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "              (intermediate_act_fn): GELUActivation()\n",
      "            )\n",
      "            (output): BertOutput(\n",
      "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (2): BertLayer(\n",
      "            (attention): BertAttention(\n",
      "              (self): BertSelfAttention(\n",
      "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "              (output): BertSelfOutput(\n",
      "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "            )\n",
      "            (intermediate): BertIntermediate(\n",
      "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "              (intermediate_act_fn): GELUActivation()\n",
      "            )\n",
      "            (output): BertOutput(\n",
      "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (3): BertLayer(\n",
      "            (attention): BertAttention(\n",
      "              (self): BertSelfAttention(\n",
      "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "              (output): BertSelfOutput(\n",
      "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "            )\n",
      "            (intermediate): BertIntermediate(\n",
      "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "              (intermediate_act_fn): GELUActivation()\n",
      "            )\n",
      "            (output): BertOutput(\n",
      "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (4): BertLayer(\n",
      "            (attention): BertAttention(\n",
      "              (self): BertSelfAttention(\n",
      "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "              (output): BertSelfOutput(\n",
      "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "            )\n",
      "            (intermediate): BertIntermediate(\n",
      "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "              (intermediate_act_fn): GELUActivation()\n",
      "            )\n",
      "            (output): BertOutput(\n",
      "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (5): BertLayer(\n",
      "            (attention): BertAttention(\n",
      "              (self): BertSelfAttention(\n",
      "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "              (output): BertSelfOutput(\n",
      "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "            )\n",
      "            (intermediate): BertIntermediate(\n",
      "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "              (intermediate_act_fn): GELUActivation()\n",
      "            )\n",
      "            (output): BertOutput(\n",
      "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (6): BertLayer(\n",
      "            (attention): BertAttention(\n",
      "              (self): BertSelfAttention(\n",
      "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "              (output): BertSelfOutput(\n",
      "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "            )\n",
      "            (intermediate): BertIntermediate(\n",
      "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "              (intermediate_act_fn): GELUActivation()\n",
      "            )\n",
      "            (output): BertOutput(\n",
      "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (7): BertLayer(\n",
      "            (attention): BertAttention(\n",
      "              (self): BertSelfAttention(\n",
      "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "              (output): BertSelfOutput(\n",
      "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "            )\n",
      "            (intermediate): BertIntermediate(\n",
      "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "              (intermediate_act_fn): GELUActivation()\n",
      "            )\n",
      "            (output): BertOutput(\n",
      "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (8): BertLayer(\n",
      "            (attention): BertAttention(\n",
      "              (self): BertSelfAttention(\n",
      "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "              (output): BertSelfOutput(\n",
      "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "            )\n",
      "            (intermediate): BertIntermediate(\n",
      "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "              (intermediate_act_fn): GELUActivation()\n",
      "            )\n",
      "            (output): BertOutput(\n",
      "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (9): BertLayer(\n",
      "            (attention): BertAttention(\n",
      "              (self): BertSelfAttention(\n",
      "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "              (output): BertSelfOutput(\n",
      "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "            )\n",
      "            (intermediate): BertIntermediate(\n",
      "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "              (intermediate_act_fn): GELUActivation()\n",
      "            )\n",
      "            (output): BertOutput(\n",
      "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (10): BertLayer(\n",
      "            (attention): BertAttention(\n",
      "              (self): BertSelfAttention(\n",
      "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "              (output): BertSelfOutput(\n",
      "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "            )\n",
      "            (intermediate): BertIntermediate(\n",
      "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "              (intermediate_act_fn): GELUActivation()\n",
      "            )\n",
      "            (output): BertOutput(\n",
      "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (11): BertLayer(\n",
      "            (attention): BertAttention(\n",
      "              (self): BertSelfAttention(\n",
      "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "              (output): BertSelfOutput(\n",
      "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "            )\n",
      "            (intermediate): BertIntermediate(\n",
      "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "              (intermediate_act_fn): GELUActivation()\n",
      "            )\n",
      "            (output): BertOutput(\n",
      "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (pooler): BertPooler(\n",
      "        (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (activation): Tanh()\n",
      "      )\n",
      "      (feature_modules): ModuleDict(\n",
      "        (distance): Embedding(5, 3, padding_idx=0)\n",
      "        (u1_depdir): Embedding(5, 3, padding_idx=0)\n",
      "        (u2_depdir): Embedding(5, 3, padding_idx=0)\n",
      "        (u2_func): Embedding(23, 5, padding_idx=0)\n",
      "        (u1_position): Embedding(12, 4, padding_idx=0)\n",
      "        (u2_position): Embedding(12, 4, padding_idx=0)\n",
      "        (sat_children): Identity()\n",
      "        (nuc_children): Identity()\n",
      "      )\n",
      "      (feature_projector): Linear(in_features=25, out_features=768, bias=True)\n",
      "    )\n",
      "  )\n",
      "  (encoder): CustomPooler2(\n",
      "    (_dropout): Dropout(p=0.0, inplace=False)\n",
      "    (pooler): BertPooler(\n",
      "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "      (activation): Tanh()\n",
      "    )\n",
      "  )\n",
      "  (module1): MyModule(\n",
      "    (feature_modules): ModuleDict(\n",
      "      (distance): Embedding(5, 3, padding_idx=0)\n",
      "      (u1_depdir): Embedding(5, 3, padding_idx=0)\n",
      "      (u2_depdir): Embedding(5, 3, padding_idx=0)\n",
      "      (u2_func): Embedding(23, 5, padding_idx=0)\n",
      "      (u1_position): Embedding(12, 4, padding_idx=0)\n",
      "      (u2_position): Embedding(12, 4, padding_idx=0)\n",
      "      (sat_children): Identity()\n",
      "      (nuc_children): Identity()\n",
      "    )\n",
      "  )\n",
      "  (dropout1): Dropout(p=0.0, inplace=False)\n",
      "  (relation_decoder): Linear(in_features=792, out_features=26, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "def writer_init(save_path_suffix):\n",
    "    writer_path = 'run1/'+save_path_suffix[:-1]+'/'\n",
    "    if os.path.isdir(writer_path):\n",
    "        filelist = [ f for f in os.listdir(writer_path) if 'events.out' in f ]\n",
    "        print(filelist)\n",
    "        for f in filelist:\n",
    "            os.remove(os.path.join(writer_path, f))\n",
    "    else:\n",
    "        os.mkdir(writer_path)\n",
    "    writer = SummaryWriter(log_dir=writer_path)\n",
    "    return writer\n",
    "\n",
    "writer = writer_init(save_path_suffix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import traceback\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from typing import Optional, Iterable, Dict, Any\n",
    "from EarlyStopperUtil import MetricTracker\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "EPOCHS = 100\n",
    "\n",
    "\n",
    "def train(model, train_loader, val_loader, optimizer, scheduler, rev_label_dict):  \n",
    "  EarlyStopper = MetricTracker(patience=12, metric_name='+accuracy')\n",
    "  best_val_acc = 0\n",
    "\n",
    "\n",
    "  for epoch in range(EPOCHS):\n",
    "    start = time.time()\n",
    "    model.train()\n",
    "    total_train_loss = 0\n",
    "    total_train_acc  = 0\n",
    "    \n",
    "    # logging for scheduler\n",
    "    losses = []\n",
    "    accuracies= []\n",
    "\n",
    "    train_size = 0\n",
    "\n",
    "    for batch_idx, (pair_token_ids, mask_ids, seg_ids, feat, y, idx) in enumerate(train_loader):\n",
    "      train_size+=1\n",
    "      optimizer.zero_grad()\n",
    "      pair_token_ids = pair_token_ids.to(device)\n",
    "      mask_ids = mask_ids.to(device)\n",
    "      seg_ids = seg_ids.to(device)\n",
    "      labels = y.to(device)\n",
    "      feat = feat.to(device)\n",
    "      outputs = model(pair_token_ids, \n",
    "                            token_type_ids=seg_ids, \n",
    "                            attention_mask=mask_ids,\n",
    "                            feat=feat)\n",
    "      criterion = nn.CrossEntropyLoss()\n",
    "      loss = criterion(outputs, labels)\n",
    "      loss.backward()\n",
    "      acc = multi_acc(outputs, labels)\n",
    "      optimizer.step()\n",
    "      total_train_loss += loss.item()\n",
    "      total_train_acc  += acc.item()\n",
    "\n",
    "      losses.append(loss)\n",
    "      accuracies.append(acc)\n",
    "      \n",
    "    mean_loss = sum(losses)/len(losses)\n",
    "    scheduler.step(mean_loss)\n",
    "\n",
    "    train_acc  = total_train_acc/len(train_loader)\n",
    "    train_loss = total_train_loss/len(train_loader)\n",
    "\n",
    "    val_acc, val_loss, cr, model, optimizer = evaluate_accuracy(model, optimizer, val_loader, rev_label_dict, label_dict, None)\n",
    "    if val_acc>best_val_acc:\n",
    "      torch.save(model.state_dict(), save_path_suffix+'_best.pt')\n",
    "      best_val_acc = val_acc\n",
    "      print(f'Epoch {epoch+1}: Best val_acc: {best_val_acc:.4f}')\n",
    "    if val_acc>=best_val_acc:\n",
    "      torch.save(model.state_dict(), save_path_suffix+'_best_latest.pt')\n",
    "      best_val_acc = val_acc\n",
    "      print(f'Epoch {epoch+1}: Best val_acc: {best_val_acc:.4f}')\n",
    "    EarlyStopper.add_metric(val_acc)\n",
    "    if EarlyStopper.should_stop_early(): break\n",
    "\n",
    "    end = time.time()\n",
    "    hours, rem = divmod(end-start, 3600)\n",
    "    minutes, seconds = divmod(rem, 60)\n",
    "\n",
    "    print(f'Epoch {epoch+1}: train_loss: {train_loss:.4f} train_acc: {train_acc:.4f} | val_loss: {val_loss:.4f} val_acc: {val_acc:.4f}')\n",
    "    print(\"{:0>2}:{:0>2}:{:05.2f}\".format(int(hours),int(minutes),seconds))\n",
    "    print(f'train_size: {train_size}')\n",
    "\n",
    "    writer.add_scalar('train_loss', train_loss, epoch)\n",
    "    writer.add_scalar('train_acc', train_acc, epoch)\n",
    "    writer.add_scalar('val_loss', val_loss, epoch)\n",
    "    writer.add_scalar('val_acc', val_acc, epoch)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "### MODIFIED\n",
    "import time\n",
    "import traceback\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from typing import Optional, Iterable, Dict, Any\n",
    "from EarlyStopperUtil import MetricTracker\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "EPOCHS = 100\n",
    "\n",
    "\n",
    "def train(model, train_loader, val_loader, optimizer, scheduler, rev_label_dict):  \n",
    "  EarlyStopper = MetricTracker(patience=12, metric_name='+accuracy')\n",
    "  best_val_acc = 0\n",
    "\n",
    "\n",
    "  for epoch in range(EPOCHS):\n",
    "    start = time.time()\n",
    "    model.train()\n",
    "    total_train_loss = 0\n",
    "    total_train_acc  = 0\n",
    "    \n",
    "    # logging for scheduler\n",
    "    losses = []\n",
    "    accuracies= []\n",
    "\n",
    "    train_size = 0\n",
    "\n",
    "    for batch_idx, (pair_token_ids, mask_ids, seg_ids, feat, y, idx) in enumerate(train_loader):\n",
    "      train_size+=1\n",
    "      optimizer.zero_grad()\n",
    "      pair_token_ids = pair_token_ids.to(device)\n",
    "      mask_ids = mask_ids.to(device)\n",
    "      seg_ids = seg_ids.to(device)\n",
    "      labels = y.to(device)\n",
    "      # feat = feat.to(device)\n",
    "      outputs = model(pair_token_ids, \n",
    "                            token_type_ids=seg_ids, \n",
    "                            attention_mask=mask_ids,\n",
    "                            feat=feat)\n",
    "      criterion = nn.CrossEntropyLoss()\n",
    "      loss = criterion(outputs, labels)\n",
    "      loss.backward()\n",
    "      acc = multi_acc(outputs, labels)\n",
    "      optimizer.step()\n",
    "      total_train_loss += loss.item()\n",
    "      total_train_acc  += acc.item()\n",
    "\n",
    "      losses.append(loss)\n",
    "      accuracies.append(acc)\n",
    "      \n",
    "    mean_loss = sum(losses)/len(losses)\n",
    "    scheduler.step(mean_loss)\n",
    "\n",
    "    train_acc  = total_train_acc/len(train_loader)\n",
    "    train_loss = total_train_loss/len(train_loader)\n",
    "\n",
    "    val_acc, val_loss, cr, model, optimizer = evaluate_accuracy(model, optimizer, val_loader, rev_label_dict, label_dict, None)\n",
    "    if val_acc>best_val_acc:\n",
    "      torch.save(model.state_dict(), save_path_suffix+'_best.pt')\n",
    "      best_val_acc = val_acc\n",
    "      print(f'Epoch {epoch+1}: Best val_acc: {best_val_acc:.4f}')\n",
    "    if val_acc>=best_val_acc:\n",
    "      torch.save(model.state_dict(), save_path_suffix+'_best_latest.pt')\n",
    "      best_val_acc = val_acc\n",
    "      print(f'Epoch {epoch+1}: Best val_acc: {best_val_acc:.4f}')\n",
    "    EarlyStopper.add_metric(val_acc)\n",
    "    if EarlyStopper.should_stop_early(): break\n",
    "\n",
    "    end = time.time()\n",
    "    hours, rem = divmod(end-start, 3600)\n",
    "    minutes, seconds = divmod(rem, 60)\n",
    "\n",
    "    print(f'Epoch {epoch+1}: train_loss: {train_loss:.4f} train_acc: {train_acc:.4f} | val_loss: {val_loss:.4f} val_acc: {val_acc:.4f}')\n",
    "    print(\"{:0>2}:{:0>2}:{:05.2f}\".format(int(hours),int(minutes),seconds))\n",
    "    print(f'train_size: {train_size}')\n",
    "\n",
    "    writer.add_scalar('train_loss', train_loss, epoch)\n",
    "    writer.add_scalar('train_acc', train_acc, epoch)\n",
    "    writer.add_scalar('val_loss', val_loss, epoch)\n",
    "    writer.add_scalar('val_acc', val_acc, epoch)\n",
    "\n",
    "    model.compute_pooler_similarity()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: Best val_acc: 0.1025\n",
      "Epoch 1: Best val_acc: 0.1025\n",
      "Epoch 1: train_loss: 3.0363 train_acc: 0.1432 | val_loss: 2.9031 val_acc: 0.1025\n",
      "00:00:11.18\n",
      "train_size: 110\n",
      "Parameter containing:\n",
      "tensor([[0.8674, 0.1795, 0.4917,  ..., 0.8036, 0.1800, 0.7561],\n",
      "        [0.2520, 0.7512, 0.4346,  ..., 0.7417, 0.5287, 0.5078],\n",
      "        [0.6663, 0.2742, 0.3046,  ..., 0.3145, 0.3703, 0.8865],\n",
      "        ...,\n",
      "        [0.4877, 0.1846, 0.8010,  ..., 0.6173, 0.2062, 0.7919],\n",
      "        [0.9214, 0.3526, 0.9716,  ..., 0.8326, 0.9009, 0.6020],\n",
      "        [0.9650, 0.1513, 0.0447,  ..., 0.4807, 0.9556, 0.3505]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[0.8675, 0.1791, 0.4914,  ..., 0.8033, 0.1800, 0.7565],\n",
      "        [0.2525, 0.7510, 0.4346,  ..., 0.7418, 0.5287, 0.5078],\n",
      "        [0.6666, 0.2743, 0.3049,  ..., 0.3149, 0.3701, 0.8864],\n",
      "        ...,\n",
      "        [0.4879, 0.1848, 0.8009,  ..., 0.6173, 0.2064, 0.7918],\n",
      "        [0.9213, 0.3528, 0.9720,  ..., 0.8329, 0.9012, 0.6019],\n",
      "        [0.9650, 0.1516, 0.0451,  ..., 0.4805, 0.9559, 0.3503]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "Epoch 2: Best val_acc: 0.1762\n",
      "Epoch 2: Best val_acc: 0.1762\n",
      "Epoch 2: train_loss: 2.7532 train_acc: 0.1318 | val_loss: 2.6613 val_acc: 0.1762\n",
      "00:00:09.32\n",
      "train_size: 110\n",
      "Parameter containing:\n",
      "tensor([[0.8675, 0.1797, 0.4920,  ..., 0.8037, 0.1802, 0.7559],\n",
      "        [0.2520, 0.7511, 0.4345,  ..., 0.7415, 0.5286, 0.5078],\n",
      "        [0.6664, 0.2743, 0.3044,  ..., 0.3145, 0.3703, 0.8864],\n",
      "        ...,\n",
      "        [0.4875, 0.1844, 0.8009,  ..., 0.6172, 0.2061, 0.7921],\n",
      "        [0.9215, 0.3526, 0.9715,  ..., 0.8326, 0.9007, 0.6020],\n",
      "        [0.9654, 0.1515, 0.0445,  ..., 0.4808, 0.9554, 0.3503]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[0.8674, 0.1795, 0.4917,  ..., 0.8036, 0.1800, 0.7561],\n",
      "        [0.2520, 0.7512, 0.4346,  ..., 0.7417, 0.5287, 0.5078],\n",
      "        [0.6663, 0.2742, 0.3046,  ..., 0.3145, 0.3703, 0.8865],\n",
      "        ...,\n",
      "        [0.4877, 0.1846, 0.8010,  ..., 0.6173, 0.2062, 0.7919],\n",
      "        [0.9214, 0.3526, 0.9716,  ..., 0.8326, 0.9009, 0.6020],\n",
      "        [0.9650, 0.1513, 0.0447,  ..., 0.4807, 0.9556, 0.3505]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "Epoch 3: Best val_acc: 0.2418\n",
      "Epoch 3: Best val_acc: 0.2418\n",
      "Epoch 3: train_loss: 2.5924 train_acc: 0.2068 | val_loss: 2.5674 val_acc: 0.2418\n",
      "00:00:09.32\n",
      "train_size: 110\n",
      "Parameter containing:\n",
      "tensor([[0.8675, 0.1797, 0.4920,  ..., 0.8037, 0.1802, 0.7559],\n",
      "        [0.2519, 0.7511, 0.4345,  ..., 0.7414, 0.5286, 0.5078],\n",
      "        [0.6664, 0.2743, 0.3043,  ..., 0.3145, 0.3703, 0.8864],\n",
      "        ...,\n",
      "        [0.4875, 0.1845, 0.8011,  ..., 0.6171, 0.2062, 0.7921],\n",
      "        [0.9216, 0.3525, 0.9713,  ..., 0.8328, 0.9006, 0.6020],\n",
      "        [0.9655, 0.1515, 0.0445,  ..., 0.4807, 0.9553, 0.3503]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[0.8675, 0.1797, 0.4920,  ..., 0.8037, 0.1802, 0.7559],\n",
      "        [0.2520, 0.7511, 0.4345,  ..., 0.7415, 0.5286, 0.5078],\n",
      "        [0.6664, 0.2743, 0.3044,  ..., 0.3145, 0.3703, 0.8864],\n",
      "        ...,\n",
      "        [0.4875, 0.1844, 0.8009,  ..., 0.6172, 0.2061, 0.7921],\n",
      "        [0.9215, 0.3526, 0.9715,  ..., 0.8326, 0.9007, 0.6020],\n",
      "        [0.9654, 0.1515, 0.0445,  ..., 0.4808, 0.9554, 0.3503]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "Epoch 4: Best val_acc: 0.2418\n",
      "Epoch 4: train_loss: 2.5374 train_acc: 0.2159 | val_loss: 2.4846 val_acc: 0.2418\n",
      "00:00:08.36\n",
      "train_size: 110\n",
      "Parameter containing:\n",
      "tensor([[0.8675, 0.1797, 0.4920,  ..., 0.8037, 0.1802, 0.7559],\n",
      "        [0.2520, 0.7511, 0.4344,  ..., 0.7414, 0.5285, 0.5077],\n",
      "        [0.6664, 0.2742, 0.3042,  ..., 0.3144, 0.3703, 0.8865],\n",
      "        ...,\n",
      "        [0.4875, 0.1845, 0.8010,  ..., 0.6169, 0.2062, 0.7922],\n",
      "        [0.9215, 0.3525, 0.9712,  ..., 0.8328, 0.9006, 0.6021],\n",
      "        [0.9655, 0.1515, 0.0444,  ..., 0.4806, 0.9552, 0.3504]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[0.8675, 0.1797, 0.4920,  ..., 0.8037, 0.1802, 0.7559],\n",
      "        [0.2519, 0.7511, 0.4345,  ..., 0.7414, 0.5286, 0.5078],\n",
      "        [0.6664, 0.2743, 0.3043,  ..., 0.3145, 0.3703, 0.8864],\n",
      "        ...,\n",
      "        [0.4875, 0.1845, 0.8011,  ..., 0.6171, 0.2062, 0.7921],\n",
      "        [0.9216, 0.3525, 0.9713,  ..., 0.8328, 0.9006, 0.6020],\n",
      "        [0.9655, 0.1515, 0.0445,  ..., 0.4807, 0.9553, 0.3503]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "Epoch 5: train_loss: 2.3682 train_acc: 0.2659 | val_loss: 2.4763 val_acc: 0.2336\n",
      "00:00:07.49\n",
      "train_size: 110\n",
      "Parameter containing:\n",
      "tensor([[0.8674, 0.1796, 0.4920,  ..., 0.8037, 0.1801, 0.7559],\n",
      "        [0.2521, 0.7511, 0.4344,  ..., 0.7416, 0.5286, 0.5076],\n",
      "        [0.6664, 0.2740, 0.3040,  ..., 0.3142, 0.3702, 0.8867],\n",
      "        ...,\n",
      "        [0.4874, 0.1843, 0.8009,  ..., 0.6167, 0.2061, 0.7924],\n",
      "        [0.9216, 0.3525, 0.9711,  ..., 0.8328, 0.9006, 0.6021],\n",
      "        [0.9654, 0.1513, 0.0445,  ..., 0.4804, 0.9553, 0.3506]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[0.8675, 0.1797, 0.4920,  ..., 0.8037, 0.1802, 0.7559],\n",
      "        [0.2520, 0.7511, 0.4344,  ..., 0.7414, 0.5285, 0.5077],\n",
      "        [0.6664, 0.2742, 0.3042,  ..., 0.3144, 0.3703, 0.8865],\n",
      "        ...,\n",
      "        [0.4875, 0.1845, 0.8010,  ..., 0.6169, 0.2062, 0.7922],\n",
      "        [0.9215, 0.3525, 0.9712,  ..., 0.8328, 0.9006, 0.6021],\n",
      "        [0.9655, 0.1515, 0.0444,  ..., 0.4806, 0.9552, 0.3504]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "Epoch 6: train_loss: 2.4044 train_acc: 0.2341 | val_loss: 2.4151 val_acc: 0.2172\n",
      "00:00:07.31\n",
      "train_size: 110\n",
      "Parameter containing:\n",
      "tensor([[0.8673, 0.1794, 0.4918,  ..., 0.8036, 0.1799, 0.7560],\n",
      "        [0.2519, 0.7510, 0.4343,  ..., 0.7415, 0.5285, 0.5076],\n",
      "        [0.6663, 0.2739, 0.3040,  ..., 0.3142, 0.3701, 0.8868],\n",
      "        ...,\n",
      "        [0.4872, 0.1843, 0.8009,  ..., 0.6166, 0.2061, 0.7924],\n",
      "        [0.9215, 0.3525, 0.9712,  ..., 0.8328, 0.9006, 0.6020],\n",
      "        [0.9655, 0.1514, 0.0445,  ..., 0.4805, 0.9553, 0.3505]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[0.8674, 0.1796, 0.4920,  ..., 0.8037, 0.1801, 0.7559],\n",
      "        [0.2521, 0.7511, 0.4344,  ..., 0.7416, 0.5286, 0.5076],\n",
      "        [0.6664, 0.2740, 0.3040,  ..., 0.3142, 0.3702, 0.8867],\n",
      "        ...,\n",
      "        [0.4874, 0.1843, 0.8009,  ..., 0.6167, 0.2061, 0.7924],\n",
      "        [0.9216, 0.3525, 0.9711,  ..., 0.8328, 0.9006, 0.6021],\n",
      "        [0.9654, 0.1513, 0.0445,  ..., 0.4804, 0.9553, 0.3506]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "Epoch 7: Best val_acc: 0.2910\n",
      "Epoch 7: Best val_acc: 0.2910\n",
      "Epoch 7: train_loss: 2.4263 train_acc: 0.2773 | val_loss: 2.3532 val_acc: 0.2910\n",
      "00:00:09.43\n",
      "train_size: 110\n",
      "Parameter containing:\n",
      "tensor([[0.8673, 0.1793, 0.4917,  ..., 0.8034, 0.1798, 0.7563],\n",
      "        [0.2521, 0.7510, 0.4342,  ..., 0.7416, 0.5285, 0.5076],\n",
      "        [0.6664, 0.2740, 0.3040,  ..., 0.3144, 0.3701, 0.8866],\n",
      "        ...,\n",
      "        [0.4872, 0.1842, 0.8008,  ..., 0.6166, 0.2060, 0.7925],\n",
      "        [0.9215, 0.3526, 0.9712,  ..., 0.8328, 0.9005, 0.6019],\n",
      "        [0.9655, 0.1515, 0.0445,  ..., 0.4806, 0.9553, 0.3504]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[0.8673, 0.1794, 0.4918,  ..., 0.8036, 0.1799, 0.7560],\n",
      "        [0.2519, 0.7510, 0.4343,  ..., 0.7415, 0.5285, 0.5076],\n",
      "        [0.6663, 0.2739, 0.3040,  ..., 0.3142, 0.3701, 0.8868],\n",
      "        ...,\n",
      "        [0.4872, 0.1843, 0.8009,  ..., 0.6166, 0.2061, 0.7924],\n",
      "        [0.9215, 0.3525, 0.9712,  ..., 0.8328, 0.9006, 0.6020],\n",
      "        [0.9655, 0.1514, 0.0445,  ..., 0.4805, 0.9553, 0.3505]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "Epoch 8: Best val_acc: 0.2992\n",
      "Epoch 8: Best val_acc: 0.2992\n",
      "Epoch 8: train_loss: 2.2772 train_acc: 0.3227 | val_loss: 2.3112 val_acc: 0.2992\n",
      "00:00:09.57\n",
      "train_size: 110\n",
      "Parameter containing:\n",
      "tensor([[0.8673, 0.1793, 0.4915,  ..., 0.8035, 0.1799, 0.7562],\n",
      "        [0.2521, 0.7510, 0.4342,  ..., 0.7416, 0.5284, 0.5075],\n",
      "        [0.6664, 0.2740, 0.3040,  ..., 0.3144, 0.3701, 0.8866],\n",
      "        ...,\n",
      "        [0.4870, 0.1842, 0.8008,  ..., 0.6165, 0.2059, 0.7925],\n",
      "        [0.9215, 0.3526, 0.9711,  ..., 0.8329, 0.9005, 0.6020],\n",
      "        [0.9655, 0.1514, 0.0443,  ..., 0.4806, 0.9553, 0.3504]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[0.8673, 0.1793, 0.4917,  ..., 0.8034, 0.1798, 0.7563],\n",
      "        [0.2521, 0.7510, 0.4342,  ..., 0.7416, 0.5285, 0.5076],\n",
      "        [0.6664, 0.2740, 0.3040,  ..., 0.3144, 0.3701, 0.8866],\n",
      "        ...,\n",
      "        [0.4872, 0.1842, 0.8008,  ..., 0.6166, 0.2060, 0.7925],\n",
      "        [0.9215, 0.3526, 0.9712,  ..., 0.8328, 0.9005, 0.6019],\n",
      "        [0.9655, 0.1515, 0.0445,  ..., 0.4806, 0.9553, 0.3504]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "Epoch 9: Best val_acc: 0.3279\n",
      "Epoch 9: Best val_acc: 0.3279\n",
      "Epoch 9: train_loss: 2.2750 train_acc: 0.3523 | val_loss: 2.2470 val_acc: 0.3279\n",
      "00:00:09.11\n",
      "train_size: 110\n",
      "Parameter containing:\n",
      "tensor([[0.8671, 0.1793, 0.4914,  ..., 0.8035, 0.1799, 0.7561],\n",
      "        [0.2521, 0.7509, 0.4341,  ..., 0.7416, 0.5284, 0.5075],\n",
      "        [0.6664, 0.2739, 0.3039,  ..., 0.3144, 0.3702, 0.8867],\n",
      "        ...,\n",
      "        [0.4871, 0.1842, 0.8008,  ..., 0.6164, 0.2060, 0.7925],\n",
      "        [0.9214, 0.3525, 0.9711,  ..., 0.8328, 0.9005, 0.6020],\n",
      "        [0.9656, 0.1515, 0.0445,  ..., 0.4808, 0.9553, 0.3503]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[0.8673, 0.1793, 0.4915,  ..., 0.8035, 0.1799, 0.7562],\n",
      "        [0.2521, 0.7510, 0.4342,  ..., 0.7416, 0.5284, 0.5075],\n",
      "        [0.6664, 0.2740, 0.3040,  ..., 0.3144, 0.3701, 0.8866],\n",
      "        ...,\n",
      "        [0.4870, 0.1842, 0.8008,  ..., 0.6165, 0.2059, 0.7925],\n",
      "        [0.9215, 0.3526, 0.9711,  ..., 0.8329, 0.9005, 0.6020],\n",
      "        [0.9655, 0.1514, 0.0443,  ..., 0.4806, 0.9553, 0.3504]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "Epoch 10: train_loss: 2.1330 train_acc: 0.3727 | val_loss: 2.2511 val_acc: 0.2992\n",
      "00:00:07.25\n",
      "train_size: 110\n",
      "Parameter containing:\n",
      "tensor([[0.8671, 0.1794, 0.4914,  ..., 0.8036, 0.1799, 0.7561],\n",
      "        [0.2520, 0.7510, 0.4341,  ..., 0.7416, 0.5284, 0.5074],\n",
      "        [0.6665, 0.2739, 0.3039,  ..., 0.3144, 0.3703, 0.8866],\n",
      "        ...,\n",
      "        [0.4870, 0.1842, 0.8007,  ..., 0.6163, 0.2060, 0.7925],\n",
      "        [0.9214, 0.3526, 0.9711,  ..., 0.8328, 0.9004, 0.6021],\n",
      "        [0.9656, 0.1514, 0.0444,  ..., 0.4810, 0.9552, 0.3502]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[0.8671, 0.1793, 0.4914,  ..., 0.8035, 0.1799, 0.7561],\n",
      "        [0.2521, 0.7509, 0.4341,  ..., 0.7416, 0.5284, 0.5075],\n",
      "        [0.6664, 0.2739, 0.3039,  ..., 0.3144, 0.3702, 0.8867],\n",
      "        ...,\n",
      "        [0.4871, 0.1842, 0.8008,  ..., 0.6164, 0.2060, 0.7925],\n",
      "        [0.9214, 0.3525, 0.9711,  ..., 0.8328, 0.9005, 0.6020],\n",
      "        [0.9656, 0.1515, 0.0445,  ..., 0.4808, 0.9553, 0.3503]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "Epoch 11: train_loss: 2.0594 train_acc: 0.3977 | val_loss: 2.2121 val_acc: 0.3197\n",
      "00:00:07.59\n",
      "train_size: 110\n",
      "Parameter containing:\n",
      "tensor([[0.8672, 0.1794, 0.4913,  ..., 0.8037, 0.1798, 0.7560],\n",
      "        [0.2519, 0.7510, 0.4341,  ..., 0.7416, 0.5284, 0.5073],\n",
      "        [0.6664, 0.2739, 0.3040,  ..., 0.3145, 0.3702, 0.8865],\n",
      "        ...,\n",
      "        [0.4869, 0.1842, 0.8007,  ..., 0.6163, 0.2060, 0.7925],\n",
      "        [0.9214, 0.3526, 0.9712,  ..., 0.8329, 0.9004, 0.6021],\n",
      "        [0.9655, 0.1512, 0.0444,  ..., 0.4810, 0.9552, 0.3502]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[0.8671, 0.1794, 0.4914,  ..., 0.8036, 0.1799, 0.7561],\n",
      "        [0.2520, 0.7510, 0.4341,  ..., 0.7416, 0.5284, 0.5074],\n",
      "        [0.6665, 0.2739, 0.3039,  ..., 0.3144, 0.3703, 0.8866],\n",
      "        ...,\n",
      "        [0.4870, 0.1842, 0.8007,  ..., 0.6163, 0.2060, 0.7925],\n",
      "        [0.9214, 0.3526, 0.9711,  ..., 0.8328, 0.9004, 0.6021],\n",
      "        [0.9656, 0.1514, 0.0444,  ..., 0.4810, 0.9552, 0.3502]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "Epoch 12: train_loss: 2.0562 train_acc: 0.4068 | val_loss: 2.1790 val_acc: 0.3074\n",
      "00:00:07.50\n",
      "train_size: 110\n",
      "Parameter containing:\n",
      "tensor([[0.8671, 0.1793, 0.4913,  ..., 0.8037, 0.1797, 0.7560],\n",
      "        [0.2519, 0.7509, 0.4340,  ..., 0.7416, 0.5284, 0.5073],\n",
      "        [0.6665, 0.2739, 0.3041,  ..., 0.3144, 0.3702, 0.8865],\n",
      "        ...,\n",
      "        [0.4868, 0.1842, 0.8006,  ..., 0.6162, 0.2059, 0.7925],\n",
      "        [0.9214, 0.3526, 0.9712,  ..., 0.8328, 0.9005, 0.6021],\n",
      "        [0.9655, 0.1513, 0.0445,  ..., 0.4810, 0.9553, 0.3502]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[0.8672, 0.1794, 0.4913,  ..., 0.8037, 0.1798, 0.7560],\n",
      "        [0.2519, 0.7510, 0.4341,  ..., 0.7416, 0.5284, 0.5073],\n",
      "        [0.6664, 0.2739, 0.3040,  ..., 0.3145, 0.3702, 0.8865],\n",
      "        ...,\n",
      "        [0.4869, 0.1842, 0.8007,  ..., 0.6163, 0.2060, 0.7925],\n",
      "        [0.9214, 0.3526, 0.9712,  ..., 0.8329, 0.9004, 0.6021],\n",
      "        [0.9655, 0.1512, 0.0444,  ..., 0.4810, 0.9552, 0.3502]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "Epoch 13: train_loss: 2.0875 train_acc: 0.3818 | val_loss: 2.2156 val_acc: 0.2992\n",
      "00:00:07.73\n",
      "train_size: 110\n",
      "Parameter containing:\n",
      "tensor([[0.8671, 0.1794, 0.4913,  ..., 0.8037, 0.1798, 0.7558],\n",
      "        [0.2518, 0.7509, 0.4340,  ..., 0.7414, 0.5285, 0.5074],\n",
      "        [0.6664, 0.2739, 0.3040,  ..., 0.3143, 0.3702, 0.8865],\n",
      "        ...,\n",
      "        [0.4868, 0.1842, 0.8007,  ..., 0.6163, 0.2058, 0.7925],\n",
      "        [0.9214, 0.3527, 0.9712,  ..., 0.8329, 0.9005, 0.6020],\n",
      "        [0.9655, 0.1513, 0.0445,  ..., 0.4811, 0.9553, 0.3502]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[0.8671, 0.1793, 0.4913,  ..., 0.8037, 0.1797, 0.7560],\n",
      "        [0.2519, 0.7509, 0.4340,  ..., 0.7416, 0.5284, 0.5073],\n",
      "        [0.6665, 0.2739, 0.3041,  ..., 0.3144, 0.3702, 0.8865],\n",
      "        ...,\n",
      "        [0.4868, 0.1842, 0.8006,  ..., 0.6162, 0.2059, 0.7925],\n",
      "        [0.9214, 0.3526, 0.9712,  ..., 0.8328, 0.9005, 0.6021],\n",
      "        [0.9655, 0.1513, 0.0445,  ..., 0.4810, 0.9553, 0.3502]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "Epoch 14: train_loss: 1.9948 train_acc: 0.4159 | val_loss: 2.1941 val_acc: 0.3033\n",
      "00:00:07.76\n",
      "train_size: 110\n",
      "Parameter containing:\n",
      "tensor([[0.8671, 0.1794, 0.4913,  ..., 0.8037, 0.1799, 0.7558],\n",
      "        [0.2518, 0.7510, 0.4340,  ..., 0.7415, 0.5285, 0.5073],\n",
      "        [0.6665, 0.2738, 0.3040,  ..., 0.3144, 0.3701, 0.8864],\n",
      "        ...,\n",
      "        [0.4867, 0.1842, 0.8006,  ..., 0.6163, 0.2058, 0.7925],\n",
      "        [0.9214, 0.3527, 0.9711,  ..., 0.8329, 0.9005, 0.6020],\n",
      "        [0.9655, 0.1513, 0.0446,  ..., 0.4811, 0.9553, 0.3502]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[0.8671, 0.1794, 0.4913,  ..., 0.8037, 0.1798, 0.7558],\n",
      "        [0.2518, 0.7509, 0.4340,  ..., 0.7414, 0.5285, 0.5074],\n",
      "        [0.6664, 0.2739, 0.3040,  ..., 0.3143, 0.3702, 0.8865],\n",
      "        ...,\n",
      "        [0.4868, 0.1842, 0.8007,  ..., 0.6163, 0.2058, 0.7925],\n",
      "        [0.9214, 0.3527, 0.9712,  ..., 0.8329, 0.9005, 0.6020],\n",
      "        [0.9655, 0.1513, 0.0445,  ..., 0.4811, 0.9553, 0.3502]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "Epoch 15: Best val_acc: 0.3443\n",
      "Epoch 15: Best val_acc: 0.3443\n",
      "Epoch 15: train_loss: 1.8724 train_acc: 0.4682 | val_loss: 2.1308 val_acc: 0.3443\n",
      "00:00:09.87\n",
      "train_size: 110\n",
      "Parameter containing:\n",
      "tensor([[0.8670, 0.1794, 0.4913,  ..., 0.8036, 0.1798, 0.7559],\n",
      "        [0.2518, 0.7510, 0.4340,  ..., 0.7415, 0.5284, 0.5073],\n",
      "        [0.6665, 0.2738, 0.3040,  ..., 0.3144, 0.3701, 0.8865],\n",
      "        ...,\n",
      "        [0.4866, 0.1841, 0.8005,  ..., 0.6161, 0.2058, 0.7925],\n",
      "        [0.9213, 0.3527, 0.9711,  ..., 0.8328, 0.9005, 0.6021],\n",
      "        [0.9656, 0.1513, 0.0446,  ..., 0.4812, 0.9553, 0.3501]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[0.8671, 0.1794, 0.4913,  ..., 0.8037, 0.1799, 0.7558],\n",
      "        [0.2518, 0.7510, 0.4340,  ..., 0.7415, 0.5285, 0.5073],\n",
      "        [0.6665, 0.2738, 0.3040,  ..., 0.3144, 0.3701, 0.8864],\n",
      "        ...,\n",
      "        [0.4867, 0.1842, 0.8006,  ..., 0.6163, 0.2058, 0.7925],\n",
      "        [0.9214, 0.3527, 0.9711,  ..., 0.8329, 0.9005, 0.6020],\n",
      "        [0.9655, 0.1513, 0.0446,  ..., 0.4811, 0.9553, 0.3502]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "Epoch 16: Best val_acc: 0.3525\n",
      "Epoch 16: Best val_acc: 0.3525\n",
      "Epoch 16: train_loss: 1.9243 train_acc: 0.4432 | val_loss: 2.1233 val_acc: 0.3525\n",
      "00:00:09.75\n",
      "train_size: 110\n",
      "Parameter containing:\n",
      "tensor([[0.8669, 0.1794, 0.4912,  ..., 0.8036, 0.1797, 0.7560],\n",
      "        [0.2518, 0.7510, 0.4340,  ..., 0.7415, 0.5284, 0.5073],\n",
      "        [0.6665, 0.2739, 0.3039,  ..., 0.3143, 0.3701, 0.8865],\n",
      "        ...,\n",
      "        [0.4865, 0.1841, 0.8005,  ..., 0.6161, 0.2057, 0.7926],\n",
      "        [0.9213, 0.3527, 0.9711,  ..., 0.8329, 0.9005, 0.6021],\n",
      "        [0.9657, 0.1513, 0.0446,  ..., 0.4812, 0.9554, 0.3501]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[0.8670, 0.1794, 0.4913,  ..., 0.8036, 0.1798, 0.7559],\n",
      "        [0.2518, 0.7510, 0.4340,  ..., 0.7415, 0.5284, 0.5073],\n",
      "        [0.6665, 0.2738, 0.3040,  ..., 0.3144, 0.3701, 0.8865],\n",
      "        ...,\n",
      "        [0.4866, 0.1841, 0.8005,  ..., 0.6161, 0.2058, 0.7925],\n",
      "        [0.9213, 0.3527, 0.9711,  ..., 0.8328, 0.9005, 0.6021],\n",
      "        [0.9656, 0.1513, 0.0446,  ..., 0.4812, 0.9553, 0.3501]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "Epoch 17: Best val_acc: 0.3689\n",
      "Epoch 17: Best val_acc: 0.3689\n",
      "Epoch 17: train_loss: 1.8310 train_acc: 0.4659 | val_loss: 2.0860 val_acc: 0.3689\n",
      "00:00:09.45\n",
      "train_size: 110\n",
      "Parameter containing:\n",
      "tensor([[0.8669, 0.1794, 0.4913,  ..., 0.8036, 0.1796, 0.7559],\n",
      "        [0.2518, 0.7510, 0.4341,  ..., 0.7415, 0.5283, 0.5073],\n",
      "        [0.6664, 0.2738, 0.3040,  ..., 0.3143, 0.3701, 0.8865],\n",
      "        ...,\n",
      "        [0.4865, 0.1841, 0.8005,  ..., 0.6160, 0.2057, 0.7926],\n",
      "        [0.9214, 0.3528, 0.9712,  ..., 0.8329, 0.9006, 0.6021],\n",
      "        [0.9657, 0.1513, 0.0446,  ..., 0.4812, 0.9554, 0.3501]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[0.8669, 0.1794, 0.4912,  ..., 0.8036, 0.1797, 0.7560],\n",
      "        [0.2518, 0.7510, 0.4340,  ..., 0.7415, 0.5284, 0.5073],\n",
      "        [0.6665, 0.2739, 0.3039,  ..., 0.3143, 0.3701, 0.8865],\n",
      "        ...,\n",
      "        [0.4865, 0.1841, 0.8005,  ..., 0.6161, 0.2057, 0.7926],\n",
      "        [0.9213, 0.3527, 0.9711,  ..., 0.8329, 0.9005, 0.6021],\n",
      "        [0.9657, 0.1513, 0.0446,  ..., 0.4812, 0.9554, 0.3501]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "Epoch 18: train_loss: 1.8170 train_acc: 0.4818 | val_loss: 2.1185 val_acc: 0.3074\n",
      "00:00:07.09\n",
      "train_size: 110\n",
      "Parameter containing:\n",
      "tensor([[0.8668, 0.1794, 0.4913,  ..., 0.8036, 0.1796, 0.7560],\n",
      "        [0.2519, 0.7510, 0.4340,  ..., 0.7416, 0.5283, 0.5073],\n",
      "        [0.6664, 0.2739, 0.3040,  ..., 0.3144, 0.3701, 0.8865],\n",
      "        ...,\n",
      "        [0.4864, 0.1841, 0.8005,  ..., 0.6161, 0.2056, 0.7926],\n",
      "        [0.9213, 0.3528, 0.9712,  ..., 0.8330, 0.9006, 0.6020],\n",
      "        [0.9656, 0.1513, 0.0446,  ..., 0.4813, 0.9554, 0.3500]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[0.8669, 0.1794, 0.4913,  ..., 0.8036, 0.1796, 0.7559],\n",
      "        [0.2518, 0.7510, 0.4341,  ..., 0.7415, 0.5283, 0.5073],\n",
      "        [0.6664, 0.2738, 0.3040,  ..., 0.3143, 0.3701, 0.8865],\n",
      "        ...,\n",
      "        [0.4865, 0.1841, 0.8005,  ..., 0.6160, 0.2057, 0.7926],\n",
      "        [0.9214, 0.3528, 0.9712,  ..., 0.8329, 0.9006, 0.6021],\n",
      "        [0.9657, 0.1513, 0.0446,  ..., 0.4812, 0.9554, 0.3501]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "Epoch 19: train_loss: 1.7694 train_acc: 0.5182 | val_loss: 2.0387 val_acc: 0.3484\n",
      "00:00:07.56\n",
      "train_size: 110\n",
      "Parameter containing:\n",
      "tensor([[0.8668, 0.1793, 0.4912,  ..., 0.8035, 0.1796, 0.7560],\n",
      "        [0.2518, 0.7511, 0.4340,  ..., 0.7416, 0.5284, 0.5072],\n",
      "        [0.6663, 0.2740, 0.3040,  ..., 0.3145, 0.3701, 0.8863],\n",
      "        ...,\n",
      "        [0.4864, 0.1840, 0.8005,  ..., 0.6161, 0.2056, 0.7926],\n",
      "        [0.9214, 0.3528, 0.9712,  ..., 0.8330, 0.9007, 0.6020],\n",
      "        [0.9656, 0.1513, 0.0446,  ..., 0.4813, 0.9554, 0.3501]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[0.8668, 0.1794, 0.4913,  ..., 0.8036, 0.1796, 0.7560],\n",
      "        [0.2519, 0.7510, 0.4340,  ..., 0.7416, 0.5283, 0.5073],\n",
      "        [0.6664, 0.2739, 0.3040,  ..., 0.3144, 0.3701, 0.8865],\n",
      "        ...,\n",
      "        [0.4864, 0.1841, 0.8005,  ..., 0.6161, 0.2056, 0.7926],\n",
      "        [0.9213, 0.3528, 0.9712,  ..., 0.8330, 0.9006, 0.6020],\n",
      "        [0.9656, 0.1513, 0.0446,  ..., 0.4813, 0.9554, 0.3500]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "Epoch 20: train_loss: 1.6553 train_acc: 0.5659 | val_loss: 2.0333 val_acc: 0.3566\n",
      "00:00:07.31\n",
      "train_size: 110\n",
      "Parameter containing:\n",
      "tensor([[0.8668, 0.1793, 0.4911,  ..., 0.8036, 0.1796, 0.7559],\n",
      "        [0.2518, 0.7511, 0.4340,  ..., 0.7416, 0.5285, 0.5072],\n",
      "        [0.6663, 0.2739, 0.3040,  ..., 0.3145, 0.3701, 0.8863],\n",
      "        ...,\n",
      "        [0.4864, 0.1840, 0.8005,  ..., 0.6162, 0.2055, 0.7925],\n",
      "        [0.9213, 0.3528, 0.9711,  ..., 0.8330, 0.9007, 0.6020],\n",
      "        [0.9657, 0.1513, 0.0446,  ..., 0.4813, 0.9555, 0.3500]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[0.8668, 0.1793, 0.4912,  ..., 0.8035, 0.1796, 0.7560],\n",
      "        [0.2518, 0.7511, 0.4340,  ..., 0.7416, 0.5284, 0.5072],\n",
      "        [0.6663, 0.2740, 0.3040,  ..., 0.3145, 0.3701, 0.8863],\n",
      "        ...,\n",
      "        [0.4864, 0.1840, 0.8005,  ..., 0.6161, 0.2056, 0.7926],\n",
      "        [0.9214, 0.3528, 0.9712,  ..., 0.8330, 0.9007, 0.6020],\n",
      "        [0.9656, 0.1513, 0.0446,  ..., 0.4813, 0.9554, 0.3501]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "Epoch 21: train_loss: 1.5475 train_acc: 0.5818 | val_loss: 2.1213 val_acc: 0.3361\n",
      "00:00:07.94\n",
      "train_size: 110\n",
      "Parameter containing:\n",
      "tensor([[0.8668, 0.1793, 0.4912,  ..., 0.8036, 0.1796, 0.7558],\n",
      "        [0.2517, 0.7511, 0.4340,  ..., 0.7416, 0.5284, 0.5072],\n",
      "        [0.6663, 0.2740, 0.3039,  ..., 0.3144, 0.3701, 0.8864],\n",
      "        ...,\n",
      "        [0.4863, 0.1840, 0.8005,  ..., 0.6161, 0.2055, 0.7925],\n",
      "        [0.9213, 0.3528, 0.9712,  ..., 0.8330, 0.9007, 0.6021],\n",
      "        [0.9656, 0.1512, 0.0445,  ..., 0.4813, 0.9555, 0.3500]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[0.8668, 0.1793, 0.4911,  ..., 0.8036, 0.1796, 0.7559],\n",
      "        [0.2518, 0.7511, 0.4340,  ..., 0.7416, 0.5285, 0.5072],\n",
      "        [0.6663, 0.2739, 0.3040,  ..., 0.3145, 0.3701, 0.8863],\n",
      "        ...,\n",
      "        [0.4864, 0.1840, 0.8005,  ..., 0.6162, 0.2055, 0.7925],\n",
      "        [0.9213, 0.3528, 0.9711,  ..., 0.8330, 0.9007, 0.6020],\n",
      "        [0.9657, 0.1513, 0.0446,  ..., 0.4813, 0.9555, 0.3500]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "Epoch 22: train_loss: 1.6831 train_acc: 0.5068 | val_loss: 2.0837 val_acc: 0.3402\n",
      "00:00:07.66\n",
      "train_size: 110\n",
      "Parameter containing:\n",
      "tensor([[0.8669, 0.1793, 0.4911,  ..., 0.8037, 0.1796, 0.7558],\n",
      "        [0.2517, 0.7511, 0.4339,  ..., 0.7416, 0.5284, 0.5072],\n",
      "        [0.6664, 0.2740, 0.3040,  ..., 0.3144, 0.3701, 0.8864],\n",
      "        ...,\n",
      "        [0.4863, 0.1840, 0.8005,  ..., 0.6161, 0.2055, 0.7925],\n",
      "        [0.9213, 0.3528, 0.9711,  ..., 0.8330, 0.9007, 0.6020],\n",
      "        [0.9657, 0.1512, 0.0446,  ..., 0.4814, 0.9554, 0.3499]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[0.8668, 0.1793, 0.4912,  ..., 0.8036, 0.1796, 0.7558],\n",
      "        [0.2517, 0.7511, 0.4340,  ..., 0.7416, 0.5284, 0.5072],\n",
      "        [0.6663, 0.2740, 0.3039,  ..., 0.3144, 0.3701, 0.8864],\n",
      "        ...,\n",
      "        [0.4863, 0.1840, 0.8005,  ..., 0.6161, 0.2055, 0.7925],\n",
      "        [0.9213, 0.3528, 0.9712,  ..., 0.8330, 0.9007, 0.6021],\n",
      "        [0.9656, 0.1512, 0.0445,  ..., 0.4813, 0.9555, 0.3500]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "Epoch 23: train_loss: 1.7017 train_acc: 0.5227 | val_loss: 2.0453 val_acc: 0.3361\n",
      "00:00:07.49\n",
      "train_size: 110\n",
      "Parameter containing:\n",
      "tensor([[0.8669, 0.1792, 0.4911,  ..., 0.8036, 0.1795, 0.7558],\n",
      "        [0.2516, 0.7511, 0.4338,  ..., 0.7416, 0.5283, 0.5071],\n",
      "        [0.6664, 0.2740, 0.3040,  ..., 0.3144, 0.3701, 0.8864],\n",
      "        ...,\n",
      "        [0.4863, 0.1840, 0.8005,  ..., 0.6161, 0.2055, 0.7925],\n",
      "        [0.9213, 0.3528, 0.9711,  ..., 0.8330, 0.9007, 0.6020],\n",
      "        [0.9656, 0.1511, 0.0446,  ..., 0.4814, 0.9554, 0.3499]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[0.8669, 0.1793, 0.4911,  ..., 0.8037, 0.1796, 0.7558],\n",
      "        [0.2517, 0.7511, 0.4339,  ..., 0.7416, 0.5284, 0.5072],\n",
      "        [0.6664, 0.2740, 0.3040,  ..., 0.3144, 0.3701, 0.8864],\n",
      "        ...,\n",
      "        [0.4863, 0.1840, 0.8005,  ..., 0.6161, 0.2055, 0.7925],\n",
      "        [0.9213, 0.3528, 0.9711,  ..., 0.8330, 0.9007, 0.6020],\n",
      "        [0.9657, 0.1512, 0.0446,  ..., 0.4814, 0.9554, 0.3499]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "Epoch 24: train_loss: 1.5436 train_acc: 0.5841 | val_loss: 2.0407 val_acc: 0.3648\n",
      "00:00:07.74\n",
      "train_size: 110\n",
      "Parameter containing:\n",
      "tensor([[0.8669, 0.1793, 0.4910,  ..., 0.8037, 0.1795, 0.7558],\n",
      "        [0.2516, 0.7511, 0.4338,  ..., 0.7416, 0.5283, 0.5071],\n",
      "        [0.6665, 0.2740, 0.3039,  ..., 0.3144, 0.3701, 0.8863],\n",
      "        ...,\n",
      "        [0.4863, 0.1840, 0.8006,  ..., 0.6162, 0.2056, 0.7924],\n",
      "        [0.9212, 0.3528, 0.9711,  ..., 0.8330, 0.9007, 0.6020],\n",
      "        [0.9657, 0.1512, 0.0446,  ..., 0.4814, 0.9554, 0.3500]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[0.8669, 0.1792, 0.4911,  ..., 0.8036, 0.1795, 0.7558],\n",
      "        [0.2516, 0.7511, 0.4338,  ..., 0.7416, 0.5283, 0.5071],\n",
      "        [0.6664, 0.2740, 0.3040,  ..., 0.3144, 0.3701, 0.8864],\n",
      "        ...,\n",
      "        [0.4863, 0.1840, 0.8005,  ..., 0.6161, 0.2055, 0.7925],\n",
      "        [0.9213, 0.3528, 0.9711,  ..., 0.8330, 0.9007, 0.6020],\n",
      "        [0.9656, 0.1511, 0.0446,  ..., 0.4814, 0.9554, 0.3499]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "Epoch 25: Best val_acc: 0.3852\n",
      "Epoch 25: Best val_acc: 0.3852\n",
      "Epoch 25: train_loss: 1.4333 train_acc: 0.6295 | val_loss: 2.0326 val_acc: 0.3852\n",
      "00:00:09.21\n",
      "train_size: 110\n",
      "Parameter containing:\n",
      "tensor([[0.8668, 0.1793, 0.4910,  ..., 0.8036, 0.1795, 0.7559],\n",
      "        [0.2515, 0.7511, 0.4338,  ..., 0.7416, 0.5283, 0.5071],\n",
      "        [0.6665, 0.2741, 0.3039,  ..., 0.3144, 0.3701, 0.8863],\n",
      "        ...,\n",
      "        [0.4863, 0.1840, 0.8005,  ..., 0.6162, 0.2055, 0.7924],\n",
      "        [0.9212, 0.3528, 0.9711,  ..., 0.8330, 0.9007, 0.6021],\n",
      "        [0.9656, 0.1510, 0.0446,  ..., 0.4813, 0.9554, 0.3500]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[0.8669, 0.1793, 0.4910,  ..., 0.8037, 0.1795, 0.7558],\n",
      "        [0.2516, 0.7511, 0.4338,  ..., 0.7416, 0.5283, 0.5071],\n",
      "        [0.6665, 0.2740, 0.3039,  ..., 0.3144, 0.3701, 0.8863],\n",
      "        ...,\n",
      "        [0.4863, 0.1840, 0.8006,  ..., 0.6162, 0.2056, 0.7924],\n",
      "        [0.9212, 0.3528, 0.9711,  ..., 0.8330, 0.9007, 0.6020],\n",
      "        [0.9657, 0.1512, 0.0446,  ..., 0.4814, 0.9554, 0.3500]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "Epoch 26: train_loss: 1.4670 train_acc: 0.6000 | val_loss: 2.0093 val_acc: 0.3607\n",
      "00:00:07.78\n",
      "train_size: 110\n",
      "Parameter containing:\n",
      "tensor([[0.8668, 0.1793, 0.4910,  ..., 0.8037, 0.1795, 0.7558],\n",
      "        [0.2515, 0.7511, 0.4338,  ..., 0.7416, 0.5284, 0.5070],\n",
      "        [0.6666, 0.2740, 0.3039,  ..., 0.3144, 0.3700, 0.8863],\n",
      "        ...,\n",
      "        [0.4864, 0.1839, 0.8005,  ..., 0.6162, 0.2055, 0.7924],\n",
      "        [0.9212, 0.3528, 0.9710,  ..., 0.8329, 0.9007, 0.6021],\n",
      "        [0.9657, 0.1511, 0.0447,  ..., 0.4814, 0.9555, 0.3500]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[0.8668, 0.1793, 0.4910,  ..., 0.8036, 0.1795, 0.7559],\n",
      "        [0.2515, 0.7511, 0.4338,  ..., 0.7416, 0.5283, 0.5071],\n",
      "        [0.6665, 0.2741, 0.3039,  ..., 0.3144, 0.3701, 0.8863],\n",
      "        ...,\n",
      "        [0.4863, 0.1840, 0.8005,  ..., 0.6162, 0.2055, 0.7924],\n",
      "        [0.9212, 0.3528, 0.9711,  ..., 0.8330, 0.9007, 0.6021],\n",
      "        [0.9656, 0.1510, 0.0446,  ..., 0.4813, 0.9554, 0.3500]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "Epoch 27: train_loss: 1.4477 train_acc: 0.6182 | val_loss: 2.0265 val_acc: 0.3607\n",
      "00:00:07.65\n",
      "train_size: 110\n",
      "Parameter containing:\n",
      "tensor([[0.8668, 0.1792, 0.4910,  ..., 0.8036, 0.1794, 0.7559],\n",
      "        [0.2515, 0.7511, 0.4338,  ..., 0.7415, 0.5283, 0.5070],\n",
      "        [0.6667, 0.2739, 0.3039,  ..., 0.3143, 0.3701, 0.8864],\n",
      "        ...,\n",
      "        [0.4864, 0.1839, 0.8005,  ..., 0.6162, 0.2055, 0.7925],\n",
      "        [0.9211, 0.3528, 0.9710,  ..., 0.8329, 0.9006, 0.6021],\n",
      "        [0.9657, 0.1511, 0.0447,  ..., 0.4815, 0.9555, 0.3499]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[0.8668, 0.1793, 0.4910,  ..., 0.8037, 0.1795, 0.7558],\n",
      "        [0.2515, 0.7511, 0.4338,  ..., 0.7416, 0.5284, 0.5070],\n",
      "        [0.6666, 0.2740, 0.3039,  ..., 0.3144, 0.3700, 0.8863],\n",
      "        ...,\n",
      "        [0.4864, 0.1839, 0.8005,  ..., 0.6162, 0.2055, 0.7924],\n",
      "        [0.9212, 0.3528, 0.9710,  ..., 0.8329, 0.9007, 0.6021],\n",
      "        [0.9657, 0.1511, 0.0447,  ..., 0.4814, 0.9555, 0.3500]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "Epoch 28: Best val_acc: 0.3852\n",
      "Epoch 28: train_loss: 1.4713 train_acc: 0.5977 | val_loss: 1.9970 val_acc: 0.3852\n",
      "00:00:08.45\n",
      "train_size: 110\n",
      "Parameter containing:\n",
      "tensor([[0.8668, 0.1793, 0.4910,  ..., 0.8037, 0.1794, 0.7558],\n",
      "        [0.2515, 0.7512, 0.4339,  ..., 0.7415, 0.5284, 0.5070],\n",
      "        [0.6667, 0.2739, 0.3040,  ..., 0.3143, 0.3701, 0.8864],\n",
      "        ...,\n",
      "        [0.4864, 0.1839, 0.8006,  ..., 0.6162, 0.2055, 0.7925],\n",
      "        [0.9211, 0.3528, 0.9710,  ..., 0.8330, 0.9007, 0.6021],\n",
      "        [0.9657, 0.1511, 0.0447,  ..., 0.4815, 0.9555, 0.3499]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[0.8668, 0.1792, 0.4910,  ..., 0.8036, 0.1794, 0.7559],\n",
      "        [0.2515, 0.7511, 0.4338,  ..., 0.7415, 0.5283, 0.5070],\n",
      "        [0.6667, 0.2739, 0.3039,  ..., 0.3143, 0.3701, 0.8864],\n",
      "        ...,\n",
      "        [0.4864, 0.1839, 0.8005,  ..., 0.6162, 0.2055, 0.7925],\n",
      "        [0.9211, 0.3528, 0.9710,  ..., 0.8329, 0.9006, 0.6021],\n",
      "        [0.9657, 0.1511, 0.0447,  ..., 0.4815, 0.9555, 0.3499]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "Epoch 29: Best val_acc: 0.3852\n",
      "Epoch 29: train_loss: 1.3625 train_acc: 0.6409 | val_loss: 1.9719 val_acc: 0.3852\n",
      "00:00:08.87\n",
      "train_size: 110\n",
      "Parameter containing:\n",
      "tensor([[0.8668, 0.1793, 0.4910,  ..., 0.8037, 0.1794, 0.7558],\n",
      "        [0.2515, 0.7513, 0.4338,  ..., 0.7415, 0.5284, 0.5070],\n",
      "        [0.6666, 0.2738, 0.3040,  ..., 0.3142, 0.3700, 0.8865],\n",
      "        ...,\n",
      "        [0.4864, 0.1838, 0.8006,  ..., 0.6162, 0.2053, 0.7925],\n",
      "        [0.9211, 0.3529, 0.9711,  ..., 0.8330, 0.9007, 0.6021],\n",
      "        [0.9657, 0.1512, 0.0446,  ..., 0.4815, 0.9555, 0.3500]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[0.8668, 0.1793, 0.4910,  ..., 0.8037, 0.1794, 0.7558],\n",
      "        [0.2515, 0.7512, 0.4339,  ..., 0.7415, 0.5284, 0.5070],\n",
      "        [0.6667, 0.2739, 0.3040,  ..., 0.3143, 0.3701, 0.8864],\n",
      "        ...,\n",
      "        [0.4864, 0.1839, 0.8006,  ..., 0.6162, 0.2055, 0.7925],\n",
      "        [0.9211, 0.3528, 0.9710,  ..., 0.8330, 0.9007, 0.6021],\n",
      "        [0.9657, 0.1511, 0.0447,  ..., 0.4815, 0.9555, 0.3499]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "Epoch 30: train_loss: 1.3219 train_acc: 0.6659 | val_loss: 1.9707 val_acc: 0.3730\n",
      "00:00:07.42\n",
      "train_size: 110\n",
      "Parameter containing:\n",
      "tensor([[0.8667, 0.1793, 0.4910,  ..., 0.8037, 0.1794, 0.7558],\n",
      "        [0.2515, 0.7512, 0.4338,  ..., 0.7415, 0.5284, 0.5070],\n",
      "        [0.6666, 0.2738, 0.3039,  ..., 0.3142, 0.3700, 0.8865],\n",
      "        ...,\n",
      "        [0.4865, 0.1839, 0.8006,  ..., 0.6161, 0.2054, 0.7925],\n",
      "        [0.9211, 0.3530, 0.9710,  ..., 0.8329, 0.9008, 0.6021],\n",
      "        [0.9656, 0.1512, 0.0446,  ..., 0.4814, 0.9555, 0.3501]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[0.8668, 0.1793, 0.4910,  ..., 0.8037, 0.1794, 0.7558],\n",
      "        [0.2515, 0.7513, 0.4338,  ..., 0.7415, 0.5284, 0.5070],\n",
      "        [0.6666, 0.2738, 0.3040,  ..., 0.3142, 0.3700, 0.8865],\n",
      "        ...,\n",
      "        [0.4864, 0.1838, 0.8006,  ..., 0.6162, 0.2053, 0.7925],\n",
      "        [0.9211, 0.3529, 0.9711,  ..., 0.8330, 0.9007, 0.6021],\n",
      "        [0.9657, 0.1512, 0.0446,  ..., 0.4815, 0.9555, 0.3500]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "Epoch 31: train_loss: 1.1978 train_acc: 0.6773 | val_loss: 2.0050 val_acc: 0.3730\n",
      "00:00:07.63\n",
      "train_size: 110\n",
      "Parameter containing:\n",
      "tensor([[0.8667, 0.1793, 0.4910,  ..., 0.8036, 0.1795, 0.7558],\n",
      "        [0.2515, 0.7512, 0.4339,  ..., 0.7415, 0.5284, 0.5070],\n",
      "        [0.6666, 0.2738, 0.3040,  ..., 0.3141, 0.3699, 0.8866],\n",
      "        ...,\n",
      "        [0.4866, 0.1839, 0.8006,  ..., 0.6162, 0.2054, 0.7925],\n",
      "        [0.9211, 0.3530, 0.9710,  ..., 0.8329, 0.9007, 0.6022],\n",
      "        [0.9656, 0.1512, 0.0446,  ..., 0.4815, 0.9555, 0.3500]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[0.8667, 0.1793, 0.4910,  ..., 0.8037, 0.1794, 0.7558],\n",
      "        [0.2515, 0.7512, 0.4338,  ..., 0.7415, 0.5284, 0.5070],\n",
      "        [0.6666, 0.2738, 0.3039,  ..., 0.3142, 0.3700, 0.8865],\n",
      "        ...,\n",
      "        [0.4865, 0.1839, 0.8006,  ..., 0.6161, 0.2054, 0.7925],\n",
      "        [0.9211, 0.3530, 0.9710,  ..., 0.8329, 0.9008, 0.6021],\n",
      "        [0.9656, 0.1512, 0.0446,  ..., 0.4814, 0.9555, 0.3501]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "Epoch 32: train_loss: 1.2087 train_acc: 0.6886 | val_loss: 1.9904 val_acc: 0.3525\n",
      "00:00:07.82\n",
      "train_size: 110\n",
      "Parameter containing:\n",
      "tensor([[0.8667, 0.1793, 0.4910,  ..., 0.8036, 0.1794, 0.7559],\n",
      "        [0.2515, 0.7512, 0.4338,  ..., 0.7415, 0.5284, 0.5070],\n",
      "        [0.6666, 0.2738, 0.3041,  ..., 0.3142, 0.3699, 0.8866],\n",
      "        ...,\n",
      "        [0.4866, 0.1839, 0.8006,  ..., 0.6161, 0.2053, 0.7925],\n",
      "        [0.9210, 0.3529, 0.9710,  ..., 0.8329, 0.9007, 0.6022],\n",
      "        [0.9657, 0.1512, 0.0446,  ..., 0.4816, 0.9556, 0.3500]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[0.8667, 0.1793, 0.4910,  ..., 0.8036, 0.1795, 0.7558],\n",
      "        [0.2515, 0.7512, 0.4339,  ..., 0.7415, 0.5284, 0.5070],\n",
      "        [0.6666, 0.2738, 0.3040,  ..., 0.3141, 0.3699, 0.8866],\n",
      "        ...,\n",
      "        [0.4866, 0.1839, 0.8006,  ..., 0.6162, 0.2054, 0.7925],\n",
      "        [0.9211, 0.3530, 0.9710,  ..., 0.8329, 0.9007, 0.6022],\n",
      "        [0.9656, 0.1512, 0.0446,  ..., 0.4815, 0.9555, 0.3500]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "Epoch 33: train_loss: 1.2389 train_acc: 0.6477 | val_loss: 1.9667 val_acc: 0.3689\n",
      "00:00:07.40\n",
      "train_size: 110\n",
      "Parameter containing:\n",
      "tensor([[0.8666, 0.1793, 0.4910,  ..., 0.8036, 0.1794, 0.7559],\n",
      "        [0.2516, 0.7512, 0.4338,  ..., 0.7415, 0.5284, 0.5070],\n",
      "        [0.6666, 0.2738, 0.3041,  ..., 0.3142, 0.3700, 0.8865],\n",
      "        ...,\n",
      "        [0.4865, 0.1839, 0.8006,  ..., 0.6161, 0.2053, 0.7925],\n",
      "        [0.9210, 0.3528, 0.9709,  ..., 0.8328, 0.9007, 0.6022],\n",
      "        [0.9656, 0.1513, 0.0446,  ..., 0.4816, 0.9556, 0.3500]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[0.8667, 0.1793, 0.4910,  ..., 0.8036, 0.1794, 0.7559],\n",
      "        [0.2515, 0.7512, 0.4338,  ..., 0.7415, 0.5284, 0.5070],\n",
      "        [0.6666, 0.2738, 0.3041,  ..., 0.3142, 0.3699, 0.8866],\n",
      "        ...,\n",
      "        [0.4866, 0.1839, 0.8006,  ..., 0.6161, 0.2053, 0.7925],\n",
      "        [0.9210, 0.3529, 0.9710,  ..., 0.8329, 0.9007, 0.6022],\n",
      "        [0.9657, 0.1512, 0.0446,  ..., 0.4816, 0.9556, 0.3500]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "Epoch 34: train_loss: 1.1328 train_acc: 0.7250 | val_loss: 1.9586 val_acc: 0.3770\n",
      "00:00:07.17\n",
      "train_size: 110\n",
      "Parameter containing:\n",
      "tensor([[0.8666, 0.1793, 0.4910,  ..., 0.8036, 0.1793, 0.7558],\n",
      "        [0.2516, 0.7512, 0.4338,  ..., 0.7414, 0.5284, 0.5071],\n",
      "        [0.6666, 0.2738, 0.3041,  ..., 0.3142, 0.3700, 0.8865],\n",
      "        ...,\n",
      "        [0.4866, 0.1839, 0.8007,  ..., 0.6162, 0.2053, 0.7925],\n",
      "        [0.9209, 0.3528, 0.9709,  ..., 0.8328, 0.9007, 0.6022],\n",
      "        [0.9656, 0.1513, 0.0446,  ..., 0.4816, 0.9556, 0.3500]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[0.8666, 0.1793, 0.4910,  ..., 0.8036, 0.1794, 0.7559],\n",
      "        [0.2516, 0.7512, 0.4338,  ..., 0.7415, 0.5284, 0.5070],\n",
      "        [0.6666, 0.2738, 0.3041,  ..., 0.3142, 0.3700, 0.8865],\n",
      "        ...,\n",
      "        [0.4865, 0.1839, 0.8006,  ..., 0.6161, 0.2053, 0.7925],\n",
      "        [0.9210, 0.3528, 0.9709,  ..., 0.8328, 0.9007, 0.6022],\n",
      "        [0.9656, 0.1513, 0.0446,  ..., 0.4816, 0.9556, 0.3500]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "Epoch 35: Best val_acc: 0.3893\n",
      "Epoch 35: Best val_acc: 0.3893\n",
      "Epoch 35: train_loss: 1.0488 train_acc: 0.7477 | val_loss: 2.0012 val_acc: 0.3893\n",
      "00:00:09.52\n",
      "train_size: 110\n",
      "Parameter containing:\n",
      "tensor([[0.8665, 0.1792, 0.4909,  ..., 0.8036, 0.1793, 0.7558],\n",
      "        [0.2516, 0.7511, 0.4337,  ..., 0.7415, 0.5283, 0.5071],\n",
      "        [0.6667, 0.2738, 0.3042,  ..., 0.3143, 0.3701, 0.8865],\n",
      "        ...,\n",
      "        [0.4866, 0.1839, 0.8006,  ..., 0.6162, 0.2054, 0.7925],\n",
      "        [0.9209, 0.3529, 0.9709,  ..., 0.8328, 0.9007, 0.6021],\n",
      "        [0.9656, 0.1513, 0.0445,  ..., 0.4817, 0.9556, 0.3500]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[0.8666, 0.1793, 0.4910,  ..., 0.8036, 0.1793, 0.7558],\n",
      "        [0.2516, 0.7512, 0.4338,  ..., 0.7414, 0.5284, 0.5071],\n",
      "        [0.6666, 0.2738, 0.3041,  ..., 0.3142, 0.3700, 0.8865],\n",
      "        ...,\n",
      "        [0.4866, 0.1839, 0.8007,  ..., 0.6162, 0.2053, 0.7925],\n",
      "        [0.9209, 0.3528, 0.9709,  ..., 0.8328, 0.9007, 0.6022],\n",
      "        [0.9656, 0.1513, 0.0446,  ..., 0.4816, 0.9556, 0.3500]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "Epoch 36: Best val_acc: 0.3975\n",
      "Epoch 36: Best val_acc: 0.3975\n",
      "Epoch 36: train_loss: 1.0890 train_acc: 0.7068 | val_loss: 1.9592 val_acc: 0.3975\n",
      "00:00:09.36\n",
      "train_size: 110\n",
      "Parameter containing:\n",
      "tensor([[0.8665, 0.1792, 0.4909,  ..., 0.8036, 0.1793, 0.7558],\n",
      "        [0.2516, 0.7511, 0.4337,  ..., 0.7414, 0.5283, 0.5071],\n",
      "        [0.6667, 0.2738, 0.3041,  ..., 0.3143, 0.3700, 0.8865],\n",
      "        ...,\n",
      "        [0.4866, 0.1839, 0.8005,  ..., 0.6162, 0.2054, 0.7925],\n",
      "        [0.9209, 0.3529, 0.9709,  ..., 0.8329, 0.9008, 0.6021],\n",
      "        [0.9657, 0.1513, 0.0446,  ..., 0.4817, 0.9556, 0.3499]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[0.8665, 0.1792, 0.4909,  ..., 0.8036, 0.1793, 0.7558],\n",
      "        [0.2516, 0.7511, 0.4337,  ..., 0.7415, 0.5283, 0.5071],\n",
      "        [0.6667, 0.2738, 0.3042,  ..., 0.3143, 0.3701, 0.8865],\n",
      "        ...,\n",
      "        [0.4866, 0.1839, 0.8006,  ..., 0.6162, 0.2054, 0.7925],\n",
      "        [0.9209, 0.3529, 0.9709,  ..., 0.8328, 0.9007, 0.6021],\n",
      "        [0.9656, 0.1513, 0.0445,  ..., 0.4817, 0.9556, 0.3500]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "Epoch 00037: reducing learning rate of group 0 to 4.0000e-06.\n",
      "Epoch 37: train_loss: 1.1078 train_acc: 0.7114 | val_loss: 1.9677 val_acc: 0.3770\n",
      "00:00:07.85\n",
      "train_size: 110\n",
      "Parameter containing:\n",
      "tensor([[0.8665, 0.1792, 0.4910,  ..., 0.8037, 0.1793, 0.7558],\n",
      "        [0.2516, 0.7511, 0.4337,  ..., 0.7414, 0.5283, 0.5071],\n",
      "        [0.6667, 0.2738, 0.3041,  ..., 0.3143, 0.3700, 0.8865],\n",
      "        ...,\n",
      "        [0.4865, 0.1839, 0.8005,  ..., 0.6161, 0.2054, 0.7925],\n",
      "        [0.9209, 0.3529, 0.9709,  ..., 0.8329, 0.9008, 0.6021],\n",
      "        [0.9656, 0.1513, 0.0445,  ..., 0.4818, 0.9556, 0.3499]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[0.8665, 0.1792, 0.4909,  ..., 0.8036, 0.1793, 0.7558],\n",
      "        [0.2516, 0.7511, 0.4337,  ..., 0.7414, 0.5283, 0.5071],\n",
      "        [0.6667, 0.2738, 0.3041,  ..., 0.3143, 0.3700, 0.8865],\n",
      "        ...,\n",
      "        [0.4866, 0.1839, 0.8005,  ..., 0.6162, 0.2054, 0.7925],\n",
      "        [0.9209, 0.3529, 0.9709,  ..., 0.8329, 0.9008, 0.6021],\n",
      "        [0.9657, 0.1513, 0.0446,  ..., 0.4817, 0.9556, 0.3499]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "Epoch 38: train_loss: 1.0719 train_acc: 0.7250 | val_loss: 1.9481 val_acc: 0.3852\n",
      "00:00:07.92\n",
      "train_size: 110\n",
      "Parameter containing:\n",
      "tensor([[0.8665, 0.1792, 0.4909,  ..., 0.8037, 0.1793, 0.7557],\n",
      "        [0.2516, 0.7511, 0.4338,  ..., 0.7414, 0.5283, 0.5071],\n",
      "        [0.6667, 0.2738, 0.3041,  ..., 0.3143, 0.3700, 0.8865],\n",
      "        ...,\n",
      "        [0.4865, 0.1839, 0.8005,  ..., 0.6162, 0.2054, 0.7925],\n",
      "        [0.9208, 0.3529, 0.9708,  ..., 0.8330, 0.9008, 0.6021],\n",
      "        [0.9657, 0.1513, 0.0445,  ..., 0.4818, 0.9557, 0.3499]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[0.8665, 0.1792, 0.4910,  ..., 0.8037, 0.1793, 0.7558],\n",
      "        [0.2516, 0.7511, 0.4337,  ..., 0.7414, 0.5283, 0.5071],\n",
      "        [0.6667, 0.2738, 0.3041,  ..., 0.3143, 0.3700, 0.8865],\n",
      "        ...,\n",
      "        [0.4865, 0.1839, 0.8005,  ..., 0.6161, 0.2054, 0.7925],\n",
      "        [0.9209, 0.3529, 0.9709,  ..., 0.8329, 0.9008, 0.6021],\n",
      "        [0.9656, 0.1513, 0.0445,  ..., 0.4818, 0.9556, 0.3499]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "Epoch 39: train_loss: 0.9815 train_acc: 0.7455 | val_loss: 2.0189 val_acc: 0.3770\n",
      "00:00:07.90\n",
      "train_size: 110\n",
      "Parameter containing:\n",
      "tensor([[0.8665, 0.1793, 0.4909,  ..., 0.8037, 0.1793, 0.7557],\n",
      "        [0.2516, 0.7511, 0.4338,  ..., 0.7414, 0.5282, 0.5071],\n",
      "        [0.6666, 0.2737, 0.3041,  ..., 0.3142, 0.3700, 0.8865],\n",
      "        ...,\n",
      "        [0.4865, 0.1839, 0.8006,  ..., 0.6162, 0.2054, 0.7925],\n",
      "        [0.9208, 0.3529, 0.9707,  ..., 0.8330, 0.9008, 0.6021],\n",
      "        [0.9657, 0.1513, 0.0445,  ..., 0.4818, 0.9557, 0.3499]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[0.8665, 0.1792, 0.4909,  ..., 0.8037, 0.1793, 0.7557],\n",
      "        [0.2516, 0.7511, 0.4338,  ..., 0.7414, 0.5283, 0.5071],\n",
      "        [0.6667, 0.2738, 0.3041,  ..., 0.3143, 0.3700, 0.8865],\n",
      "        ...,\n",
      "        [0.4865, 0.1839, 0.8005,  ..., 0.6162, 0.2054, 0.7925],\n",
      "        [0.9208, 0.3529, 0.9708,  ..., 0.8330, 0.9008, 0.6021],\n",
      "        [0.9657, 0.1513, 0.0445,  ..., 0.4818, 0.9557, 0.3499]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "Epoch 40: train_loss: 0.9581 train_acc: 0.7705 | val_loss: 1.9758 val_acc: 0.3770\n",
      "00:00:07.93\n",
      "train_size: 110\n",
      "Parameter containing:\n",
      "tensor([[0.8665, 0.1793, 0.4910,  ..., 0.8038, 0.1793, 0.7557],\n",
      "        [0.2516, 0.7511, 0.4338,  ..., 0.7414, 0.5282, 0.5071],\n",
      "        [0.6666, 0.2737, 0.3041,  ..., 0.3142, 0.3700, 0.8865],\n",
      "        ...,\n",
      "        [0.4865, 0.1839, 0.8007,  ..., 0.6162, 0.2054, 0.7925],\n",
      "        [0.9207, 0.3529, 0.9707,  ..., 0.8329, 0.9008, 0.6021],\n",
      "        [0.9657, 0.1514, 0.0445,  ..., 0.4818, 0.9558, 0.3499]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[0.8665, 0.1793, 0.4909,  ..., 0.8037, 0.1793, 0.7557],\n",
      "        [0.2516, 0.7511, 0.4338,  ..., 0.7414, 0.5282, 0.5071],\n",
      "        [0.6666, 0.2737, 0.3041,  ..., 0.3142, 0.3700, 0.8865],\n",
      "        ...,\n",
      "        [0.4865, 0.1839, 0.8006,  ..., 0.6162, 0.2054, 0.7925],\n",
      "        [0.9208, 0.3529, 0.9707,  ..., 0.8330, 0.9008, 0.6021],\n",
      "        [0.9657, 0.1513, 0.0445,  ..., 0.4818, 0.9557, 0.3499]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "Epoch 41: train_loss: 0.8887 train_acc: 0.8000 | val_loss: 1.9719 val_acc: 0.3730\n",
      "00:00:07.93\n",
      "train_size: 110\n",
      "Parameter containing:\n",
      "tensor([[0.8665, 0.1794, 0.4910,  ..., 0.8038, 0.1794, 0.7556],\n",
      "        [0.2517, 0.7511, 0.4339,  ..., 0.7414, 0.5282, 0.5071],\n",
      "        [0.6665, 0.2737, 0.3041,  ..., 0.3143, 0.3700, 0.8865],\n",
      "        ...,\n",
      "        [0.4865, 0.1839, 0.8007,  ..., 0.6163, 0.2054, 0.7925],\n",
      "        [0.9206, 0.3528, 0.9707,  ..., 0.8329, 0.9007, 0.6022],\n",
      "        [0.9657, 0.1514, 0.0445,  ..., 0.4818, 0.9558, 0.3499]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[0.8665, 0.1793, 0.4910,  ..., 0.8038, 0.1793, 0.7557],\n",
      "        [0.2516, 0.7511, 0.4338,  ..., 0.7414, 0.5282, 0.5071],\n",
      "        [0.6666, 0.2737, 0.3041,  ..., 0.3142, 0.3700, 0.8865],\n",
      "        ...,\n",
      "        [0.4865, 0.1839, 0.8007,  ..., 0.6162, 0.2054, 0.7925],\n",
      "        [0.9207, 0.3529, 0.9707,  ..., 0.8329, 0.9008, 0.6021],\n",
      "        [0.9657, 0.1514, 0.0445,  ..., 0.4818, 0.9558, 0.3499]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "Epoch 42: train_loss: 0.9466 train_acc: 0.7545 | val_loss: 2.0095 val_acc: 0.3607\n",
      "00:00:07.55\n",
      "train_size: 110\n",
      "Parameter containing:\n",
      "tensor([[0.8665, 0.1794, 0.4910,  ..., 0.8038, 0.1794, 0.7556],\n",
      "        [0.2517, 0.7511, 0.4339,  ..., 0.7414, 0.5282, 0.5071],\n",
      "        [0.6665, 0.2736, 0.3041,  ..., 0.3142, 0.3700, 0.8865],\n",
      "        ...,\n",
      "        [0.4864, 0.1839, 0.8008,  ..., 0.6163, 0.2054, 0.7925],\n",
      "        [0.9206, 0.3528, 0.9706,  ..., 0.8329, 0.9006, 0.6022],\n",
      "        [0.9656, 0.1514, 0.0445,  ..., 0.4818, 0.9558, 0.3499]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[0.8665, 0.1794, 0.4910,  ..., 0.8038, 0.1794, 0.7556],\n",
      "        [0.2517, 0.7511, 0.4339,  ..., 0.7414, 0.5282, 0.5071],\n",
      "        [0.6665, 0.2737, 0.3041,  ..., 0.3143, 0.3700, 0.8865],\n",
      "        ...,\n",
      "        [0.4865, 0.1839, 0.8007,  ..., 0.6163, 0.2054, 0.7925],\n",
      "        [0.9206, 0.3528, 0.9707,  ..., 0.8329, 0.9007, 0.6022],\n",
      "        [0.9657, 0.1514, 0.0445,  ..., 0.4818, 0.9558, 0.3499]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "Epoch 43: train_loss: 0.9203 train_acc: 0.7818 | val_loss: 2.0054 val_acc: 0.3566\n",
      "00:00:07.90\n",
      "train_size: 110\n",
      "Parameter containing:\n",
      "tensor([[0.8665, 0.1794, 0.4910,  ..., 0.8038, 0.1794, 0.7556],\n",
      "        [0.2516, 0.7511, 0.4339,  ..., 0.7414, 0.5282, 0.5071],\n",
      "        [0.6665, 0.2736, 0.3041,  ..., 0.3143, 0.3700, 0.8864],\n",
      "        ...,\n",
      "        [0.4864, 0.1839, 0.8007,  ..., 0.6163, 0.2053, 0.7925],\n",
      "        [0.9206, 0.3528, 0.9706,  ..., 0.8329, 0.9006, 0.6023],\n",
      "        [0.9656, 0.1513, 0.0445,  ..., 0.4818, 0.9558, 0.3499]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[0.8665, 0.1794, 0.4910,  ..., 0.8038, 0.1794, 0.7556],\n",
      "        [0.2517, 0.7511, 0.4339,  ..., 0.7414, 0.5282, 0.5071],\n",
      "        [0.6665, 0.2736, 0.3041,  ..., 0.3142, 0.3700, 0.8865],\n",
      "        ...,\n",
      "        [0.4864, 0.1839, 0.8008,  ..., 0.6163, 0.2054, 0.7925],\n",
      "        [0.9206, 0.3528, 0.9706,  ..., 0.8329, 0.9006, 0.6022],\n",
      "        [0.9656, 0.1514, 0.0445,  ..., 0.4818, 0.9558, 0.3499]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "Epoch 44: train_loss: 0.8252 train_acc: 0.8205 | val_loss: 2.0126 val_acc: 0.3484\n",
      "00:00:07.92\n",
      "train_size: 110\n",
      "Parameter containing:\n",
      "tensor([[0.8665, 0.1794, 0.4910,  ..., 0.8038, 0.1794, 0.7556],\n",
      "        [0.2516, 0.7510, 0.4339,  ..., 0.7414, 0.5282, 0.5071],\n",
      "        [0.6666, 0.2736, 0.3042,  ..., 0.3143, 0.3699, 0.8864],\n",
      "        ...,\n",
      "        [0.4864, 0.1839, 0.8008,  ..., 0.6163, 0.2053, 0.7924],\n",
      "        [0.9206, 0.3528, 0.9707,  ..., 0.8330, 0.9006, 0.6022],\n",
      "        [0.9656, 0.1513, 0.0446,  ..., 0.4819, 0.9558, 0.3499]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[0.8665, 0.1794, 0.4910,  ..., 0.8038, 0.1794, 0.7556],\n",
      "        [0.2516, 0.7511, 0.4339,  ..., 0.7414, 0.5282, 0.5071],\n",
      "        [0.6665, 0.2736, 0.3041,  ..., 0.3143, 0.3700, 0.8864],\n",
      "        ...,\n",
      "        [0.4864, 0.1839, 0.8007,  ..., 0.6163, 0.2053, 0.7925],\n",
      "        [0.9206, 0.3528, 0.9706,  ..., 0.8329, 0.9006, 0.6023],\n",
      "        [0.9656, 0.1513, 0.0445,  ..., 0.4818, 0.9558, 0.3499]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "Epoch 45: train_loss: 0.7657 train_acc: 0.8295 | val_loss: 1.9861 val_acc: 0.3934\n",
      "00:00:07.93\n",
      "train_size: 110\n",
      "Parameter containing:\n",
      "tensor([[0.8666, 0.1794, 0.4909,  ..., 0.8039, 0.1795, 0.7555],\n",
      "        [0.2517, 0.7510, 0.4339,  ..., 0.7414, 0.5282, 0.5071],\n",
      "        [0.6666, 0.2736, 0.3042,  ..., 0.3142, 0.3699, 0.8865],\n",
      "        ...,\n",
      "        [0.4863, 0.1839, 0.8008,  ..., 0.6164, 0.2053, 0.7924],\n",
      "        [0.9206, 0.3528, 0.9707,  ..., 0.8330, 0.9006, 0.6022],\n",
      "        [0.9656, 0.1513, 0.0446,  ..., 0.4819, 0.9557, 0.3499]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[0.8665, 0.1794, 0.4910,  ..., 0.8038, 0.1794, 0.7556],\n",
      "        [0.2516, 0.7510, 0.4339,  ..., 0.7414, 0.5282, 0.5071],\n",
      "        [0.6666, 0.2736, 0.3042,  ..., 0.3143, 0.3699, 0.8864],\n",
      "        ...,\n",
      "        [0.4864, 0.1839, 0.8008,  ..., 0.6163, 0.2053, 0.7924],\n",
      "        [0.9206, 0.3528, 0.9707,  ..., 0.8330, 0.9006, 0.6022],\n",
      "        [0.9656, 0.1513, 0.0446,  ..., 0.4819, 0.9558, 0.3499]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "Epoch 46: train_loss: 0.7496 train_acc: 0.8477 | val_loss: 2.0444 val_acc: 0.3730\n",
      "00:00:07.82\n",
      "train_size: 110\n",
      "Parameter containing:\n",
      "tensor([[0.8665, 0.1794, 0.4909,  ..., 0.8038, 0.1795, 0.7556],\n",
      "        [0.2517, 0.7510, 0.4338,  ..., 0.7414, 0.5282, 0.5071],\n",
      "        [0.6666, 0.2736, 0.3042,  ..., 0.3142, 0.3699, 0.8865],\n",
      "        ...,\n",
      "        [0.4863, 0.1839, 0.8009,  ..., 0.6164, 0.2053, 0.7924],\n",
      "        [0.9206, 0.3528, 0.9706,  ..., 0.8330, 0.9006, 0.6022],\n",
      "        [0.9655, 0.1513, 0.0446,  ..., 0.4818, 0.9557, 0.3499]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[0.8666, 0.1794, 0.4909,  ..., 0.8039, 0.1795, 0.7555],\n",
      "        [0.2517, 0.7510, 0.4339,  ..., 0.7414, 0.5282, 0.5071],\n",
      "        [0.6666, 0.2736, 0.3042,  ..., 0.3142, 0.3699, 0.8865],\n",
      "        ...,\n",
      "        [0.4863, 0.1839, 0.8008,  ..., 0.6164, 0.2053, 0.7924],\n",
      "        [0.9206, 0.3528, 0.9707,  ..., 0.8330, 0.9006, 0.6022],\n",
      "        [0.9656, 0.1513, 0.0446,  ..., 0.4819, 0.9557, 0.3499]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "Epoch 47: train_loss: 0.8743 train_acc: 0.7705 | val_loss: 2.0114 val_acc: 0.3607\n",
      "00:00:07.73\n",
      "train_size: 110\n",
      "Parameter containing:\n",
      "tensor([[0.8665, 0.1794, 0.4909,  ..., 0.8038, 0.1794, 0.7556],\n",
      "        [0.2517, 0.7510, 0.4338,  ..., 0.7414, 0.5282, 0.5071],\n",
      "        [0.6666, 0.2736, 0.3042,  ..., 0.3142, 0.3699, 0.8865],\n",
      "        ...,\n",
      "        [0.4863, 0.1838, 0.8009,  ..., 0.6164, 0.2052, 0.7924],\n",
      "        [0.9205, 0.3527, 0.9706,  ..., 0.8330, 0.9005, 0.6022],\n",
      "        [0.9656, 0.1513, 0.0445,  ..., 0.4818, 0.9558, 0.3499]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[0.8665, 0.1794, 0.4909,  ..., 0.8038, 0.1795, 0.7556],\n",
      "        [0.2517, 0.7510, 0.4338,  ..., 0.7414, 0.5282, 0.5071],\n",
      "        [0.6666, 0.2736, 0.3042,  ..., 0.3142, 0.3699, 0.8865],\n",
      "        ...,\n",
      "        [0.4863, 0.1839, 0.8009,  ..., 0.6164, 0.2053, 0.7924],\n",
      "        [0.9206, 0.3528, 0.9706,  ..., 0.8330, 0.9006, 0.6022],\n",
      "        [0.9655, 0.1513, 0.0446,  ..., 0.4818, 0.9557, 0.3499]],\n",
      "       device='cuda:0', requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "from sklearn.exceptions import DataConversionWarning\n",
    "with warnings.catch_warnings():\n",
    "    warnings.simplefilter(\"ignore\")\n",
    "\n",
    "    train(model, train_loader, val_loader, optimizer, scheduler, rev_label_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.9999999103602022, 0.9999999792004625, 0.9999999878151963, 0.9999999902987232, 0.9999999892121801, 0.9999999874271452, 0.9999999960418791, 0.9999999988358468, 0.9999999955762178, 1.0000000009313226, 0.9999999975164732, 0.9999999982925752, 0.9999999970508119, 0.9999999964299301, 0.9999999928598603, 0.9999999984477957, 0.9999999985254059, 0.9999999979045242, 1.0000000029491882, 0.999999999611949, 1.0000000034148495, 1.000000001474594, 1.0000000000776101, 1.0000000022506963, 1.000000005044664, 1.000000001086543, 0.9999999997671694, 0.9999999965075403, 0.9999999992238978, 1.0000000016298145, 1.0000000030267984, 1.0000000018626451, 1.0000000022506963, 1.0000000027939677, 1.0000000003104408, 1.0000000055879354, 0.9999999991462877, 0.9999999982149651, 1.0000000034924597, 1.0000000064416479, 1.0000000071401398, 1.0000000049670537, 1.0000000024059166, 1.000000003259629, 1.0000000096236665, 1.0000000098564972, 1.0000000006208818]\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "y = model.history_w['cos']\n",
    "y = [x[0] for x in y]\n",
    "print(y)\n",
    "\n",
    "import pylab\n",
    "import matplotlib.pyplot as plt\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(2, 1, 1)\n",
    "line, = ax.plot(y, color='blue')\n",
    "ax.set_yscale('log')\n",
    "plt.title('Row wise average cos similarity')\n",
    "pylab.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MyModule(\n",
      "  (feature_modules): ModuleDict(\n",
      "    (distance): Embedding(5, 3, padding_idx=0)\n",
      "    (u1_depdir): Embedding(5, 3, padding_idx=0)\n",
      "    (u2_depdir): Embedding(5, 3, padding_idx=0)\n",
      "    (u2_func): Embedding(23, 5, padding_idx=0)\n",
      "    (u1_position): Embedding(12, 4, padding_idx=0)\n",
      "    (u2_position): Embedding(12, 4, padding_idx=0)\n",
      "    (sat_children): Identity()\n",
      "    (nuc_children): Identity()\n",
      "  )\n",
      ")\n",
      "ModuleDict(\n",
      "  (distance): Embedding(5, 3, padding_idx=0)\n",
      "  (u1_depdir): Embedding(5, 3, padding_idx=0)\n",
      "  (u2_depdir): Embedding(5, 3, padding_idx=0)\n",
      "  (u2_func): Embedding(23, 5, padding_idx=0)\n",
      "  (u1_position): Embedding(12, 4, padding_idx=0)\n",
      "  (u2_position): Embedding(12, 4, padding_idx=0)\n",
      "  (sat_children): Identity()\n",
      "  (nuc_children): Identity()\n",
      ")\n",
      "Embedding(5, 3, padding_idx=0)\n",
      "Embedding(5, 3, padding_idx=0)\n",
      "Embedding(5, 3, padding_idx=0)\n",
      "Embedding(23, 5, padding_idx=0)\n",
      "Embedding(12, 4, padding_idx=0)\n",
      "Embedding(12, 4, padding_idx=0)\n",
      "Identity()\n",
      "Identity()\n"
     ]
    }
   ],
   "source": [
    "for i in model.module1.modules():\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/VD/kaveri/anaconda3/envs/py310/lib/python3.10/site-packages/transformers/modeling_utils.py:763: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test_loss: 2.2308 test_acc: 0.3654\n",
      "00:00:00.83\n",
      "                precision    recall  f1-score   support\n",
      "\n",
      "    antithesis       0.54      0.39      0.45        18\n",
      "    background       0.33      0.41      0.37        17\n",
      "         cause       0.22      1.00      0.36         2\n",
      "  circumstance       0.45      0.33      0.38        15\n",
      "    concession       0.44      0.54      0.48        13\n",
      "     condition       0.53      0.89      0.67         9\n",
      "   conjunction       0.30      0.43      0.35         7\n",
      "      contrast       0.00      0.00      0.00         8\n",
      " e-elaboration       0.70      0.64      0.67        11\n",
      "   elaboration       0.12      0.30      0.17        10\n",
      "  evaluation-n       0.00      0.00      0.00         3\n",
      "  evaluation-s       0.33      0.06      0.10        17\n",
      "      evidence       0.00      0.00      0.00        10\n",
      "interpretation       0.06      0.08      0.07        12\n",
      "         joint       0.34      0.52      0.41        29\n",
      "          list       0.44      0.42      0.43        26\n",
      "         means       0.00      0.00      0.00         2\n",
      "   preparation       0.67      0.50      0.57         4\n",
      "       purpose       0.67      0.67      0.67         3\n",
      "        reason       0.47      0.41      0.44        34\n",
      "   restatement       0.00      0.00      0.00         1\n",
      "      sequence       0.00      0.00      0.00         7\n",
      "  solutionhood       0.00      0.00      0.00         1\n",
      "       summary       0.00      0.00      0.00         1\n",
      "\n",
      "      accuracy                           0.37       260\n",
      "     macro avg       0.28      0.32      0.27       260\n",
      "  weighted avg       0.36      0.37      0.34       260\n",
      "\n",
      "Test Loss: 2.231 |  Test Acc: 36.54%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/VD/kaveri/anaconda3/envs/py310/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1334: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/VD/kaveri/anaconda3/envs/py310/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1334: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/VD/kaveri/anaconda3/envs/py310/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1334: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "#latest\n",
    "def validate(model, test_loader, optimizer, rev_label_dict, label_dict):\n",
    "  start = time.time()\n",
    "  test_acc, test_loss, cr, model, optimizer = evaluate_accuracy(model, optimizer, test_loader, rev_label_dict, label_dict, is_training=False)\n",
    "  end = time.time()\n",
    "  hours, rem = divmod(end-start, 3600)\n",
    "  minutes, seconds = divmod(rem, 60)\n",
    "\n",
    "  print(f'Test_loss: {test_loss:.4f} test_acc: {test_acc:.4f}')\n",
    "  print(\"{:0>2}:{:0>2}:{:05.2f}\".format(int(hours),int(minutes),seconds))\n",
    "  print(cr)\n",
    "\n",
    "  return test_loss, test_acc\n",
    "\n",
    "\n",
    "test_loss, test_acc = validate(model, test_loader, optimizer, rev_label_dict, label_dict)\n",
    "writer.add_scalar('test_loss_latest', test_loss, 1)\n",
    "writer.add_scalar('test_acc_latest', test_acc, 1)\n",
    "print(f'Test Loss: {test_loss:.3f} |  Test Acc: {test_acc*100:.2f}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/VD/kaveri/anaconda3/envs/py310/lib/python3.10/site-packages/transformers/modeling_utils.py:763: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test_loss: 2.2349 test_acc: 0.3231\n",
      "00:00:00.99\n",
      "                precision    recall  f1-score   support\n",
      "\n",
      "    antithesis       0.50      0.22      0.31        18\n",
      "    background       0.25      0.47      0.33        17\n",
      "         cause       0.50      0.50      0.50         2\n",
      "  circumstance       0.44      0.27      0.33        15\n",
      "    concession       0.55      0.46      0.50        13\n",
      "     condition       0.41      0.78      0.54         9\n",
      "   conjunction       0.33      0.43      0.38         7\n",
      "      contrast       0.00      0.00      0.00         8\n",
      " e-elaboration       0.73      0.73      0.73        11\n",
      "   elaboration       0.12      0.30      0.17        10\n",
      "  evaluation-n       0.00      0.00      0.00         3\n",
      "  evaluation-s       0.00      0.00      0.00        17\n",
      "      evidence       0.00      0.00      0.00        10\n",
      "interpretation       0.07      0.25      0.11        12\n",
      "         joint       0.26      0.34      0.29        29\n",
      "          list       0.62      0.38      0.48        26\n",
      "         means       0.00      0.00      0.00         2\n",
      "   preparation       0.60      0.75      0.67         4\n",
      "       purpose       0.67      0.67      0.67         3\n",
      "        reason       0.40      0.35      0.38        34\n",
      "   restatement       0.00      0.00      0.00         1\n",
      "      sequence       0.00      0.00      0.00         7\n",
      "  solutionhood       0.00      0.00      0.00         1\n",
      "       summary       0.00      0.00      0.00         1\n",
      "\n",
      "      accuracy                           0.32       260\n",
      "     macro avg       0.27      0.29      0.27       260\n",
      "  weighted avg       0.33      0.32      0.31       260\n",
      "\n",
      "Latest Test Loss: 2.235 |  Latest Test Acc: 32.31%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/VD/kaveri/anaconda3/envs/py310/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1334: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/VD/kaveri/anaconda3/envs/py310/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1334: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/VD/kaveri/anaconda3/envs/py310/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1334: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "#best earliest\n",
    "model.load_state_dict(torch.load(save_path_suffix+'_best.pt'))\n",
    "test_loss, test_acc = validate(model, test_loader, optimizer, rev_label_dict, label_dict)\n",
    "writer.add_scalar('test_loss_best_earliest', test_loss, 1)\n",
    "writer.add_scalar('test_acc_best_earliest', test_acc, 1)\n",
    "print(f'Latest Test Loss: {test_loss:.3f} |  Latest Test Acc: {test_acc*100:.2f}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/VD/kaveri/anaconda3/envs/py310/lib/python3.10/site-packages/transformers/modeling_utils.py:763: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test_loss: 2.2349 test_acc: 0.3231\n",
      "00:00:01.05\n",
      "                precision    recall  f1-score   support\n",
      "\n",
      "    antithesis       0.50      0.22      0.31        18\n",
      "    background       0.25      0.47      0.33        17\n",
      "         cause       0.50      0.50      0.50         2\n",
      "  circumstance       0.44      0.27      0.33        15\n",
      "    concession       0.55      0.46      0.50        13\n",
      "     condition       0.41      0.78      0.54         9\n",
      "   conjunction       0.33      0.43      0.38         7\n",
      "      contrast       0.00      0.00      0.00         8\n",
      " e-elaboration       0.73      0.73      0.73        11\n",
      "   elaboration       0.12      0.30      0.17        10\n",
      "  evaluation-n       0.00      0.00      0.00         3\n",
      "  evaluation-s       0.00      0.00      0.00        17\n",
      "      evidence       0.00      0.00      0.00        10\n",
      "interpretation       0.07      0.25      0.11        12\n",
      "         joint       0.26      0.34      0.29        29\n",
      "          list       0.62      0.38      0.48        26\n",
      "         means       0.00      0.00      0.00         2\n",
      "   preparation       0.60      0.75      0.67         4\n",
      "       purpose       0.67      0.67      0.67         3\n",
      "        reason       0.40      0.35      0.38        34\n",
      "   restatement       0.00      0.00      0.00         1\n",
      "      sequence       0.00      0.00      0.00         7\n",
      "  solutionhood       0.00      0.00      0.00         1\n",
      "       summary       0.00      0.00      0.00         1\n",
      "\n",
      "      accuracy                           0.32       260\n",
      "     macro avg       0.27      0.29      0.27       260\n",
      "  weighted avg       0.33      0.32      0.31       260\n",
      "\n",
      "Best Test Loss: 2.235 |  Best Test Acc: 32.31%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/VD/kaveri/anaconda3/envs/py310/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1334: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/VD/kaveri/anaconda3/envs/py310/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1334: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/VD/kaveri/anaconda3/envs/py310/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1334: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "#best lastest\n",
    "model.load_state_dict(torch.load(save_path_suffix+'_best_latest.pt'))\n",
    "test_loss, test_acc = validate(model, test_loader, optimizer, rev_label_dict, label_dict)\n",
    "writer.add_scalar('test_loss_best_latest', test_loss, 1)\n",
    "writer.add_scalar('test_acc_best_latest', test_acc, 1)\n",
    "print(f'Best Test Loss: {test_loss:.3f} |  Best Test Acc: {test_acc*100:.2f}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/VD/kaveri/anaconda3/envs/py310/lib/python3.10/site-packages/transformers/modeling_utils.py:763: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test_loss: 1.9785 test_acc: 0.3852\n",
      "00:00:00.80\n",
      "                precision    recall  f1-score   support\n",
      "\n",
      "    antithesis       0.25      0.36      0.30        11\n",
      "    background       0.32      0.53      0.40        17\n",
      "         cause       0.00      0.00      0.00         7\n",
      "  circumstance       0.25      0.08      0.12        13\n",
      "    concession       0.57      0.36      0.44        11\n",
      "     condition       0.41      0.88      0.56         8\n",
      "   conjunction       0.67      0.75      0.71         8\n",
      "      contrast       0.00      0.00      0.00         3\n",
      " e-elaboration       0.78      0.54      0.64        13\n",
      "   elaboration       0.31      0.32      0.32        28\n",
      "  evaluation-n       0.00      0.00      0.00         8\n",
      "  evaluation-s       0.00      0.00      0.00         5\n",
      "      evidence       0.20      0.12      0.15         8\n",
      "interpretation       0.24      0.46      0.32        13\n",
      "         joint       0.24      0.28      0.26        18\n",
      "          list       0.47      0.44      0.46        18\n",
      "         means       0.00      0.00      0.00         1\n",
      "   preparation       0.80      0.73      0.76        11\n",
      "       purpose       1.00      0.40      0.57         5\n",
      "        reason       0.42      0.61      0.50        28\n",
      "   restatement       0.00      0.00      0.00         1\n",
      "        result       0.00      0.00      0.00         3\n",
      "  solutionhood       0.00      0.00      0.00         1\n",
      "       summary       0.00      0.00      0.00         2\n",
      "\n",
      "      accuracy                           0.39       241\n",
      "     macro avg       0.29      0.29      0.27       241\n",
      "  weighted avg       0.37      0.39      0.36       241\n",
      "\n",
      "Val Loss: 1.979 |  Val Acc: 38.52%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/VD/kaveri/anaconda3/envs/py310/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1334: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/VD/kaveri/anaconda3/envs/py310/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1334: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/VD/kaveri/anaconda3/envs/py310/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1334: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "#best val acc\n",
    "model.load_state_dict(torch.load(save_path_suffix+'_best_latest.pt'))\n",
    "test_loss, test_acc = validate(model, val_loader, optimizer, rev_label_dict, label_dict)\n",
    "writer.add_scalar('val_loss_best_latest', test_loss, 1)\n",
    "writer.add_scalar('val_acc_best_latest', test_acc, 1)\n",
    "print(f'Val Loss: {test_loss:.3f} |  Val Acc: {test_acc*100:.2f}%')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.6 ('py310')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "e3409ea685db85227fbd9509d1b1ace14d085473eb2d57f3ba9dd0302d25f838"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

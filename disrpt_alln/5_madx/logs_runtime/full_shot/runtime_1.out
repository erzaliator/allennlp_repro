-------------------------------------------------------------------
Lang1:  deu.rst.pcc    Lang2:  eng.pdtb.pdtb
Saving run to:  runs/full_shot/FullShot=v4_deu.rst.pcc_eng.pdtb.pdtb_lrfunc=Adafactor_lr=2e-05_plm=bert-base-multilingual-cased
Running with params: BERT_MODEL=bert-base-multilingual-cased lr=2e-05
read 2164 examples
read 241 examples
read 260 examples
read 43920 examples
read 1674 examples
read 2257 examples
Total prediction labels:  49
BertAdapterModel(
  (shared_parameters): ModuleDict()
  (bert): BertModel(
    (shared_parameters): ModuleDict()
    (invertible_adapters): ModuleDict(
      (de): NICECouplingBlock(
        (F): Sequential(
          (0): Linear(in_features=384, out_features=192, bias=True)
          (1): Activation_Function_Class(
            (f): NewGELUActivation()
          )
          (2): Linear(in_features=192, out_features=384, bias=True)
        )
        (G): Sequential(
          (0): Linear(in_features=384, out_features=192, bias=True)
          (1): Activation_Function_Class(
            (f): NewGELUActivation()
          )
          (2): Linear(in_features=192, out_features=384, bias=True)
        )
      )
      (en): NICECouplingBlock(
        (F): Sequential(
          (0): Linear(in_features=384, out_features=192, bias=True)
          (1): Activation_Function_Class(
            (f): ReLU()
          )
          (2): Linear(in_features=192, out_features=384, bias=True)
        )
        (G): Sequential(
          (0): Linear(in_features=384, out_features=192, bias=True)
          (1): Activation_Function_Class(
            (f): ReLU()
          )
          (2): Linear(in_features=192, out_features=384, bias=True)
        )
      )
    )
    (embeddings): BertEmbeddings(
      (word_embeddings): Embedding(119547, 768, padding_idx=0)
      (position_embeddings): Embedding(512, 768)
      (token_type_embeddings): Embedding(2, 768)
      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
      (dropout): Dropout(p=0.1, inplace=False)
    )
    (encoder): BertEncoder(
      (layer): ModuleList(
        (0): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (de): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (en): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (1): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (de): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (en): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (2): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (de): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (en): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (3): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (de): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (en): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (4): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (de): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (en): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (5): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (de): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (en): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (6): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (de): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (en): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (7): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (de): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (en): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (8): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (de): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (en): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (9): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (de): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (en): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (10): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (de): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (en): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (11): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (de): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (en): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
      )
    )
    (pooler): BertPooler(
      (dense): Linear(in_features=768, out_features=768, bias=True)
      (activation): Tanh()
    )
    (prefix_tuning): PrefixTuningPool(
      (prefix_tunings): ModuleDict()
    )
  )
  (heads): ModuleDict(
    (disrpt): ClassificationHead(
      (0): Dropout(p=0.1, inplace=False)
      (1): Linear(in_features=768, out_features=768, bias=True)
      (2): Activation_Function_Class(
        (f): Tanh()
      )
      (3): Dropout(p=0.1, inplace=False)
      (4): Linear(in_features=768, out_features=49, bias=True)
    )
  )
)
{'train@deu.rst.pcc_loss': 3.2885210514068604, 'train@deu.rst.pcc_accuracy@deu.rst.pcc': 0.10166358595194085, 'train@deu.rst.pcc_f1@deu.rst.pcc': 0.01680022533124688, 'train@deu.rst.pcc_precision@deu.rst.pcc': 0.05700590911717461, 'train@deu.rst.pcc_recall@deu.rst.pcc': 0.04173776425283222, 'train@deu.rst.pcc_loss@deu.rst.pcc': 3.2885212898254395, 'train@deu.rst.pcc_runtime': 27.0484, 'train@deu.rst.pcc_samples_per_second': 80.005, 'train@deu.rst.pcc_steps_per_second': 2.514, 'epoch': 1.0}
{'loss': 3.6299, 'learning_rate': 1.8333333333333333e-05, 'epoch': 1.0}
{'eval_loss': 3.3191146850585938, 'eval_accuracy@deu.rst.pcc': 0.0995850622406639, 'eval_f1@deu.rst.pcc': 0.019841459680814418, 'eval_precision@deu.rst.pcc': 0.021277205040091635, 'eval_recall@deu.rst.pcc': 0.04811507936507936, 'eval_loss@deu.rst.pcc': 3.3191146850585938, 'eval_runtime': 3.4524, 'eval_samples_per_second': 69.806, 'eval_steps_per_second': 2.317, 'epoch': 1.0}
{'train@deu.rst.pcc_loss': 3.005258083343506, 'train@deu.rst.pcc_accuracy@deu.rst.pcc': 0.11182994454713494, 'train@deu.rst.pcc_f1@deu.rst.pcc': 0.018859256049291166, 'train@deu.rst.pcc_precision@deu.rst.pcc': 0.038556295498559816, 'train@deu.rst.pcc_recall@deu.rst.pcc': 0.04508408533092946, 'train@deu.rst.pcc_loss@deu.rst.pcc': 3.005258083343506, 'train@deu.rst.pcc_runtime': 26.7545, 'train@deu.rst.pcc_samples_per_second': 80.884, 'train@deu.rst.pcc_steps_per_second': 2.542, 'epoch': 2.0}
{'loss': 3.1377, 'learning_rate': 1.6666666666666667e-05, 'epoch': 2.0}
{'eval_loss': 3.045344352722168, 'eval_accuracy@deu.rst.pcc': 0.11203319502074689, 'eval_f1@deu.rst.pcc': 0.011475832596522252, 'eval_precision@deu.rst.pcc': 0.006913021618903972, 'eval_recall@deu.rst.pcc': 0.04189560439560439, 'eval_loss@deu.rst.pcc': 3.045344352722168, 'eval_runtime': 3.5109, 'eval_samples_per_second': 68.644, 'eval_steps_per_second': 2.279, 'epoch': 2.0}
{'train@deu.rst.pcc_loss': 2.922327756881714, 'train@deu.rst.pcc_accuracy@deu.rst.pcc': 0.14417744916820702, 'train@deu.rst.pcc_f1@deu.rst.pcc': 0.03633269689619979, 'train@deu.rst.pcc_precision@deu.rst.pcc': 0.07120994084859637, 'train@deu.rst.pcc_recall@deu.rst.pcc': 0.06095632379055996, 'train@deu.rst.pcc_loss@deu.rst.pcc': 2.9223275184631348, 'train@deu.rst.pcc_runtime': 26.7224, 'train@deu.rst.pcc_samples_per_second': 80.981, 'train@deu.rst.pcc_steps_per_second': 2.545, 'epoch': 3.0}
{'loss': 2.9893, 'learning_rate': 1.5000000000000002e-05, 'epoch': 3.0}
{'eval_loss': 2.9731404781341553, 'eval_accuracy@deu.rst.pcc': 0.13278008298755187, 'eval_f1@deu.rst.pcc': 0.02608331673259531, 'eval_precision@deu.rst.pcc': 0.021341691616766467, 'eval_recall@deu.rst.pcc': 0.05518671143671144, 'eval_loss@deu.rst.pcc': 2.973140001296997, 'eval_runtime': 3.4246, 'eval_samples_per_second': 70.374, 'eval_steps_per_second': 2.336, 'epoch': 3.0}
{'train@deu.rst.pcc_loss': 2.8682377338409424, 'train@deu.rst.pcc_accuracy@deu.rst.pcc': 0.18253234750462108, 'train@deu.rst.pcc_f1@deu.rst.pcc': 0.06667668591028994, 'train@deu.rst.pcc_precision@deu.rst.pcc': 0.09949472590761786, 'train@deu.rst.pcc_recall@deu.rst.pcc': 0.09489530775215436, 'train@deu.rst.pcc_loss@deu.rst.pcc': 2.8682377338409424, 'train@deu.rst.pcc_runtime': 26.6833, 'train@deu.rst.pcc_samples_per_second': 81.1, 'train@deu.rst.pcc_steps_per_second': 2.548, 'epoch': 4.0}
{'loss': 2.9321, 'learning_rate': 1.3333333333333333e-05, 'epoch': 4.0}
{'eval_loss': 2.928144693374634, 'eval_accuracy@deu.rst.pcc': 0.15767634854771784, 'eval_f1@deu.rst.pcc': 0.05122511345337433, 'eval_precision@deu.rst.pcc': 0.04452006327006327, 'eval_recall@deu.rst.pcc': 0.08354319291819291, 'eval_loss@deu.rst.pcc': 2.928144693374634, 'eval_runtime': 3.4288, 'eval_samples_per_second': 70.287, 'eval_steps_per_second': 2.333, 'epoch': 4.0}
{'train@deu.rst.pcc_loss': 2.824598789215088, 'train@deu.rst.pcc_accuracy@deu.rst.pcc': 0.20609981515711645, 'train@deu.rst.pcc_f1@deu.rst.pcc': 0.07423819193251104, 'train@deu.rst.pcc_precision@deu.rst.pcc': 0.0955296904738476, 'train@deu.rst.pcc_recall@deu.rst.pcc': 0.11672999473052556, 'train@deu.rst.pcc_loss@deu.rst.pcc': 2.824598789215088, 'train@deu.rst.pcc_runtime': 26.8179, 'train@deu.rst.pcc_samples_per_second': 80.692, 'train@deu.rst.pcc_steps_per_second': 2.536, 'epoch': 5.0}
{'loss': 2.8829, 'learning_rate': 1.1666666666666668e-05, 'epoch': 5.0}
{'eval_loss': 2.8903961181640625, 'eval_accuracy@deu.rst.pcc': 0.17427385892116182, 'eval_f1@deu.rst.pcc': 0.061773249024576427, 'eval_precision@deu.rst.pcc': 0.08610637446844344, 'eval_recall@deu.rst.pcc': 0.10526683964183965, 'eval_loss@deu.rst.pcc': 2.8903958797454834, 'eval_runtime': 3.4841, 'eval_samples_per_second': 69.171, 'eval_steps_per_second': 2.296, 'epoch': 5.0}
{'train@deu.rst.pcc_loss': 2.7869536876678467, 'train@deu.rst.pcc_accuracy@deu.rst.pcc': 0.21025878003696857, 'train@deu.rst.pcc_f1@deu.rst.pcc': 0.07636571839915705, 'train@deu.rst.pcc_precision@deu.rst.pcc': 0.09702632952282324, 'train@deu.rst.pcc_recall@deu.rst.pcc': 0.12232293402206042, 'train@deu.rst.pcc_loss@deu.rst.pcc': 2.7869534492492676, 'train@deu.rst.pcc_runtime': 26.6717, 'train@deu.rst.pcc_samples_per_second': 81.135, 'train@deu.rst.pcc_steps_per_second': 2.55, 'epoch': 6.0}
{'loss': 2.8413, 'learning_rate': 1e-05, 'epoch': 6.0}
{'eval_loss': 2.856003522872925, 'eval_accuracy@deu.rst.pcc': 0.17842323651452283, 'eval_f1@deu.rst.pcc': 0.0667589363374985, 'eval_precision@deu.rst.pcc': 0.0804914367094674, 'eval_recall@deu.rst.pcc': 0.1242432336182336, 'eval_loss@deu.rst.pcc': 2.856003999710083, 'eval_runtime': 3.4152, 'eval_samples_per_second': 70.567, 'eval_steps_per_second': 2.342, 'epoch': 6.0}
{'train@deu.rst.pcc_loss': 2.7580513954162598, 'train@deu.rst.pcc_accuracy@deu.rst.pcc': 0.2121072088724584, 'train@deu.rst.pcc_f1@deu.rst.pcc': 0.07862299824253624, 'train@deu.rst.pcc_precision@deu.rst.pcc': 0.09996408301238602, 'train@deu.rst.pcc_recall@deu.rst.pcc': 0.12518862690530105, 'train@deu.rst.pcc_loss@deu.rst.pcc': 2.7580513954162598, 'train@deu.rst.pcc_runtime': 27.7071, 'train@deu.rst.pcc_samples_per_second': 78.103, 'train@deu.rst.pcc_steps_per_second': 2.454, 'epoch': 7.0}
{'loss': 2.8098, 'learning_rate': 8.333333333333334e-06, 'epoch': 7.0}
{'eval_loss': 2.830354928970337, 'eval_accuracy@deu.rst.pcc': 0.18672199170124482, 'eval_f1@deu.rst.pcc': 0.06768181577550168, 'eval_precision@deu.rst.pcc': 0.08283931214746156, 'eval_recall@deu.rst.pcc': 0.12715583028083027, 'eval_loss@deu.rst.pcc': 2.830354928970337, 'eval_runtime': 3.4211, 'eval_samples_per_second': 70.446, 'eval_steps_per_second': 2.338, 'epoch': 7.0}
{'train@deu.rst.pcc_loss': 2.7353289127349854, 'train@deu.rst.pcc_accuracy@deu.rst.pcc': 0.21534195933456562, 'train@deu.rst.pcc_f1@deu.rst.pcc': 0.08164716359012479, 'train@deu.rst.pcc_precision@deu.rst.pcc': 0.09765410530694939, 'train@deu.rst.pcc_recall@deu.rst.pcc': 0.12794831104672666, 'train@deu.rst.pcc_loss@deu.rst.pcc': 2.735328435897827, 'train@deu.rst.pcc_runtime': 26.7534, 'train@deu.rst.pcc_samples_per_second': 80.887, 'train@deu.rst.pcc_steps_per_second': 2.542, 'epoch': 8.0}
{'loss': 2.7837, 'learning_rate': 6.666666666666667e-06, 'epoch': 8.0}
{'eval_loss': 2.8111860752105713, 'eval_accuracy@deu.rst.pcc': 0.18672199170124482, 'eval_f1@deu.rst.pcc': 0.06906021666731448, 'eval_precision@deu.rst.pcc': 0.08036765619002462, 'eval_recall@deu.rst.pcc': 0.1281097374847375, 'eval_loss@deu.rst.pcc': 2.811185836791992, 'eval_runtime': 3.5673, 'eval_samples_per_second': 67.558, 'eval_steps_per_second': 2.243, 'epoch': 8.0}
{'train@deu.rst.pcc_loss': 2.717475652694702, 'train@deu.rst.pcc_accuracy@deu.rst.pcc': 0.21903881700554528, 'train@deu.rst.pcc_f1@deu.rst.pcc': 0.08517607474074804, 'train@deu.rst.pcc_precision@deu.rst.pcc': 0.09363589294004197, 'train@deu.rst.pcc_recall@deu.rst.pcc': 0.13057970735456972, 'train@deu.rst.pcc_loss@deu.rst.pcc': 2.717475414276123, 'train@deu.rst.pcc_runtime': 26.6811, 'train@deu.rst.pcc_samples_per_second': 81.106, 'train@deu.rst.pcc_steps_per_second': 2.549, 'epoch': 9.0}
{'loss': 2.7633, 'learning_rate': 5e-06, 'epoch': 9.0}
{'eval_loss': 2.798205852508545, 'eval_accuracy@deu.rst.pcc': 0.1991701244813278, 'eval_f1@deu.rst.pcc': 0.08167893713268132, 'eval_precision@deu.rst.pcc': 0.0837977858811192, 'eval_recall@deu.rst.pcc': 0.1376615282865283, 'eval_loss@deu.rst.pcc': 2.7982065677642822, 'eval_runtime': 3.4374, 'eval_samples_per_second': 70.112, 'eval_steps_per_second': 2.327, 'epoch': 9.0}
{'train@deu.rst.pcc_loss': 2.7054226398468018, 'train@deu.rst.pcc_accuracy@deu.rst.pcc': 0.22134935304990758, 'train@deu.rst.pcc_f1@deu.rst.pcc': 0.08684832265183465, 'train@deu.rst.pcc_precision@deu.rst.pcc': 0.09408947521413984, 'train@deu.rst.pcc_recall@deu.rst.pcc': 0.13246724742329413, 'train@deu.rst.pcc_loss@deu.rst.pcc': 2.7054226398468018, 'train@deu.rst.pcc_runtime': 26.7474, 'train@deu.rst.pcc_samples_per_second': 80.905, 'train@deu.rst.pcc_steps_per_second': 2.542, 'epoch': 10.0}
{'loss': 2.7459, 'learning_rate': 3.3333333333333333e-06, 'epoch': 10.0}
{'eval_loss': 2.785285472869873, 'eval_accuracy@deu.rst.pcc': 0.1991701244813278, 'eval_f1@deu.rst.pcc': 0.07976937529447696, 'eval_precision@deu.rst.pcc': 0.08302534207139471, 'eval_recall@deu.rst.pcc': 0.13683480870980871, 'eval_loss@deu.rst.pcc': 2.7852859497070312, 'eval_runtime': 3.4549, 'eval_samples_per_second': 69.756, 'eval_steps_per_second': 2.316, 'epoch': 10.0}
{'train@deu.rst.pcc_loss': 2.6981008052825928, 'train@deu.rst.pcc_accuracy@deu.rst.pcc': 0.2245841035120148, 'train@deu.rst.pcc_f1@deu.rst.pcc': 0.08769688217475083, 'train@deu.rst.pcc_precision@deu.rst.pcc': 0.0907193533629107, 'train@deu.rst.pcc_recall@deu.rst.pcc': 0.13424015272615086, 'train@deu.rst.pcc_loss@deu.rst.pcc': 2.698101043701172, 'train@deu.rst.pcc_runtime': 26.8564, 'train@deu.rst.pcc_samples_per_second': 80.577, 'train@deu.rst.pcc_steps_per_second': 2.532, 'epoch': 11.0}
{'loss': 2.7331, 'learning_rate': 1.6666666666666667e-06, 'epoch': 11.0}
{'eval_loss': 2.7790894508361816, 'eval_accuracy@deu.rst.pcc': 0.1950207468879668, 'eval_f1@deu.rst.pcc': 0.07815916387344958, 'eval_precision@deu.rst.pcc': 0.08243718958695738, 'eval_recall@deu.rst.pcc': 0.13362968050468052, 'eval_loss@deu.rst.pcc': 2.7790896892547607, 'eval_runtime': 3.4529, 'eval_samples_per_second': 69.796, 'eval_steps_per_second': 2.317, 'epoch': 11.0}
{'train@deu.rst.pcc_loss': 2.695636749267578, 'train@deu.rst.pcc_accuracy@deu.rst.pcc': 0.22643253234750463, 'train@deu.rst.pcc_f1@deu.rst.pcc': 0.088437040198655, 'train@deu.rst.pcc_precision@deu.rst.pcc': 0.09133248919173997, 'train@deu.rst.pcc_recall@deu.rst.pcc': 0.13543414807642631, 'train@deu.rst.pcc_loss@deu.rst.pcc': 2.695636749267578, 'train@deu.rst.pcc_runtime': 26.8112, 'train@deu.rst.pcc_samples_per_second': 80.713, 'train@deu.rst.pcc_steps_per_second': 2.536, 'epoch': 12.0}
{'loss': 2.7305, 'learning_rate': 0.0, 'epoch': 12.0}
{'eval_loss': 2.7764360904693604, 'eval_accuracy@deu.rst.pcc': 0.1950207468879668, 'eval_f1@deu.rst.pcc': 0.0789468581135248, 'eval_precision@deu.rst.pcc': 0.0829904536889831, 'eval_recall@deu.rst.pcc': 0.13362968050468052, 'eval_loss@deu.rst.pcc': 2.776435375213623, 'eval_runtime': 3.4943, 'eval_samples_per_second': 68.97, 'eval_steps_per_second': 2.289, 'epoch': 12.0}
{'train_runtime': 1040.9414, 'train_samples_per_second': 24.947, 'train_steps_per_second': 0.784, 'train_loss': 2.9149423113056256, 'epoch': 12.0}
***** train_log metrics *****
  epoch                    =       12.0
  train_loss               =     2.9149
  train_runtime            = 0:17:20.94
  train_samples_per_second =     24.947
  train_steps_per_second   =      0.784
{'train@eng.pdtb.pdtb_loss': 1.2447469234466553, 'train@eng.pdtb.pdtb_accuracy@eng.pdtb.pdtb': 0.602823315118397, 'train@eng.pdtb.pdtb_f1@eng.pdtb.pdtb': 0.2916575424464744, 'train@eng.pdtb.pdtb_precision@eng.pdtb.pdtb': 0.3572767705148428, 'train@eng.pdtb.pdtb_recall@eng.pdtb.pdtb': 0.278774255142763, 'train@eng.pdtb.pdtb_loss@eng.pdtb.pdtb': 1.2447469234466553, 'train@eng.pdtb.pdtb_runtime': 522.0136, 'train@eng.pdtb.pdtb_samples_per_second': 84.136, 'train@eng.pdtb.pdtb_steps_per_second': 2.63, 'epoch': 1.0}
{'loss': 1.7403, 'learning_rate': 1.8333333333333333e-05, 'epoch': 1.0}
{'eval_loss': 1.1723971366882324, 'eval_accuracy@eng.pdtb.pdtb': 0.6397849462365591, 'eval_f1@eng.pdtb.pdtb': 0.34784801926315206, 'eval_precision@eng.pdtb.pdtb': 0.436656838817302, 'eval_recall@eng.pdtb.pdtb': 0.3338925149253487, 'eval_loss@eng.pdtb.pdtb': 1.1723971366882324, 'eval_runtime': 20.4505, 'eval_samples_per_second': 81.856, 'eval_steps_per_second': 2.592, 'epoch': 1.0}
{'train@eng.pdtb.pdtb_loss': 1.0930671691894531, 'train@eng.pdtb.pdtb_accuracy@eng.pdtb.pdtb': 0.6417805100182149, 'train@eng.pdtb.pdtb_f1@eng.pdtb.pdtb': 0.3703922871942231, 'train@eng.pdtb.pdtb_precision@eng.pdtb.pdtb': 0.4590095378235355, 'train@eng.pdtb.pdtb_recall@eng.pdtb.pdtb': 0.3579491289160696, 'train@eng.pdtb.pdtb_loss@eng.pdtb.pdtb': 1.0930671691894531, 'train@eng.pdtb.pdtb_runtime': 522.1603, 'train@eng.pdtb.pdtb_samples_per_second': 84.112, 'train@eng.pdtb.pdtb_steps_per_second': 2.629, 'epoch': 2.0}
{'loss': 1.21, 'learning_rate': 1.6666666666666667e-05, 'epoch': 2.0}
{'eval_loss': 1.0373934507369995, 'eval_accuracy@eng.pdtb.pdtb': 0.6666666666666666, 'eval_f1@eng.pdtb.pdtb': 0.42701873680336133, 'eval_precision@eng.pdtb.pdtb': 0.495793247237164, 'eval_recall@eng.pdtb.pdtb': 0.4124885554291155, 'eval_loss@eng.pdtb.pdtb': 1.037393569946289, 'eval_runtime': 20.4279, 'eval_samples_per_second': 81.947, 'eval_steps_per_second': 2.594, 'epoch': 2.0}
{'train@eng.pdtb.pdtb_loss': 1.049291729927063, 'train@eng.pdtb.pdtb_accuracy@eng.pdtb.pdtb': 0.6524817850637523, 'train@eng.pdtb.pdtb_f1@eng.pdtb.pdtb': 0.42336930791325317, 'train@eng.pdtb.pdtb_precision@eng.pdtb.pdtb': 0.4747467922389802, 'train@eng.pdtb.pdtb_recall@eng.pdtb.pdtb': 0.40251780132365017, 'train@eng.pdtb.pdtb_loss@eng.pdtb.pdtb': 1.049291729927063, 'train@eng.pdtb.pdtb_runtime': 521.9647, 'train@eng.pdtb.pdtb_samples_per_second': 84.144, 'train@eng.pdtb.pdtb_steps_per_second': 2.63, 'epoch': 3.0}
{'loss': 1.1219, 'learning_rate': 1.5000000000000002e-05, 'epoch': 3.0}
{'eval_loss': 0.9988798499107361, 'eval_accuracy@eng.pdtb.pdtb': 0.6804062126642771, 'eval_f1@eng.pdtb.pdtb': 0.46179386253495525, 'eval_precision@eng.pdtb.pdtb': 0.5166579695845707, 'eval_recall@eng.pdtb.pdtb': 0.4417250738117642, 'eval_loss@eng.pdtb.pdtb': 0.9988799095153809, 'eval_runtime': 20.4976, 'eval_samples_per_second': 81.668, 'eval_steps_per_second': 2.586, 'epoch': 3.0}
{'train@eng.pdtb.pdtb_loss': 1.0018620491027832, 'train@eng.pdtb.pdtb_accuracy@eng.pdtb.pdtb': 0.6657103825136612, 'train@eng.pdtb.pdtb_f1@eng.pdtb.pdtb': 0.4455270551402576, 'train@eng.pdtb.pdtb_precision@eng.pdtb.pdtb': 0.5186976327854377, 'train@eng.pdtb.pdtb_recall@eng.pdtb.pdtb': 0.43395165041890565, 'train@eng.pdtb.pdtb_loss@eng.pdtb.pdtb': 1.0018620491027832, 'train@eng.pdtb.pdtb_runtime': 521.7901, 'train@eng.pdtb.pdtb_samples_per_second': 84.172, 'train@eng.pdtb.pdtb_steps_per_second': 2.631, 'epoch': 4.0}
{'loss': 1.0725, 'learning_rate': 1.3333333333333333e-05, 'epoch': 4.0}
{'eval_loss': 0.9565660357475281, 'eval_accuracy@eng.pdtb.pdtb': 0.6833930704898447, 'eval_f1@eng.pdtb.pdtb': 0.48029545564209847, 'eval_precision@eng.pdtb.pdtb': 0.5358796619211316, 'eval_recall@eng.pdtb.pdtb': 0.45840621989097985, 'eval_loss@eng.pdtb.pdtb': 0.9565658569335938, 'eval_runtime': 20.4734, 'eval_samples_per_second': 81.765, 'eval_steps_per_second': 2.589, 'epoch': 4.0}
{'train@eng.pdtb.pdtb_loss': 0.9788819551467896, 'train@eng.pdtb.pdtb_accuracy@eng.pdtb.pdtb': 0.6729735883424408, 'train@eng.pdtb.pdtb_f1@eng.pdtb.pdtb': 0.45489997412602257, 'train@eng.pdtb.pdtb_precision@eng.pdtb.pdtb': 0.5217402882552803, 'train@eng.pdtb.pdtb_recall@eng.pdtb.pdtb': 0.4484952786039591, 'train@eng.pdtb.pdtb_loss@eng.pdtb.pdtb': 0.9788819551467896, 'train@eng.pdtb.pdtb_runtime': 522.1886, 'train@eng.pdtb.pdtb_samples_per_second': 84.108, 'train@eng.pdtb.pdtb_steps_per_second': 2.629, 'epoch': 5.0}
{'loss': 1.0453, 'learning_rate': 1.1666666666666668e-05, 'epoch': 5.0}
{'eval_loss': 0.9416821002960205, 'eval_accuracy@eng.pdtb.pdtb': 0.6857825567502986, 'eval_f1@eng.pdtb.pdtb': 0.5156257050522017, 'eval_precision@eng.pdtb.pdtb': 0.5416930876168166, 'eval_recall@eng.pdtb.pdtb': 0.5094066779603104, 'eval_loss@eng.pdtb.pdtb': 0.9416821002960205, 'eval_runtime': 20.4118, 'eval_samples_per_second': 82.011, 'eval_steps_per_second': 2.597, 'epoch': 5.0}
{'train@eng.pdtb.pdtb_loss': 0.9593784809112549, 'train@eng.pdtb.pdtb_accuracy@eng.pdtb.pdtb': 0.6775045537340619, 'train@eng.pdtb.pdtb_f1@eng.pdtb.pdtb': 0.4620696477532849, 'train@eng.pdtb.pdtb_precision@eng.pdtb.pdtb': 0.5129173121671287, 'train@eng.pdtb.pdtb_recall@eng.pdtb.pdtb': 0.4554413265584106, 'train@eng.pdtb.pdtb_loss@eng.pdtb.pdtb': 0.9593783617019653, 'train@eng.pdtb.pdtb_runtime': 521.8566, 'train@eng.pdtb.pdtb_samples_per_second': 84.161, 'train@eng.pdtb.pdtb_steps_per_second': 2.631, 'epoch': 6.0}
{'loss': 1.0213, 'learning_rate': 1e-05, 'epoch': 6.0}
{'eval_loss': 0.93341463804245, 'eval_accuracy@eng.pdtb.pdtb': 0.6881720430107527, 'eval_f1@eng.pdtb.pdtb': 0.5339217394306115, 'eval_precision@eng.pdtb.pdtb': 0.5968942228140316, 'eval_recall@eng.pdtb.pdtb': 0.5207602863670773, 'eval_loss@eng.pdtb.pdtb': 0.93341463804245, 'eval_runtime': 20.3824, 'eval_samples_per_second': 82.13, 'eval_steps_per_second': 2.6, 'epoch': 6.0}
{'train@eng.pdtb.pdtb_loss': 0.9476413130760193, 'train@eng.pdtb.pdtb_accuracy@eng.pdtb.pdtb': 0.6792577413479053, 'train@eng.pdtb.pdtb_f1@eng.pdtb.pdtb': 0.4655702342105686, 'train@eng.pdtb.pdtb_precision@eng.pdtb.pdtb': 0.5353864788151993, 'train@eng.pdtb.pdtb_recall@eng.pdtb.pdtb': 0.45506715109769125, 'train@eng.pdtb.pdtb_loss@eng.pdtb.pdtb': 0.9476413726806641, 'train@eng.pdtb.pdtb_runtime': 522.0095, 'train@eng.pdtb.pdtb_samples_per_second': 84.136, 'train@eng.pdtb.pdtb_steps_per_second': 2.63, 'epoch': 7.0}
{'loss': 1.0073, 'learning_rate': 8.333333333333334e-06, 'epoch': 7.0}
{'eval_loss': 0.9176300168037415, 'eval_accuracy@eng.pdtb.pdtb': 0.6911589008363201, 'eval_f1@eng.pdtb.pdtb': 0.5379949466219089, 'eval_precision@eng.pdtb.pdtb': 0.6169968398329579, 'eval_recall@eng.pdtb.pdtb': 0.5178569696030294, 'eval_loss@eng.pdtb.pdtb': 0.9176300764083862, 'eval_runtime': 20.465, 'eval_samples_per_second': 81.798, 'eval_steps_per_second': 2.59, 'epoch': 7.0}
{'train@eng.pdtb.pdtb_loss': 0.9353342056274414, 'train@eng.pdtb.pdtb_accuracy@eng.pdtb.pdtb': 0.6836976320582878, 'train@eng.pdtb.pdtb_f1@eng.pdtb.pdtb': 0.4699679470847129, 'train@eng.pdtb.pdtb_precision@eng.pdtb.pdtb': 0.5130132387654907, 'train@eng.pdtb.pdtb_recall@eng.pdtb.pdtb': 0.46576035843349567, 'train@eng.pdtb.pdtb_loss@eng.pdtb.pdtb': 0.9353342056274414, 'train@eng.pdtb.pdtb_runtime': 521.6835, 'train@eng.pdtb.pdtb_samples_per_second': 84.189, 'train@eng.pdtb.pdtb_steps_per_second': 2.632, 'epoch': 8.0}
{'loss': 0.9962, 'learning_rate': 6.666666666666667e-06, 'epoch': 8.0}
{'eval_loss': 0.9130423069000244, 'eval_accuracy@eng.pdtb.pdtb': 0.6917562724014337, 'eval_f1@eng.pdtb.pdtb': 0.5333514203704165, 'eval_precision@eng.pdtb.pdtb': 0.57279509523752, 'eval_recall@eng.pdtb.pdtb': 0.5248432643099379, 'eval_loss@eng.pdtb.pdtb': 0.9130423069000244, 'eval_runtime': 20.5675, 'eval_samples_per_second': 81.39, 'eval_steps_per_second': 2.577, 'epoch': 8.0}
{'train@eng.pdtb.pdtb_loss': 0.9267736673355103, 'train@eng.pdtb.pdtb_accuracy@eng.pdtb.pdtb': 0.6866575591985428, 'train@eng.pdtb.pdtb_f1@eng.pdtb.pdtb': 0.4740257614259398, 'train@eng.pdtb.pdtb_precision@eng.pdtb.pdtb': 0.5232364939721859, 'train@eng.pdtb.pdtb_recall@eng.pdtb.pdtb': 0.46720235245763164, 'train@eng.pdtb.pdtb_loss@eng.pdtb.pdtb': 0.9267735481262207, 'train@eng.pdtb.pdtb_runtime': 521.755, 'train@eng.pdtb.pdtb_samples_per_second': 84.177, 'train@eng.pdtb.pdtb_steps_per_second': 2.632, 'epoch': 9.0}
{'loss': 0.9821, 'learning_rate': 5e-06, 'epoch': 9.0}
{'eval_loss': 0.9104881882667542, 'eval_accuracy@eng.pdtb.pdtb': 0.6929510155316607, 'eval_f1@eng.pdtb.pdtb': 0.5388228823595282, 'eval_precision@eng.pdtb.pdtb': 0.5841085166400658, 'eval_recall@eng.pdtb.pdtb': 0.52359558132122, 'eval_loss@eng.pdtb.pdtb': 0.9104882478713989, 'eval_runtime': 20.4964, 'eval_samples_per_second': 81.673, 'eval_steps_per_second': 2.586, 'epoch': 9.0}
{'train@eng.pdtb.pdtb_loss': 0.9227684736251831, 'train@eng.pdtb.pdtb_accuracy@eng.pdtb.pdtb': 0.6872950819672131, 'train@eng.pdtb.pdtb_f1@eng.pdtb.pdtb': 0.47514852137149294, 'train@eng.pdtb.pdtb_precision@eng.pdtb.pdtb': 0.5240379984457553, 'train@eng.pdtb.pdtb_recall@eng.pdtb.pdtb': 0.46885305192941545, 'train@eng.pdtb.pdtb_loss@eng.pdtb.pdtb': 0.922768771648407, 'train@eng.pdtb.pdtb_runtime': 521.4465, 'train@eng.pdtb.pdtb_samples_per_second': 84.227, 'train@eng.pdtb.pdtb_steps_per_second': 2.633, 'epoch': 10.0}
{'loss': 0.9774, 'learning_rate': 3.3333333333333333e-06, 'epoch': 10.0}
{'eval_loss': 0.9048112034797668, 'eval_accuracy@eng.pdtb.pdtb': 0.6929510155316607, 'eval_f1@eng.pdtb.pdtb': 0.5459892562885761, 'eval_precision@eng.pdtb.pdtb': 0.6136639017948917, 'eval_recall@eng.pdtb.pdtb': 0.5321695468870289, 'eval_loss@eng.pdtb.pdtb': 0.9048111438751221, 'eval_runtime': 20.4455, 'eval_samples_per_second': 81.876, 'eval_steps_per_second': 2.592, 'epoch': 10.0}
{'train@eng.pdtb.pdtb_loss': 0.9185274839401245, 'train@eng.pdtb.pdtb_accuracy@eng.pdtb.pdtb': 0.6893670309653916, 'train@eng.pdtb.pdtb_f1@eng.pdtb.pdtb': 0.4779686423444769, 'train@eng.pdtb.pdtb_precision@eng.pdtb.pdtb': 0.5207566305952764, 'train@eng.pdtb.pdtb_recall@eng.pdtb.pdtb': 0.4724266133939516, 'train@eng.pdtb.pdtb_loss@eng.pdtb.pdtb': 0.918527364730835, 'train@eng.pdtb.pdtb_runtime': 515.4396, 'train@eng.pdtb.pdtb_samples_per_second': 85.209, 'train@eng.pdtb.pdtb_steps_per_second': 2.664, 'epoch': 11.0}
{'loss': 0.9741, 'learning_rate': 1.6666666666666667e-06, 'epoch': 11.0}
{'eval_loss': 0.9050949215888977, 'eval_accuracy@eng.pdtb.pdtb': 0.6947431302270012, 'eval_f1@eng.pdtb.pdtb': 0.5482403891672629, 'eval_precision@eng.pdtb.pdtb': 0.5851523147197597, 'eval_recall@eng.pdtb.pdtb': 0.5390990788796548, 'eval_loss@eng.pdtb.pdtb': 0.9050948619842529, 'eval_runtime': 20.0699, 'eval_samples_per_second': 83.408, 'eval_steps_per_second': 2.641, 'epoch': 11.0}
{'train@eng.pdtb.pdtb_loss': 0.91732257604599, 'train@eng.pdtb.pdtb_accuracy@eng.pdtb.pdtb': 0.6890710382513662, 'train@eng.pdtb.pdtb_f1@eng.pdtb.pdtb': 0.47704392138773344, 'train@eng.pdtb.pdtb_precision@eng.pdtb.pdtb': 0.5228989584853815, 'train@eng.pdtb.pdtb_recall@eng.pdtb.pdtb': 0.4698507783075829, 'train@eng.pdtb.pdtb_loss@eng.pdtb.pdtb': 0.9173225164413452, 'train@eng.pdtb.pdtb_runtime': 516.2733, 'train@eng.pdtb.pdtb_samples_per_second': 85.071, 'train@eng.pdtb.pdtb_steps_per_second': 2.659, 'epoch': 12.0}
{'loss': 0.967, 'learning_rate': 0.0, 'epoch': 12.0}
{'eval_loss': 0.90318363904953, 'eval_accuracy@eng.pdtb.pdtb': 0.6929510155316607, 'eval_f1@eng.pdtb.pdtb': 0.5439494578309615, 'eval_precision@eng.pdtb.pdtb': 0.5834036135122254, 'eval_recall@eng.pdtb.pdtb': 0.5341331787069616, 'eval_loss@eng.pdtb.pdtb': 0.9031837582588196, 'eval_runtime': 20.0964, 'eval_samples_per_second': 83.299, 'eval_steps_per_second': 2.637, 'epoch': 12.0}
{'train_runtime': 19820.3156, 'train_samples_per_second': 26.591, 'train_steps_per_second': 0.831, 'train_loss': 1.092930241098795, 'epoch': 12.0}
***** train_logger metrics *****
  epoch                    =       12.0
  train_loss               =     2.9149
  train_runtime            = 0:17:20.94
  train_samples_per_second =     24.947
  train_steps_per_second   =      0.784
-------------------------------------------------------------------
Lang1:  eng.rst.gum    Lang2:  eng.pdtb.pdtb
Saving run to:  runs/full_shot/FullShot=v4_eng.rst.gum_eng.pdtb.pdtb_lrfunc=Adafactor_lr=2e-05_plm=bert-base-multilingual-cased
Running with params: BERT_MODEL=bert-base-multilingual-cased lr=2e-05
read 13897 examples
read 2149 examples
read 2091 examples
read 43920 examples
read 1674 examples
read 2257 examples
Total prediction labels:  46
BertAdapterModel(
  (shared_parameters): ModuleDict()
  (bert): BertModel(
    (shared_parameters): ModuleDict()
    (invertible_adapters): ModuleDict(
      (en): NICECouplingBlock(
        (F): Sequential(
          (0): Linear(in_features=384, out_features=192, bias=True)
          (1): Activation_Function_Class(
            (f): ReLU()
          )
          (2): Linear(in_features=192, out_features=384, bias=True)
        )
        (G): Sequential(
          (0): Linear(in_features=384, out_features=192, bias=True)
          (1): Activation_Function_Class(
            (f): ReLU()
          )
          (2): Linear(in_features=192, out_features=384, bias=True)
        )
      )
    )
    (embeddings): BertEmbeddings(
      (word_embeddings): Embedding(119547, 768, padding_idx=0)
      (position_embeddings): Embedding(512, 768)
      (token_type_embeddings): Embedding(2, 768)
      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
      (dropout): Dropout(p=0.1, inplace=False)
    )
    (encoder): BertEncoder(
      (layer): ModuleList(
        (0): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (en): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (1): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (en): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (2): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (en): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (3): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (en): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (4): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (en): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (5): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (en): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (6): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (en): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (7): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (en): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (8): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (en): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (9): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (en): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (10): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (en): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (11): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (en): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
      )
    )
    (pooler): BertPooler(
      (dense): Linear(in_features=768, out_features=768, bias=True)
      (activation): Tanh()
    )
    (prefix_tuning): PrefixTuningPool(
      (prefix_tunings): ModuleDict()
    )
  )
  (heads): ModuleDict(
    (disrpt): ClassificationHead(
      (0): Dropout(p=0.1, inplace=False)
      (1): Linear(in_features=768, out_features=768, bias=True)
      (2): Activation_Function_Class(
        (f): Tanh()
      )
      (3): Dropout(p=0.1, inplace=False)
      (4): Linear(in_features=768, out_features=46, bias=True)
    )
  )
)
{'train@eng.rst.gum_loss': 2.581017255783081, 'train@eng.rst.gum_accuracy@eng.rst.gum': 0.23004965100381378, 'train@eng.rst.gum_f1@eng.rst.gum': 0.025448528704127688, 'train@eng.rst.gum_precision@eng.rst.gum': 0.018969317996968188, 'train@eng.rst.gum_recall@eng.rst.gum': 0.04855016796962692, 'train@eng.rst.gum_loss@eng.rst.gum': 2.5810177326202393, 'train@eng.rst.gum_runtime': 163.0846, 'train@eng.rst.gum_samples_per_second': 85.213, 'train@eng.rst.gum_steps_per_second': 2.667, 'epoch': 1.0}
{'loss': 2.8916, 'learning_rate': 1.8333333333333333e-05, 'epoch': 1.0}
{'eval_loss': 2.6507437229156494, 'eval_accuracy@eng.rst.gum': 0.22987436016751978, 'eval_f1@eng.rst.gum': 0.026568771520461592, 'eval_precision@eng.rst.gum': 0.019554103844522053, 'eval_recall@eng.rst.gum': 0.050911629009205164, 'eval_loss@eng.rst.gum': 2.6507439613342285, 'eval_runtime': 25.6277, 'eval_samples_per_second': 83.854, 'eval_steps_per_second': 2.653, 'epoch': 1.0}
{'train@eng.rst.gum_loss': 2.2159953117370605, 'train@eng.rst.gum_accuracy@eng.rst.gum': 0.33683528819169606, 'train@eng.rst.gum_f1@eng.rst.gum': 0.08563193890503454, 'train@eng.rst.gum_precision@eng.rst.gum': 0.21223292997710264, 'train@eng.rst.gum_recall@eng.rst.gum': 0.11070545746356265, 'train@eng.rst.gum_loss@eng.rst.gum': 2.2159950733184814, 'train@eng.rst.gum_runtime': 163.2705, 'train@eng.rst.gum_samples_per_second': 85.116, 'train@eng.rst.gum_steps_per_second': 2.664, 'epoch': 2.0}
{'loss': 2.4627, 'learning_rate': 1.6666666666666667e-05, 'epoch': 2.0}
{'eval_loss': 2.2965404987335205, 'eval_accuracy@eng.rst.gum': 0.32061423918101445, 'eval_f1@eng.rst.gum': 0.08235537757673243, 'eval_precision@eng.rst.gum': 0.14067177090325042, 'eval_recall@eng.rst.gum': 0.11346797023712661, 'eval_loss@eng.rst.gum': 2.2965404987335205, 'eval_runtime': 25.6535, 'eval_samples_per_second': 83.77, 'eval_steps_per_second': 2.651, 'epoch': 2.0}
{'train@eng.rst.gum_loss': 1.9120110273361206, 'train@eng.rst.gum_accuracy@eng.rst.gum': 0.45275958840037417, 'train@eng.rst.gum_f1@eng.rst.gum': 0.23558502021470443, 'train@eng.rst.gum_precision@eng.rst.gum': 0.27099161912885394, 'train@eng.rst.gum_recall@eng.rst.gum': 0.24841709430045522, 'train@eng.rst.gum_loss@eng.rst.gum': 1.9120110273361206, 'train@eng.rst.gum_runtime': 162.9252, 'train@eng.rst.gum_samples_per_second': 85.297, 'train@eng.rst.gum_steps_per_second': 2.67, 'epoch': 3.0}
{'loss': 2.1232, 'learning_rate': 1.5000000000000002e-05, 'epoch': 3.0}
{'eval_loss': 2.0187020301818848, 'eval_accuracy@eng.rst.gum': 0.4243834341554211, 'eval_f1@eng.rst.gum': 0.22919194004572163, 'eval_precision@eng.rst.gum': 0.23776426402557468, 'eval_recall@eng.rst.gum': 0.24557139958316015, 'eval_loss@eng.rst.gum': 2.018702268600464, 'eval_runtime': 25.6148, 'eval_samples_per_second': 83.897, 'eval_steps_per_second': 2.655, 'epoch': 3.0}
{'train@eng.rst.gum_loss': 1.7495124340057373, 'train@eng.rst.gum_accuracy@eng.rst.gum': 0.48434914010218033, 'train@eng.rst.gum_f1@eng.rst.gum': 0.2699406256614637, 'train@eng.rst.gum_precision@eng.rst.gum': 0.4045158995425487, 'train@eng.rst.gum_recall@eng.rst.gum': 0.28416721450811055, 'train@eng.rst.gum_loss@eng.rst.gum': 1.7495123147964478, 'train@eng.rst.gum_runtime': 163.2244, 'train@eng.rst.gum_samples_per_second': 85.14, 'train@eng.rst.gum_steps_per_second': 2.665, 'epoch': 4.0}
{'loss': 1.9069, 'learning_rate': 1.3333333333333333e-05, 'epoch': 4.0}
{'eval_loss': 1.8905885219573975, 'eval_accuracy@eng.rst.gum': 0.4611447184737087, 'eval_f1@eng.rst.gum': 0.2689936402278302, 'eval_precision@eng.rst.gum': 0.33154559836538444, 'eval_recall@eng.rst.gum': 0.283379994942989, 'eval_loss@eng.rst.gum': 1.890588402748108, 'eval_runtime': 25.6182, 'eval_samples_per_second': 83.886, 'eval_steps_per_second': 2.654, 'epoch': 4.0}
{'train@eng.rst.gum_loss': 1.6619517803192139, 'train@eng.rst.gum_accuracy@eng.rst.gum': 0.5066561128301073, 'train@eng.rst.gum_f1@eng.rst.gum': 0.318973535130491, 'train@eng.rst.gum_precision@eng.rst.gum': 0.43487873941014316, 'train@eng.rst.gum_recall@eng.rst.gum': 0.325951842471822, 'train@eng.rst.gum_loss@eng.rst.gum': 1.661952018737793, 'train@eng.rst.gum_runtime': 163.2137, 'train@eng.rst.gum_samples_per_second': 85.146, 'train@eng.rst.gum_steps_per_second': 2.665, 'epoch': 5.0}
{'loss': 1.7845, 'learning_rate': 1.1666666666666668e-05, 'epoch': 5.0}
{'eval_loss': 1.8260586261749268, 'eval_accuracy@eng.rst.gum': 0.467194043741275, 'eval_f1@eng.rst.gum': 0.2971301025715828, 'eval_precision@eng.rst.gum': 0.3624984994401043, 'eval_recall@eng.rst.gum': 0.31422006164661304, 'eval_loss@eng.rst.gum': 1.8260587453842163, 'eval_runtime': 25.6394, 'eval_samples_per_second': 83.816, 'eval_steps_per_second': 2.652, 'epoch': 5.0}
{'train@eng.rst.gum_loss': 1.6037191152572632, 'train@eng.rst.gum_accuracy@eng.rst.gum': 0.5255091026840325, 'train@eng.rst.gum_f1@eng.rst.gum': 0.35666716885799316, 'train@eng.rst.gum_precision@eng.rst.gum': 0.4438707592595522, 'train@eng.rst.gum_recall@eng.rst.gum': 0.3565126527807393, 'train@eng.rst.gum_loss@eng.rst.gum': 1.6037191152572632, 'train@eng.rst.gum_runtime': 163.0448, 'train@eng.rst.gum_samples_per_second': 85.234, 'train@eng.rst.gum_steps_per_second': 2.668, 'epoch': 6.0}
{'loss': 1.7124, 'learning_rate': 1e-05, 'epoch': 6.0}
{'eval_loss': 1.783996820449829, 'eval_accuracy@eng.rst.gum': 0.48348068869241506, 'eval_f1@eng.rst.gum': 0.33365189488842983, 'eval_precision@eng.rst.gum': 0.42799616330210627, 'eval_recall@eng.rst.gum': 0.33913926653731064, 'eval_loss@eng.rst.gum': 1.7839969396591187, 'eval_runtime': 25.6335, 'eval_samples_per_second': 83.836, 'eval_steps_per_second': 2.653, 'epoch': 6.0}
{'train@eng.rst.gum_loss': 1.563864827156067, 'train@eng.rst.gum_accuracy@eng.rst.gum': 0.5342879758221198, 'train@eng.rst.gum_f1@eng.rst.gum': 0.36829107834390845, 'train@eng.rst.gum_precision@eng.rst.gum': 0.5181347582675526, 'train@eng.rst.gum_recall@eng.rst.gum': 0.37151556512746253, 'train@eng.rst.gum_loss@eng.rst.gum': 1.5638649463653564, 'train@eng.rst.gum_runtime': 163.2968, 'train@eng.rst.gum_samples_per_second': 85.103, 'train@eng.rst.gum_steps_per_second': 2.664, 'epoch': 7.0}
{'loss': 1.6628, 'learning_rate': 8.333333333333334e-06, 'epoch': 7.0}
{'eval_loss': 1.7572274208068848, 'eval_accuracy@eng.rst.gum': 0.48859934853420195, 'eval_f1@eng.rst.gum': 0.3427153975623368, 'eval_precision@eng.rst.gum': 0.428659505707366, 'eval_recall@eng.rst.gum': 0.3517370898484483, 'eval_loss@eng.rst.gum': 1.7572275400161743, 'eval_runtime': 25.6517, 'eval_samples_per_second': 83.776, 'eval_steps_per_second': 2.651, 'epoch': 7.0}
{'train@eng.rst.gum_loss': 1.535880446434021, 'train@eng.rst.gum_accuracy@eng.rst.gum': 0.5446499244441246, 'train@eng.rst.gum_f1@eng.rst.gum': 0.3836464957702243, 'train@eng.rst.gum_precision@eng.rst.gum': 0.5165106216880619, 'train@eng.rst.gum_recall@eng.rst.gum': 0.38383177213031283, 'train@eng.rst.gum_loss@eng.rst.gum': 1.535880446434021, 'train@eng.rst.gum_runtime': 163.3054, 'train@eng.rst.gum_samples_per_second': 85.098, 'train@eng.rst.gum_steps_per_second': 2.664, 'epoch': 8.0}
{'loss': 1.6258, 'learning_rate': 6.666666666666667e-06, 'epoch': 8.0}
{'eval_loss': 1.739748239517212, 'eval_accuracy@eng.rst.gum': 0.495114006514658, 'eval_f1@eng.rst.gum': 0.3535104316535911, 'eval_precision@eng.rst.gum': 0.42125193012979606, 'eval_recall@eng.rst.gum': 0.3648879556636706, 'eval_loss@eng.rst.gum': 1.739748239517212, 'eval_runtime': 25.6097, 'eval_samples_per_second': 83.913, 'eval_steps_per_second': 2.655, 'epoch': 8.0}
{'train@eng.rst.gum_loss': 1.512820839881897, 'train@eng.rst.gum_accuracy@eng.rst.gum': 0.549830898755127, 'train@eng.rst.gum_f1@eng.rst.gum': 0.3930095878413352, 'train@eng.rst.gum_precision@eng.rst.gum': 0.5133015699215274, 'train@eng.rst.gum_recall@eng.rst.gum': 0.3953360763211633, 'train@eng.rst.gum_loss@eng.rst.gum': 1.512820839881897, 'train@eng.rst.gum_runtime': 163.151, 'train@eng.rst.gum_samples_per_second': 85.179, 'train@eng.rst.gum_steps_per_second': 2.666, 'epoch': 9.0}
{'loss': 1.6014, 'learning_rate': 5e-06, 'epoch': 9.0}
{'eval_loss': 1.721635341644287, 'eval_accuracy@eng.rst.gum': 0.5025593299208935, 'eval_f1@eng.rst.gum': 0.36752302593317604, 'eval_precision@eng.rst.gum': 0.4660832053864392, 'eval_recall@eng.rst.gum': 0.3801039066350203, 'eval_loss@eng.rst.gum': 1.7216352224349976, 'eval_runtime': 25.6347, 'eval_samples_per_second': 83.832, 'eval_steps_per_second': 2.653, 'epoch': 9.0}
{'train@eng.rst.gum_loss': 1.4976195096969604, 'train@eng.rst.gum_accuracy@eng.rst.gum': 0.5539325034180039, 'train@eng.rst.gum_f1@eng.rst.gum': 0.3994643238750481, 'train@eng.rst.gum_precision@eng.rst.gum': 0.557853394299121, 'train@eng.rst.gum_recall@eng.rst.gum': 0.39860555469411824, 'train@eng.rst.gum_loss@eng.rst.gum': 1.49761962890625, 'train@eng.rst.gum_runtime': 163.158, 'train@eng.rst.gum_samples_per_second': 85.175, 'train@eng.rst.gum_steps_per_second': 2.666, 'epoch': 10.0}
{'loss': 1.5849, 'learning_rate': 3.3333333333333333e-06, 'epoch': 10.0}
{'eval_loss': 1.7128410339355469, 'eval_accuracy@eng.rst.gum': 0.5025593299208935, 'eval_f1@eng.rst.gum': 0.3701588585246017, 'eval_precision@eng.rst.gum': 0.46419866344390015, 'eval_recall@eng.rst.gum': 0.3785685655907232, 'eval_loss@eng.rst.gum': 1.7128410339355469, 'eval_runtime': 25.6471, 'eval_samples_per_second': 83.791, 'eval_steps_per_second': 2.651, 'epoch': 10.0}
{'train@eng.rst.gum_loss': 1.4893471002578735, 'train@eng.rst.gum_accuracy@eng.rst.gum': 0.5560912427142549, 'train@eng.rst.gum_f1@eng.rst.gum': 0.40437876087325203, 'train@eng.rst.gum_precision@eng.rst.gum': 0.5510512831897852, 'train@eng.rst.gum_recall@eng.rst.gum': 0.40606566444066555, 'train@eng.rst.gum_loss@eng.rst.gum': 1.4893471002578735, 'train@eng.rst.gum_runtime': 163.0232, 'train@eng.rst.gum_samples_per_second': 85.246, 'train@eng.rst.gum_steps_per_second': 2.668, 'epoch': 11.0}
{'loss': 1.5741, 'learning_rate': 1.6666666666666667e-06, 'epoch': 11.0}
{'eval_loss': 1.7063580751419067, 'eval_accuracy@eng.rst.gum': 0.5095393206142392, 'eval_f1@eng.rst.gum': 0.376460060556794, 'eval_precision@eng.rst.gum': 0.4598024560233052, 'eval_recall@eng.rst.gum': 0.38892267367334865, 'eval_loss@eng.rst.gum': 1.7063581943511963, 'eval_runtime': 25.6595, 'eval_samples_per_second': 83.751, 'eval_steps_per_second': 2.65, 'epoch': 11.0}
{'train@eng.rst.gum_loss': 1.4865014553070068, 'train@eng.rst.gum_accuracy@eng.rst.gum': 0.5563790746204217, 'train@eng.rst.gum_f1@eng.rst.gum': 0.40353319511140734, 'train@eng.rst.gum_precision@eng.rst.gum': 0.5536345951863778, 'train@eng.rst.gum_recall@eng.rst.gum': 0.40468602837482276, 'train@eng.rst.gum_loss@eng.rst.gum': 1.4865014553070068, 'train@eng.rst.gum_runtime': 162.984, 'train@eng.rst.gum_samples_per_second': 85.266, 'train@eng.rst.gum_steps_per_second': 2.669, 'epoch': 12.0}
{'loss': 1.5595, 'learning_rate': 0.0, 'epoch': 12.0}
{'eval_loss': 1.7048115730285645, 'eval_accuracy@eng.rst.gum': 0.5072126570497906, 'eval_f1@eng.rst.gum': 0.37572346335173695, 'eval_precision@eng.rst.gum': 0.4583902936457096, 'eval_recall@eng.rst.gum': 0.3878093596396057, 'eval_loss@eng.rst.gum': 1.704811692237854, 'eval_runtime': 25.6506, 'eval_samples_per_second': 83.78, 'eval_steps_per_second': 2.651, 'epoch': 12.0}
{'train_runtime': 6400.6994, 'train_samples_per_second': 26.054, 'train_steps_per_second': 0.816, 'train_loss': 1.8741493429717433, 'epoch': 12.0}
***** train_log metrics *****
  epoch                    =       12.0
  train_loss               =     1.8741
  train_runtime            = 1:46:40.69
  train_samples_per_second =     26.054
  train_steps_per_second   =      0.816
{'train@eng.pdtb.pdtb_loss': 1.2069370746612549, 'train@eng.pdtb.pdtb_accuracy@eng.pdtb.pdtb': 0.6102686703096539, 'train@eng.pdtb.pdtb_f1@eng.pdtb.pdtb': 0.30937368280821104, 'train@eng.pdtb.pdtb_precision@eng.pdtb.pdtb': 0.3494928315678089, 'train@eng.pdtb.pdtb_recall@eng.pdtb.pdtb': 0.2955726254378717, 'train@eng.pdtb.pdtb_loss@eng.pdtb.pdtb': 1.2069370746612549, 'train@eng.pdtb.pdtb_runtime': 515.3646, 'train@eng.pdtb.pdtb_samples_per_second': 85.221, 'train@eng.pdtb.pdtb_steps_per_second': 2.664, 'epoch': 1.0}
{'loss': 1.6287, 'learning_rate': 1.8333333333333333e-05, 'epoch': 1.0}
{'eval_loss': 1.1390248537063599, 'eval_accuracy@eng.pdtb.pdtb': 0.6415770609318996, 'eval_f1@eng.pdtb.pdtb': 0.3626167600062617, 'eval_precision@eng.pdtb.pdtb': 0.4286590388998023, 'eval_recall@eng.pdtb.pdtb': 0.3449981201961957, 'eval_loss@eng.pdtb.pdtb': 1.1390248537063599, 'eval_runtime': 20.0764, 'eval_samples_per_second': 83.382, 'eval_steps_per_second': 2.64, 'epoch': 1.0}
{'train@eng.pdtb.pdtb_loss': 1.0779832601547241, 'train@eng.pdtb.pdtb_accuracy@eng.pdtb.pdtb': 0.6460610200364298, 'train@eng.pdtb.pdtb_f1@eng.pdtb.pdtb': 0.41813086762354607, 'train@eng.pdtb.pdtb_precision@eng.pdtb.pdtb': 0.4623442009290719, 'train@eng.pdtb.pdtb_recall@eng.pdtb.pdtb': 0.40766493017727773, 'train@eng.pdtb.pdtb_loss@eng.pdtb.pdtb': 1.0779832601547241, 'train@eng.pdtb.pdtb_runtime': 515.6942, 'train@eng.pdtb.pdtb_samples_per_second': 85.167, 'train@eng.pdtb.pdtb_steps_per_second': 2.662, 'epoch': 2.0}
{'loss': 1.1938, 'learning_rate': 1.6666666666666667e-05, 'epoch': 2.0}
{'eval_loss': 1.0216774940490723, 'eval_accuracy@eng.pdtb.pdtb': 0.6672640382317802, 'eval_f1@eng.pdtb.pdtb': 0.45308294705154595, 'eval_precision@eng.pdtb.pdtb': 0.5235854817618277, 'eval_recall@eng.pdtb.pdtb': 0.43181691681272183, 'eval_loss@eng.pdtb.pdtb': 1.0216774940490723, 'eval_runtime': 20.1019, 'eval_samples_per_second': 83.276, 'eval_steps_per_second': 2.637, 'epoch': 2.0}
{'train@eng.pdtb.pdtb_loss': 1.0370116233825684, 'train@eng.pdtb.pdtb_accuracy@eng.pdtb.pdtb': 0.6552595628415301, 'train@eng.pdtb.pdtb_f1@eng.pdtb.pdtb': 0.4388922646843266, 'train@eng.pdtb.pdtb_precision@eng.pdtb.pdtb': 0.5030330663725805, 'train@eng.pdtb.pdtb_recall@eng.pdtb.pdtb': 0.43288631354315243, 'train@eng.pdtb.pdtb_loss@eng.pdtb.pdtb': 1.0370115041732788, 'train@eng.pdtb.pdtb_runtime': 515.5369, 'train@eng.pdtb.pdtb_samples_per_second': 85.193, 'train@eng.pdtb.pdtb_steps_per_second': 2.663, 'epoch': 3.0}
{'loss': 1.1128, 'learning_rate': 1.5000000000000002e-05, 'epoch': 3.0}
{'eval_loss': 0.9851779341697693, 'eval_accuracy@eng.pdtb.pdtb': 0.6774193548387096, 'eval_f1@eng.pdtb.pdtb': 0.5059497404219522, 'eval_precision@eng.pdtb.pdtb': 0.5443588395686255, 'eval_recall@eng.pdtb.pdtb': 0.498982534802405, 'eval_loss@eng.pdtb.pdtb': 0.9851779341697693, 'eval_runtime': 20.11, 'eval_samples_per_second': 83.242, 'eval_steps_per_second': 2.636, 'epoch': 3.0}
{'train@eng.pdtb.pdtb_loss': 0.9965301752090454, 'train@eng.pdtb.pdtb_accuracy@eng.pdtb.pdtb': 0.6670765027322404, 'train@eng.pdtb.pdtb_f1@eng.pdtb.pdtb': 0.44821874656008115, 'train@eng.pdtb.pdtb_precision@eng.pdtb.pdtb': 0.5097577845995814, 'train@eng.pdtb.pdtb_recall@eng.pdtb.pdtb': 0.44408660987486925, 'train@eng.pdtb.pdtb_loss@eng.pdtb.pdtb': 0.996530294418335, 'train@eng.pdtb.pdtb_runtime': 515.9241, 'train@eng.pdtb.pdtb_samples_per_second': 85.129, 'train@eng.pdtb.pdtb_steps_per_second': 2.661, 'epoch': 4.0}
{'loss': 1.0712, 'learning_rate': 1.3333333333333333e-05, 'epoch': 4.0}
{'eval_loss': 0.9488795399665833, 'eval_accuracy@eng.pdtb.pdtb': 0.6887694145758662, 'eval_f1@eng.pdtb.pdtb': 0.5113662577647022, 'eval_precision@eng.pdtb.pdtb': 0.5458816870748439, 'eval_recall@eng.pdtb.pdtb': 0.5023661579364415, 'eval_loss@eng.pdtb.pdtb': 0.9488795399665833, 'eval_runtime': 20.0678, 'eval_samples_per_second': 83.417, 'eval_steps_per_second': 2.641, 'epoch': 4.0}
{'train@eng.pdtb.pdtb_loss': 0.974262535572052, 'train@eng.pdtb.pdtb_accuracy@eng.pdtb.pdtb': 0.6729963570127504, 'train@eng.pdtb.pdtb_f1@eng.pdtb.pdtb': 0.4552750412430368, 'train@eng.pdtb.pdtb_precision@eng.pdtb.pdtb': 0.5133031425369443, 'train@eng.pdtb.pdtb_recall@eng.pdtb.pdtb': 0.4541631723271542, 'train@eng.pdtb.pdtb_loss@eng.pdtb.pdtb': 0.9742625951766968, 'train@eng.pdtb.pdtb_runtime': 515.5542, 'train@eng.pdtb.pdtb_samples_per_second': 85.19, 'train@eng.pdtb.pdtb_steps_per_second': 2.663, 'epoch': 5.0}
{'loss': 1.045, 'learning_rate': 1.1666666666666668e-05, 'epoch': 5.0}
{'eval_loss': 0.9338982701301575, 'eval_accuracy@eng.pdtb.pdtb': 0.6851851851851852, 'eval_f1@eng.pdtb.pdtb': 0.5139356852116451, 'eval_precision@eng.pdtb.pdtb': 0.5402514765828508, 'eval_recall@eng.pdtb.pdtb': 0.5118132681670222, 'eval_loss@eng.pdtb.pdtb': 0.9338982105255127, 'eval_runtime': 20.0925, 'eval_samples_per_second': 83.315, 'eval_steps_per_second': 2.638, 'epoch': 5.0}
{'train@eng.pdtb.pdtb_loss': 0.9559381604194641, 'train@eng.pdtb.pdtb_accuracy@eng.pdtb.pdtb': 0.6770947176684882, 'train@eng.pdtb.pdtb_f1@eng.pdtb.pdtb': 0.4622657366637533, 'train@eng.pdtb.pdtb_precision@eng.pdtb.pdtb': 0.5054127984068222, 'train@eng.pdtb.pdtb_recall@eng.pdtb.pdtb': 0.4593168260298497, 'train@eng.pdtb.pdtb_loss@eng.pdtb.pdtb': 0.9559381604194641, 'train@eng.pdtb.pdtb_runtime': 515.8202, 'train@eng.pdtb.pdtb_samples_per_second': 85.146, 'train@eng.pdtb.pdtb_steps_per_second': 2.662, 'epoch': 6.0}
{'loss': 1.0223, 'learning_rate': 1e-05, 'epoch': 6.0}
{'eval_loss': 0.9266541004180908, 'eval_accuracy@eng.pdtb.pdtb': 0.6893667861409797, 'eval_f1@eng.pdtb.pdtb': 0.5219567382914672, 'eval_precision@eng.pdtb.pdtb': 0.546816097740945, 'eval_recall@eng.pdtb.pdtb': 0.5168954725430263, 'eval_loss@eng.pdtb.pdtb': 0.9266542196273804, 'eval_runtime': 20.0397, 'eval_samples_per_second': 83.534, 'eval_steps_per_second': 2.645, 'epoch': 6.0}
{'train@eng.pdtb.pdtb_loss': 0.9457975625991821, 'train@eng.pdtb.pdtb_accuracy@eng.pdtb.pdtb': 0.6787340619307832, 'train@eng.pdtb.pdtb_f1@eng.pdtb.pdtb': 0.48285382532957555, 'train@eng.pdtb.pdtb_precision@eng.pdtb.pdtb': 0.5747325384733604, 'train@eng.pdtb.pdtb_recall@eng.pdtb.pdtb': 0.46753386541019326, 'train@eng.pdtb.pdtb_loss@eng.pdtb.pdtb': 0.9457975029945374, 'train@eng.pdtb.pdtb_runtime': 516.1661, 'train@eng.pdtb.pdtb_samples_per_second': 85.089, 'train@eng.pdtb.pdtb_steps_per_second': 2.66, 'epoch': 7.0}
{'loss': 1.0069, 'learning_rate': 8.333333333333334e-06, 'epoch': 7.0}
{'eval_loss': 0.9131190180778503, 'eval_accuracy@eng.pdtb.pdtb': 0.6881720430107527, 'eval_f1@eng.pdtb.pdtb': 0.5333241866535732, 'eval_precision@eng.pdtb.pdtb': 0.5714361062859643, 'eval_recall@eng.pdtb.pdtb': 0.5183428725859025, 'eval_loss@eng.pdtb.pdtb': 0.9131189584732056, 'eval_runtime': 20.1008, 'eval_samples_per_second': 83.28, 'eval_steps_per_second': 2.637, 'epoch': 7.0}
{'train@eng.pdtb.pdtb_loss': 0.9335460066795349, 'train@eng.pdtb.pdtb_accuracy@eng.pdtb.pdtb': 0.6831511839708561, 'train@eng.pdtb.pdtb_f1@eng.pdtb.pdtb': 0.48948663920882, 'train@eng.pdtb.pdtb_precision@eng.pdtb.pdtb': 0.551258744049368, 'train@eng.pdtb.pdtb_recall@eng.pdtb.pdtb': 0.48000635242996703, 'train@eng.pdtb.pdtb_loss@eng.pdtb.pdtb': 0.9335460066795349, 'train@eng.pdtb.pdtb_runtime': 515.9662, 'train@eng.pdtb.pdtb_samples_per_second': 85.122, 'train@eng.pdtb.pdtb_steps_per_second': 2.661, 'epoch': 8.0}
{'loss': 0.9972, 'learning_rate': 6.666666666666667e-06, 'epoch': 8.0}
{'eval_loss': 0.9068894982337952, 'eval_accuracy@eng.pdtb.pdtb': 0.6965352449223416, 'eval_f1@eng.pdtb.pdtb': 0.5344810620548676, 'eval_precision@eng.pdtb.pdtb': 0.5621983576341062, 'eval_recall@eng.pdtb.pdtb': 0.5267123619916545, 'eval_loss@eng.pdtb.pdtb': 0.9068896174430847, 'eval_runtime': 20.1509, 'eval_samples_per_second': 83.073, 'eval_steps_per_second': 2.63, 'epoch': 8.0}
{'train@eng.pdtb.pdtb_loss': 0.9252035617828369, 'train@eng.pdtb.pdtb_accuracy@eng.pdtb.pdtb': 0.6866803278688525, 'train@eng.pdtb.pdtb_f1@eng.pdtb.pdtb': 0.4940220325057785, 'train@eng.pdtb.pdtb_precision@eng.pdtb.pdtb': 0.5607581355193619, 'train@eng.pdtb.pdtb_recall@eng.pdtb.pdtb': 0.48341559566352144, 'train@eng.pdtb.pdtb_loss@eng.pdtb.pdtb': 0.9252035617828369, 'train@eng.pdtb.pdtb_runtime': 515.9603, 'train@eng.pdtb.pdtb_samples_per_second': 85.123, 'train@eng.pdtb.pdtb_steps_per_second': 2.661, 'epoch': 9.0}
{'loss': 0.9855, 'learning_rate': 5e-06, 'epoch': 9.0}
{'eval_loss': 0.9058254957199097, 'eval_accuracy@eng.pdtb.pdtb': 0.6953405017921147, 'eval_f1@eng.pdtb.pdtb': 0.5423843813580744, 'eval_precision@eng.pdtb.pdtb': 0.571747038152538, 'eval_recall@eng.pdtb.pdtb': 0.5330573789751093, 'eval_loss@eng.pdtb.pdtb': 0.9058256149291992, 'eval_runtime': 20.0735, 'eval_samples_per_second': 83.394, 'eval_steps_per_second': 2.64, 'epoch': 9.0}
{'train@eng.pdtb.pdtb_loss': 0.9227530360221863, 'train@eng.pdtb.pdtb_accuracy@eng.pdtb.pdtb': 0.6871812386156648, 'train@eng.pdtb.pdtb_f1@eng.pdtb.pdtb': 0.49510116661648257, 'train@eng.pdtb.pdtb_precision@eng.pdtb.pdtb': 0.5799346340897431, 'train@eng.pdtb.pdtb_recall@eng.pdtb.pdtb': 0.48267914072129375, 'train@eng.pdtb.pdtb_loss@eng.pdtb.pdtb': 0.9227529168128967, 'train@eng.pdtb.pdtb_runtime': 516.1309, 'train@eng.pdtb.pdtb_samples_per_second': 85.095, 'train@eng.pdtb.pdtb_steps_per_second': 2.66, 'epoch': 10.0}
{'loss': 0.9799, 'learning_rate': 3.3333333333333333e-06, 'epoch': 10.0}
{'eval_loss': 0.9021744132041931, 'eval_accuracy@eng.pdtb.pdtb': 0.6917562724014337, 'eval_f1@eng.pdtb.pdtb': 0.5415962860930671, 'eval_precision@eng.pdtb.pdtb': 0.5758182121260381, 'eval_recall@eng.pdtb.pdtb': 0.5307725826129668, 'eval_loss@eng.pdtb.pdtb': 0.9021743535995483, 'eval_runtime': 20.1102, 'eval_samples_per_second': 83.241, 'eval_steps_per_second': 2.635, 'epoch': 10.0}
{'train@eng.pdtb.pdtb_loss': 0.91744065284729, 'train@eng.pdtb.pdtb_accuracy@eng.pdtb.pdtb': 0.6883652094717668, 'train@eng.pdtb.pdtb_f1@eng.pdtb.pdtb': 0.4963496787471385, 'train@eng.pdtb.pdtb_precision@eng.pdtb.pdtb': 0.5519915955692118, 'train@eng.pdtb.pdtb_recall@eng.pdtb.pdtb': 0.487069779864677, 'train@eng.pdtb.pdtb_loss@eng.pdtb.pdtb': 0.9174405336380005, 'train@eng.pdtb.pdtb_runtime': 515.9319, 'train@eng.pdtb.pdtb_samples_per_second': 85.128, 'train@eng.pdtb.pdtb_steps_per_second': 2.661, 'epoch': 11.0}
{'loss': 0.9735, 'learning_rate': 1.6666666666666667e-06, 'epoch': 11.0}
{'eval_loss': 0.9000391960144043, 'eval_accuracy@eng.pdtb.pdtb': 0.6977299880525687, 'eval_f1@eng.pdtb.pdtb': 0.5407884224618962, 'eval_precision@eng.pdtb.pdtb': 0.5644907071411309, 'eval_recall@eng.pdtb.pdtb': 0.5339787972039718, 'eval_loss@eng.pdtb.pdtb': 0.9000391960144043, 'eval_runtime': 20.1066, 'eval_samples_per_second': 83.256, 'eval_steps_per_second': 2.636, 'epoch': 11.0}
{'train@eng.pdtb.pdtb_loss': 0.9165560007095337, 'train@eng.pdtb.pdtb_accuracy@eng.pdtb.pdtb': 0.6885928961748634, 'train@eng.pdtb.pdtb_f1@eng.pdtb.pdtb': 0.49593613220104016, 'train@eng.pdtb.pdtb_precision@eng.pdtb.pdtb': 0.5590843172042924, 'train@eng.pdtb.pdtb_recall@eng.pdtb.pdtb': 0.4848185738817596, 'train@eng.pdtb.pdtb_loss@eng.pdtb.pdtb': 0.9165560007095337, 'train@eng.pdtb.pdtb_runtime': 515.9808, 'train@eng.pdtb.pdtb_samples_per_second': 85.119, 'train@eng.pdtb.pdtb_steps_per_second': 2.661, 'epoch': 12.0}
{'loss': 0.9693, 'learning_rate': 0.0, 'epoch': 12.0}
{'eval_loss': 0.8991729617118835, 'eval_accuracy@eng.pdtb.pdtb': 0.6935483870967742, 'eval_f1@eng.pdtb.pdtb': 0.5406800220793689, 'eval_precision@eng.pdtb.pdtb': 0.5696090159825092, 'eval_recall@eng.pdtb.pdtb': 0.5299630319835635, 'eval_loss@eng.pdtb.pdtb': 0.8991729617118835, 'eval_runtime': 20.0698, 'eval_samples_per_second': 83.409, 'eval_steps_per_second': 2.641, 'epoch': 12.0}
{'train_runtime': 19516.9446, 'train_samples_per_second': 27.004, 'train_steps_per_second': 0.844, 'train_loss': 1.0821777477111594, 'epoch': 12.0}
***** train_logger metrics *****
  epoch                    =       12.0
  train_loss               =     1.8741
  train_runtime            = 1:46:40.69
  train_samples_per_second =     26.054
  train_steps_per_second   =      0.816
-------------------------------------------------------------------
Lang1:  eng.rst.rstdt    Lang2:  eng.pdtb.pdtb
Saving run to:  runs/full_shot/FullShot=v4_eng.rst.rstdt_eng.pdtb.pdtb_lrfunc=Adafactor_lr=2e-05_plm=bert-base-multilingual-cased
Running with params: BERT_MODEL=bert-base-multilingual-cased lr=2e-05
read 16002 examples
read 1621 examples
read 2155 examples
read 43920 examples
read 1674 examples
read 2257 examples
Total prediction labels:  40
BertAdapterModel(
  (shared_parameters): ModuleDict()
  (bert): BertModel(
    (shared_parameters): ModuleDict()
    (invertible_adapters): ModuleDict(
      (en): NICECouplingBlock(
        (F): Sequential(
          (0): Linear(in_features=384, out_features=192, bias=True)
          (1): Activation_Function_Class(
            (f): ReLU()
          )
          (2): Linear(in_features=192, out_features=384, bias=True)
        )
        (G): Sequential(
          (0): Linear(in_features=384, out_features=192, bias=True)
          (1): Activation_Function_Class(
            (f): ReLU()
          )
          (2): Linear(in_features=192, out_features=384, bias=True)
        )
      )
    )
    (embeddings): BertEmbeddings(
      (word_embeddings): Embedding(119547, 768, padding_idx=0)
      (position_embeddings): Embedding(512, 768)
      (token_type_embeddings): Embedding(2, 768)
      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
      (dropout): Dropout(p=0.1, inplace=False)
    )
    (encoder): BertEncoder(
      (layer): ModuleList(
        (0): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (en): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (1): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (en): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (2): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (en): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (3): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (en): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (4): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (en): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (5): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (en): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (6): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (en): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (7): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (en): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (8): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (en): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (9): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (en): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (10): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (en): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (11): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (en): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
      )
    )
    (pooler): BertPooler(
      (dense): Linear(in_features=768, out_features=768, bias=True)
      (activation): Tanh()
    )
    (prefix_tuning): PrefixTuningPool(
      (prefix_tunings): ModuleDict()
    )
  )
  (heads): ModuleDict(
    (disrpt): ClassificationHead(
      (0): Dropout(p=0.1, inplace=False)
      (1): Linear(in_features=768, out_features=768, bias=True)
      (2): Activation_Function_Class(
        (f): Tanh()
      )
      (3): Dropout(p=0.1, inplace=False)
      (4): Linear(in_features=768, out_features=40, bias=True)
    )
  )
)
{'train@eng.rst.rstdt_loss': 1.7409735918045044, 'train@eng.rst.rstdt_accuracy@eng.rst.rstdt': 0.509936257967754, 'train@eng.rst.rstdt_f1@eng.rst.rstdt': 0.08224231012679141, 'train@eng.rst.rstdt_precision@eng.rst.rstdt': 0.11002635276349158, 'train@eng.rst.rstdt_recall@eng.rst.rstdt': 0.11054840888435429, 'train@eng.rst.rstdt_loss@eng.rst.rstdt': 1.7409734725952148, 'train@eng.rst.rstdt_runtime': 188.1076, 'train@eng.rst.rstdt_samples_per_second': 85.068, 'train@eng.rst.rstdt_steps_per_second': 2.663, 'epoch': 1.0}
{'loss': 2.2036, 'learning_rate': 1.8333333333333333e-05, 'epoch': 1.0}
{'eval_loss': 1.7211124897003174, 'eval_accuracy@eng.rst.rstdt': 0.5175817396668723, 'eval_f1@eng.rst.rstdt': 0.08089991512654675, 'eval_precision@eng.rst.rstdt': 0.06514586847777626, 'eval_recall@eng.rst.rstdt': 0.10794914175033637, 'eval_loss@eng.rst.rstdt': 1.721112608909607, 'eval_runtime': 19.3926, 'eval_samples_per_second': 83.589, 'eval_steps_per_second': 2.63, 'epoch': 1.0}
{'train@eng.rst.rstdt_loss': 1.431071162223816, 'train@eng.rst.rstdt_accuracy@eng.rst.rstdt': 0.5993000874890638, 'train@eng.rst.rstdt_f1@eng.rst.rstdt': 0.19894703585925472, 'train@eng.rst.rstdt_precision@eng.rst.rstdt': 0.2357795647814926, 'train@eng.rst.rstdt_recall@eng.rst.rstdt': 0.21003868831853745, 'train@eng.rst.rstdt_loss@eng.rst.rstdt': 1.431071162223816, 'train@eng.rst.rstdt_runtime': 188.1402, 'train@eng.rst.rstdt_samples_per_second': 85.054, 'train@eng.rst.rstdt_steps_per_second': 2.663, 'epoch': 2.0}
{'loss': 1.6119, 'learning_rate': 1.6666666666666667e-05, 'epoch': 2.0}
{'eval_loss': 1.437852144241333, 'eval_accuracy@eng.rst.rstdt': 0.6113510178901912, 'eval_f1@eng.rst.rstdt': 0.20621574796963008, 'eval_precision@eng.rst.rstdt': 0.23620935669132118, 'eval_recall@eng.rst.rstdt': 0.21324951508877066, 'eval_loss@eng.rst.rstdt': 1.437852144241333, 'eval_runtime': 19.4225, 'eval_samples_per_second': 83.46, 'eval_steps_per_second': 2.626, 'epoch': 2.0}
{'train@eng.rst.rstdt_loss': 1.3279958963394165, 'train@eng.rst.rstdt_accuracy@eng.rst.rstdt': 0.6226721659792526, 'train@eng.rst.rstdt_f1@eng.rst.rstdt': 0.2416788656446315, 'train@eng.rst.rstdt_precision@eng.rst.rstdt': 0.4194498311093686, 'train@eng.rst.rstdt_recall@eng.rst.rstdt': 0.24641372850046192, 'train@eng.rst.rstdt_loss@eng.rst.rstdt': 1.3279958963394165, 'train@eng.rst.rstdt_runtime': 187.9465, 'train@eng.rst.rstdt_samples_per_second': 85.141, 'train@eng.rst.rstdt_steps_per_second': 2.666, 'epoch': 3.0}
{'loss': 1.4357, 'learning_rate': 1.5000000000000002e-05, 'epoch': 3.0}
{'eval_loss': 1.3626515865325928, 'eval_accuracy@eng.rst.rstdt': 0.6310919185687847, 'eval_f1@eng.rst.rstdt': 0.24841452523545904, 'eval_precision@eng.rst.rstdt': 0.38073546400326036, 'eval_recall@eng.rst.rstdt': 0.25020833005602106, 'eval_loss@eng.rst.rstdt': 1.3626517057418823, 'eval_runtime': 19.4098, 'eval_samples_per_second': 83.515, 'eval_steps_per_second': 2.628, 'epoch': 3.0}
{'train@eng.rst.rstdt_loss': 1.2654526233673096, 'train@eng.rst.rstdt_accuracy@eng.rst.rstdt': 0.6405449318835146, 'train@eng.rst.rstdt_f1@eng.rst.rstdt': 0.3199283861734946, 'train@eng.rst.rstdt_precision@eng.rst.rstdt': 0.42617998827578824, 'train@eng.rst.rstdt_recall@eng.rst.rstdt': 0.3059799304817211, 'train@eng.rst.rstdt_loss@eng.rst.rstdt': 1.2654526233673096, 'train@eng.rst.rstdt_runtime': 188.3323, 'train@eng.rst.rstdt_samples_per_second': 84.967, 'train@eng.rst.rstdt_steps_per_second': 2.66, 'epoch': 4.0}
{'loss': 1.3484, 'learning_rate': 1.3333333333333333e-05, 'epoch': 4.0}
{'eval_loss': 1.3269860744476318, 'eval_accuracy@eng.rst.rstdt': 0.6304750154225787, 'eval_f1@eng.rst.rstdt': 0.3204952799711925, 'eval_precision@eng.rst.rstdt': 0.4323365517286921, 'eval_recall@eng.rst.rstdt': 0.31169831090234007, 'eval_loss@eng.rst.rstdt': 1.3269860744476318, 'eval_runtime': 19.4036, 'eval_samples_per_second': 83.541, 'eval_steps_per_second': 2.628, 'epoch': 4.0}
{'train@eng.rst.rstdt_loss': 1.2141773700714111, 'train@eng.rst.rstdt_accuracy@eng.rst.rstdt': 0.654230721159855, 'train@eng.rst.rstdt_f1@eng.rst.rstdt': 0.3411382456723417, 'train@eng.rst.rstdt_precision@eng.rst.rstdt': 0.44185624154648157, 'train@eng.rst.rstdt_recall@eng.rst.rstdt': 0.3201927761068843, 'train@eng.rst.rstdt_loss@eng.rst.rstdt': 1.2141774892807007, 'train@eng.rst.rstdt_runtime': 188.027, 'train@eng.rst.rstdt_samples_per_second': 85.105, 'train@eng.rst.rstdt_steps_per_second': 2.665, 'epoch': 5.0}
{'loss': 1.2942, 'learning_rate': 1.1666666666666668e-05, 'epoch': 5.0}
{'eval_loss': 1.2790569067001343, 'eval_accuracy@eng.rst.rstdt': 0.6458975940777298, 'eval_f1@eng.rst.rstdt': 0.3243575801561759, 'eval_precision@eng.rst.rstdt': 0.4070877421749478, 'eval_recall@eng.rst.rstdt': 0.31493180843775015, 'eval_loss@eng.rst.rstdt': 1.2790571451187134, 'eval_runtime': 19.409, 'eval_samples_per_second': 83.518, 'eval_steps_per_second': 2.628, 'epoch': 5.0}
{'train@eng.rst.rstdt_loss': 1.1781703233718872, 'train@eng.rst.rstdt_accuracy@eng.rst.rstdt': 0.6614173228346457, 'train@eng.rst.rstdt_f1@eng.rst.rstdt': 0.3655972373530607, 'train@eng.rst.rstdt_precision@eng.rst.rstdt': 0.5740192460923644, 'train@eng.rst.rstdt_recall@eng.rst.rstdt': 0.3368446808381464, 'train@eng.rst.rstdt_loss@eng.rst.rstdt': 1.1781704425811768, 'train@eng.rst.rstdt_runtime': 188.1508, 'train@eng.rst.rstdt_samples_per_second': 85.049, 'train@eng.rst.rstdt_steps_per_second': 2.663, 'epoch': 6.0}
{'loss': 1.2486, 'learning_rate': 1e-05, 'epoch': 6.0}
{'eval_loss': 1.252807378768921, 'eval_accuracy@eng.rst.rstdt': 0.6452806909315237, 'eval_f1@eng.rst.rstdt': 0.3273125839360662, 'eval_precision@eng.rst.rstdt': 0.41145693811838063, 'eval_recall@eng.rst.rstdt': 0.32021752123991154, 'eval_loss@eng.rst.rstdt': 1.2528072595596313, 'eval_runtime': 19.4003, 'eval_samples_per_second': 83.556, 'eval_steps_per_second': 2.629, 'epoch': 6.0}
{'train@eng.rst.rstdt_loss': 1.157420039176941, 'train@eng.rst.rstdt_accuracy@eng.rst.rstdt': 0.6661667291588551, 'train@eng.rst.rstdt_f1@eng.rst.rstdt': 0.3828216523990027, 'train@eng.rst.rstdt_precision@eng.rst.rstdt': 0.6197479972058069, 'train@eng.rst.rstdt_recall@eng.rst.rstdt': 0.3498926945152976, 'train@eng.rst.rstdt_loss@eng.rst.rstdt': 1.157420039176941, 'train@eng.rst.rstdt_runtime': 188.0932, 'train@eng.rst.rstdt_samples_per_second': 85.075, 'train@eng.rst.rstdt_steps_per_second': 2.664, 'epoch': 7.0}
{'loss': 1.2267, 'learning_rate': 8.333333333333334e-06, 'epoch': 7.0}
{'eval_loss': 1.2347605228424072, 'eval_accuracy@eng.rst.rstdt': 0.6434299814929056, 'eval_f1@eng.rst.rstdt': 0.3385101341855739, 'eval_precision@eng.rst.rstdt': 0.46505432956635745, 'eval_recall@eng.rst.rstdt': 0.32807932216016494, 'eval_loss@eng.rst.rstdt': 1.2347606420516968, 'eval_runtime': 19.4212, 'eval_samples_per_second': 83.465, 'eval_steps_per_second': 2.626, 'epoch': 7.0}
{'train@eng.rst.rstdt_loss': 1.1386305093765259, 'train@eng.rst.rstdt_accuracy@eng.rst.rstdt': 0.6692288463942008, 'train@eng.rst.rstdt_f1@eng.rst.rstdt': 0.3974148400921691, 'train@eng.rst.rstdt_precision@eng.rst.rstdt': 0.5794369055271923, 'train@eng.rst.rstdt_recall@eng.rst.rstdt': 0.36304110419757707, 'train@eng.rst.rstdt_loss@eng.rst.rstdt': 1.1386306285858154, 'train@eng.rst.rstdt_runtime': 188.1048, 'train@eng.rst.rstdt_samples_per_second': 85.07, 'train@eng.rst.rstdt_steps_per_second': 2.663, 'epoch': 8.0}
{'loss': 1.2038, 'learning_rate': 6.666666666666667e-06, 'epoch': 8.0}
{'eval_loss': 1.2276370525360107, 'eval_accuracy@eng.rst.rstdt': 0.6452806909315237, 'eval_f1@eng.rst.rstdt': 0.3555544091884043, 'eval_precision@eng.rst.rstdt': 0.4799954598842704, 'eval_recall@eng.rst.rstdt': 0.34448063713003274, 'eval_loss@eng.rst.rstdt': 1.2276370525360107, 'eval_runtime': 19.4237, 'eval_samples_per_second': 83.455, 'eval_steps_per_second': 2.626, 'epoch': 8.0}
{'train@eng.rst.rstdt_loss': 1.1260486841201782, 'train@eng.rst.rstdt_accuracy@eng.rst.rstdt': 0.6712285964254469, 'train@eng.rst.rstdt_f1@eng.rst.rstdt': 0.4045949063330209, 'train@eng.rst.rstdt_precision@eng.rst.rstdt': 0.5769643983054932, 'train@eng.rst.rstdt_recall@eng.rst.rstdt': 0.3684641175078884, 'train@eng.rst.rstdt_loss@eng.rst.rstdt': 1.1260486841201782, 'train@eng.rst.rstdt_runtime': 188.1942, 'train@eng.rst.rstdt_samples_per_second': 85.029, 'train@eng.rst.rstdt_steps_per_second': 2.662, 'epoch': 9.0}
{'loss': 1.1853, 'learning_rate': 5e-06, 'epoch': 9.0}
{'eval_loss': 1.2174030542373657, 'eval_accuracy@eng.rst.rstdt': 0.6434299814929056, 'eval_f1@eng.rst.rstdt': 0.3495872405642023, 'eval_precision@eng.rst.rstdt': 0.4703471175691626, 'eval_recall@eng.rst.rstdt': 0.33778714284256695, 'eval_loss@eng.rst.rstdt': 1.2174030542373657, 'eval_runtime': 19.3974, 'eval_samples_per_second': 83.568, 'eval_steps_per_second': 2.629, 'epoch': 9.0}
{'train@eng.rst.rstdt_loss': 1.120560646057129, 'train@eng.rst.rstdt_accuracy@eng.rst.rstdt': 0.6696662917135358, 'train@eng.rst.rstdt_f1@eng.rst.rstdt': 0.41745174858181533, 'train@eng.rst.rstdt_precision@eng.rst.rstdt': 0.6039706902716362, 'train@eng.rst.rstdt_recall@eng.rst.rstdt': 0.3816859966846208, 'train@eng.rst.rstdt_loss@eng.rst.rstdt': 1.120560646057129, 'train@eng.rst.rstdt_runtime': 188.1223, 'train@eng.rst.rstdt_samples_per_second': 85.062, 'train@eng.rst.rstdt_steps_per_second': 2.663, 'epoch': 10.0}
{'loss': 1.174, 'learning_rate': 3.3333333333333333e-06, 'epoch': 10.0}
{'eval_loss': 1.2224153280258179, 'eval_accuracy@eng.rst.rstdt': 0.6366440468846392, 'eval_f1@eng.rst.rstdt': 0.35777650839953035, 'eval_precision@eng.rst.rstdt': 0.4586869115747728, 'eval_recall@eng.rst.rstdt': 0.34811006221987356, 'eval_loss@eng.rst.rstdt': 1.2224153280258179, 'eval_runtime': 19.417, 'eval_samples_per_second': 83.484, 'eval_steps_per_second': 2.627, 'epoch': 10.0}
{'train@eng.rst.rstdt_loss': 1.1130205392837524, 'train@eng.rst.rstdt_accuracy@eng.rst.rstdt': 0.6733533308336458, 'train@eng.rst.rstdt_f1@eng.rst.rstdt': 0.4154130325862657, 'train@eng.rst.rstdt_precision@eng.rst.rstdt': 0.6053315973841349, 'train@eng.rst.rstdt_recall@eng.rst.rstdt': 0.37803433946358345, 'train@eng.rst.rstdt_loss@eng.rst.rstdt': 1.113020420074463, 'train@eng.rst.rstdt_runtime': 188.0604, 'train@eng.rst.rstdt_samples_per_second': 85.09, 'train@eng.rst.rstdt_steps_per_second': 2.664, 'epoch': 11.0}
{'loss': 1.1706, 'learning_rate': 1.6666666666666667e-06, 'epoch': 11.0}
{'eval_loss': 1.2131006717681885, 'eval_accuracy@eng.rst.rstdt': 0.6465144972239358, 'eval_f1@eng.rst.rstdt': 0.36542443688451653, 'eval_precision@eng.rst.rstdt': 0.48064501352216904, 'eval_recall@eng.rst.rstdt': 0.3502472992791629, 'eval_loss@eng.rst.rstdt': 1.2131006717681885, 'eval_runtime': 19.4207, 'eval_samples_per_second': 83.468, 'eval_steps_per_second': 2.626, 'epoch': 11.0}
{'train@eng.rst.rstdt_loss': 1.1112966537475586, 'train@eng.rst.rstdt_accuracy@eng.rst.rstdt': 0.6739782527184102, 'train@eng.rst.rstdt_f1@eng.rst.rstdt': 0.4165972049929263, 'train@eng.rst.rstdt_precision@eng.rst.rstdt': 0.6143958960662995, 'train@eng.rst.rstdt_recall@eng.rst.rstdt': 0.378204853425388, 'train@eng.rst.rstdt_loss@eng.rst.rstdt': 1.1112966537475586, 'train@eng.rst.rstdt_runtime': 188.0437, 'train@eng.rst.rstdt_samples_per_second': 85.097, 'train@eng.rst.rstdt_steps_per_second': 2.664, 'epoch': 12.0}
{'loss': 1.1642, 'learning_rate': 0.0, 'epoch': 12.0}
{'eval_loss': 1.2107934951782227, 'eval_accuracy@eng.rst.rstdt': 0.6465144972239358, 'eval_f1@eng.rst.rstdt': 0.3665971046209602, 'eval_precision@eng.rst.rstdt': 0.48237182579939514, 'eval_recall@eng.rst.rstdt': 0.35040821082527374, 'eval_loss@eng.rst.rstdt': 1.2107934951782227, 'eval_runtime': 19.4316, 'eval_samples_per_second': 83.421, 'eval_steps_per_second': 2.625, 'epoch': 12.0}
{'train_runtime': 7254.1816, 'train_samples_per_second': 26.471, 'train_steps_per_second': 0.829, 'train_loss': 1.3555751351301304, 'epoch': 12.0}
***** train_log metrics *****
  epoch                    =       12.0
  train_loss               =     1.3556
  train_runtime            = 2:00:54.18
  train_samples_per_second =     26.471
  train_steps_per_second   =      0.829
{'train@eng.pdtb.pdtb_loss': 1.230914831161499, 'train@eng.pdtb.pdtb_accuracy@eng.pdtb.pdtb': 0.6050774134790529, 'train@eng.pdtb.pdtb_f1@eng.pdtb.pdtb': 0.2955229056136687, 'train@eng.pdtb.pdtb_precision@eng.pdtb.pdtb': 0.3583055043798784, 'train@eng.pdtb.pdtb_recall@eng.pdtb.pdtb': 0.2892102867529034, 'train@eng.pdtb.pdtb_loss@eng.pdtb.pdtb': 1.230914831161499, 'train@eng.pdtb.pdtb_runtime': 516.0937, 'train@eng.pdtb.pdtb_samples_per_second': 85.101, 'train@eng.pdtb.pdtb_steps_per_second': 2.66, 'epoch': 1.0}
{'loss': 1.6723, 'learning_rate': 1.8333333333333333e-05, 'epoch': 1.0}
{'eval_loss': 1.1652555465698242, 'eval_accuracy@eng.pdtb.pdtb': 0.6332138590203107, 'eval_f1@eng.pdtb.pdtb': 0.35677488429562193, 'eval_precision@eng.pdtb.pdtb': 0.42165839690236206, 'eval_recall@eng.pdtb.pdtb': 0.35466019861951714, 'eval_loss@eng.pdtb.pdtb': 1.1652556657791138, 'eval_runtime': 20.0427, 'eval_samples_per_second': 83.522, 'eval_steps_per_second': 2.644, 'epoch': 1.0}
{'train@eng.pdtb.pdtb_loss': 1.0869404077529907, 'train@eng.pdtb.pdtb_accuracy@eng.pdtb.pdtb': 0.6430327868852459, 'train@eng.pdtb.pdtb_f1@eng.pdtb.pdtb': 0.3724706994672551, 'train@eng.pdtb.pdtb_precision@eng.pdtb.pdtb': 0.49738528134749627, 'train@eng.pdtb.pdtb_recall@eng.pdtb.pdtb': 0.3541611491544834, 'train@eng.pdtb.pdtb_loss@eng.pdtb.pdtb': 1.0869404077529907, 'train@eng.pdtb.pdtb_runtime': 515.8845, 'train@eng.pdtb.pdtb_samples_per_second': 85.135, 'train@eng.pdtb.pdtb_steps_per_second': 2.661, 'epoch': 2.0}
{'loss': 1.2035, 'learning_rate': 1.6666666666666667e-05, 'epoch': 2.0}
{'eval_loss': 1.0349586009979248, 'eval_accuracy@eng.pdtb.pdtb': 0.6648745519713262, 'eval_f1@eng.pdtb.pdtb': 0.4126484141874812, 'eval_precision@eng.pdtb.pdtb': 0.47541170402532285, 'eval_recall@eng.pdtb.pdtb': 0.39938568534593444, 'eval_loss@eng.pdtb.pdtb': 1.0349586009979248, 'eval_runtime': 20.0677, 'eval_samples_per_second': 83.418, 'eval_steps_per_second': 2.641, 'epoch': 2.0}
{'train@eng.pdtb.pdtb_loss': 1.0450468063354492, 'train@eng.pdtb.pdtb_accuracy@eng.pdtb.pdtb': 0.6543260473588343, 'train@eng.pdtb.pdtb_f1@eng.pdtb.pdtb': 0.42385936761923404, 'train@eng.pdtb.pdtb_precision@eng.pdtb.pdtb': 0.4709070613493907, 'train@eng.pdtb.pdtb_recall@eng.pdtb.pdtb': 0.40441180046674435, 'train@eng.pdtb.pdtb_loss@eng.pdtb.pdtb': 1.0450470447540283, 'train@eng.pdtb.pdtb_runtime': 516.22, 'train@eng.pdtb.pdtb_samples_per_second': 85.08, 'train@eng.pdtb.pdtb_steps_per_second': 2.66, 'epoch': 3.0}
{'loss': 1.1183, 'learning_rate': 1.5000000000000002e-05, 'epoch': 3.0}
{'eval_loss': 0.9992356896400452, 'eval_accuracy@eng.pdtb.pdtb': 0.6750298685782556, 'eval_f1@eng.pdtb.pdtb': 0.48626821928685765, 'eval_precision@eng.pdtb.pdtb': 0.5656888880498967, 'eval_recall@eng.pdtb.pdtb': 0.45722209201793246, 'eval_loss@eng.pdtb.pdtb': 0.9992358088493347, 'eval_runtime': 20.0559, 'eval_samples_per_second': 83.467, 'eval_steps_per_second': 2.643, 'epoch': 3.0}
{'train@eng.pdtb.pdtb_loss': 0.9949480891227722, 'train@eng.pdtb.pdtb_accuracy@eng.pdtb.pdtb': 0.6690346083788706, 'train@eng.pdtb.pdtb_f1@eng.pdtb.pdtb': 0.4475834592486162, 'train@eng.pdtb.pdtb_precision@eng.pdtb.pdtb': 0.49231186100733815, 'train@eng.pdtb.pdtb_recall@eng.pdtb.pdtb': 0.43142004873667106, 'train@eng.pdtb.pdtb_loss@eng.pdtb.pdtb': 0.9949482083320618, 'train@eng.pdtb.pdtb_runtime': 516.2191, 'train@eng.pdtb.pdtb_samples_per_second': 85.08, 'train@eng.pdtb.pdtb_steps_per_second': 2.66, 'epoch': 4.0}
{'loss': 1.0739, 'learning_rate': 1.3333333333333333e-05, 'epoch': 4.0}
{'eval_loss': 0.9546834826469421, 'eval_accuracy@eng.pdtb.pdtb': 0.6887694145758662, 'eval_f1@eng.pdtb.pdtb': 0.49272672982731114, 'eval_precision@eng.pdtb.pdtb': 0.5552426085270372, 'eval_recall@eng.pdtb.pdtb': 0.4638111462253856, 'eval_loss@eng.pdtb.pdtb': 0.9546834230422974, 'eval_runtime': 20.0177, 'eval_samples_per_second': 83.626, 'eval_steps_per_second': 2.648, 'epoch': 4.0}
{'train@eng.pdtb.pdtb_loss': 0.9742013216018677, 'train@eng.pdtb.pdtb_accuracy@eng.pdtb.pdtb': 0.6751593806921676, 'train@eng.pdtb.pdtb_f1@eng.pdtb.pdtb': 0.4572843102599264, 'train@eng.pdtb.pdtb_precision@eng.pdtb.pdtb': 0.49814264733664476, 'train@eng.pdtb.pdtb_recall@eng.pdtb.pdtb': 0.44682742766025657, 'train@eng.pdtb.pdtb_loss@eng.pdtb.pdtb': 0.9742012023925781, 'train@eng.pdtb.pdtb_runtime': 515.9282, 'train@eng.pdtb.pdtb_samples_per_second': 85.128, 'train@eng.pdtb.pdtb_steps_per_second': 2.661, 'epoch': 5.0}
{'loss': 1.043, 'learning_rate': 1.1666666666666668e-05, 'epoch': 5.0}
{'eval_loss': 0.9397751688957214, 'eval_accuracy@eng.pdtb.pdtb': 0.6863799283154122, 'eval_f1@eng.pdtb.pdtb': 0.507728732352368, 'eval_precision@eng.pdtb.pdtb': 0.5654901466638537, 'eval_recall@eng.pdtb.pdtb': 0.48167103565023395, 'eval_loss@eng.pdtb.pdtb': 0.9397750496864319, 'eval_runtime': 20.006, 'eval_samples_per_second': 83.675, 'eval_steps_per_second': 2.649, 'epoch': 5.0}
{'train@eng.pdtb.pdtb_loss': 0.954528272151947, 'train@eng.pdtb.pdtb_accuracy@eng.pdtb.pdtb': 0.6800546448087431, 'train@eng.pdtb.pdtb_f1@eng.pdtb.pdtb': 0.46473555470264905, 'train@eng.pdtb.pdtb_precision@eng.pdtb.pdtb': 0.5082510617321225, 'train@eng.pdtb.pdtb_recall@eng.pdtb.pdtb': 0.4547455088085247, 'train@eng.pdtb.pdtb_loss@eng.pdtb.pdtb': 0.9545282125473022, 'train@eng.pdtb.pdtb_runtime': 516.1823, 'train@eng.pdtb.pdtb_samples_per_second': 85.086, 'train@eng.pdtb.pdtb_steps_per_second': 2.66, 'epoch': 6.0}
{'loss': 1.0219, 'learning_rate': 1e-05, 'epoch': 6.0}
{'eval_loss': 0.9288531541824341, 'eval_accuracy@eng.pdtb.pdtb': 0.6845878136200717, 'eval_f1@eng.pdtb.pdtb': 0.5254984170637073, 'eval_precision@eng.pdtb.pdtb': 0.6088257785581669, 'eval_recall@eng.pdtb.pdtb': 0.4979412597900694, 'eval_loss@eng.pdtb.pdtb': 0.9288529753684998, 'eval_runtime': 20.0751, 'eval_samples_per_second': 83.387, 'eval_steps_per_second': 2.64, 'epoch': 6.0}
{'train@eng.pdtb.pdtb_loss': 0.9447876811027527, 'train@eng.pdtb.pdtb_accuracy@eng.pdtb.pdtb': 0.6821265938069216, 'train@eng.pdtb.pdtb_f1@eng.pdtb.pdtb': 0.4696186319143452, 'train@eng.pdtb.pdtb_precision@eng.pdtb.pdtb': 0.5243235860702731, 'train@eng.pdtb.pdtb_recall@eng.pdtb.pdtb': 0.4550438034372573, 'train@eng.pdtb.pdtb_loss@eng.pdtb.pdtb': 0.9447876811027527, 'train@eng.pdtb.pdtb_runtime': 516.0332, 'train@eng.pdtb.pdtb_samples_per_second': 85.111, 'train@eng.pdtb.pdtb_steps_per_second': 2.661, 'epoch': 7.0}
{'loss': 1.0073, 'learning_rate': 8.333333333333334e-06, 'epoch': 7.0}
{'eval_loss': 0.915654718875885, 'eval_accuracy@eng.pdtb.pdtb': 0.6857825567502986, 'eval_f1@eng.pdtb.pdtb': 0.5207980361858912, 'eval_precision@eng.pdtb.pdtb': 0.6229120304247553, 'eval_recall@eng.pdtb.pdtb': 0.4828781489464101, 'eval_loss@eng.pdtb.pdtb': 0.915654718875885, 'eval_runtime': 20.0708, 'eval_samples_per_second': 83.405, 'eval_steps_per_second': 2.641, 'epoch': 7.0}
{'train@eng.pdtb.pdtb_loss': 0.9316971302032471, 'train@eng.pdtb.pdtb_accuracy@eng.pdtb.pdtb': 0.6841985428051002, 'train@eng.pdtb.pdtb_f1@eng.pdtb.pdtb': 0.4739282597228544, 'train@eng.pdtb.pdtb_precision@eng.pdtb.pdtb': 0.5154028676224744, 'train@eng.pdtb.pdtb_recall@eng.pdtb.pdtb': 0.46635398449420545, 'train@eng.pdtb.pdtb_loss@eng.pdtb.pdtb': 0.9316971302032471, 'train@eng.pdtb.pdtb_runtime': 515.9288, 'train@eng.pdtb.pdtb_samples_per_second': 85.128, 'train@eng.pdtb.pdtb_steps_per_second': 2.661, 'epoch': 8.0}
{'loss': 0.9931, 'learning_rate': 6.666666666666667e-06, 'epoch': 8.0}
{'eval_loss': 0.9121840596199036, 'eval_accuracy@eng.pdtb.pdtb': 0.6923536439665472, 'eval_f1@eng.pdtb.pdtb': 0.5345293402847164, 'eval_precision@eng.pdtb.pdtb': 0.5865282406492593, 'eval_recall@eng.pdtb.pdtb': 0.5095131739883791, 'eval_loss@eng.pdtb.pdtb': 0.9121841192245483, 'eval_runtime': 20.0614, 'eval_samples_per_second': 83.444, 'eval_steps_per_second': 2.642, 'epoch': 8.0}
{'train@eng.pdtb.pdtb_loss': 0.9234907031059265, 'train@eng.pdtb.pdtb_accuracy@eng.pdtb.pdtb': 0.6874089253187614, 'train@eng.pdtb.pdtb_f1@eng.pdtb.pdtb': 0.47673857281958515, 'train@eng.pdtb.pdtb_precision@eng.pdtb.pdtb': 0.5227625116521668, 'train@eng.pdtb.pdtb_recall@eng.pdtb.pdtb': 0.47000899960620934, 'train@eng.pdtb.pdtb_loss@eng.pdtb.pdtb': 0.9234907627105713, 'train@eng.pdtb.pdtb_runtime': 516.0177, 'train@eng.pdtb.pdtb_samples_per_second': 85.113, 'train@eng.pdtb.pdtb_steps_per_second': 2.661, 'epoch': 9.0}
{'loss': 0.9821, 'learning_rate': 5e-06, 'epoch': 9.0}
{'eval_loss': 0.9085552096366882, 'eval_accuracy@eng.pdtb.pdtb': 0.6887694145758662, 'eval_f1@eng.pdtb.pdtb': 0.5384835323738096, 'eval_precision@eng.pdtb.pdtb': 0.6161467224266598, 'eval_recall@eng.pdtb.pdtb': 0.5149242822199016, 'eval_loss@eng.pdtb.pdtb': 0.9085552096366882, 'eval_runtime': 20.0473, 'eval_samples_per_second': 83.503, 'eval_steps_per_second': 2.644, 'epoch': 9.0}
{'train@eng.pdtb.pdtb_loss': 0.9189726114273071, 'train@eng.pdtb.pdtb_accuracy@eng.pdtb.pdtb': 0.6895947176684881, 'train@eng.pdtb.pdtb_f1@eng.pdtb.pdtb': 0.47802774084374067, 'train@eng.pdtb.pdtb_precision@eng.pdtb.pdtb': 0.5287317609186666, 'train@eng.pdtb.pdtb_recall@eng.pdtb.pdtb': 0.46948353879812016, 'train@eng.pdtb.pdtb_loss@eng.pdtb.pdtb': 0.9189726114273071, 'train@eng.pdtb.pdtb_runtime': 515.9941, 'train@eng.pdtb.pdtb_samples_per_second': 85.117, 'train@eng.pdtb.pdtb_steps_per_second': 2.661, 'epoch': 10.0}
{'loss': 0.9768, 'learning_rate': 3.3333333333333333e-06, 'epoch': 10.0}
{'eval_loss': 0.9029635190963745, 'eval_accuracy@eng.pdtb.pdtb': 0.6935483870967742, 'eval_f1@eng.pdtb.pdtb': 0.5467358553299828, 'eval_precision@eng.pdtb.pdtb': 0.6266525837503448, 'eval_recall@eng.pdtb.pdtb': 0.5253139705051934, 'eval_loss@eng.pdtb.pdtb': 0.9029635190963745, 'eval_runtime': 20.0226, 'eval_samples_per_second': 83.606, 'eval_steps_per_second': 2.647, 'epoch': 10.0}
{'train@eng.pdtb.pdtb_loss': 0.9151055812835693, 'train@eng.pdtb.pdtb_accuracy@eng.pdtb.pdtb': 0.6892531876138434, 'train@eng.pdtb.pdtb_f1@eng.pdtb.pdtb': 0.48039070122921934, 'train@eng.pdtb.pdtb_precision@eng.pdtb.pdtb': 0.5238729965193001, 'train@eng.pdtb.pdtb_recall@eng.pdtb.pdtb': 0.47325518312322673, 'train@eng.pdtb.pdtb_loss@eng.pdtb.pdtb': 0.9151055812835693, 'train@eng.pdtb.pdtb_runtime': 516.1582, 'train@eng.pdtb.pdtb_samples_per_second': 85.09, 'train@eng.pdtb.pdtb_steps_per_second': 2.66, 'epoch': 11.0}
{'loss': 0.972, 'learning_rate': 1.6666666666666667e-06, 'epoch': 11.0}
{'eval_loss': 0.903159499168396, 'eval_accuracy@eng.pdtb.pdtb': 0.6923536439665472, 'eval_f1@eng.pdtb.pdtb': 0.548315359922085, 'eval_precision@eng.pdtb.pdtb': 0.597678611254145, 'eval_recall@eng.pdtb.pdtb': 0.5289581610450136, 'eval_loss@eng.pdtb.pdtb': 0.9031594395637512, 'eval_runtime': 20.0615, 'eval_samples_per_second': 83.443, 'eval_steps_per_second': 2.642, 'epoch': 11.0}
{'train@eng.pdtb.pdtb_loss': 0.9140701293945312, 'train@eng.pdtb.pdtb_accuracy@eng.pdtb.pdtb': 0.690528233151184, 'train@eng.pdtb.pdtb_f1@eng.pdtb.pdtb': 0.48130157342472496, 'train@eng.pdtb.pdtb_precision@eng.pdtb.pdtb': 0.5304625085623792, 'train@eng.pdtb.pdtb_recall@eng.pdtb.pdtb': 0.4722444726475468, 'train@eng.pdtb.pdtb_loss@eng.pdtb.pdtb': 0.9140701293945312, 'train@eng.pdtb.pdtb_runtime': 515.9103, 'train@eng.pdtb.pdtb_samples_per_second': 85.131, 'train@eng.pdtb.pdtb_steps_per_second': 2.661, 'epoch': 12.0}
{'loss': 0.9651, 'learning_rate': 0.0, 'epoch': 12.0}
{'eval_loss': 0.9012660384178162, 'eval_accuracy@eng.pdtb.pdtb': 0.6929510155316607, 'eval_f1@eng.pdtb.pdtb': 0.5478882641381153, 'eval_precision@eng.pdtb.pdtb': 0.6237737605096674, 'eval_recall@eng.pdtb.pdtb': 0.5265387483349134, 'eval_loss@eng.pdtb.pdtb': 0.9012659788131714, 'eval_runtime': 20.0168, 'eval_samples_per_second': 83.63, 'eval_steps_per_second': 2.648, 'epoch': 12.0}
{'train_runtime': 19525.5503, 'train_samples_per_second': 26.992, 'train_steps_per_second': 0.844, 'train_loss': 1.0857734504212566, 'epoch': 12.0}
***** train_logger metrics *****
  epoch                    =       12.0
  train_loss               =     1.3556
  train_runtime            = 2:00:54.18
  train_samples_per_second =     26.471
  train_steps_per_second   =      0.829
-------------------------------------------------------------------
Lang1:  eng.sdrt.stac    Lang2:  eng.pdtb.pdtb
Saving run to:  runs/full_shot/FullShot=v4_eng.sdrt.stac_eng.pdtb.pdtb_lrfunc=Adafactor_lr=2e-05_plm=bert-base-multilingual-cased
Running with params: BERT_MODEL=bert-base-multilingual-cased lr=2e-05
read 9580 examples
read 1145 examples
read 1510 examples
read 43920 examples
read 1674 examples
read 2257 examples
Total prediction labels:  39
BertAdapterModel(
  (shared_parameters): ModuleDict()
  (bert): BertModel(
    (shared_parameters): ModuleDict()
    (invertible_adapters): ModuleDict(
      (en): NICECouplingBlock(
        (F): Sequential(
          (0): Linear(in_features=384, out_features=192, bias=True)
          (1): Activation_Function_Class(
            (f): ReLU()
          )
          (2): Linear(in_features=192, out_features=384, bias=True)
        )
        (G): Sequential(
          (0): Linear(in_features=384, out_features=192, bias=True)
          (1): Activation_Function_Class(
            (f): ReLU()
          )
          (2): Linear(in_features=192, out_features=384, bias=True)
        )
      )
    )
    (embeddings): BertEmbeddings(
      (word_embeddings): Embedding(119547, 768, padding_idx=0)
      (position_embeddings): Embedding(512, 768)
      (token_type_embeddings): Embedding(2, 768)
      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
      (dropout): Dropout(p=0.1, inplace=False)
    )
    (encoder): BertEncoder(
      (layer): ModuleList(
        (0): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (en): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (1): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (en): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (2): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (en): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (3): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (en): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (4): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (en): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (5): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (en): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (6): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (en): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (7): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (en): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (8): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (en): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (9): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (en): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (10): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (en): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (11): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (en): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
      )
    )
    (pooler): BertPooler(
      (dense): Linear(in_features=768, out_features=768, bias=True)
      (activation): Tanh()
    )
    (prefix_tuning): PrefixTuningPool(
      (prefix_tunings): ModuleDict()
    )
  )
  (heads): ModuleDict(
    (disrpt): ClassificationHead(
      (0): Dropout(p=0.1, inplace=False)
      (1): Linear(in_features=768, out_features=768, bias=True)
      (2): Activation_Function_Class(
        (f): Tanh()
      )
      (3): Dropout(p=0.1, inplace=False)
      (4): Linear(in_features=768, out_features=39, bias=True)
    )
  )
)
{'train@eng.sdrt.stac_loss': 2.1172330379486084, 'train@eng.sdrt.stac_accuracy@eng.sdrt.stac': 0.35198329853862215, 'train@eng.sdrt.stac_f1@eng.sdrt.stac': 0.06593133382795793, 'train@eng.sdrt.stac_precision@eng.sdrt.stac': 0.07193567032251988, 'train@eng.sdrt.stac_recall@eng.sdrt.stac': 0.10955019627041471, 'train@eng.sdrt.stac_loss@eng.sdrt.stac': 2.1172327995300293, 'train@eng.sdrt.stac_runtime': 112.411, 'train@eng.sdrt.stac_samples_per_second': 85.223, 'train@eng.sdrt.stac_steps_per_second': 2.669, 'epoch': 1.0}
{'loss': 2.5639, 'learning_rate': 1.8333333333333333e-05, 'epoch': 1.0}
{'eval_loss': 2.0817813873291016, 'eval_accuracy@eng.sdrt.stac': 0.3537117903930131, 'eval_f1@eng.sdrt.stac': 0.0652824388778694, 'eval_precision@eng.sdrt.stac': 0.06482197758663558, 'eval_recall@eng.sdrt.stac': 0.10909119358805364, 'eval_loss@eng.sdrt.stac': 2.0817813873291016, 'eval_runtime': 13.7781, 'eval_samples_per_second': 83.103, 'eval_steps_per_second': 2.613, 'epoch': 1.0}
{'train@eng.sdrt.stac_loss': 1.8922548294067383, 'train@eng.sdrt.stac_accuracy@eng.sdrt.stac': 0.4083507306889353, 'train@eng.sdrt.stac_f1@eng.sdrt.stac': 0.11868314050331508, 'train@eng.sdrt.stac_precision@eng.sdrt.stac': 0.12735977893054443, 'train@eng.sdrt.stac_recall@eng.sdrt.stac': 0.14956182262774148, 'train@eng.sdrt.stac_loss@eng.sdrt.stac': 1.8922545909881592, 'train@eng.sdrt.stac_runtime': 112.3672, 'train@eng.sdrt.stac_samples_per_second': 85.256, 'train@eng.sdrt.stac_steps_per_second': 2.67, 'epoch': 2.0}
{'loss': 2.04, 'learning_rate': 1.6666666666666667e-05, 'epoch': 2.0}
{'eval_loss': 1.8528902530670166, 'eval_accuracy@eng.sdrt.stac': 0.4218340611353712, 'eval_f1@eng.sdrt.stac': 0.12001509182077959, 'eval_precision@eng.sdrt.stac': 0.12843169519868258, 'eval_recall@eng.sdrt.stac': 0.15345671531747407, 'eval_loss@eng.sdrt.stac': 1.8528902530670166, 'eval_runtime': 13.788, 'eval_samples_per_second': 83.043, 'eval_steps_per_second': 2.611, 'epoch': 2.0}
{'train@eng.sdrt.stac_loss': 1.7392653226852417, 'train@eng.sdrt.stac_accuracy@eng.sdrt.stac': 0.4559498956158664, 'train@eng.sdrt.stac_f1@eng.sdrt.stac': 0.15735331523200044, 'train@eng.sdrt.stac_precision@eng.sdrt.stac': 0.17234016362494953, 'train@eng.sdrt.stac_recall@eng.sdrt.stac': 0.19706553499538249, 'train@eng.sdrt.stac_loss@eng.sdrt.stac': 1.7392652034759521, 'train@eng.sdrt.stac_runtime': 112.4241, 'train@eng.sdrt.stac_samples_per_second': 85.213, 'train@eng.sdrt.stac_steps_per_second': 2.668, 'epoch': 3.0}
{'loss': 1.8478, 'learning_rate': 1.5000000000000002e-05, 'epoch': 3.0}
{'eval_loss': 1.6967647075653076, 'eval_accuracy@eng.sdrt.stac': 0.4663755458515284, 'eval_f1@eng.sdrt.stac': 0.15095341052233213, 'eval_precision@eng.sdrt.stac': 0.13625974847217084, 'eval_recall@eng.sdrt.stac': 0.19426030127683186, 'eval_loss@eng.sdrt.stac': 1.6967647075653076, 'eval_runtime': 13.7921, 'eval_samples_per_second': 83.019, 'eval_steps_per_second': 2.61, 'epoch': 3.0}
{'train@eng.sdrt.stac_loss': 1.6585074663162231, 'train@eng.sdrt.stac_accuracy@eng.sdrt.stac': 0.47473903966597075, 'train@eng.sdrt.stac_f1@eng.sdrt.stac': 0.1906240555864051, 'train@eng.sdrt.stac_precision@eng.sdrt.stac': 0.22649583918094734, 'train@eng.sdrt.stac_recall@eng.sdrt.stac': 0.21721849556678735, 'train@eng.sdrt.stac_loss@eng.sdrt.stac': 1.6585074663162231, 'train@eng.sdrt.stac_runtime': 112.3678, 'train@eng.sdrt.stac_samples_per_second': 85.256, 'train@eng.sdrt.stac_steps_per_second': 2.67, 'epoch': 4.0}
{'loss': 1.7382, 'learning_rate': 1.3333333333333333e-05, 'epoch': 4.0}
{'eval_loss': 1.6143949031829834, 'eval_accuracy@eng.sdrt.stac': 0.4724890829694323, 'eval_f1@eng.sdrt.stac': 0.16926874219119953, 'eval_precision@eng.sdrt.stac': 0.22297892906495662, 'eval_recall@eng.sdrt.stac': 0.20289556987385748, 'eval_loss@eng.sdrt.stac': 1.6143949031829834, 'eval_runtime': 13.7803, 'eval_samples_per_second': 83.09, 'eval_steps_per_second': 2.612, 'epoch': 4.0}
{'train@eng.sdrt.stac_loss': 1.6077054738998413, 'train@eng.sdrt.stac_accuracy@eng.sdrt.stac': 0.49269311064718163, 'train@eng.sdrt.stac_f1@eng.sdrt.stac': 0.2198789000519738, 'train@eng.sdrt.stac_precision@eng.sdrt.stac': 0.23709945867926857, 'train@eng.sdrt.stac_recall@eng.sdrt.stac': 0.24208073152323284, 'train@eng.sdrt.stac_loss@eng.sdrt.stac': 1.6077054738998413, 'train@eng.sdrt.stac_runtime': 112.506, 'train@eng.sdrt.stac_samples_per_second': 85.151, 'train@eng.sdrt.stac_steps_per_second': 2.667, 'epoch': 5.0}
{'loss': 1.6757, 'learning_rate': 1.1666666666666668e-05, 'epoch': 5.0}
{'eval_loss': 1.5716742277145386, 'eval_accuracy@eng.sdrt.stac': 0.4943231441048035, 'eval_f1@eng.sdrt.stac': 0.20352981523088653, 'eval_precision@eng.sdrt.stac': 0.21545697979966744, 'eval_recall@eng.sdrt.stac': 0.22657614594137712, 'eval_loss@eng.sdrt.stac': 1.5716742277145386, 'eval_runtime': 13.7723, 'eval_samples_per_second': 83.138, 'eval_steps_per_second': 2.614, 'epoch': 5.0}
{'train@eng.sdrt.stac_loss': 1.5641446113586426, 'train@eng.sdrt.stac_accuracy@eng.sdrt.stac': 0.4968684759916493, 'train@eng.sdrt.stac_f1@eng.sdrt.stac': 0.2346720196529653, 'train@eng.sdrt.stac_precision@eng.sdrt.stac': 0.2855529423666773, 'train@eng.sdrt.stac_recall@eng.sdrt.stac': 0.2496239896302947, 'train@eng.sdrt.stac_loss@eng.sdrt.stac': 1.5641446113586426, 'train@eng.sdrt.stac_runtime': 112.4534, 'train@eng.sdrt.stac_samples_per_second': 85.191, 'train@eng.sdrt.stac_steps_per_second': 2.668, 'epoch': 6.0}
{'loss': 1.6263, 'learning_rate': 1e-05, 'epoch': 6.0}
{'eval_loss': 1.5515034198760986, 'eval_accuracy@eng.sdrt.stac': 0.4925764192139738, 'eval_f1@eng.sdrt.stac': 0.211764776438648, 'eval_precision@eng.sdrt.stac': 0.25094293431799913, 'eval_recall@eng.sdrt.stac': 0.23376313960508727, 'eval_loss@eng.sdrt.stac': 1.5515035390853882, 'eval_runtime': 13.7827, 'eval_samples_per_second': 83.075, 'eval_steps_per_second': 2.612, 'epoch': 6.0}
{'train@eng.sdrt.stac_loss': 1.5439255237579346, 'train@eng.sdrt.stac_accuracy@eng.sdrt.stac': 0.5105427974947808, 'train@eng.sdrt.stac_f1@eng.sdrt.stac': 0.27870882719214307, 'train@eng.sdrt.stac_precision@eng.sdrt.stac': 0.40409855856428967, 'train@eng.sdrt.stac_recall@eng.sdrt.stac': 0.2931064013312463, 'train@eng.sdrt.stac_loss@eng.sdrt.stac': 1.5439256429672241, 'train@eng.sdrt.stac_runtime': 112.4941, 'train@eng.sdrt.stac_samples_per_second': 85.16, 'train@eng.sdrt.stac_steps_per_second': 2.667, 'epoch': 7.0}
{'loss': 1.5908, 'learning_rate': 8.333333333333334e-06, 'epoch': 7.0}
{'eval_loss': 1.5377533435821533, 'eval_accuracy@eng.sdrt.stac': 0.5074235807860262, 'eval_f1@eng.sdrt.stac': 0.24400851150554587, 'eval_precision@eng.sdrt.stac': 0.25575913412474144, 'eval_recall@eng.sdrt.stac': 0.2591950302676609, 'eval_loss@eng.sdrt.stac': 1.5377533435821533, 'eval_runtime': 13.821, 'eval_samples_per_second': 82.845, 'eval_steps_per_second': 2.605, 'epoch': 7.0}
{'train@eng.sdrt.stac_loss': 1.5042356252670288, 'train@eng.sdrt.stac_accuracy@eng.sdrt.stac': 0.5242171189979123, 'train@eng.sdrt.stac_f1@eng.sdrt.stac': 0.29941758514608746, 'train@eng.sdrt.stac_precision@eng.sdrt.stac': 0.37644754694659166, 'train@eng.sdrt.stac_recall@eng.sdrt.stac': 0.3135465633802207, 'train@eng.sdrt.stac_loss@eng.sdrt.stac': 1.5042357444763184, 'train@eng.sdrt.stac_runtime': 112.3915, 'train@eng.sdrt.stac_samples_per_second': 85.238, 'train@eng.sdrt.stac_steps_per_second': 2.669, 'epoch': 8.0}
{'loss': 1.5628, 'learning_rate': 6.666666666666667e-06, 'epoch': 8.0}
{'eval_loss': 1.4989572763442993, 'eval_accuracy@eng.sdrt.stac': 0.5213973799126638, 'eval_f1@eng.sdrt.stac': 0.2602551814039029, 'eval_precision@eng.sdrt.stac': 0.29734466845401103, 'eval_recall@eng.sdrt.stac': 0.2761428911648391, 'eval_loss@eng.sdrt.stac': 1.4989572763442993, 'eval_runtime': 13.7804, 'eval_samples_per_second': 83.089, 'eval_steps_per_second': 2.612, 'epoch': 8.0}
{'train@eng.sdrt.stac_loss': 1.4902143478393555, 'train@eng.sdrt.stac_accuracy@eng.sdrt.stac': 0.5293319415448852, 'train@eng.sdrt.stac_f1@eng.sdrt.stac': 0.315170111619818, 'train@eng.sdrt.stac_precision@eng.sdrt.stac': 0.369370033530096, 'train@eng.sdrt.stac_recall@eng.sdrt.stac': 0.3276159796077729, 'train@eng.sdrt.stac_loss@eng.sdrt.stac': 1.490214228630066, 'train@eng.sdrt.stac_runtime': 112.2265, 'train@eng.sdrt.stac_samples_per_second': 85.363, 'train@eng.sdrt.stac_steps_per_second': 2.673, 'epoch': 9.0}
{'loss': 1.5404, 'learning_rate': 5e-06, 'epoch': 9.0}
{'eval_loss': 1.4913480281829834, 'eval_accuracy@eng.sdrt.stac': 0.5275109170305677, 'eval_f1@eng.sdrt.stac': 0.2882514987946916, 'eval_precision@eng.sdrt.stac': 0.36698779072333193, 'eval_recall@eng.sdrt.stac': 0.3029047035568996, 'eval_loss@eng.sdrt.stac': 1.491348147392273, 'eval_runtime': 13.7929, 'eval_samples_per_second': 83.014, 'eval_steps_per_second': 2.61, 'epoch': 9.0}
{'train@eng.sdrt.stac_loss': 1.4727553129196167, 'train@eng.sdrt.stac_accuracy@eng.sdrt.stac': 0.5327766179540709, 'train@eng.sdrt.stac_f1@eng.sdrt.stac': 0.3253053980917904, 'train@eng.sdrt.stac_precision@eng.sdrt.stac': 0.4255380108604993, 'train@eng.sdrt.stac_recall@eng.sdrt.stac': 0.33614138428387086, 'train@eng.sdrt.stac_loss@eng.sdrt.stac': 1.4727551937103271, 'train@eng.sdrt.stac_runtime': 112.4501, 'train@eng.sdrt.stac_samples_per_second': 85.193, 'train@eng.sdrt.stac_steps_per_second': 2.668, 'epoch': 10.0}
{'loss': 1.5297, 'learning_rate': 3.3333333333333333e-06, 'epoch': 10.0}
{'eval_loss': 1.4778386354446411, 'eval_accuracy@eng.sdrt.stac': 0.5301310043668123, 'eval_f1@eng.sdrt.stac': 0.29099488708987753, 'eval_precision@eng.sdrt.stac': 0.36683977241403665, 'eval_recall@eng.sdrt.stac': 0.3072657572143854, 'eval_loss@eng.sdrt.stac': 1.4778386354446411, 'eval_runtime': 17.0178, 'eval_samples_per_second': 67.282, 'eval_steps_per_second': 2.115, 'epoch': 10.0}
{'train@eng.sdrt.stac_loss': 1.46499502658844, 'train@eng.sdrt.stac_accuracy@eng.sdrt.stac': 0.5355949895615867, 'train@eng.sdrt.stac_f1@eng.sdrt.stac': 0.3322858886534901, 'train@eng.sdrt.stac_precision@eng.sdrt.stac': 0.4223253550834578, 'train@eng.sdrt.stac_recall@eng.sdrt.stac': 0.34222313963857676, 'train@eng.sdrt.stac_loss@eng.sdrt.stac': 1.4649947881698608, 'train@eng.sdrt.stac_runtime': 112.4378, 'train@eng.sdrt.stac_samples_per_second': 85.203, 'train@eng.sdrt.stac_steps_per_second': 2.668, 'epoch': 11.0}
{'loss': 1.5168, 'learning_rate': 1.6666666666666667e-06, 'epoch': 11.0}
{'eval_loss': 1.4715420007705688, 'eval_accuracy@eng.sdrt.stac': 0.5344978165938865, 'eval_f1@eng.sdrt.stac': 0.29644318321803864, 'eval_precision@eng.sdrt.stac': 0.3705621430768685, 'eval_recall@eng.sdrt.stac': 0.3116877575574051, 'eval_loss@eng.sdrt.stac': 1.4715418815612793, 'eval_runtime': 13.8007, 'eval_samples_per_second': 82.967, 'eval_steps_per_second': 2.609, 'epoch': 11.0}
{'train@eng.sdrt.stac_loss': 1.4638360738754272, 'train@eng.sdrt.stac_accuracy@eng.sdrt.stac': 0.5358037578288101, 'train@eng.sdrt.stac_f1@eng.sdrt.stac': 0.331211838961024, 'train@eng.sdrt.stac_precision@eng.sdrt.stac': 0.4206019357428126, 'train@eng.sdrt.stac_recall@eng.sdrt.stac': 0.3416259322439606, 'train@eng.sdrt.stac_loss@eng.sdrt.stac': 1.4638359546661377, 'train@eng.sdrt.stac_runtime': 112.4424, 'train@eng.sdrt.stac_samples_per_second': 85.199, 'train@eng.sdrt.stac_steps_per_second': 2.668, 'epoch': 12.0}
{'loss': 1.5091, 'learning_rate': 0.0, 'epoch': 12.0}
{'eval_loss': 1.4716110229492188, 'eval_accuracy@eng.sdrt.stac': 0.5301310043668123, 'eval_f1@eng.sdrt.stac': 0.29310187553635303, 'eval_precision@eng.sdrt.stac': 0.36817697825494916, 'eval_recall@eng.sdrt.stac': 0.3091406333984853, 'eval_loss@eng.sdrt.stac': 1.4716111421585083, 'eval_runtime': 13.793, 'eval_samples_per_second': 83.013, 'eval_steps_per_second': 2.61, 'epoch': 12.0}
{'train_runtime': 4365.851, 'train_samples_per_second': 26.332, 'train_steps_per_second': 0.825, 'train_loss': 1.7284528435601128, 'epoch': 12.0}
***** train_log metrics *****
  epoch                    =       12.0
  train_loss               =     1.7285
  train_runtime            = 1:12:45.85
  train_samples_per_second =     26.332
  train_steps_per_second   =      0.825
{'train@eng.pdtb.pdtb_loss': 1.2713602781295776, 'train@eng.pdtb.pdtb_accuracy@eng.pdtb.pdtb': 0.593351548269581, 'train@eng.pdtb.pdtb_f1@eng.pdtb.pdtb': 0.32131210500768764, 'train@eng.pdtb.pdtb_precision@eng.pdtb.pdtb': 0.40672577156905887, 'train@eng.pdtb.pdtb_recall@eng.pdtb.pdtb': 0.32318832669172404, 'train@eng.pdtb.pdtb_loss@eng.pdtb.pdtb': 1.2713603973388672, 'train@eng.pdtb.pdtb_runtime': 515.607, 'train@eng.pdtb.pdtb_samples_per_second': 85.181, 'train@eng.pdtb.pdtb_steps_per_second': 2.663, 'epoch': 1.0}
{'loss': 1.7131, 'learning_rate': 1.8333333333333333e-05, 'epoch': 1.0}
{'eval_loss': 1.2040765285491943, 'eval_accuracy@eng.pdtb.pdtb': 0.6206690561529271, 'eval_f1@eng.pdtb.pdtb': 0.37283028403164115, 'eval_precision@eng.pdtb.pdtb': 0.43794673728385136, 'eval_recall@eng.pdtb.pdtb': 0.3751635452166761, 'eval_loss@eng.pdtb.pdtb': 1.2040765285491943, 'eval_runtime': 20.0085, 'eval_samples_per_second': 83.664, 'eval_steps_per_second': 2.649, 'epoch': 1.0}
{'train@eng.pdtb.pdtb_loss': 1.1123398542404175, 'train@eng.pdtb.pdtb_accuracy@eng.pdtb.pdtb': 0.6360883424408015, 'train@eng.pdtb.pdtb_f1@eng.pdtb.pdtb': 0.40101529443585265, 'train@eng.pdtb.pdtb_precision@eng.pdtb.pdtb': 0.47670165733981934, 'train@eng.pdtb.pdtb_recall@eng.pdtb.pdtb': 0.3997340740966031, 'train@eng.pdtb.pdtb_loss@eng.pdtb.pdtb': 1.1123398542404175, 'train@eng.pdtb.pdtb_runtime': 515.311, 'train@eng.pdtb.pdtb_samples_per_second': 85.23, 'train@eng.pdtb.pdtb_steps_per_second': 2.664, 'epoch': 2.0}
{'loss': 1.2312, 'learning_rate': 1.6666666666666667e-05, 'epoch': 2.0}
{'eval_loss': 1.0540146827697754, 'eval_accuracy@eng.pdtb.pdtb': 0.6618876941457587, 'eval_f1@eng.pdtb.pdtb': 0.45398122808201063, 'eval_precision@eng.pdtb.pdtb': 0.5175955090147768, 'eval_recall@eng.pdtb.pdtb': 0.445833129476493, 'eval_loss@eng.pdtb.pdtb': 1.0540145635604858, 'eval_runtime': 19.9789, 'eval_samples_per_second': 83.788, 'eval_steps_per_second': 2.653, 'epoch': 2.0}
{'train@eng.pdtb.pdtb_loss': 1.0666429996490479, 'train@eng.pdtb.pdtb_accuracy@eng.pdtb.pdtb': 0.647495446265938, 'train@eng.pdtb.pdtb_f1@eng.pdtb.pdtb': 0.43004629779805054, 'train@eng.pdtb.pdtb_precision@eng.pdtb.pdtb': 0.49106385150885784, 'train@eng.pdtb.pdtb_recall@eng.pdtb.pdtb': 0.42464295091966026, 'train@eng.pdtb.pdtb_loss@eng.pdtb.pdtb': 1.0666429996490479, 'train@eng.pdtb.pdtb_runtime': 515.9649, 'train@eng.pdtb.pdtb_samples_per_second': 85.122, 'train@eng.pdtb.pdtb_steps_per_second': 2.661, 'epoch': 3.0}
{'loss': 1.1379, 'learning_rate': 1.5000000000000002e-05, 'epoch': 3.0}
{'eval_loss': 1.0171207189559937, 'eval_accuracy@eng.pdtb.pdtb': 0.6666666666666666, 'eval_f1@eng.pdtb.pdtb': 0.49631755247665527, 'eval_precision@eng.pdtb.pdtb': 0.552838476936002, 'eval_recall@eng.pdtb.pdtb': 0.4845109708050762, 'eval_loss@eng.pdtb.pdtb': 1.0171207189559937, 'eval_runtime': 20.0001, 'eval_samples_per_second': 83.7, 'eval_steps_per_second': 2.65, 'epoch': 3.0}
{'train@eng.pdtb.pdtb_loss': 1.0127830505371094, 'train@eng.pdtb.pdtb_accuracy@eng.pdtb.pdtb': 0.6618169398907103, 'train@eng.pdtb.pdtb_f1@eng.pdtb.pdtb': 0.446920199716097, 'train@eng.pdtb.pdtb_precision@eng.pdtb.pdtb': 0.5034908610926258, 'train@eng.pdtb.pdtb_recall@eng.pdtb.pdtb': 0.4431488774652424, 'train@eng.pdtb.pdtb_loss@eng.pdtb.pdtb': 1.0127830505371094, 'train@eng.pdtb.pdtb_runtime': 515.0146, 'train@eng.pdtb.pdtb_samples_per_second': 85.279, 'train@eng.pdtb.pdtb_steps_per_second': 2.666, 'epoch': 4.0}
{'loss': 1.0912, 'learning_rate': 1.3333333333333333e-05, 'epoch': 4.0}
{'eval_loss': 0.9721961617469788, 'eval_accuracy@eng.pdtb.pdtb': 0.6804062126642771, 'eval_f1@eng.pdtb.pdtb': 0.5297044778575214, 'eval_precision@eng.pdtb.pdtb': 0.625899123002548, 'eval_recall@eng.pdtb.pdtb': 0.5030794768464749, 'eval_loss@eng.pdtb.pdtb': 0.9721961617469788, 'eval_runtime': 20.0061, 'eval_samples_per_second': 83.674, 'eval_steps_per_second': 2.649, 'epoch': 4.0}
{'train@eng.pdtb.pdtb_loss': 0.9916385412216187, 'train@eng.pdtb.pdtb_accuracy@eng.pdtb.pdtb': 0.668943533697632, 'train@eng.pdtb.pdtb_f1@eng.pdtb.pdtb': 0.45400408315366414, 'train@eng.pdtb.pdtb_precision@eng.pdtb.pdtb': 0.5100332683015172, 'train@eng.pdtb.pdtb_recall@eng.pdtb.pdtb': 0.45218692189864995, 'train@eng.pdtb.pdtb_loss@eng.pdtb.pdtb': 0.9916384816169739, 'train@eng.pdtb.pdtb_runtime': 515.2396, 'train@eng.pdtb.pdtb_samples_per_second': 85.242, 'train@eng.pdtb.pdtb_steps_per_second': 2.665, 'epoch': 5.0}
{'loss': 1.0594, 'learning_rate': 1.1666666666666668e-05, 'epoch': 5.0}
{'eval_loss': 0.956666111946106, 'eval_accuracy@eng.pdtb.pdtb': 0.6845878136200717, 'eval_f1@eng.pdtb.pdtb': 0.5383343120624317, 'eval_precision@eng.pdtb.pdtb': 0.6281627219041162, 'eval_recall@eng.pdtb.pdtb': 0.5153451309761892, 'eval_loss@eng.pdtb.pdtb': 0.956666111946106, 'eval_runtime': 19.9954, 'eval_samples_per_second': 83.719, 'eval_steps_per_second': 2.651, 'epoch': 5.0}
{'train@eng.pdtb.pdtb_loss': 0.9698796272277832, 'train@eng.pdtb.pdtb_accuracy@eng.pdtb.pdtb': 0.6752276867030965, 'train@eng.pdtb.pdtb_f1@eng.pdtb.pdtb': 0.46269797113289474, 'train@eng.pdtb.pdtb_precision@eng.pdtb.pdtb': 0.5114768590694797, 'train@eng.pdtb.pdtb_recall@eng.pdtb.pdtb': 0.4607025614987225, 'train@eng.pdtb.pdtb_loss@eng.pdtb.pdtb': 0.9698796272277832, 'train@eng.pdtb.pdtb_runtime': 515.6063, 'train@eng.pdtb.pdtb_samples_per_second': 85.181, 'train@eng.pdtb.pdtb_steps_per_second': 2.663, 'epoch': 6.0}
{'loss': 1.0364, 'learning_rate': 1e-05, 'epoch': 6.0}
{'eval_loss': 0.9448647499084473, 'eval_accuracy@eng.pdtb.pdtb': 0.6839904420549582, 'eval_f1@eng.pdtb.pdtb': 0.5346009438673055, 'eval_precision@eng.pdtb.pdtb': 0.6177260910574025, 'eval_recall@eng.pdtb.pdtb': 0.5099716722090926, 'eval_loss@eng.pdtb.pdtb': 0.9448646903038025, 'eval_runtime': 20.0154, 'eval_samples_per_second': 83.635, 'eval_steps_per_second': 2.648, 'epoch': 6.0}
{'train@eng.pdtb.pdtb_loss': 0.9580150246620178, 'train@eng.pdtb.pdtb_accuracy@eng.pdtb.pdtb': 0.6765255009107468, 'train@eng.pdtb.pdtb_f1@eng.pdtb.pdtb': 0.4649050202301641, 'train@eng.pdtb.pdtb_precision@eng.pdtb.pdtb': 0.5273177007154238, 'train@eng.pdtb.pdtb_recall@eng.pdtb.pdtb': 0.45704131549953886, 'train@eng.pdtb.pdtb_loss@eng.pdtb.pdtb': 0.9580150246620178, 'train@eng.pdtb.pdtb_runtime': 515.1874, 'train@eng.pdtb.pdtb_samples_per_second': 85.251, 'train@eng.pdtb.pdtb_steps_per_second': 2.665, 'epoch': 7.0}
{'loss': 1.017, 'learning_rate': 8.333333333333334e-06, 'epoch': 7.0}
{'eval_loss': 0.9334335327148438, 'eval_accuracy@eng.pdtb.pdtb': 0.6839904420549582, 'eval_f1@eng.pdtb.pdtb': 0.5369541067794603, 'eval_precision@eng.pdtb.pdtb': 0.6329837603164148, 'eval_recall@eng.pdtb.pdtb': 0.5065572917004937, 'eval_loss@eng.pdtb.pdtb': 0.9334335327148438, 'eval_runtime': 20.0295, 'eval_samples_per_second': 83.577, 'eval_steps_per_second': 2.646, 'epoch': 7.0}
{'train@eng.pdtb.pdtb_loss': 0.9461681842803955, 'train@eng.pdtb.pdtb_accuracy@eng.pdtb.pdtb': 0.6800318761384335, 'train@eng.pdtb.pdtb_f1@eng.pdtb.pdtb': 0.4690704106923233, 'train@eng.pdtb.pdtb_precision@eng.pdtb.pdtb': 0.5160580403345418, 'train@eng.pdtb.pdtb_recall@eng.pdtb.pdtb': 0.4673422450636022, 'train@eng.pdtb.pdtb_loss@eng.pdtb.pdtb': 0.9461681246757507, 'train@eng.pdtb.pdtb_runtime': 515.2542, 'train@eng.pdtb.pdtb_samples_per_second': 85.239, 'train@eng.pdtb.pdtb_steps_per_second': 2.665, 'epoch': 8.0}
{'loss': 1.0061, 'learning_rate': 6.666666666666667e-06, 'epoch': 8.0}
{'eval_loss': 0.9276590943336487, 'eval_accuracy@eng.pdtb.pdtb': 0.6935483870967742, 'eval_f1@eng.pdtb.pdtb': 0.5404755998336785, 'eval_precision@eng.pdtb.pdtb': 0.5995831691641674, 'eval_recall@eng.pdtb.pdtb': 0.5170979283798324, 'eval_loss@eng.pdtb.pdtb': 0.9276590347290039, 'eval_runtime': 20.0212, 'eval_samples_per_second': 83.611, 'eval_steps_per_second': 2.647, 'epoch': 8.0}
{'train@eng.pdtb.pdtb_loss': 0.936037540435791, 'train@eng.pdtb.pdtb_accuracy@eng.pdtb.pdtb': 0.6846994535519125, 'train@eng.pdtb.pdtb_f1@eng.pdtb.pdtb': 0.4742865411854563, 'train@eng.pdtb.pdtb_precision@eng.pdtb.pdtb': 0.5258903454504295, 'train@eng.pdtb.pdtb_recall@eng.pdtb.pdtb': 0.469791438316383, 'train@eng.pdtb.pdtb_loss@eng.pdtb.pdtb': 0.9360373616218567, 'train@eng.pdtb.pdtb_runtime': 515.6992, 'train@eng.pdtb.pdtb_samples_per_second': 85.166, 'train@eng.pdtb.pdtb_steps_per_second': 2.662, 'epoch': 9.0}
{'loss': 0.9936, 'learning_rate': 5e-06, 'epoch': 9.0}
{'eval_loss': 0.9217604994773865, 'eval_accuracy@eng.pdtb.pdtb': 0.6905615292712067, 'eval_f1@eng.pdtb.pdtb': 0.5386566243668189, 'eval_precision@eng.pdtb.pdtb': 0.6284519649859605, 'eval_recall@eng.pdtb.pdtb': 0.5117652768316403, 'eval_loss@eng.pdtb.pdtb': 0.9217605590820312, 'eval_runtime': 20.0129, 'eval_samples_per_second': 83.646, 'eval_steps_per_second': 2.648, 'epoch': 9.0}
{'train@eng.pdtb.pdtb_loss': 0.9321332573890686, 'train@eng.pdtb.pdtb_accuracy@eng.pdtb.pdtb': 0.6855646630236795, 'train@eng.pdtb.pdtb_f1@eng.pdtb.pdtb': 0.4741402271441347, 'train@eng.pdtb.pdtb_precision@eng.pdtb.pdtb': 0.5285004106282581, 'train@eng.pdtb.pdtb_recall@eng.pdtb.pdtb': 0.4719812771244936, 'train@eng.pdtb.pdtb_loss@eng.pdtb.pdtb': 0.9321333765983582, 'train@eng.pdtb.pdtb_runtime': 515.7796, 'train@eng.pdtb.pdtb_samples_per_second': 85.153, 'train@eng.pdtb.pdtb_steps_per_second': 2.662, 'epoch': 10.0}
{'loss': 0.9886, 'learning_rate': 3.3333333333333333e-06, 'epoch': 10.0}
{'eval_loss': 0.9167398810386658, 'eval_accuracy@eng.pdtb.pdtb': 0.6875746714456392, 'eval_f1@eng.pdtb.pdtb': 0.5552875986469862, 'eval_precision@eng.pdtb.pdtb': 0.6306682148302324, 'eval_recall@eng.pdtb.pdtb': 0.5384310559361067, 'eval_loss@eng.pdtb.pdtb': 0.9167398810386658, 'eval_runtime': 20.0799, 'eval_samples_per_second': 83.367, 'eval_steps_per_second': 2.639, 'epoch': 10.0}
{'train@eng.pdtb.pdtb_loss': 0.9279595613479614, 'train@eng.pdtb.pdtb_accuracy@eng.pdtb.pdtb': 0.6871129326047359, 'train@eng.pdtb.pdtb_f1@eng.pdtb.pdtb': 0.49505977849504035, 'train@eng.pdtb.pdtb_precision@eng.pdtb.pdtb': 0.5665981761288835, 'train@eng.pdtb.pdtb_recall@eng.pdtb.pdtb': 0.4857371569348321, 'train@eng.pdtb.pdtb_loss@eng.pdtb.pdtb': 0.927959680557251, 'train@eng.pdtb.pdtb_runtime': 515.7971, 'train@eng.pdtb.pdtb_samples_per_second': 85.15, 'train@eng.pdtb.pdtb_steps_per_second': 2.662, 'epoch': 11.0}
{'loss': 0.9806, 'learning_rate': 1.6666666666666667e-06, 'epoch': 11.0}
{'eval_loss': 0.91680908203125, 'eval_accuracy@eng.pdtb.pdtb': 0.6929510155316607, 'eval_f1@eng.pdtb.pdtb': 0.5504162017220656, 'eval_precision@eng.pdtb.pdtb': 0.5996159146669212, 'eval_recall@eng.pdtb.pdtb': 0.5312204966105611, 'eval_loss@eng.pdtb.pdtb': 0.9168091416358948, 'eval_runtime': 19.9884, 'eval_samples_per_second': 83.749, 'eval_steps_per_second': 2.652, 'epoch': 11.0}
{'train@eng.pdtb.pdtb_loss': 0.926490306854248, 'train@eng.pdtb.pdtb_accuracy@eng.pdtb.pdtb': 0.6876138433515483, 'train@eng.pdtb.pdtb_f1@eng.pdtb.pdtb': 0.49493124363583263, 'train@eng.pdtb.pdtb_precision@eng.pdtb.pdtb': 0.5689695351119667, 'train@eng.pdtb.pdtb_recall@eng.pdtb.pdtb': 0.4844424382490212, 'train@eng.pdtb.pdtb_loss@eng.pdtb.pdtb': 0.926490306854248, 'train@eng.pdtb.pdtb_runtime': 515.3998, 'train@eng.pdtb.pdtb_samples_per_second': 85.215, 'train@eng.pdtb.pdtb_steps_per_second': 2.664, 'epoch': 12.0}
{'loss': 0.9784, 'learning_rate': 0.0, 'epoch': 12.0}
{'eval_loss': 0.9146145582199097, 'eval_accuracy@eng.pdtb.pdtb': 0.6887694145758662, 'eval_f1@eng.pdtb.pdtb': 0.5490557927890684, 'eval_precision@eng.pdtb.pdtb': 0.5998160044321599, 'eval_recall@eng.pdtb.pdtb': 0.5289078218451353, 'eval_loss@eng.pdtb.pdtb': 0.9146145582199097, 'eval_runtime': 19.9652, 'eval_samples_per_second': 83.846, 'eval_steps_per_second': 2.655, 'epoch': 12.0}
{'train_runtime': 19505.8051, 'train_samples_per_second': 27.02, 'train_steps_per_second': 0.845, 'train_loss': 1.1027835291200079, 'epoch': 12.0}
***** train_logger metrics *****
  epoch                    =       12.0
  train_loss               =     1.7285
  train_runtime            = 1:12:45.85
  train_samples_per_second =     26.332
  train_steps_per_second   =      0.825
-------------------------------------------------------------------
Lang1:  fas.rst.prstc    Lang2:  eng.pdtb.pdtb
Saving run to:  runs/full_shot/FullShot=v4_fas.rst.prstc_eng.pdtb.pdtb_lrfunc=Adafactor_lr=2e-05_plm=bert-base-multilingual-cased
Running with params: BERT_MODEL=bert-base-multilingual-cased lr=2e-05
read 4100 examples
read 499 examples
read 592 examples
read 43920 examples
read 1674 examples
read 2257 examples
Total prediction labels:  40
BertAdapterModel(
  (shared_parameters): ModuleDict()
  (bert): BertModel(
    (shared_parameters): ModuleDict()
    (invertible_adapters): ModuleDict(
      (ar): NICECouplingBlock(
        (F): Sequential(
          (0): Linear(in_features=384, out_features=192, bias=True)
          (1): Activation_Function_Class(
            (f): ReLU()
          )
          (2): Linear(in_features=192, out_features=384, bias=True)
        )
        (G): Sequential(
          (0): Linear(in_features=384, out_features=192, bias=True)
          (1): Activation_Function_Class(
            (f): ReLU()
          )
          (2): Linear(in_features=192, out_features=384, bias=True)
        )
      )
      (en): NICECouplingBlock(
        (F): Sequential(
          (0): Linear(in_features=384, out_features=192, bias=True)
          (1): Activation_Function_Class(
            (f): ReLU()
          )
          (2): Linear(in_features=192, out_features=384, bias=True)
        )
        (G): Sequential(
          (0): Linear(in_features=384, out_features=192, bias=True)
          (1): Activation_Function_Class(
            (f): ReLU()
          )
          (2): Linear(in_features=192, out_features=384, bias=True)
        )
      )
    )
    (embeddings): BertEmbeddings(
      (word_embeddings): Embedding(119547, 768, padding_idx=0)
      (position_embeddings): Embedding(512, 768)
      (token_type_embeddings): Embedding(2, 768)
      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
      (dropout): Dropout(p=0.1, inplace=False)
    )
    (encoder): BertEncoder(
      (layer): ModuleList(
        (0): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (ar): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (en): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (1): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (ar): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (en): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (2): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (ar): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (en): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (3): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (ar): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (en): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (4): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (ar): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (en): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (5): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (ar): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (en): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (6): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (ar): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (en): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (7): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (ar): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (en): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (8): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (ar): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (en): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (9): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (ar): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (en): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (10): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (ar): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (en): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (11): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (ar): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (en): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
      )
    )
    (pooler): BertPooler(
      (dense): Linear(in_features=768, out_features=768, bias=True)
      (activation): Tanh()
    )
    (prefix_tuning): PrefixTuningPool(
      (prefix_tunings): ModuleDict()
    )
  )
  (heads): ModuleDict(
    (disrpt): ClassificationHead(
      (0): Dropout(p=0.1, inplace=False)
      (1): Linear(in_features=768, out_features=768, bias=True)
      (2): Activation_Function_Class(
        (f): Tanh()
      )
      (3): Dropout(p=0.1, inplace=False)
      (4): Linear(in_features=768, out_features=40, bias=True)
    )
  )
)
{'train@fas.rst.prstc_loss': 2.4289143085479736, 'train@fas.rst.prstc_accuracy@fas.rst.prstc': 0.23780487804878048, 'train@fas.rst.prstc_f1@fas.rst.prstc': 0.02260214430599826, 'train@fas.rst.prstc_precision@fas.rst.prstc': 0.013988522238163558, 'train@fas.rst.prstc_recall@fas.rst.prstc': 0.058823529411764705, 'train@fas.rst.prstc_loss@fas.rst.prstc': 2.4289143085479736, 'train@fas.rst.prstc_runtime': 48.1765, 'train@fas.rst.prstc_samples_per_second': 85.104, 'train@fas.rst.prstc_steps_per_second': 2.678, 'epoch': 1.0}
{'loss': 2.854, 'learning_rate': 1.8333333333333333e-05, 'epoch': 1.0}
{'eval_loss': 2.346639633178711, 'eval_accuracy@fas.rst.prstc': 0.24448897795591182, 'eval_f1@fas.rst.prstc': 0.026194310252281264, 'eval_precision@fas.rst.prstc': 0.016299265197060788, 'eval_recall@fas.rst.prstc': 0.06666666666666667, 'eval_loss@fas.rst.prstc': 2.346639633178711, 'eval_runtime': 6.2045, 'eval_samples_per_second': 80.425, 'eval_steps_per_second': 2.579, 'epoch': 1.0}
{'train@fas.rst.prstc_loss': 2.3665597438812256, 'train@fas.rst.prstc_accuracy@fas.rst.prstc': 0.2553658536585366, 'train@fas.rst.prstc_f1@fas.rst.prstc': 0.03810619207916083, 'train@fas.rst.prstc_precision@fas.rst.prstc': 0.029289728144491288, 'train@fas.rst.prstc_recall@fas.rst.prstc': 0.06520971727479863, 'train@fas.rst.prstc_loss@fas.rst.prstc': 2.3665595054626465, 'train@fas.rst.prstc_runtime': 48.3775, 'train@fas.rst.prstc_samples_per_second': 84.75, 'train@fas.rst.prstc_steps_per_second': 2.667, 'epoch': 2.0}
{'loss': 2.4156, 'learning_rate': 1.6666666666666667e-05, 'epoch': 2.0}
{'eval_loss': 2.276686906814575, 'eval_accuracy@fas.rst.prstc': 0.28857715430861725, 'eval_f1@fas.rst.prstc': 0.048984508314651856, 'eval_precision@fas.rst.prstc': 0.03825247650495301, 'eval_recall@fas.rst.prstc': 0.07988595866001426, 'eval_loss@fas.rst.prstc': 2.276686668395996, 'eval_runtime': 6.2393, 'eval_samples_per_second': 79.977, 'eval_steps_per_second': 2.564, 'epoch': 2.0}
{'train@fas.rst.prstc_loss': 2.3473851680755615, 'train@fas.rst.prstc_accuracy@fas.rst.prstc': 0.23780487804878048, 'train@fas.rst.prstc_f1@fas.rst.prstc': 0.02260214430599826, 'train@fas.rst.prstc_precision@fas.rst.prstc': 0.013988522238163558, 'train@fas.rst.prstc_recall@fas.rst.prstc': 0.058823529411764705, 'train@fas.rst.prstc_loss@fas.rst.prstc': 2.3473854064941406, 'train@fas.rst.prstc_runtime': 48.2683, 'train@fas.rst.prstc_samples_per_second': 84.942, 'train@fas.rst.prstc_steps_per_second': 2.673, 'epoch': 3.0}
{'loss': 2.3679, 'learning_rate': 1.5000000000000002e-05, 'epoch': 3.0}
{'eval_loss': 2.259504795074463, 'eval_accuracy@fas.rst.prstc': 0.24649298597194388, 'eval_f1@fas.rst.prstc': 0.027385984427141265, 'eval_precision@fas.rst.prstc': 0.08299866131191432, 'eval_recall@fas.rst.prstc': 0.0672463768115942, 'eval_loss@fas.rst.prstc': 2.259504795074463, 'eval_runtime': 6.2329, 'eval_samples_per_second': 80.059, 'eval_steps_per_second': 2.567, 'epoch': 3.0}
{'train@fas.rst.prstc_loss': 2.3334476947784424, 'train@fas.rst.prstc_accuracy@fas.rst.prstc': 0.23780487804878048, 'train@fas.rst.prstc_f1@fas.rst.prstc': 0.02260214430599826, 'train@fas.rst.prstc_precision@fas.rst.prstc': 0.013988522238163558, 'train@fas.rst.prstc_recall@fas.rst.prstc': 0.058823529411764705, 'train@fas.rst.prstc_loss@fas.rst.prstc': 2.3334476947784424, 'train@fas.rst.prstc_runtime': 48.281, 'train@fas.rst.prstc_samples_per_second': 84.92, 'train@fas.rst.prstc_steps_per_second': 2.672, 'epoch': 4.0}
{'loss': 2.3558, 'learning_rate': 1.3333333333333333e-05, 'epoch': 4.0}
{'eval_loss': 2.2506816387176514, 'eval_accuracy@fas.rst.prstc': 0.24448897795591182, 'eval_f1@fas.rst.prstc': 0.026194310252281264, 'eval_precision@fas.rst.prstc': 0.016299265197060788, 'eval_recall@fas.rst.prstc': 0.06666666666666667, 'eval_loss@fas.rst.prstc': 2.2506816387176514, 'eval_runtime': 6.2257, 'eval_samples_per_second': 80.151, 'eval_steps_per_second': 2.57, 'epoch': 4.0}
{'train@fas.rst.prstc_loss': 2.323042154312134, 'train@fas.rst.prstc_accuracy@fas.rst.prstc': 0.24804878048780488, 'train@fas.rst.prstc_f1@fas.rst.prstc': 0.028989395040693474, 'train@fas.rst.prstc_precision@fas.rst.prstc': 0.03238582018173123, 'train@fas.rst.prstc_recall@fas.rst.prstc': 0.061817442743600445, 'train@fas.rst.prstc_loss@fas.rst.prstc': 2.323042392730713, 'train@fas.rst.prstc_runtime': 48.3251, 'train@fas.rst.prstc_samples_per_second': 84.842, 'train@fas.rst.prstc_steps_per_second': 2.669, 'epoch': 5.0}
{'loss': 2.3459, 'learning_rate': 1.1666666666666668e-05, 'epoch': 5.0}
{'eval_loss': 2.237773895263672, 'eval_accuracy@fas.rst.prstc': 0.2625250501002004, 'eval_f1@fas.rst.prstc': 0.036640654031958375, 'eval_precision@fas.rst.prstc': 0.0459322859578614, 'eval_recall@fas.rst.prstc': 0.07191732002851034, 'eval_loss@fas.rst.prstc': 2.237773895263672, 'eval_runtime': 6.2273, 'eval_samples_per_second': 80.131, 'eval_steps_per_second': 2.569, 'epoch': 5.0}
{'train@fas.rst.prstc_loss': 2.312800884246826, 'train@fas.rst.prstc_accuracy@fas.rst.prstc': 0.23829268292682926, 'train@fas.rst.prstc_f1@fas.rst.prstc': 0.022900925714224946, 'train@fas.rst.prstc_precision@fas.rst.prstc': 0.03753501400560224, 'train@fas.rst.prstc_recall@fas.rst.prstc': 0.058962592129050195, 'train@fas.rst.prstc_loss@fas.rst.prstc': 2.312800645828247, 'train@fas.rst.prstc_runtime': 48.3549, 'train@fas.rst.prstc_samples_per_second': 84.79, 'train@fas.rst.prstc_steps_per_second': 2.668, 'epoch': 6.0}
{'loss': 2.3274, 'learning_rate': 1e-05, 'epoch': 6.0}
{'eval_loss': 2.233609437942505, 'eval_accuracy@fas.rst.prstc': 0.24649298597194388, 'eval_f1@fas.rst.prstc': 0.027418545673796077, 'eval_precision@fas.rst.prstc': 0.049698189134808855, 'eval_recall@fas.rst.prstc': 0.0672463768115942, 'eval_loss@fas.rst.prstc': 2.233609914779663, 'eval_runtime': 6.2215, 'eval_samples_per_second': 80.206, 'eval_steps_per_second': 2.572, 'epoch': 6.0}
{'train@fas.rst.prstc_loss': 2.2937204837799072, 'train@fas.rst.prstc_accuracy@fas.rst.prstc': 0.27463414634146344, 'train@fas.rst.prstc_f1@fas.rst.prstc': 0.0405210906428868, 'train@fas.rst.prstc_precision@fas.rst.prstc': 0.03227092330513343, 'train@fas.rst.prstc_recall@fas.rst.prstc': 0.06992073425114727, 'train@fas.rst.prstc_loss@fas.rst.prstc': 2.2937204837799072, 'train@fas.rst.prstc_runtime': 48.5052, 'train@fas.rst.prstc_samples_per_second': 84.527, 'train@fas.rst.prstc_steps_per_second': 2.66, 'epoch': 7.0}
{'loss': 2.3174, 'learning_rate': 8.333333333333334e-06, 'epoch': 7.0}
{'eval_loss': 2.2087225914001465, 'eval_accuracy@fas.rst.prstc': 0.2845691382765531, 'eval_f1@fas.rst.prstc': 0.046570883898265684, 'eval_precision@fas.rst.prstc': 0.03949359010454793, 'eval_recall@fas.rst.prstc': 0.07849370396768829, 'eval_loss@fas.rst.prstc': 2.2087228298187256, 'eval_runtime': 6.2253, 'eval_samples_per_second': 80.157, 'eval_steps_per_second': 2.57, 'epoch': 7.0}
{'train@fas.rst.prstc_loss': 2.265507221221924, 'train@fas.rst.prstc_accuracy@fas.rst.prstc': 0.2995121951219512, 'train@fas.rst.prstc_f1@fas.rst.prstc': 0.04800524991438662, 'train@fas.rst.prstc_precision@fas.rst.prstc': 0.03463995055114866, 'train@fas.rst.prstc_recall@fas.rst.prstc': 0.07826406940299306, 'train@fas.rst.prstc_loss@fas.rst.prstc': 2.265507459640503, 'train@fas.rst.prstc_runtime': 48.3957, 'train@fas.rst.prstc_samples_per_second': 84.718, 'train@fas.rst.prstc_steps_per_second': 2.666, 'epoch': 8.0}
{'loss': 2.3015, 'learning_rate': 6.666666666666667e-06, 'epoch': 8.0}
{'eval_loss': 2.1807403564453125, 'eval_accuracy@fas.rst.prstc': 0.3186372745490982, 'eval_f1@fas.rst.prstc': 0.057060334239029294, 'eval_precision@fas.rst.prstc': 0.04209405886825242, 'eval_recall@fas.rst.prstc': 0.08901401758137324, 'eval_loss@fas.rst.prstc': 2.1807401180267334, 'eval_runtime': 6.2236, 'eval_samples_per_second': 80.179, 'eval_steps_per_second': 2.571, 'epoch': 8.0}
{'train@fas.rst.prstc_loss': 2.241460084915161, 'train@fas.rst.prstc_accuracy@fas.rst.prstc': 0.29804878048780487, 'train@fas.rst.prstc_f1@fas.rst.prstc': 0.0485795111607963, 'train@fas.rst.prstc_precision@fas.rst.prstc': 0.035315623655392744, 'train@fas.rst.prstc_recall@fas.rst.prstc': 0.07838045420022892, 'train@fas.rst.prstc_loss@fas.rst.prstc': 2.241459846496582, 'train@fas.rst.prstc_runtime': 48.3957, 'train@fas.rst.prstc_samples_per_second': 84.718, 'train@fas.rst.prstc_steps_per_second': 2.666, 'epoch': 9.0}
{'loss': 2.2741, 'learning_rate': 5e-06, 'epoch': 9.0}
{'eval_loss': 2.151676893234253, 'eval_accuracy@fas.rst.prstc': 0.31462925851703405, 'eval_f1@fas.rst.prstc': 0.05729166666666667, 'eval_precision@fas.rst.prstc': 0.04260761812402349, 'eval_recall@fas.rst.prstc': 0.08808743169398907, 'eval_loss@fas.rst.prstc': 2.151677370071411, 'eval_runtime': 6.231, 'eval_samples_per_second': 80.083, 'eval_steps_per_second': 2.568, 'epoch': 9.0}
{'train@fas.rst.prstc_loss': 2.2237613201141357, 'train@fas.rst.prstc_accuracy@fas.rst.prstc': 0.29853658536585365, 'train@fas.rst.prstc_f1@fas.rst.prstc': 0.049170243169302054, 'train@fas.rst.prstc_precision@fas.rst.prstc': 0.036130494463723045, 'train@fas.rst.prstc_recall@fas.rst.prstc': 0.0788323010600858, 'train@fas.rst.prstc_loss@fas.rst.prstc': 2.2237613201141357, 'train@fas.rst.prstc_runtime': 48.413, 'train@fas.rst.prstc_samples_per_second': 84.688, 'train@fas.rst.prstc_steps_per_second': 2.665, 'epoch': 10.0}
{'loss': 2.2555, 'learning_rate': 3.3333333333333333e-06, 'epoch': 10.0}
{'eval_loss': 2.130579710006714, 'eval_accuracy@fas.rst.prstc': 0.32064128256513025, 'eval_f1@fas.rst.prstc': 0.059166666666666666, 'eval_precision@fas.rst.prstc': 0.04488517511773325, 'eval_recall@fas.rst.prstc': 0.09002613447374673, 'eval_loss@fas.rst.prstc': 2.130579710006714, 'eval_runtime': 6.2356, 'eval_samples_per_second': 80.025, 'eval_steps_per_second': 2.566, 'epoch': 10.0}
{'train@fas.rst.prstc_loss': 2.2152349948883057, 'train@fas.rst.prstc_accuracy@fas.rst.prstc': 0.3019512195121951, 'train@fas.rst.prstc_f1@fas.rst.prstc': 0.049824847648958226, 'train@fas.rst.prstc_precision@fas.rst.prstc': 0.03671665318697736, 'train@fas.rst.prstc_recall@fas.rst.prstc': 0.07988853588352962, 'train@fas.rst.prstc_loss@fas.rst.prstc': 2.2152349948883057, 'train@fas.rst.prstc_runtime': 48.3733, 'train@fas.rst.prstc_samples_per_second': 84.757, 'train@fas.rst.prstc_steps_per_second': 2.667, 'epoch': 11.0}
{'loss': 2.242, 'learning_rate': 1.6666666666666667e-06, 'epoch': 11.0}
{'eval_loss': 2.1197755336761475, 'eval_accuracy@fas.rst.prstc': 0.3246492985971944, 'eval_f1@fas.rst.prstc': 0.06004549510487753, 'eval_precision@fas.rst.prstc': 0.045753891654531705, 'eval_recall@fas.rst.prstc': 0.09118555476360181, 'eval_loss@fas.rst.prstc': 2.1197755336761475, 'eval_runtime': 6.2222, 'eval_samples_per_second': 80.197, 'eval_steps_per_second': 2.571, 'epoch': 11.0}
{'train@fas.rst.prstc_loss': 2.215073585510254, 'train@fas.rst.prstc_accuracy@fas.rst.prstc': 0.3029268292682927, 'train@fas.rst.prstc_f1@fas.rst.prstc': 0.04997657212970654, 'train@fas.rst.prstc_precision@fas.rst.prstc': 0.03681658897220963, 'train@fas.rst.prstc_recall@fas.rst.prstc': 0.08012986318368046, 'train@fas.rst.prstc_loss@fas.rst.prstc': 2.215073585510254, 'train@fas.rst.prstc_runtime': 48.3515, 'train@fas.rst.prstc_samples_per_second': 84.796, 'train@fas.rst.prstc_steps_per_second': 2.668, 'epoch': 12.0}
{'loss': 2.2371, 'learning_rate': 0.0, 'epoch': 12.0}
{'eval_loss': 2.119368076324463, 'eval_accuracy@fas.rst.prstc': 0.3246492985971944, 'eval_f1@fas.rst.prstc': 0.05994744886051362, 'eval_precision@fas.rst.prstc': 0.04557804768331084, 'eval_recall@fas.rst.prstc': 0.09118555476360181, 'eval_loss@fas.rst.prstc': 2.119368076324463, 'eval_runtime': 6.206, 'eval_samples_per_second': 80.405, 'eval_steps_per_second': 2.578, 'epoch': 12.0}
{'train_runtime': 1879.5862, 'train_samples_per_second': 26.176, 'train_steps_per_second': 0.824, 'train_loss': 2.35785377364442, 'epoch': 12.0}
***** train_log metrics *****
  epoch                    =       12.0
  train_loss               =     2.3579
  train_runtime            = 0:31:19.58
  train_samples_per_second =     26.176
  train_steps_per_second   =      0.824
{'train@eng.pdtb.pdtb_loss': 1.316820502281189, 'train@eng.pdtb.pdtb_accuracy@eng.pdtb.pdtb': 0.5839708561020036, 'train@eng.pdtb.pdtb_f1@eng.pdtb.pdtb': 0.25133978196451245, 'train@eng.pdtb.pdtb_precision@eng.pdtb.pdtb': 0.3572882801346076, 'train@eng.pdtb.pdtb_recall@eng.pdtb.pdtb': 0.24417209670702208, 'train@eng.pdtb.pdtb_loss@eng.pdtb.pdtb': 1.316820502281189, 'train@eng.pdtb.pdtb_runtime': 515.7577, 'train@eng.pdtb.pdtb_samples_per_second': 85.156, 'train@eng.pdtb.pdtb_steps_per_second': 2.662, 'epoch': 1.0}
{'loss': 1.8385, 'learning_rate': 1.8333333333333333e-05, 'epoch': 1.0}
{'eval_loss': 1.2442595958709717, 'eval_accuracy@eng.pdtb.pdtb': 0.6158900836320191, 'eval_f1@eng.pdtb.pdtb': 0.29783369400230886, 'eval_precision@eng.pdtb.pdtb': 0.39418707865138325, 'eval_recall@eng.pdtb.pdtb': 0.29393208464975007, 'eval_loss@eng.pdtb.pdtb': 1.2442595958709717, 'eval_runtime': 20.0248, 'eval_samples_per_second': 83.596, 'eval_steps_per_second': 2.647, 'epoch': 1.0}
{'train@eng.pdtb.pdtb_loss': 1.1141043901443481, 'train@eng.pdtb.pdtb_accuracy@eng.pdtb.pdtb': 0.635200364298725, 'train@eng.pdtb.pdtb_f1@eng.pdtb.pdtb': 0.36789085954411876, 'train@eng.pdtb.pdtb_precision@eng.pdtb.pdtb': 0.46395093817071875, 'train@eng.pdtb.pdtb_recall@eng.pdtb.pdtb': 0.35084969703910857, 'train@eng.pdtb.pdtb_loss@eng.pdtb.pdtb': 1.1141042709350586, 'train@eng.pdtb.pdtb_runtime': 516.451, 'train@eng.pdtb.pdtb_samples_per_second': 85.042, 'train@eng.pdtb.pdtb_steps_per_second': 2.659, 'epoch': 2.0}
{'loss': 1.2487, 'learning_rate': 1.6666666666666667e-05, 'epoch': 2.0}
{'eval_loss': 1.0531150102615356, 'eval_accuracy@eng.pdtb.pdtb': 0.6577060931899642, 'eval_f1@eng.pdtb.pdtb': 0.40595282395666643, 'eval_precision@eng.pdtb.pdtb': 0.493798200376256, 'eval_recall@eng.pdtb.pdtb': 0.38717694768814465, 'eval_loss@eng.pdtb.pdtb': 1.0531151294708252, 'eval_runtime': 20.0246, 'eval_samples_per_second': 83.597, 'eval_steps_per_second': 2.647, 'epoch': 2.0}
{'train@eng.pdtb.pdtb_loss': 1.05903959274292, 'train@eng.pdtb.pdtb_accuracy@eng.pdtb.pdtb': 0.648816029143898, 'train@eng.pdtb.pdtb_f1@eng.pdtb.pdtb': 0.41823065358354883, 'train@eng.pdtb.pdtb_precision@eng.pdtb.pdtb': 0.4634805353040428, 'train@eng.pdtb.pdtb_recall@eng.pdtb.pdtb': 0.398678327791904, 'train@eng.pdtb.pdtb_loss@eng.pdtb.pdtb': 1.05903959274292, 'train@eng.pdtb.pdtb_runtime': 515.3502, 'train@eng.pdtb.pdtb_samples_per_second': 85.224, 'train@eng.pdtb.pdtb_steps_per_second': 2.664, 'epoch': 3.0}
{'loss': 1.1328, 'learning_rate': 1.5000000000000002e-05, 'epoch': 3.0}
{'eval_loss': 1.0083134174346924, 'eval_accuracy@eng.pdtb.pdtb': 0.6756272401433692, 'eval_f1@eng.pdtb.pdtb': 0.4606490319088736, 'eval_precision@eng.pdtb.pdtb': 0.5104966559008511, 'eval_recall@eng.pdtb.pdtb': 0.44177533428640736, 'eval_loss@eng.pdtb.pdtb': 1.0083134174346924, 'eval_runtime': 20.0827, 'eval_samples_per_second': 83.355, 'eval_steps_per_second': 2.639, 'epoch': 3.0}
{'train@eng.pdtb.pdtb_loss': 1.0086590051651, 'train@eng.pdtb.pdtb_accuracy@eng.pdtb.pdtb': 0.6644808743169399, 'train@eng.pdtb.pdtb_f1@eng.pdtb.pdtb': 0.44405128645462894, 'train@eng.pdtb.pdtb_precision@eng.pdtb.pdtb': 0.5137182651110421, 'train@eng.pdtb.pdtb_recall@eng.pdtb.pdtb': 0.4356408111648802, 'train@eng.pdtb.pdtb_loss@eng.pdtb.pdtb': 1.0086590051651, 'train@eng.pdtb.pdtb_runtime': 515.358, 'train@eng.pdtb.pdtb_samples_per_second': 85.222, 'train@eng.pdtb.pdtb_steps_per_second': 2.664, 'epoch': 4.0}
{'loss': 1.0832, 'learning_rate': 1.3333333333333333e-05, 'epoch': 4.0}
{'eval_loss': 0.9644238948822021, 'eval_accuracy@eng.pdtb.pdtb': 0.6875746714456392, 'eval_f1@eng.pdtb.pdtb': 0.5029053329777322, 'eval_precision@eng.pdtb.pdtb': 0.5410002357522996, 'eval_recall@eng.pdtb.pdtb': 0.4852500109983441, 'eval_loss@eng.pdtb.pdtb': 0.9644239544868469, 'eval_runtime': 20.0145, 'eval_samples_per_second': 83.639, 'eval_steps_per_second': 2.648, 'epoch': 4.0}
{'train@eng.pdtb.pdtb_loss': 0.984268069267273, 'train@eng.pdtb.pdtb_accuracy@eng.pdtb.pdtb': 0.6709699453551913, 'train@eng.pdtb.pdtb_f1@eng.pdtb.pdtb': 0.4517597914354692, 'train@eng.pdtb.pdtb_precision@eng.pdtb.pdtb': 0.5191126297421349, 'train@eng.pdtb.pdtb_recall@eng.pdtb.pdtb': 0.44626436760590577, 'train@eng.pdtb.pdtb_loss@eng.pdtb.pdtb': 0.984268069267273, 'train@eng.pdtb.pdtb_runtime': 515.5101, 'train@eng.pdtb.pdtb_samples_per_second': 85.197, 'train@eng.pdtb.pdtb_steps_per_second': 2.663, 'epoch': 5.0}
{'loss': 1.052, 'learning_rate': 1.1666666666666668e-05, 'epoch': 5.0}
{'eval_loss': 0.9475991725921631, 'eval_accuracy@eng.pdtb.pdtb': 0.6845878136200717, 'eval_f1@eng.pdtb.pdtb': 0.5104804145191051, 'eval_precision@eng.pdtb.pdtb': 0.5490708462950876, 'eval_recall@eng.pdtb.pdtb': 0.5029339644302575, 'eval_loss@eng.pdtb.pdtb': 0.9475991129875183, 'eval_runtime': 23.2866, 'eval_samples_per_second': 71.887, 'eval_steps_per_second': 2.276, 'epoch': 5.0}
{'train@eng.pdtb.pdtb_loss': 0.964229166507721, 'train@eng.pdtb.pdtb_accuracy@eng.pdtb.pdtb': 0.6757513661202186, 'train@eng.pdtb.pdtb_f1@eng.pdtb.pdtb': 0.45985299827809356, 'train@eng.pdtb.pdtb_precision@eng.pdtb.pdtb': 0.5250057938149632, 'train@eng.pdtb.pdtb_recall@eng.pdtb.pdtb': 0.4533854924013645, 'train@eng.pdtb.pdtb_loss@eng.pdtb.pdtb': 0.964229166507721, 'train@eng.pdtb.pdtb_runtime': 516.0749, 'train@eng.pdtb.pdtb_samples_per_second': 85.104, 'train@eng.pdtb.pdtb_steps_per_second': 2.66, 'epoch': 6.0}
{'loss': 1.0283, 'learning_rate': 1e-05, 'epoch': 6.0}
{'eval_loss': 0.9379402995109558, 'eval_accuracy@eng.pdtb.pdtb': 0.6851851851851852, 'eval_f1@eng.pdtb.pdtb': 0.5144167124820334, 'eval_precision@eng.pdtb.pdtb': 0.5538106096815059, 'eval_recall@eng.pdtb.pdtb': 0.5043176219960893, 'eval_loss@eng.pdtb.pdtb': 0.9379402995109558, 'eval_runtime': 20.0953, 'eval_samples_per_second': 83.303, 'eval_steps_per_second': 2.637, 'epoch': 6.0}
{'train@eng.pdtb.pdtb_loss': 0.9523705840110779, 'train@eng.pdtb.pdtb_accuracy@eng.pdtb.pdtb': 0.6797131147540983, 'train@eng.pdtb.pdtb_f1@eng.pdtb.pdtb': 0.4852368098294982, 'train@eng.pdtb.pdtb_precision@eng.pdtb.pdtb': 0.5770901380071396, 'train@eng.pdtb.pdtb_recall@eng.pdtb.pdtb': 0.4690056206156261, 'train@eng.pdtb.pdtb_loss@eng.pdtb.pdtb': 0.9523705840110779, 'train@eng.pdtb.pdtb_runtime': 515.8307, 'train@eng.pdtb.pdtb_samples_per_second': 85.144, 'train@eng.pdtb.pdtb_steps_per_second': 2.662, 'epoch': 7.0}
{'loss': 1.0117, 'learning_rate': 8.333333333333334e-06, 'epoch': 7.0}
{'eval_loss': 0.925277829170227, 'eval_accuracy@eng.pdtb.pdtb': 0.6863799283154122, 'eval_f1@eng.pdtb.pdtb': 0.5321829522311244, 'eval_precision@eng.pdtb.pdtb': 0.5763602298691979, 'eval_recall@eng.pdtb.pdtb': 0.5203780592796751, 'eval_loss@eng.pdtb.pdtb': 0.925277829170227, 'eval_runtime': 20.0261, 'eval_samples_per_second': 83.591, 'eval_steps_per_second': 2.647, 'epoch': 7.0}
{'train@eng.pdtb.pdtb_loss': 0.9404225945472717, 'train@eng.pdtb.pdtb_accuracy@eng.pdtb.pdtb': 0.6829690346083789, 'train@eng.pdtb.pdtb_f1@eng.pdtb.pdtb': 0.4925237968067968, 'train@eng.pdtb.pdtb_precision@eng.pdtb.pdtb': 0.5560616073557708, 'train@eng.pdtb.pdtb_recall@eng.pdtb.pdtb': 0.48171960594410246, 'train@eng.pdtb.pdtb_loss@eng.pdtb.pdtb': 0.9404224753379822, 'train@eng.pdtb.pdtb_runtime': 515.7622, 'train@eng.pdtb.pdtb_samples_per_second': 85.156, 'train@eng.pdtb.pdtb_steps_per_second': 2.662, 'epoch': 8.0}
{'loss': 1.0, 'learning_rate': 6.666666666666667e-06, 'epoch': 8.0}
{'eval_loss': 0.9194499254226685, 'eval_accuracy@eng.pdtb.pdtb': 0.6905615292712067, 'eval_f1@eng.pdtb.pdtb': 0.5452063676294183, 'eval_precision@eng.pdtb.pdtb': 0.5789861296652382, 'eval_recall@eng.pdtb.pdtb': 0.5411253909720134, 'eval_loss@eng.pdtb.pdtb': 0.9194499850273132, 'eval_runtime': 20.013, 'eval_samples_per_second': 83.646, 'eval_steps_per_second': 2.648, 'epoch': 8.0}
{'train@eng.pdtb.pdtb_loss': 0.9328835606575012, 'train@eng.pdtb.pdtb_accuracy@eng.pdtb.pdtb': 0.6841757741347906, 'train@eng.pdtb.pdtb_f1@eng.pdtb.pdtb': 0.496972667038224, 'train@eng.pdtb.pdtb_precision@eng.pdtb.pdtb': 0.5702093134465761, 'train@eng.pdtb.pdtb_recall@eng.pdtb.pdtb': 0.48310885618371835, 'train@eng.pdtb.pdtb_loss@eng.pdtb.pdtb': 0.9328835606575012, 'train@eng.pdtb.pdtb_runtime': 515.9606, 'train@eng.pdtb.pdtb_samples_per_second': 85.123, 'train@eng.pdtb.pdtb_steps_per_second': 2.661, 'epoch': 9.0}
{'loss': 0.9866, 'learning_rate': 5e-06, 'epoch': 9.0}
{'eval_loss': 0.9175682663917542, 'eval_accuracy@eng.pdtb.pdtb': 0.6911589008363201, 'eval_f1@eng.pdtb.pdtb': 0.5390680707187061, 'eval_precision@eng.pdtb.pdtb': 0.5742238460207169, 'eval_recall@eng.pdtb.pdtb': 0.5289126681307847, 'eval_loss@eng.pdtb.pdtb': 0.9175682067871094, 'eval_runtime': 20.0358, 'eval_samples_per_second': 83.551, 'eval_steps_per_second': 2.645, 'epoch': 9.0}
{'train@eng.pdtb.pdtb_loss': 0.9282941818237305, 'train@eng.pdtb.pdtb_accuracy@eng.pdtb.pdtb': 0.6862249544626594, 'train@eng.pdtb.pdtb_f1@eng.pdtb.pdtb': 0.49805937489866897, 'train@eng.pdtb.pdtb_precision@eng.pdtb.pdtb': 0.5754111946559327, 'train@eng.pdtb.pdtb_recall@eng.pdtb.pdtb': 0.48505824145012605, 'train@eng.pdtb.pdtb_loss@eng.pdtb.pdtb': 0.9282941818237305, 'train@eng.pdtb.pdtb_runtime': 516.0332, 'train@eng.pdtb.pdtb_samples_per_second': 85.111, 'train@eng.pdtb.pdtb_steps_per_second': 2.661, 'epoch': 10.0}
{'loss': 0.9815, 'learning_rate': 3.3333333333333333e-06, 'epoch': 10.0}
{'eval_loss': 0.9119323492050171, 'eval_accuracy@eng.pdtb.pdtb': 0.6929510155316607, 'eval_f1@eng.pdtb.pdtb': 0.5467595529505465, 'eval_precision@eng.pdtb.pdtb': 0.5811997352449194, 'eval_recall@eng.pdtb.pdtb': 0.540297199460006, 'eval_loss@eng.pdtb.pdtb': 0.9119323492050171, 'eval_runtime': 20.0408, 'eval_samples_per_second': 83.529, 'eval_steps_per_second': 2.645, 'epoch': 10.0}
{'train@eng.pdtb.pdtb_loss': 0.9243344664573669, 'train@eng.pdtb.pdtb_accuracy@eng.pdtb.pdtb': 0.687568306010929, 'train@eng.pdtb.pdtb_f1@eng.pdtb.pdtb': 0.5015347346633405, 'train@eng.pdtb.pdtb_precision@eng.pdtb.pdtb': 0.559314337342548, 'train@eng.pdtb.pdtb_recall@eng.pdtb.pdtb': 0.4906836297476842, 'train@eng.pdtb.pdtb_loss@eng.pdtb.pdtb': 0.9243342876434326, 'train@eng.pdtb.pdtb_runtime': 515.9255, 'train@eng.pdtb.pdtb_samples_per_second': 85.129, 'train@eng.pdtb.pdtb_steps_per_second': 2.661, 'epoch': 11.0}
{'loss': 0.9769, 'learning_rate': 1.6666666666666667e-06, 'epoch': 11.0}
{'eval_loss': 0.9113349318504333, 'eval_accuracy@eng.pdtb.pdtb': 0.6923536439665472, 'eval_f1@eng.pdtb.pdtb': 0.5441079211780668, 'eval_precision@eng.pdtb.pdtb': 0.5650690614605779, 'eval_recall@eng.pdtb.pdtb': 0.5404045542011477, 'eval_loss@eng.pdtb.pdtb': 0.9113350510597229, 'eval_runtime': 20.069, 'eval_samples_per_second': 83.412, 'eval_steps_per_second': 2.641, 'epoch': 11.0}
{'train@eng.pdtb.pdtb_loss': 0.9228954911231995, 'train@eng.pdtb.pdtb_accuracy@eng.pdtb.pdtb': 0.6881830601092896, 'train@eng.pdtb.pdtb_f1@eng.pdtb.pdtb': 0.5004677709428883, 'train@eng.pdtb.pdtb_precision@eng.pdtb.pdtb': 0.5661315526576552, 'train@eng.pdtb.pdtb_recall@eng.pdtb.pdtb': 0.48785759057948, 'train@eng.pdtb.pdtb_loss@eng.pdtb.pdtb': 0.9228954911231995, 'train@eng.pdtb.pdtb_runtime': 515.9543, 'train@eng.pdtb.pdtb_samples_per_second': 85.124, 'train@eng.pdtb.pdtb_steps_per_second': 2.661, 'epoch': 12.0}
{'loss': 0.9713, 'learning_rate': 0.0, 'epoch': 12.0}
{'eval_loss': 0.9094600677490234, 'eval_accuracy@eng.pdtb.pdtb': 0.6911589008363201, 'eval_f1@eng.pdtb.pdtb': 0.5444264708326224, 'eval_precision@eng.pdtb.pdtb': 0.5743207360288864, 'eval_recall@eng.pdtb.pdtb': 0.5379634177857289, 'eval_loss@eng.pdtb.pdtb': 0.9094601273536682, 'eval_runtime': 19.9974, 'eval_samples_per_second': 83.711, 'eval_steps_per_second': 2.65, 'epoch': 12.0}
{'train_runtime': 19553.745, 'train_samples_per_second': 26.953, 'train_steps_per_second': 0.843, 'train_loss': 1.1092938568789452, 'epoch': 12.0}
***** train_logger metrics *****
  epoch                    =       12.0
  train_loss               =     2.3579
  train_runtime            = 0:31:19.58
  train_samples_per_second =     26.176
  train_steps_per_second   =      0.824
-------------------------------------------------------------------
Lang1:  fra.sdrt.annodis    Lang2:  eng.pdtb.pdtb
Saving run to:  runs/full_shot/FullShot=v4_fra.sdrt.annodis_eng.pdtb.pdtb_lrfunc=Adafactor_lr=2e-05_plm=bert-base-multilingual-cased
Running with params: BERT_MODEL=bert-base-multilingual-cased lr=2e-05
read 2185 examples
read 528 examples
read 625 examples
read 43920 examples
read 1674 examples
read 2257 examples
Total prediction labels:  41
BertAdapterModel(
  (shared_parameters): ModuleDict()
  (bert): BertModel(
    (shared_parameters): ModuleDict()
    (invertible_adapters): ModuleDict(
      (fr): NICECouplingBlock(
        (F): Sequential(
          (0): Linear(in_features=384, out_features=192, bias=True)
          (1): Activation_Function_Class(
            (f): NewGELUActivation()
          )
          (2): Linear(in_features=192, out_features=384, bias=True)
        )
        (G): Sequential(
          (0): Linear(in_features=384, out_features=192, bias=True)
          (1): Activation_Function_Class(
            (f): NewGELUActivation()
          )
          (2): Linear(in_features=192, out_features=384, bias=True)
        )
      )
      (en): NICECouplingBlock(
        (F): Sequential(
          (0): Linear(in_features=384, out_features=192, bias=True)
          (1): Activation_Function_Class(
            (f): ReLU()
          )
          (2): Linear(in_features=192, out_features=384, bias=True)
        )
        (G): Sequential(
          (0): Linear(in_features=384, out_features=192, bias=True)
          (1): Activation_Function_Class(
            (f): ReLU()
          )
          (2): Linear(in_features=192, out_features=384, bias=True)
        )
      )
    )
    (embeddings): BertEmbeddings(
      (word_embeddings): Embedding(119547, 768, padding_idx=0)
      (position_embeddings): Embedding(512, 768)
      (token_type_embeddings): Embedding(2, 768)
      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
      (dropout): Dropout(p=0.1, inplace=False)
    )
    (encoder): BertEncoder(
      (layer): ModuleList(
        (0): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (fr): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (en): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (1): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (fr): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (en): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (2): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (fr): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (en): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (3): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (fr): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (en): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (4): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (fr): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (en): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (5): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (fr): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (en): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (6): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (fr): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (en): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (7): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (fr): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (en): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (8): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (fr): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (en): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (9): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (fr): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (en): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (10): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (fr): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (en): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (11): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (fr): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (en): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
      )
    )
    (pooler): BertPooler(
      (dense): Linear(in_features=768, out_features=768, bias=True)
      (activation): Tanh()
    )
    (prefix_tuning): PrefixTuningPool(
      (prefix_tunings): ModuleDict()
    )
  )
  (heads): ModuleDict(
    (disrpt): ClassificationHead(
      (0): Dropout(p=0.1, inplace=False)
      (1): Linear(in_features=768, out_features=768, bias=True)
      (2): Activation_Function_Class(
        (f): Tanh()
      )
      (3): Dropout(p=0.1, inplace=False)
      (4): Linear(in_features=768, out_features=41, bias=True)
    )
  )
)
{'train@fra.sdrt.annodis_loss': 2.7092182636260986, 'train@fra.sdrt.annodis_accuracy@fra.sdrt.annodis': 0.23386727688787184, 'train@fra.sdrt.annodis_f1@fra.sdrt.annodis': 0.04982215609067245, 'train@fra.sdrt.annodis_precision@fra.sdrt.annodis': 0.03913390483875964, 'train@fra.sdrt.annodis_recall@fra.sdrt.annodis': 0.07058147179351579, 'train@fra.sdrt.annodis_loss@fra.sdrt.annodis': 2.7092182636260986, 'train@fra.sdrt.annodis_runtime': 26.2809, 'train@fra.sdrt.annodis_samples_per_second': 83.14, 'train@fra.sdrt.annodis_steps_per_second': 2.625, 'epoch': 1.0}
{'loss': 3.2254, 'learning_rate': 1.8333333333333333e-05, 'epoch': 1.0}
{'eval_loss': 2.722702980041504, 'eval_accuracy@fra.sdrt.annodis': 0.2537878787878788, 'eval_f1@fra.sdrt.annodis': 0.053220944344741526, 'eval_precision@fra.sdrt.annodis': 0.04238626692857965, 'eval_recall@fra.sdrt.annodis': 0.073063717923531, 'eval_loss@fra.sdrt.annodis': 2.722703456878662, 'eval_runtime': 6.646, 'eval_samples_per_second': 79.446, 'eval_steps_per_second': 2.558, 'epoch': 1.0}
{'train@fra.sdrt.annodis_loss': 2.4063196182250977, 'train@fra.sdrt.annodis_accuracy@fra.sdrt.annodis': 0.2750572082379863, 'train@fra.sdrt.annodis_f1@fra.sdrt.annodis': 0.059439428492657376, 'train@fra.sdrt.annodis_precision@fra.sdrt.annodis': 0.04784567623581734, 'train@fra.sdrt.annodis_recall@fra.sdrt.annodis': 0.08206135376118068, 'train@fra.sdrt.annodis_loss@fra.sdrt.annodis': 2.4063198566436768, 'train@fra.sdrt.annodis_runtime': 26.3728, 'train@fra.sdrt.annodis_samples_per_second': 82.851, 'train@fra.sdrt.annodis_steps_per_second': 2.616, 'epoch': 2.0}
{'loss': 2.5491, 'learning_rate': 1.6666666666666667e-05, 'epoch': 2.0}
{'eval_loss': 2.417858123779297, 'eval_accuracy@fra.sdrt.annodis': 0.2746212121212121, 'eval_f1@fra.sdrt.annodis': 0.05728977582351505, 'eval_precision@fra.sdrt.annodis': 0.04724801199248802, 'eval_recall@fra.sdrt.annodis': 0.07830901167695907, 'eval_loss@fra.sdrt.annodis': 2.417858123779297, 'eval_runtime': 6.6666, 'eval_samples_per_second': 79.201, 'eval_steps_per_second': 2.55, 'epoch': 2.0}
{'train@fra.sdrt.annodis_loss': 2.3246419429779053, 'train@fra.sdrt.annodis_accuracy@fra.sdrt.annodis': 0.2755148741418764, 'train@fra.sdrt.annodis_f1@fra.sdrt.annodis': 0.059693254424546476, 'train@fra.sdrt.annodis_precision@fra.sdrt.annodis': 0.05509856566391323, 'train@fra.sdrt.annodis_recall@fra.sdrt.annodis': 0.0806347177175971, 'train@fra.sdrt.annodis_loss@fra.sdrt.annodis': 2.3246419429779053, 'train@fra.sdrt.annodis_runtime': 26.4703, 'train@fra.sdrt.annodis_samples_per_second': 82.545, 'train@fra.sdrt.annodis_steps_per_second': 2.607, 'epoch': 3.0}
{'loss': 2.3944, 'learning_rate': 1.5000000000000002e-05, 'epoch': 3.0}
{'eval_loss': 2.340208053588867, 'eval_accuracy@fra.sdrt.annodis': 0.26515151515151514, 'eval_f1@fra.sdrt.annodis': 0.054654958910278065, 'eval_precision@fra.sdrt.annodis': 0.050510645663839365, 'eval_recall@fra.sdrt.annodis': 0.07456677861662285, 'eval_loss@fra.sdrt.annodis': 2.340208053588867, 'eval_runtime': 6.6713, 'eval_samples_per_second': 79.145, 'eval_steps_per_second': 2.548, 'epoch': 3.0}
{'train@fra.sdrt.annodis_loss': 2.2698822021484375, 'train@fra.sdrt.annodis_accuracy@fra.sdrt.annodis': 0.2851258581235698, 'train@fra.sdrt.annodis_f1@fra.sdrt.annodis': 0.06208976328177359, 'train@fra.sdrt.annodis_precision@fra.sdrt.annodis': 0.056334021524766756, 'train@fra.sdrt.annodis_recall@fra.sdrt.annodis': 0.08379360351600634, 'train@fra.sdrt.annodis_loss@fra.sdrt.annodis': 2.2698819637298584, 'train@fra.sdrt.annodis_runtime': 26.4556, 'train@fra.sdrt.annodis_samples_per_second': 82.591, 'train@fra.sdrt.annodis_steps_per_second': 2.608, 'epoch': 4.0}
{'loss': 2.3323, 'learning_rate': 1.3333333333333333e-05, 'epoch': 4.0}
{'eval_loss': 2.2897255420684814, 'eval_accuracy@fra.sdrt.annodis': 0.2784090909090909, 'eval_f1@fra.sdrt.annodis': 0.059269015591257954, 'eval_precision@fra.sdrt.annodis': 0.05356156479458465, 'eval_recall@fra.sdrt.annodis': 0.07912637455731951, 'eval_loss@fra.sdrt.annodis': 2.2897257804870605, 'eval_runtime': 6.7049, 'eval_samples_per_second': 78.749, 'eval_steps_per_second': 2.535, 'epoch': 4.0}
{'train@fra.sdrt.annodis_loss': 2.2232601642608643, 'train@fra.sdrt.annodis_accuracy@fra.sdrt.annodis': 0.32128146453089246, 'train@fra.sdrt.annodis_f1@fra.sdrt.annodis': 0.06932276892723473, 'train@fra.sdrt.annodis_precision@fra.sdrt.annodis': 0.0543463355665487, 'train@fra.sdrt.annodis_recall@fra.sdrt.annodis': 0.09796231783463367, 'train@fra.sdrt.annodis_loss@fra.sdrt.annodis': 2.2232604026794434, 'train@fra.sdrt.annodis_runtime': 26.4309, 'train@fra.sdrt.annodis_samples_per_second': 82.668, 'train@fra.sdrt.annodis_steps_per_second': 2.611, 'epoch': 5.0}
{'loss': 2.2801, 'learning_rate': 1.1666666666666668e-05, 'epoch': 5.0}
{'eval_loss': 2.251173496246338, 'eval_accuracy@fra.sdrt.annodis': 0.2935606060606061, 'eval_f1@fra.sdrt.annodis': 0.06260889017858279, 'eval_precision@fra.sdrt.annodis': 0.04891278150946659, 'eval_recall@fra.sdrt.annodis': 0.08748572040368509, 'eval_loss@fra.sdrt.annodis': 2.251173734664917, 'eval_runtime': 6.6689, 'eval_samples_per_second': 79.173, 'eval_steps_per_second': 2.549, 'epoch': 5.0}
{'train@fra.sdrt.annodis_loss': 2.1794509887695312, 'train@fra.sdrt.annodis_accuracy@fra.sdrt.annodis': 0.34645308924485124, 'train@fra.sdrt.annodis_f1@fra.sdrt.annodis': 0.08702018964899903, 'train@fra.sdrt.annodis_precision@fra.sdrt.annodis': 0.09678759345857284, 'train@fra.sdrt.annodis_recall@fra.sdrt.annodis': 0.11192842260228969, 'train@fra.sdrt.annodis_loss@fra.sdrt.annodis': 2.179450750350952, 'train@fra.sdrt.annodis_runtime': 26.4101, 'train@fra.sdrt.annodis_samples_per_second': 82.733, 'train@fra.sdrt.annodis_steps_per_second': 2.613, 'epoch': 6.0}
{'loss': 2.2338, 'learning_rate': 1e-05, 'epoch': 6.0}
{'eval_loss': 2.2151923179626465, 'eval_accuracy@fra.sdrt.annodis': 0.32196969696969696, 'eval_f1@fra.sdrt.annodis': 0.0715398762915081, 'eval_precision@fra.sdrt.annodis': 0.10903510240526813, 'eval_recall@fra.sdrt.annodis': 0.09820321105011513, 'eval_loss@fra.sdrt.annodis': 2.2151923179626465, 'eval_runtime': 6.6844, 'eval_samples_per_second': 78.989, 'eval_steps_per_second': 2.543, 'epoch': 6.0}
{'train@fra.sdrt.annodis_loss': 2.1380527019500732, 'train@fra.sdrt.annodis_accuracy@fra.sdrt.annodis': 0.36613272311212813, 'train@fra.sdrt.annodis_f1@fra.sdrt.annodis': 0.0998947331082734, 'train@fra.sdrt.annodis_precision@fra.sdrt.annodis': 0.11810584675553051, 'train@fra.sdrt.annodis_recall@fra.sdrt.annodis': 0.12506062503269724, 'train@fra.sdrt.annodis_loss@fra.sdrt.annodis': 2.1380529403686523, 'train@fra.sdrt.annodis_runtime': 26.4431, 'train@fra.sdrt.annodis_samples_per_second': 82.63, 'train@fra.sdrt.annodis_steps_per_second': 2.609, 'epoch': 7.0}
{'loss': 2.1977, 'learning_rate': 8.333333333333334e-06, 'epoch': 7.0}
{'eval_loss': 2.181391954421997, 'eval_accuracy@fra.sdrt.annodis': 0.32765151515151514, 'eval_f1@fra.sdrt.annodis': 0.08254225286405452, 'eval_precision@fra.sdrt.annodis': 0.10022546144100028, 'eval_recall@fra.sdrt.annodis': 0.10412639646752815, 'eval_loss@fra.sdrt.annodis': 2.181392192840576, 'eval_runtime': 6.6905, 'eval_samples_per_second': 78.917, 'eval_steps_per_second': 2.541, 'epoch': 7.0}
{'train@fra.sdrt.annodis_loss': 2.1011500358581543, 'train@fra.sdrt.annodis_accuracy@fra.sdrt.annodis': 0.37848970251716246, 'train@fra.sdrt.annodis_f1@fra.sdrt.annodis': 0.10850086277538575, 'train@fra.sdrt.annodis_precision@fra.sdrt.annodis': 0.11849270444512748, 'train@fra.sdrt.annodis_recall@fra.sdrt.annodis': 0.1365493423882197, 'train@fra.sdrt.annodis_loss@fra.sdrt.annodis': 2.1011502742767334, 'train@fra.sdrt.annodis_runtime': 26.4821, 'train@fra.sdrt.annodis_samples_per_second': 82.508, 'train@fra.sdrt.annodis_steps_per_second': 2.606, 'epoch': 8.0}
{'loss': 2.1671, 'learning_rate': 6.666666666666667e-06, 'epoch': 8.0}
{'eval_loss': 2.1521308422088623, 'eval_accuracy@fra.sdrt.annodis': 0.3409090909090909, 'eval_f1@fra.sdrt.annodis': 0.08603301118284892, 'eval_precision@fra.sdrt.annodis': 0.08408770469686433, 'eval_recall@fra.sdrt.annodis': 0.10926509960370649, 'eval_loss@fra.sdrt.annodis': 2.1521308422088623, 'eval_runtime': 6.6925, 'eval_samples_per_second': 78.894, 'eval_steps_per_second': 2.54, 'epoch': 8.0}
{'train@fra.sdrt.annodis_loss': 2.0730996131896973, 'train@fra.sdrt.annodis_accuracy@fra.sdrt.annodis': 0.3862700228832952, 'train@fra.sdrt.annodis_f1@fra.sdrt.annodis': 0.11121275367578695, 'train@fra.sdrt.annodis_precision@fra.sdrt.annodis': 0.11419568821821044, 'train@fra.sdrt.annodis_recall@fra.sdrt.annodis': 0.1437745190232262, 'train@fra.sdrt.annodis_loss@fra.sdrt.annodis': 2.0730996131896973, 'train@fra.sdrt.annodis_runtime': 26.4219, 'train@fra.sdrt.annodis_samples_per_second': 82.697, 'train@fra.sdrt.annodis_steps_per_second': 2.611, 'epoch': 9.0}
{'loss': 2.1334, 'learning_rate': 5e-06, 'epoch': 9.0}
{'eval_loss': 2.1291513442993164, 'eval_accuracy@fra.sdrt.annodis': 0.3560606060606061, 'eval_f1@fra.sdrt.annodis': 0.09572503535872245, 'eval_precision@fra.sdrt.annodis': 0.087382425966241, 'eval_recall@fra.sdrt.annodis': 0.11914061660055592, 'eval_loss@fra.sdrt.annodis': 2.1291515827178955, 'eval_runtime': 6.694, 'eval_samples_per_second': 78.877, 'eval_steps_per_second': 2.54, 'epoch': 9.0}
{'train@fra.sdrt.annodis_loss': 2.0525600910186768, 'train@fra.sdrt.annodis_accuracy@fra.sdrt.annodis': 0.39267734553775746, 'train@fra.sdrt.annodis_f1@fra.sdrt.annodis': 0.11437866289842827, 'train@fra.sdrt.annodis_precision@fra.sdrt.annodis': 0.11451641524181475, 'train@fra.sdrt.annodis_recall@fra.sdrt.annodis': 0.1489608352785826, 'train@fra.sdrt.annodis_loss@fra.sdrt.annodis': 2.0525600910186768, 'train@fra.sdrt.annodis_runtime': 26.4545, 'train@fra.sdrt.annodis_samples_per_second': 82.595, 'train@fra.sdrt.annodis_steps_per_second': 2.608, 'epoch': 10.0}
{'loss': 2.1061, 'learning_rate': 3.3333333333333333e-06, 'epoch': 10.0}
{'eval_loss': 2.1133995056152344, 'eval_accuracy@fra.sdrt.annodis': 0.35795454545454547, 'eval_f1@fra.sdrt.annodis': 0.09546207853748688, 'eval_precision@fra.sdrt.annodis': 0.08459082530657613, 'eval_recall@fra.sdrt.annodis': 0.11986390825496561, 'eval_loss@fra.sdrt.annodis': 2.1133995056152344, 'eval_runtime': 6.6707, 'eval_samples_per_second': 79.152, 'eval_steps_per_second': 2.548, 'epoch': 10.0}
{'train@fra.sdrt.annodis_loss': 2.0400705337524414, 'train@fra.sdrt.annodis_accuracy@fra.sdrt.annodis': 0.39633867276887874, 'train@fra.sdrt.annodis_f1@fra.sdrt.annodis': 0.11647376528022026, 'train@fra.sdrt.annodis_precision@fra.sdrt.annodis': 0.11950535524545675, 'train@fra.sdrt.annodis_recall@fra.sdrt.annodis': 0.15127690723224305, 'train@fra.sdrt.annodis_loss@fra.sdrt.annodis': 2.0400705337524414, 'train@fra.sdrt.annodis_runtime': 26.4265, 'train@fra.sdrt.annodis_samples_per_second': 82.682, 'train@fra.sdrt.annodis_steps_per_second': 2.611, 'epoch': 11.0}
{'loss': 2.0844, 'learning_rate': 1.6666666666666667e-06, 'epoch': 11.0}
{'eval_loss': 2.1026413440704346, 'eval_accuracy@fra.sdrt.annodis': 0.3541666666666667, 'eval_f1@fra.sdrt.annodis': 0.09365762911894333, 'eval_precision@fra.sdrt.annodis': 0.08074440861866011, 'eval_recall@fra.sdrt.annodis': 0.11884419695488048, 'eval_loss@fra.sdrt.annodis': 2.1026413440704346, 'eval_runtime': 6.7074, 'eval_samples_per_second': 78.719, 'eval_steps_per_second': 2.535, 'epoch': 11.0}
{'train@fra.sdrt.annodis_loss': 2.035733222961426, 'train@fra.sdrt.annodis_accuracy@fra.sdrt.annodis': 0.39725400457665905, 'train@fra.sdrt.annodis_f1@fra.sdrt.annodis': 0.11672499388663603, 'train@fra.sdrt.annodis_precision@fra.sdrt.annodis': 0.1182064715310587, 'train@fra.sdrt.annodis_recall@fra.sdrt.annodis': 0.1515938836162926, 'train@fra.sdrt.annodis_loss@fra.sdrt.annodis': 2.035733222961426, 'train@fra.sdrt.annodis_runtime': 26.4696, 'train@fra.sdrt.annodis_samples_per_second': 82.548, 'train@fra.sdrt.annodis_steps_per_second': 2.607, 'epoch': 12.0}
{'loss': 2.0815, 'learning_rate': 0.0, 'epoch': 12.0}
{'eval_loss': 2.0994412899017334, 'eval_accuracy@fra.sdrt.annodis': 0.3541666666666667, 'eval_f1@fra.sdrt.annodis': 0.09362646956213326, 'eval_precision@fra.sdrt.annodis': 0.0807137027706042, 'eval_recall@fra.sdrt.annodis': 0.11884419695488048, 'eval_loss@fra.sdrt.annodis': 2.0994412899017334, 'eval_runtime': 6.7177, 'eval_samples_per_second': 78.598, 'eval_steps_per_second': 2.531, 'epoch': 12.0}
{'train_runtime': 1062.3154, 'train_samples_per_second': 24.682, 'train_steps_per_second': 0.779, 'train_loss': 2.315454031534241, 'epoch': 12.0}
***** train_log metrics *****
  epoch                    =       12.0
  train_loss               =     2.3155
  train_runtime            = 0:17:42.31
  train_samples_per_second =     24.682
  train_steps_per_second   =      0.779
{'train@eng.pdtb.pdtb_loss': 1.2333990335464478, 'train@eng.pdtb.pdtb_accuracy@eng.pdtb.pdtb': 0.6035291438979964, 'train@eng.pdtb.pdtb_f1@eng.pdtb.pdtb': 0.2809682484004174, 'train@eng.pdtb.pdtb_precision@eng.pdtb.pdtb': 0.35455022894639815, 'train@eng.pdtb.pdtb_recall@eng.pdtb.pdtb': 0.27191946312683957, 'train@eng.pdtb.pdtb_loss@eng.pdtb.pdtb': 1.2333991527557373, 'train@eng.pdtb.pdtb_runtime': 515.4252, 'train@eng.pdtb.pdtb_samples_per_second': 85.211, 'train@eng.pdtb.pdtb_steps_per_second': 2.664, 'epoch': 1.0}
{'loss': 1.7442, 'learning_rate': 1.8333333333333333e-05, 'epoch': 1.0}
{'eval_loss': 1.1626983880996704, 'eval_accuracy@eng.pdtb.pdtb': 0.6356033452807647, 'eval_f1@eng.pdtb.pdtb': 0.3288437204087034, 'eval_precision@eng.pdtb.pdtb': 0.4195549150735475, 'eval_recall@eng.pdtb.pdtb': 0.3199515313699912, 'eval_loss@eng.pdtb.pdtb': 1.1626983880996704, 'eval_runtime': 20.0285, 'eval_samples_per_second': 83.581, 'eval_steps_per_second': 2.646, 'epoch': 1.0}
{'train@eng.pdtb.pdtb_loss': 1.08632230758667, 'train@eng.pdtb.pdtb_accuracy@eng.pdtb.pdtb': 0.6418715846994536, 'train@eng.pdtb.pdtb_f1@eng.pdtb.pdtb': 0.3737580864203015, 'train@eng.pdtb.pdtb_precision@eng.pdtb.pdtb': 0.44797718755584126, 'train@eng.pdtb.pdtb_recall@eng.pdtb.pdtb': 0.35852027619503707, 'train@eng.pdtb.pdtb_loss@eng.pdtb.pdtb': 1.08632230758667, 'train@eng.pdtb.pdtb_runtime': 515.5728, 'train@eng.pdtb.pdtb_samples_per_second': 85.187, 'train@eng.pdtb.pdtb_steps_per_second': 2.663, 'epoch': 2.0}
{'loss': 1.1989, 'learning_rate': 1.6666666666666667e-05, 'epoch': 2.0}
{'eval_loss': 1.02509343624115, 'eval_accuracy@eng.pdtb.pdtb': 0.6600955794504182, 'eval_f1@eng.pdtb.pdtb': 0.41024320886877597, 'eval_precision@eng.pdtb.pdtb': 0.4911326241436421, 'eval_recall@eng.pdtb.pdtb': 0.39530964369598565, 'eval_loss@eng.pdtb.pdtb': 1.0250935554504395, 'eval_runtime': 19.9967, 'eval_samples_per_second': 83.714, 'eval_steps_per_second': 2.65, 'epoch': 2.0}
{'train@eng.pdtb.pdtb_loss': 1.0393102169036865, 'train@eng.pdtb.pdtb_accuracy@eng.pdtb.pdtb': 0.6548724954462659, 'train@eng.pdtb.pdtb_f1@eng.pdtb.pdtb': 0.41959525672967507, 'train@eng.pdtb.pdtb_precision@eng.pdtb.pdtb': 0.4698019770176252, 'train@eng.pdtb.pdtb_recall@eng.pdtb.pdtb': 0.40134424006290303, 'train@eng.pdtb.pdtb_loss@eng.pdtb.pdtb': 1.039310097694397, 'train@eng.pdtb.pdtb_runtime': 515.6119, 'train@eng.pdtb.pdtb_samples_per_second': 85.18, 'train@eng.pdtb.pdtb_steps_per_second': 2.663, 'epoch': 3.0}
{'loss': 1.1112, 'learning_rate': 1.5000000000000002e-05, 'epoch': 3.0}
{'eval_loss': 0.9872749447822571, 'eval_accuracy@eng.pdtb.pdtb': 0.6786140979689367, 'eval_f1@eng.pdtb.pdtb': 0.4633004755577018, 'eval_precision@eng.pdtb.pdtb': 0.5119049819665582, 'eval_recall@eng.pdtb.pdtb': 0.4457182788344916, 'eval_loss@eng.pdtb.pdtb': 0.9872749447822571, 'eval_runtime': 19.9944, 'eval_samples_per_second': 83.723, 'eval_steps_per_second': 2.651, 'epoch': 3.0}
{'train@eng.pdtb.pdtb_loss': 0.9952026605606079, 'train@eng.pdtb.pdtb_accuracy@eng.pdtb.pdtb': 0.669011839708561, 'train@eng.pdtb.pdtb_f1@eng.pdtb.pdtb': 0.44831925113853094, 'train@eng.pdtb.pdtb_precision@eng.pdtb.pdtb': 0.5194511071294737, 'train@eng.pdtb.pdtb_recall@eng.pdtb.pdtb': 0.4373271578986643, 'train@eng.pdtb.pdtb_loss@eng.pdtb.pdtb': 0.9952027201652527, 'train@eng.pdtb.pdtb_runtime': 515.6996, 'train@eng.pdtb.pdtb_samples_per_second': 85.166, 'train@eng.pdtb.pdtb_steps_per_second': 2.662, 'epoch': 4.0}
{'loss': 1.0676, 'learning_rate': 1.3333333333333333e-05, 'epoch': 4.0}
{'eval_loss': 0.948647677898407, 'eval_accuracy@eng.pdtb.pdtb': 0.6821983273596177, 'eval_f1@eng.pdtb.pdtb': 0.483884438366008, 'eval_precision@eng.pdtb.pdtb': 0.5311646957429185, 'eval_recall@eng.pdtb.pdtb': 0.4611238079844663, 'eval_loss@eng.pdtb.pdtb': 0.948647677898407, 'eval_runtime': 19.9961, 'eval_samples_per_second': 83.716, 'eval_steps_per_second': 2.651, 'epoch': 4.0}
{'train@eng.pdtb.pdtb_loss': 0.9729956388473511, 'train@eng.pdtb.pdtb_accuracy@eng.pdtb.pdtb': 0.67433970856102, 'train@eng.pdtb.pdtb_f1@eng.pdtb.pdtb': 0.456088997078839, 'train@eng.pdtb.pdtb_precision@eng.pdtb.pdtb': 0.5164862151815833, 'train@eng.pdtb.pdtb_recall@eng.pdtb.pdtb': 0.45259468473823655, 'train@eng.pdtb.pdtb_loss@eng.pdtb.pdtb': 0.9729956388473511, 'train@eng.pdtb.pdtb_runtime': 515.6563, 'train@eng.pdtb.pdtb_samples_per_second': 85.173, 'train@eng.pdtb.pdtb_steps_per_second': 2.663, 'epoch': 5.0}
{'loss': 1.0359, 'learning_rate': 1.1666666666666668e-05, 'epoch': 5.0}
{'eval_loss': 0.932906448841095, 'eval_accuracy@eng.pdtb.pdtb': 0.6851851851851852, 'eval_f1@eng.pdtb.pdtb': 0.5237886113739549, 'eval_precision@eng.pdtb.pdtb': 0.5487940412981663, 'eval_recall@eng.pdtb.pdtb': 0.5155108348806982, 'eval_loss@eng.pdtb.pdtb': 0.9329065680503845, 'eval_runtime': 20.0349, 'eval_samples_per_second': 83.554, 'eval_steps_per_second': 2.645, 'epoch': 5.0}
{'train@eng.pdtb.pdtb_loss': 0.9536010026931763, 'train@eng.pdtb.pdtb_accuracy@eng.pdtb.pdtb': 0.6794854280510019, 'train@eng.pdtb.pdtb_f1@eng.pdtb.pdtb': 0.4629963077216107, 'train@eng.pdtb.pdtb_precision@eng.pdtb.pdtb': 0.5047922044804369, 'train@eng.pdtb.pdtb_recall@eng.pdtb.pdtb': 0.4596470525328317, 'train@eng.pdtb.pdtb_loss@eng.pdtb.pdtb': 0.9536009430885315, 'train@eng.pdtb.pdtb_runtime': 515.5317, 'train@eng.pdtb.pdtb_samples_per_second': 85.194, 'train@eng.pdtb.pdtb_steps_per_second': 2.663, 'epoch': 6.0}
{'loss': 1.0153, 'learning_rate': 1e-05, 'epoch': 6.0}
{'eval_loss': 0.927707314491272, 'eval_accuracy@eng.pdtb.pdtb': 0.6804062126642771, 'eval_f1@eng.pdtb.pdtb': 0.522164423991039, 'eval_precision@eng.pdtb.pdtb': 0.5491050820125919, 'eval_recall@eng.pdtb.pdtb': 0.5122722099192799, 'eval_loss@eng.pdtb.pdtb': 0.927707314491272, 'eval_runtime': 20.0742, 'eval_samples_per_second': 83.391, 'eval_steps_per_second': 2.64, 'epoch': 6.0}
{'train@eng.pdtb.pdtb_loss': 0.9427019953727722, 'train@eng.pdtb.pdtb_accuracy@eng.pdtb.pdtb': 0.6813752276867031, 'train@eng.pdtb.pdtb_f1@eng.pdtb.pdtb': 0.4681328879370659, 'train@eng.pdtb.pdtb_precision@eng.pdtb.pdtb': 0.52114150942515, 'train@eng.pdtb.pdtb_recall@eng.pdtb.pdtb': 0.46260992514609023, 'train@eng.pdtb.pdtb_loss@eng.pdtb.pdtb': 0.9427019953727722, 'train@eng.pdtb.pdtb_runtime': 515.8885, 'train@eng.pdtb.pdtb_samples_per_second': 85.135, 'train@eng.pdtb.pdtb_steps_per_second': 2.661, 'epoch': 7.0}
{'loss': 1.0027, 'learning_rate': 8.333333333333334e-06, 'epoch': 7.0}
{'eval_loss': 0.9155191779136658, 'eval_accuracy@eng.pdtb.pdtb': 0.6869772998805257, 'eval_f1@eng.pdtb.pdtb': 0.5433969135086032, 'eval_precision@eng.pdtb.pdtb': 0.574373921974053, 'eval_recall@eng.pdtb.pdtb': 0.5328566659682534, 'eval_loss@eng.pdtb.pdtb': 0.9155192375183105, 'eval_runtime': 20.069, 'eval_samples_per_second': 83.412, 'eval_steps_per_second': 2.641, 'epoch': 7.0}
{'train@eng.pdtb.pdtb_loss': 0.9315560460090637, 'train@eng.pdtb.pdtb_accuracy@eng.pdtb.pdtb': 0.6858378870673952, 'train@eng.pdtb.pdtb_f1@eng.pdtb.pdtb': 0.47408411981163523, 'train@eng.pdtb.pdtb_precision@eng.pdtb.pdtb': 0.5118215654102907, 'train@eng.pdtb.pdtb_recall@eng.pdtb.pdtb': 0.4725512979320368, 'train@eng.pdtb.pdtb_loss@eng.pdtb.pdtb': 0.9315559267997742, 'train@eng.pdtb.pdtb_runtime': 515.8746, 'train@eng.pdtb.pdtb_samples_per_second': 85.137, 'train@eng.pdtb.pdtb_steps_per_second': 2.661, 'epoch': 8.0}
{'loss': 0.9908, 'learning_rate': 6.666666666666667e-06, 'epoch': 8.0}
{'eval_loss': 0.910518229007721, 'eval_accuracy@eng.pdtb.pdtb': 0.6887694145758662, 'eval_f1@eng.pdtb.pdtb': 0.550029032850101, 'eval_precision@eng.pdtb.pdtb': 0.5827783681867749, 'eval_recall@eng.pdtb.pdtb': 0.5435751192077224, 'eval_loss@eng.pdtb.pdtb': 0.9105182886123657, 'eval_runtime': 20.0459, 'eval_samples_per_second': 83.508, 'eval_steps_per_second': 2.644, 'epoch': 8.0}
{'train@eng.pdtb.pdtb_loss': 0.9228389263153076, 'train@eng.pdtb.pdtb_accuracy@eng.pdtb.pdtb': 0.6884562841530054, 'train@eng.pdtb.pdtb_f1@eng.pdtb.pdtb': 0.4769066647144758, 'train@eng.pdtb.pdtb_precision@eng.pdtb.pdtb': 0.5214510304993304, 'train@eng.pdtb.pdtb_recall@eng.pdtb.pdtb': 0.4743137529538652, 'train@eng.pdtb.pdtb_loss@eng.pdtb.pdtb': 0.9228390455245972, 'train@eng.pdtb.pdtb_runtime': 515.8564, 'train@eng.pdtb.pdtb_samples_per_second': 85.14, 'train@eng.pdtb.pdtb_steps_per_second': 2.662, 'epoch': 9.0}
{'loss': 0.9787, 'learning_rate': 5e-06, 'epoch': 9.0}
{'eval_loss': 0.9096924662590027, 'eval_accuracy@eng.pdtb.pdtb': 0.6863799283154122, 'eval_f1@eng.pdtb.pdtb': 0.5487621675601182, 'eval_precision@eng.pdtb.pdtb': 0.584792228061337, 'eval_recall@eng.pdtb.pdtb': 0.5409583167534395, 'eval_loss@eng.pdtb.pdtb': 0.9096924662590027, 'eval_runtime': 20.0435, 'eval_samples_per_second': 83.518, 'eval_steps_per_second': 2.644, 'epoch': 9.0}
{'train@eng.pdtb.pdtb_loss': 0.9198929071426392, 'train@eng.pdtb.pdtb_accuracy@eng.pdtb.pdtb': 0.6888433515482696, 'train@eng.pdtb.pdtb_f1@eng.pdtb.pdtb': 0.4970734471110697, 'train@eng.pdtb.pdtb_precision@eng.pdtb.pdtb': 0.5731951967610189, 'train@eng.pdtb.pdtb_recall@eng.pdtb.pdtb': 0.48578296398738724, 'train@eng.pdtb.pdtb_loss@eng.pdtb.pdtb': 0.9198929667472839, 'train@eng.pdtb.pdtb_runtime': 515.531, 'train@eng.pdtb.pdtb_samples_per_second': 85.194, 'train@eng.pdtb.pdtb_steps_per_second': 2.663, 'epoch': 10.0}
{'loss': 0.9739, 'learning_rate': 3.3333333333333333e-06, 'epoch': 10.0}
{'eval_loss': 0.903976559638977, 'eval_accuracy@eng.pdtb.pdtb': 0.6869772998805257, 'eval_f1@eng.pdtb.pdtb': 0.5506412003650663, 'eval_precision@eng.pdtb.pdtb': 0.5907377511547247, 'eval_recall@eng.pdtb.pdtb': 0.5388423876655681, 'eval_loss@eng.pdtb.pdtb': 0.903976559638977, 'eval_runtime': 20.0665, 'eval_samples_per_second': 83.423, 'eval_steps_per_second': 2.641, 'epoch': 10.0}
{'train@eng.pdtb.pdtb_loss': 0.9155396223068237, 'train@eng.pdtb.pdtb_accuracy@eng.pdtb.pdtb': 0.6903233151183971, 'train@eng.pdtb.pdtb_f1@eng.pdtb.pdtb': 0.5010436058390942, 'train@eng.pdtb.pdtb_precision@eng.pdtb.pdtb': 0.5606219640379123, 'train@eng.pdtb.pdtb_recall@eng.pdtb.pdtb': 0.49249345124905886, 'train@eng.pdtb.pdtb_loss@eng.pdtb.pdtb': 0.9155396223068237, 'train@eng.pdtb.pdtb_runtime': 515.6081, 'train@eng.pdtb.pdtb_samples_per_second': 85.181, 'train@eng.pdtb.pdtb_steps_per_second': 2.663, 'epoch': 11.0}
{'loss': 0.968, 'learning_rate': 1.6666666666666667e-06, 'epoch': 11.0}
{'eval_loss': 0.9040359854698181, 'eval_accuracy@eng.pdtb.pdtb': 0.6887694145758662, 'eval_f1@eng.pdtb.pdtb': 0.5536439869974678, 'eval_precision@eng.pdtb.pdtb': 0.586412667597158, 'eval_recall@eng.pdtb.pdtb': 0.5433112098809211, 'eval_loss@eng.pdtb.pdtb': 0.9040359854698181, 'eval_runtime': 20.0067, 'eval_samples_per_second': 83.672, 'eval_steps_per_second': 2.649, 'epoch': 11.0}
{'train@eng.pdtb.pdtb_loss': 0.9142954349517822, 'train@eng.pdtb.pdtb_accuracy@eng.pdtb.pdtb': 0.6912568306010929, 'train@eng.pdtb.pdtb_f1@eng.pdtb.pdtb': 0.5005994603198767, 'train@eng.pdtb.pdtb_precision@eng.pdtb.pdtb': 0.569414519291422, 'train@eng.pdtb.pdtb_recall@eng.pdtb.pdtb': 0.489679261556841, 'train@eng.pdtb.pdtb_loss@eng.pdtb.pdtb': 0.9142953157424927, 'train@eng.pdtb.pdtb_runtime': 516.2454, 'train@eng.pdtb.pdtb_samples_per_second': 85.076, 'train@eng.pdtb.pdtb_steps_per_second': 2.66, 'epoch': 12.0}
{'loss': 0.9642, 'learning_rate': 0.0, 'epoch': 12.0}
{'eval_loss': 0.9021567106246948, 'eval_accuracy@eng.pdtb.pdtb': 0.6869772998805257, 'eval_f1@eng.pdtb.pdtb': 0.5523120147778754, 'eval_precision@eng.pdtb.pdtb': 0.5879796253360208, 'eval_recall@eng.pdtb.pdtb': 0.5411895323643614, 'eval_loss@eng.pdtb.pdtb': 0.9021567106246948, 'eval_runtime': 20.0623, 'eval_samples_per_second': 83.44, 'eval_steps_per_second': 2.642, 'epoch': 12.0}
{'train_runtime': 19540.7271, 'train_samples_per_second': 26.971, 'train_steps_per_second': 0.843, 'train_loss': 1.087629636360274, 'epoch': 12.0}
***** train_logger metrics *****
  epoch                    =       12.0
  train_loss               =     2.3155
  train_runtime            = 0:17:42.31
  train_samples_per_second =     24.682
  train_steps_per_second   =      0.779
-------------------------------------------------------------------
Lang1:  nld.rst.nldt    Lang2:  eng.pdtb.pdtb
Saving run to:  runs/full_shot/FullShot=v4_nld.rst.nldt_eng.pdtb.pdtb_lrfunc=Adafactor_lr=2e-05_plm=bert-base-multilingual-cased
Running with params: BERT_MODEL=bert-base-multilingual-cased lr=2e-05
read 1608 examples
read 331 examples
read 326 examples
read 43920 examples
read 1674 examples
read 2257 examples
Total prediction labels:  56
BertAdapterModel(
  (shared_parameters): ModuleDict()
  (bert): BertModel(
    (shared_parameters): ModuleDict()
    (invertible_adapters): ModuleDict(
      (de): NICECouplingBlock(
        (F): Sequential(
          (0): Linear(in_features=384, out_features=192, bias=True)
          (1): Activation_Function_Class(
            (f): NewGELUActivation()
          )
          (2): Linear(in_features=192, out_features=384, bias=True)
        )
        (G): Sequential(
          (0): Linear(in_features=384, out_features=192, bias=True)
          (1): Activation_Function_Class(
            (f): NewGELUActivation()
          )
          (2): Linear(in_features=192, out_features=384, bias=True)
        )
      )
      (en): NICECouplingBlock(
        (F): Sequential(
          (0): Linear(in_features=384, out_features=192, bias=True)
          (1): Activation_Function_Class(
            (f): ReLU()
          )
          (2): Linear(in_features=192, out_features=384, bias=True)
        )
        (G): Sequential(
          (0): Linear(in_features=384, out_features=192, bias=True)
          (1): Activation_Function_Class(
            (f): ReLU()
          )
          (2): Linear(in_features=192, out_features=384, bias=True)
        )
      )
    )
    (embeddings): BertEmbeddings(
      (word_embeddings): Embedding(119547, 768, padding_idx=0)
      (position_embeddings): Embedding(512, 768)
      (token_type_embeddings): Embedding(2, 768)
      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
      (dropout): Dropout(p=0.1, inplace=False)
    )
    (encoder): BertEncoder(
      (layer): ModuleList(
        (0): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (de): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (en): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (1): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (de): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (en): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (2): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (de): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (en): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (3): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (de): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (en): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (4): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (de): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (en): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (5): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (de): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (en): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (6): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (de): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (en): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (7): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (de): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (en): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (8): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (de): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (en): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (9): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (de): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (en): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (10): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (de): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (en): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (11): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (de): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (en): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
      )
    )
    (pooler): BertPooler(
      (dense): Linear(in_features=768, out_features=768, bias=True)
      (activation): Tanh()
    )
    (prefix_tuning): PrefixTuningPool(
      (prefix_tunings): ModuleDict()
    )
  )
  (heads): ModuleDict(
    (disrpt): ClassificationHead(
      (0): Dropout(p=0.1, inplace=False)
      (1): Linear(in_features=768, out_features=768, bias=True)
      (2): Activation_Function_Class(
        (f): Tanh()
      )
      (3): Dropout(p=0.1, inplace=False)
      (4): Linear(in_features=768, out_features=56, bias=True)
    )
  )
)
{'train@nld.rst.nldt_loss': 3.4494502544403076, 'train@nld.rst.nldt_accuracy@nld.rst.nldt': 0.26430348258706465, 'train@nld.rst.nldt_f1@nld.rst.nldt': 0.01306566650270536, 'train@nld.rst.nldt_precision@nld.rst.nldt': 0.00825948383084577, 'train@nld.rst.nldt_recall@nld.rst.nldt': 0.03125, 'train@nld.rst.nldt_loss@nld.rst.nldt': 3.4494504928588867, 'train@nld.rst.nldt_runtime': 22.9835, 'train@nld.rst.nldt_samples_per_second': 69.963, 'train@nld.rst.nldt_steps_per_second': 2.219, 'epoch': 1.0}
{'loss': 3.7836, 'learning_rate': 1.8333333333333333e-05, 'epoch': 1.0}
{'eval_loss': 3.4235663414001465, 'eval_accuracy@nld.rst.nldt': 0.27794561933534745, 'eval_f1@nld.rst.nldt': 0.016110673321075214, 'eval_precision@nld.rst.nldt': 0.010294282197605462, 'eval_recall@nld.rst.nldt': 0.037037037037037035, 'eval_loss@nld.rst.nldt': 3.4235665798187256, 'eval_runtime': 4.4528, 'eval_samples_per_second': 74.334, 'eval_steps_per_second': 2.47, 'epoch': 1.0}
{'train@nld.rst.nldt_loss': 3.007732629776001, 'train@nld.rst.nldt_accuracy@nld.rst.nldt': 0.26430348258706465, 'train@nld.rst.nldt_f1@nld.rst.nldt': 0.01306566650270536, 'train@nld.rst.nldt_precision@nld.rst.nldt': 0.00825948383084577, 'train@nld.rst.nldt_recall@nld.rst.nldt': 0.03125, 'train@nld.rst.nldt_loss@nld.rst.nldt': 3.007732391357422, 'train@nld.rst.nldt_runtime': 19.7384, 'train@nld.rst.nldt_samples_per_second': 81.466, 'train@nld.rst.nldt_steps_per_second': 2.584, 'epoch': 2.0}
{'loss': 3.2152, 'learning_rate': 1.6666666666666667e-05, 'epoch': 2.0}
{'eval_loss': 2.943591594696045, 'eval_accuracy@nld.rst.nldt': 0.27794561933534745, 'eval_f1@nld.rst.nldt': 0.016110673321075214, 'eval_precision@nld.rst.nldt': 0.010294282197605462, 'eval_recall@nld.rst.nldt': 0.037037037037037035, 'eval_loss@nld.rst.nldt': 2.9435911178588867, 'eval_runtime': 4.5029, 'eval_samples_per_second': 73.507, 'eval_steps_per_second': 2.443, 'epoch': 2.0}
{'train@nld.rst.nldt_loss': 2.8670167922973633, 'train@nld.rst.nldt_accuracy@nld.rst.nldt': 0.26616915422885573, 'train@nld.rst.nldt_f1@nld.rst.nldt': 0.014896523762515865, 'train@nld.rst.nldt_precision@nld.rst.nldt': 0.039545596502186135, 'train@nld.rst.nldt_recall@nld.rst.nldt': 0.03216911764705882, 'train@nld.rst.nldt_loss@nld.rst.nldt': 2.8670167922973633, 'train@nld.rst.nldt_runtime': 20.0531, 'train@nld.rst.nldt_samples_per_second': 80.187, 'train@nld.rst.nldt_steps_per_second': 2.543, 'epoch': 3.0}
{'loss': 2.9686, 'learning_rate': 1.5000000000000002e-05, 'epoch': 3.0}
{'eval_loss': 2.808358669281006, 'eval_accuracy@nld.rst.nldt': 0.27794561933534745, 'eval_f1@nld.rst.nldt': 0.016110673321075214, 'eval_precision@nld.rst.nldt': 0.010294282197605462, 'eval_recall@nld.rst.nldt': 0.037037037037037035, 'eval_loss@nld.rst.nldt': 2.808358907699585, 'eval_runtime': 4.4884, 'eval_samples_per_second': 73.746, 'eval_steps_per_second': 2.451, 'epoch': 3.0}
{'train@nld.rst.nldt_loss': 2.801412343978882, 'train@nld.rst.nldt_accuracy@nld.rst.nldt': 0.27611940298507465, 'train@nld.rst.nldt_f1@nld.rst.nldt': 0.020880251028435878, 'train@nld.rst.nldt_precision@nld.rst.nldt': 0.017828725362374415, 'train@nld.rst.nldt_recall@nld.rst.nldt': 0.037303921568627454, 'train@nld.rst.nldt_loss@nld.rst.nldt': 2.801412582397461, 'train@nld.rst.nldt_runtime': 19.7394, 'train@nld.rst.nldt_samples_per_second': 81.462, 'train@nld.rst.nldt_steps_per_second': 2.584, 'epoch': 4.0}
{'loss': 2.8434, 'learning_rate': 1.3333333333333333e-05, 'epoch': 4.0}
{'eval_loss': 2.751100778579712, 'eval_accuracy@nld.rst.nldt': 0.2990936555891239, 'eval_f1@nld.rst.nldt': 0.03290668119099492, 'eval_precision@nld.rst.nldt': 0.029301453352086265, 'eval_recall@nld.rst.nldt': 0.05144032921810699, 'eval_loss@nld.rst.nldt': 2.751100778579712, 'eval_runtime': 4.4954, 'eval_samples_per_second': 73.631, 'eval_steps_per_second': 2.447, 'epoch': 4.0}
{'train@nld.rst.nldt_loss': 2.762223482131958, 'train@nld.rst.nldt_accuracy@nld.rst.nldt': 0.2848258706467662, 'train@nld.rst.nldt_f1@nld.rst.nldt': 0.025969852585984908, 'train@nld.rst.nldt_precision@nld.rst.nldt': 0.025707166649318207, 'train@nld.rst.nldt_recall@nld.rst.nldt': 0.04217436974789916, 'train@nld.rst.nldt_loss@nld.rst.nldt': 2.762223482131958, 'train@nld.rst.nldt_runtime': 19.735, 'train@nld.rst.nldt_samples_per_second': 81.48, 'train@nld.rst.nldt_steps_per_second': 2.584, 'epoch': 5.0}
{'loss': 2.8084, 'learning_rate': 1.1666666666666668e-05, 'epoch': 5.0}
{'eval_loss': 2.7159268856048584, 'eval_accuracy@nld.rst.nldt': 0.2990936555891239, 'eval_f1@nld.rst.nldt': 0.032004509144991276, 'eval_precision@nld.rst.nldt': 0.02501669491960754, 'eval_recall@nld.rst.nldt': 0.05309536589729826, 'eval_loss@nld.rst.nldt': 2.7159271240234375, 'eval_runtime': 4.4773, 'eval_samples_per_second': 73.929, 'eval_steps_per_second': 2.457, 'epoch': 5.0}
{'train@nld.rst.nldt_loss': 2.7269234657287598, 'train@nld.rst.nldt_accuracy@nld.rst.nldt': 0.2916666666666667, 'train@nld.rst.nldt_f1@nld.rst.nldt': 0.02892408777825444, 'train@nld.rst.nldt_precision@nld.rst.nldt': 0.027449682435186647, 'train@nld.rst.nldt_recall@nld.rst.nldt': 0.04664974323062558, 'train@nld.rst.nldt_loss@nld.rst.nldt': 2.7269234657287598, 'train@nld.rst.nldt_runtime': 19.7488, 'train@nld.rst.nldt_samples_per_second': 81.423, 'train@nld.rst.nldt_steps_per_second': 2.582, 'epoch': 6.0}
{'loss': 2.7683, 'learning_rate': 1e-05, 'epoch': 6.0}
{'eval_loss': 2.686136484146118, 'eval_accuracy@nld.rst.nldt': 0.311178247734139, 'eval_f1@nld.rst.nldt': 0.03784213135001272, 'eval_precision@nld.rst.nldt': 0.05009653905901147, 'eval_recall@nld.rst.nldt': 0.05932699435114894, 'eval_loss@nld.rst.nldt': 2.686136245727539, 'eval_runtime': 4.4928, 'eval_samples_per_second': 73.674, 'eval_steps_per_second': 2.448, 'epoch': 6.0}
{'train@nld.rst.nldt_loss': 2.6965720653533936, 'train@nld.rst.nldt_accuracy@nld.rst.nldt': 0.2972636815920398, 'train@nld.rst.nldt_f1@nld.rst.nldt': 0.03180149361949297, 'train@nld.rst.nldt_precision@nld.rst.nldt': 0.026697518589222376, 'train@nld.rst.nldt_recall@nld.rst.nldt': 0.05074463118580766, 'train@nld.rst.nldt_loss@nld.rst.nldt': 2.6965720653533936, 'train@nld.rst.nldt_runtime': 19.7288, 'train@nld.rst.nldt_samples_per_second': 81.505, 'train@nld.rst.nldt_steps_per_second': 2.585, 'epoch': 7.0}
{'loss': 2.7304, 'learning_rate': 8.333333333333334e-06, 'epoch': 7.0}
{'eval_loss': 2.660853147506714, 'eval_accuracy@nld.rst.nldt': 0.3293051359516616, 'eval_f1@nld.rst.nldt': 0.0430173078541289, 'eval_precision@nld.rst.nldt': 0.04117489645366276, 'eval_recall@nld.rst.nldt': 0.06867443703192494, 'eval_loss@nld.rst.nldt': 2.6608526706695557, 'eval_runtime': 4.4759, 'eval_samples_per_second': 73.951, 'eval_steps_per_second': 2.458, 'epoch': 7.0}
{'train@nld.rst.nldt_loss': 2.673891305923462, 'train@nld.rst.nldt_accuracy@nld.rst.nldt': 0.3034825870646766, 'train@nld.rst.nldt_f1@nld.rst.nldt': 0.033240814685968664, 'train@nld.rst.nldt_precision@nld.rst.nldt': 0.026564677633116783, 'train@nld.rst.nldt_recall@nld.rst.nldt': 0.05444794584500467, 'train@nld.rst.nldt_loss@nld.rst.nldt': 2.673891544342041, 'train@nld.rst.nldt_runtime': 19.7249, 'train@nld.rst.nldt_samples_per_second': 81.521, 'train@nld.rst.nldt_steps_per_second': 2.586, 'epoch': 8.0}
{'loss': 2.7163, 'learning_rate': 6.666666666666667e-06, 'epoch': 8.0}
{'eval_loss': 2.6423165798187256, 'eval_accuracy@nld.rst.nldt': 0.3323262839879154, 'eval_f1@nld.rst.nldt': 0.050155128725394155, 'eval_precision@nld.rst.nldt': 0.07792797253581567, 'eval_recall@nld.rst.nldt': 0.0747654832195895, 'eval_loss@nld.rst.nldt': 2.6423163414001465, 'eval_runtime': 4.5114, 'eval_samples_per_second': 73.37, 'eval_steps_per_second': 2.438, 'epoch': 8.0}
{'train@nld.rst.nldt_loss': 2.660093069076538, 'train@nld.rst.nldt_accuracy@nld.rst.nldt': 0.30659203980099503, 'train@nld.rst.nldt_f1@nld.rst.nldt': 0.035013056224972396, 'train@nld.rst.nldt_precision@nld.rst.nldt': 0.02770696212580468, 'train@nld.rst.nldt_recall@nld.rst.nldt': 0.055804154995331465, 'train@nld.rst.nldt_loss@nld.rst.nldt': 2.660093069076538, 'train@nld.rst.nldt_runtime': 19.7183, 'train@nld.rst.nldt_samples_per_second': 81.549, 'train@nld.rst.nldt_steps_per_second': 2.586, 'epoch': 9.0}
{'loss': 2.6961, 'learning_rate': 5e-06, 'epoch': 9.0}
{'eval_loss': 2.6309289932250977, 'eval_accuracy@nld.rst.nldt': 0.3323262839879154, 'eval_f1@nld.rst.nldt': 0.04984242120658956, 'eval_precision@nld.rst.nldt': 0.07570632861569848, 'eval_recall@nld.rst.nldt': 0.0747654832195895, 'eval_loss@nld.rst.nldt': 2.6309289932250977, 'eval_runtime': 4.5113, 'eval_samples_per_second': 73.372, 'eval_steps_per_second': 2.438, 'epoch': 9.0}
{'train@nld.rst.nldt_loss': 2.6497976779937744, 'train@nld.rst.nldt_accuracy@nld.rst.nldt': 0.30783582089552236, 'train@nld.rst.nldt_f1@nld.rst.nldt': 0.03537653494853058, 'train@nld.rst.nldt_precision@nld.rst.nldt': 0.027962596678470314, 'train@nld.rst.nldt_recall@nld.rst.nldt': 0.05641690009337068, 'train@nld.rst.nldt_loss@nld.rst.nldt': 2.649797201156616, 'train@nld.rst.nldt_runtime': 19.7507, 'train@nld.rst.nldt_samples_per_second': 81.415, 'train@nld.rst.nldt_steps_per_second': 2.582, 'epoch': 10.0}
{'loss': 2.6804, 'learning_rate': 3.3333333333333333e-06, 'epoch': 10.0}
{'eval_loss': 2.621802568435669, 'eval_accuracy@nld.rst.nldt': 0.3323262839879154, 'eval_f1@nld.rst.nldt': 0.04965111069649202, 'eval_precision@nld.rst.nldt': 0.06265795206971676, 'eval_recall@nld.rst.nldt': 0.0747654832195895, 'eval_loss@nld.rst.nldt': 2.62180233001709, 'eval_runtime': 4.5035, 'eval_samples_per_second': 73.498, 'eval_steps_per_second': 2.443, 'epoch': 10.0}
{'train@nld.rst.nldt_loss': 2.6427979469299316, 'train@nld.rst.nldt_accuracy@nld.rst.nldt': 0.30845771144278605, 'train@nld.rst.nldt_f1@nld.rst.nldt': 0.035354424703072915, 'train@nld.rst.nldt_precision@nld.rst.nldt': 0.027642397540644084, 'train@nld.rst.nldt_recall@nld.rst.nldt': 0.056897759103641456, 'train@nld.rst.nldt_loss@nld.rst.nldt': 2.6427979469299316, 'train@nld.rst.nldt_runtime': 19.6972, 'train@nld.rst.nldt_samples_per_second': 81.636, 'train@nld.rst.nldt_steps_per_second': 2.589, 'epoch': 11.0}
{'loss': 2.6645, 'learning_rate': 1.6666666666666667e-06, 'epoch': 11.0}
{'eval_loss': 2.6161375045776367, 'eval_accuracy@nld.rst.nldt': 0.3323262839879154, 'eval_f1@nld.rst.nldt': 0.049481952561662705, 'eval_precision@nld.rst.nldt': 0.06250209485503602, 'eval_recall@nld.rst.nldt': 0.0747654832195895, 'eval_loss@nld.rst.nldt': 2.6161375045776367, 'eval_runtime': 4.4639, 'eval_samples_per_second': 74.151, 'eval_steps_per_second': 2.464, 'epoch': 11.0}
{'train@nld.rst.nldt_loss': 2.6399481296539307, 'train@nld.rst.nldt_accuracy@nld.rst.nldt': 0.30845771144278605, 'train@nld.rst.nldt_f1@nld.rst.nldt': 0.0352891256612839, 'train@nld.rst.nldt_precision@nld.rst.nldt': 0.0273932282680067, 'train@nld.rst.nldt_recall@nld.rst.nldt': 0.05713060224089636, 'train@nld.rst.nldt_loss@nld.rst.nldt': 2.6399478912353516, 'train@nld.rst.nldt_runtime': 19.6838, 'train@nld.rst.nldt_samples_per_second': 81.692, 'train@nld.rst.nldt_steps_per_second': 2.591, 'epoch': 12.0}
{'loss': 2.6715, 'learning_rate': 0.0, 'epoch': 12.0}
{'eval_loss': 2.614283561706543, 'eval_accuracy@nld.rst.nldt': 0.3323262839879154, 'eval_f1@nld.rst.nldt': 0.04916320334160709, 'eval_precision@nld.rst.nldt': 0.062214504380401006, 'eval_recall@nld.rst.nldt': 0.0747654832195895, 'eval_loss@nld.rst.nldt': 2.614283323287964, 'eval_runtime': 4.5245, 'eval_samples_per_second': 73.156, 'eval_steps_per_second': 2.431, 'epoch': 12.0}
{'train_runtime': 783.9655, 'train_samples_per_second': 24.613, 'train_steps_per_second': 0.781, 'train_loss': 2.878891439998851, 'epoch': 12.0}
***** train_log metrics *****
  epoch                    =       12.0
  train_loss               =     2.8789
  train_runtime            = 0:13:03.96
  train_samples_per_second =     24.613
  train_steps_per_second   =      0.781
{'train@eng.pdtb.pdtb_loss': 1.2510255575180054, 'train@eng.pdtb.pdtb_accuracy@eng.pdtb.pdtb': 0.6013433515482696, 'train@eng.pdtb.pdtb_f1@eng.pdtb.pdtb': 0.27315435793490495, 'train@eng.pdtb.pdtb_precision@eng.pdtb.pdtb': 0.36127519102190747, 'train@eng.pdtb.pdtb_recall@eng.pdtb.pdtb': 0.26230658731448797, 'train@eng.pdtb.pdtb_loss@eng.pdtb.pdtb': 1.2510255575180054, 'train@eng.pdtb.pdtb_runtime': 516.702, 'train@eng.pdtb.pdtb_samples_per_second': 85.001, 'train@eng.pdtb.pdtb_steps_per_second': 2.657, 'epoch': 1.0}
{'loss': 1.8102, 'learning_rate': 1.8333333333333333e-05, 'epoch': 1.0}
{'eval_loss': 1.1762932538986206, 'eval_accuracy@eng.pdtb.pdtb': 0.6308243727598566, 'eval_f1@eng.pdtb.pdtb': 0.3198423466276666, 'eval_precision@eng.pdtb.pdtb': 0.4000593691017884, 'eval_recall@eng.pdtb.pdtb': 0.31196144819479155, 'eval_loss@eng.pdtb.pdtb': 1.1762932538986206, 'eval_runtime': 20.2288, 'eval_samples_per_second': 82.753, 'eval_steps_per_second': 2.62, 'epoch': 1.0}
{'train@eng.pdtb.pdtb_loss': 1.094452142715454, 'train@eng.pdtb.pdtb_accuracy@eng.pdtb.pdtb': 0.6402777777777777, 'train@eng.pdtb.pdtb_f1@eng.pdtb.pdtb': 0.3598349518737576, 'train@eng.pdtb.pdtb_precision@eng.pdtb.pdtb': 0.42822679637173083, 'train@eng.pdtb.pdtb_recall@eng.pdtb.pdtb': 0.3471653772518339, 'train@eng.pdtb.pdtb_loss@eng.pdtb.pdtb': 1.094451904296875, 'train@eng.pdtb.pdtb_runtime': 516.2299, 'train@eng.pdtb.pdtb_samples_per_second': 85.078, 'train@eng.pdtb.pdtb_steps_per_second': 2.66, 'epoch': 2.0}
{'loss': 1.2104, 'learning_rate': 1.6666666666666667e-05, 'epoch': 2.0}
{'eval_loss': 1.0311379432678223, 'eval_accuracy@eng.pdtb.pdtb': 0.6648745519713262, 'eval_f1@eng.pdtb.pdtb': 0.410102690912133, 'eval_precision@eng.pdtb.pdtb': 0.4900891091645829, 'eval_recall@eng.pdtb.pdtb': 0.39551302857864673, 'eval_loss@eng.pdtb.pdtb': 1.0311379432678223, 'eval_runtime': 20.2153, 'eval_samples_per_second': 82.809, 'eval_steps_per_second': 2.622, 'epoch': 2.0}
{'train@eng.pdtb.pdtb_loss': 1.043570876121521, 'train@eng.pdtb.pdtb_accuracy@eng.pdtb.pdtb': 0.6553734061930784, 'train@eng.pdtb.pdtb_f1@eng.pdtb.pdtb': 0.4330377737207069, 'train@eng.pdtb.pdtb_precision@eng.pdtb.pdtb': 0.4722717924809993, 'train@eng.pdtb.pdtb_recall@eng.pdtb.pdtb': 0.4177859775995573, 'train@eng.pdtb.pdtb_loss@eng.pdtb.pdtb': 1.043570876121521, 'train@eng.pdtb.pdtb_runtime': 516.5289, 'train@eng.pdtb.pdtb_samples_per_second': 85.029, 'train@eng.pdtb.pdtb_steps_per_second': 2.658, 'epoch': 3.0}
{'loss': 1.116, 'learning_rate': 1.5000000000000002e-05, 'epoch': 3.0}
{'eval_loss': 0.9906619191169739, 'eval_accuracy@eng.pdtb.pdtb': 0.6804062126642771, 'eval_f1@eng.pdtb.pdtb': 0.4909737541283731, 'eval_precision@eng.pdtb.pdtb': 0.5639013967801059, 'eval_recall@eng.pdtb.pdtb': 0.46299540018166735, 'eval_loss@eng.pdtb.pdtb': 0.9906618595123291, 'eval_runtime': 20.2099, 'eval_samples_per_second': 82.831, 'eval_steps_per_second': 2.622, 'epoch': 3.0}
{'train@eng.pdtb.pdtb_loss': 1.0002832412719727, 'train@eng.pdtb.pdtb_accuracy@eng.pdtb.pdtb': 0.6668488160291439, 'train@eng.pdtb.pdtb_f1@eng.pdtb.pdtb': 0.4467759132363668, 'train@eng.pdtb.pdtb_precision@eng.pdtb.pdtb': 0.4719821284263562, 'train@eng.pdtb.pdtb_recall@eng.pdtb.pdtb': 0.44282542115292656, 'train@eng.pdtb.pdtb_loss@eng.pdtb.pdtb': 1.0002832412719727, 'train@eng.pdtb.pdtb_runtime': 516.2369, 'train@eng.pdtb.pdtb_samples_per_second': 85.077, 'train@eng.pdtb.pdtb_steps_per_second': 2.66, 'epoch': 4.0}
{'loss': 1.0729, 'learning_rate': 1.3333333333333333e-05, 'epoch': 4.0}
{'eval_loss': 0.952991247177124, 'eval_accuracy@eng.pdtb.pdtb': 0.6851851851851852, 'eval_f1@eng.pdtb.pdtb': 0.5059658912635262, 'eval_precision@eng.pdtb.pdtb': 0.5576198403641495, 'eval_recall@eng.pdtb.pdtb': 0.4930386857724656, 'eval_loss@eng.pdtb.pdtb': 0.952991247177124, 'eval_runtime': 20.2076, 'eval_samples_per_second': 82.84, 'eval_steps_per_second': 2.623, 'epoch': 4.0}
{'train@eng.pdtb.pdtb_loss': 0.977199375629425, 'train@eng.pdtb.pdtb_accuracy@eng.pdtb.pdtb': 0.6740437158469945, 'train@eng.pdtb.pdtb_f1@eng.pdtb.pdtb': 0.45711802290821174, 'train@eng.pdtb.pdtb_precision@eng.pdtb.pdtb': 0.5195926611099406, 'train@eng.pdtb.pdtb_recall@eng.pdtb.pdtb': 0.4546649997004773, 'train@eng.pdtb.pdtb_loss@eng.pdtb.pdtb': 0.9771993160247803, 'train@eng.pdtb.pdtb_runtime': 516.5261, 'train@eng.pdtb.pdtb_samples_per_second': 85.03, 'train@eng.pdtb.pdtb_steps_per_second': 2.658, 'epoch': 5.0}
{'loss': 1.0408, 'learning_rate': 1.1666666666666668e-05, 'epoch': 5.0}
{'eval_loss': 0.9375030398368835, 'eval_accuracy@eng.pdtb.pdtb': 0.6845878136200717, 'eval_f1@eng.pdtb.pdtb': 0.5236604792988088, 'eval_precision@eng.pdtb.pdtb': 0.558225865642992, 'eval_recall@eng.pdtb.pdtb': 0.5142986710651829, 'eval_loss@eng.pdtb.pdtb': 0.9375029802322388, 'eval_runtime': 20.9687, 'eval_samples_per_second': 79.833, 'eval_steps_per_second': 2.528, 'epoch': 5.0}
{'train@eng.pdtb.pdtb_loss': 0.958644449710846, 'train@eng.pdtb.pdtb_accuracy@eng.pdtb.pdtb': 0.6795537340619308, 'train@eng.pdtb.pdtb_f1@eng.pdtb.pdtb': 0.46318787606076833, 'train@eng.pdtb.pdtb_precision@eng.pdtb.pdtb': 0.5248024386280083, 'train@eng.pdtb.pdtb_recall@eng.pdtb.pdtb': 0.45997324179169413, 'train@eng.pdtb.pdtb_loss@eng.pdtb.pdtb': 0.9586445093154907, 'train@eng.pdtb.pdtb_runtime': 516.3263, 'train@eng.pdtb.pdtb_samples_per_second': 85.062, 'train@eng.pdtb.pdtb_steps_per_second': 2.659, 'epoch': 6.0}
{'loss': 1.0193, 'learning_rate': 1e-05, 'epoch': 6.0}
{'eval_loss': 0.9296883940696716, 'eval_accuracy@eng.pdtb.pdtb': 0.6857825567502986, 'eval_f1@eng.pdtb.pdtb': 0.5288417976863862, 'eval_precision@eng.pdtb.pdtb': 0.561849985318158, 'eval_recall@eng.pdtb.pdtb': 0.5172025041240697, 'eval_loss@eng.pdtb.pdtb': 0.9296883940696716, 'eval_runtime': 20.2433, 'eval_samples_per_second': 82.694, 'eval_steps_per_second': 2.618, 'epoch': 6.0}
{'train@eng.pdtb.pdtb_loss': 0.947329044342041, 'train@eng.pdtb.pdtb_accuracy@eng.pdtb.pdtb': 0.6804417122040073, 'train@eng.pdtb.pdtb_f1@eng.pdtb.pdtb': 0.46636228345497854, 'train@eng.pdtb.pdtb_precision@eng.pdtb.pdtb': 0.5330728194212063, 'train@eng.pdtb.pdtb_recall@eng.pdtb.pdtb': 0.4591208716338525, 'train@eng.pdtb.pdtb_loss@eng.pdtb.pdtb': 0.9473289251327515, 'train@eng.pdtb.pdtb_runtime': 516.0377, 'train@eng.pdtb.pdtb_samples_per_second': 85.11, 'train@eng.pdtb.pdtb_steps_per_second': 2.661, 'epoch': 7.0}
{'loss': 1.0052, 'learning_rate': 8.333333333333334e-06, 'epoch': 7.0}
{'eval_loss': 0.9162036776542664, 'eval_accuracy@eng.pdtb.pdtb': 0.6893667861409797, 'eval_f1@eng.pdtb.pdtb': 0.5305509722064952, 'eval_precision@eng.pdtb.pdtb': 0.5718845595194657, 'eval_recall@eng.pdtb.pdtb': 0.515340992520801, 'eval_loss@eng.pdtb.pdtb': 0.9162037372589111, 'eval_runtime': 20.1816, 'eval_samples_per_second': 82.947, 'eval_steps_per_second': 2.626, 'epoch': 7.0}
{'train@eng.pdtb.pdtb_loss': 0.9367822408676147, 'train@eng.pdtb.pdtb_accuracy@eng.pdtb.pdtb': 0.6856785063752276, 'train@eng.pdtb.pdtb_f1@eng.pdtb.pdtb': 0.4727264485652039, 'train@eng.pdtb.pdtb_precision@eng.pdtb.pdtb': 0.5561490311789326, 'train@eng.pdtb.pdtb_recall@eng.pdtb.pdtb': 0.4704031445775417, 'train@eng.pdtb.pdtb_loss@eng.pdtb.pdtb': 0.9367822408676147, 'train@eng.pdtb.pdtb_runtime': 516.4453, 'train@eng.pdtb.pdtb_samples_per_second': 85.043, 'train@eng.pdtb.pdtb_steps_per_second': 2.659, 'epoch': 8.0}
{'loss': 0.9943, 'learning_rate': 6.666666666666667e-06, 'epoch': 8.0}
{'eval_loss': 0.9139531850814819, 'eval_accuracy@eng.pdtb.pdtb': 0.6929510155316607, 'eval_f1@eng.pdtb.pdtb': 0.5335654064805391, 'eval_precision@eng.pdtb.pdtb': 0.5642887351130612, 'eval_recall@eng.pdtb.pdtb': 0.5233759805368998, 'eval_loss@eng.pdtb.pdtb': 0.9139530658721924, 'eval_runtime': 20.2279, 'eval_samples_per_second': 82.757, 'eval_steps_per_second': 2.62, 'epoch': 8.0}
{'train@eng.pdtb.pdtb_loss': 0.9274441003799438, 'train@eng.pdtb.pdtb_accuracy@eng.pdtb.pdtb': 0.6875910746812386, 'train@eng.pdtb.pdtb_f1@eng.pdtb.pdtb': 0.4769032042458472, 'train@eng.pdtb.pdtb_precision@eng.pdtb.pdtb': 0.5781522931927315, 'train@eng.pdtb.pdtb_recall@eng.pdtb.pdtb': 0.4714748040650778, 'train@eng.pdtb.pdtb_loss@eng.pdtb.pdtb': 0.9274441599845886, 'train@eng.pdtb.pdtb_runtime': 516.2483, 'train@eng.pdtb.pdtb_samples_per_second': 85.075, 'train@eng.pdtb.pdtb_steps_per_second': 2.66, 'epoch': 9.0}
{'loss': 0.9837, 'learning_rate': 5e-06, 'epoch': 9.0}
{'eval_loss': 0.9103703498840332, 'eval_accuracy@eng.pdtb.pdtb': 0.6875746714456392, 'eval_f1@eng.pdtb.pdtb': 0.5292178450816631, 'eval_precision@eng.pdtb.pdtb': 0.5644492110144486, 'eval_recall@eng.pdtb.pdtb': 0.5165505412582025, 'eval_loss@eng.pdtb.pdtb': 0.9103703498840332, 'eval_runtime': 20.1557, 'eval_samples_per_second': 83.054, 'eval_steps_per_second': 2.63, 'epoch': 9.0}
{'train@eng.pdtb.pdtb_loss': 0.9244726896286011, 'train@eng.pdtb.pdtb_accuracy@eng.pdtb.pdtb': 0.6888205828779599, 'train@eng.pdtb.pdtb_f1@eng.pdtb.pdtb': 0.4936993532547914, 'train@eng.pdtb.pdtb_precision@eng.pdtb.pdtb': 0.5812338665230442, 'train@eng.pdtb.pdtb_recall@eng.pdtb.pdtb': 0.482041963005769, 'train@eng.pdtb.pdtb_loss@eng.pdtb.pdtb': 0.9244726896286011, 'train@eng.pdtb.pdtb_runtime': 515.93, 'train@eng.pdtb.pdtb_samples_per_second': 85.128, 'train@eng.pdtb.pdtb_steps_per_second': 2.661, 'epoch': 10.0}
{'loss': 0.979, 'learning_rate': 3.3333333333333333e-06, 'epoch': 10.0}
{'eval_loss': 0.9053236246109009, 'eval_accuracy@eng.pdtb.pdtb': 0.6869772998805257, 'eval_f1@eng.pdtb.pdtb': 0.5399461969056522, 'eval_precision@eng.pdtb.pdtb': 0.5745847876830699, 'eval_recall@eng.pdtb.pdtb': 0.526620303954729, 'eval_loss@eng.pdtb.pdtb': 0.9053236246109009, 'eval_runtime': 20.1986, 'eval_samples_per_second': 82.877, 'eval_steps_per_second': 2.624, 'epoch': 10.0}
{'train@eng.pdtb.pdtb_loss': 0.9199737310409546, 'train@eng.pdtb.pdtb_accuracy@eng.pdtb.pdtb': 0.6897768670309654, 'train@eng.pdtb.pdtb_f1@eng.pdtb.pdtb': 0.49513338358431136, 'train@eng.pdtb.pdtb_precision@eng.pdtb.pdtb': 0.5756534466476867, 'train@eng.pdtb.pdtb_recall@eng.pdtb.pdtb': 0.485692754930891, 'train@eng.pdtb.pdtb_loss@eng.pdtb.pdtb': 0.9199737906455994, 'train@eng.pdtb.pdtb_runtime': 516.9296, 'train@eng.pdtb.pdtb_samples_per_second': 84.963, 'train@eng.pdtb.pdtb_steps_per_second': 2.656, 'epoch': 11.0}
{'loss': 0.9711, 'learning_rate': 1.6666666666666667e-06, 'epoch': 11.0}
{'eval_loss': 0.9046506285667419, 'eval_accuracy@eng.pdtb.pdtb': 0.6965352449223416, 'eval_f1@eng.pdtb.pdtb': 0.5549447552828486, 'eval_precision@eng.pdtb.pdtb': 0.5791561490108379, 'eval_recall@eng.pdtb.pdtb': 0.544983814971311, 'eval_loss@eng.pdtb.pdtb': 0.9046506285667419, 'eval_runtime': 21.2637, 'eval_samples_per_second': 78.726, 'eval_steps_per_second': 2.493, 'epoch': 11.0}
{'train@eng.pdtb.pdtb_loss': 0.9187355637550354, 'train@eng.pdtb.pdtb_accuracy@eng.pdtb.pdtb': 0.6903233151183971, 'train@eng.pdtb.pdtb_f1@eng.pdtb.pdtb': 0.4946903430417362, 'train@eng.pdtb.pdtb_precision@eng.pdtb.pdtb': 0.5731898356067443, 'train@eng.pdtb.pdtb_recall@eng.pdtb.pdtb': 0.4845525200844219, 'train@eng.pdtb.pdtb_loss@eng.pdtb.pdtb': 0.9187355637550354, 'train@eng.pdtb.pdtb_runtime': 516.7001, 'train@eng.pdtb.pdtb_samples_per_second': 85.001, 'train@eng.pdtb.pdtb_steps_per_second': 2.657, 'epoch': 12.0}
{'loss': 0.967, 'learning_rate': 0.0, 'epoch': 12.0}
{'eval_loss': 0.9031209945678711, 'eval_accuracy@eng.pdtb.pdtb': 0.6947431302270012, 'eval_f1@eng.pdtb.pdtb': 0.5506748205105596, 'eval_precision@eng.pdtb.pdtb': 0.5769460337788885, 'eval_recall@eng.pdtb.pdtb': 0.5411095922708802, 'eval_loss@eng.pdtb.pdtb': 0.9031209945678711, 'eval_runtime': 20.1779, 'eval_samples_per_second': 82.962, 'eval_steps_per_second': 2.627, 'epoch': 12.0}
{'train_runtime': 19562.9162, 'train_samples_per_second': 26.941, 'train_steps_per_second': 0.842, 'train_loss': 1.0974830947879681, 'epoch': 12.0}
***** train_logger metrics *****
  epoch                    =       12.0
  train_loss               =     2.8789
  train_runtime            = 0:13:03.96
  train_samples_per_second =     24.613
  train_steps_per_second   =      0.781
-------------------------------------------------------------------
Lang1:  por.rst.cstn    Lang2:  eng.pdtb.pdtb
Saving run to:  runs/full_shot/FullShot=v4_por.rst.cstn_eng.pdtb.pdtb_lrfunc=Adafactor_lr=2e-05_plm=bert-base-multilingual-cased
Running with params: BERT_MODEL=bert-base-multilingual-cased lr=2e-05
read 4148 examples
read 573 examples
read 272 examples
read 43920 examples
read 1674 examples
read 2257 examples
Total prediction labels:  55
BertAdapterModel(
  (shared_parameters): ModuleDict()
  (bert): BertModel(
    (shared_parameters): ModuleDict()
    (invertible_adapters): ModuleDict(
      (pt): NICECouplingBlock(
        (F): Sequential(
          (0): Linear(in_features=384, out_features=192, bias=True)
          (1): Activation_Function_Class(
            (f): NewGELUActivation()
          )
          (2): Linear(in_features=192, out_features=384, bias=True)
        )
        (G): Sequential(
          (0): Linear(in_features=384, out_features=192, bias=True)
          (1): Activation_Function_Class(
            (f): NewGELUActivation()
          )
          (2): Linear(in_features=192, out_features=384, bias=True)
        )
      )
      (en): NICECouplingBlock(
        (F): Sequential(
          (0): Linear(in_features=384, out_features=192, bias=True)
          (1): Activation_Function_Class(
            (f): ReLU()
          )
          (2): Linear(in_features=192, out_features=384, bias=True)
        )
        (G): Sequential(
          (0): Linear(in_features=384, out_features=192, bias=True)
          (1): Activation_Function_Class(
            (f): ReLU()
          )
          (2): Linear(in_features=192, out_features=384, bias=True)
        )
      )
    )
    (embeddings): BertEmbeddings(
      (word_embeddings): Embedding(119547, 768, padding_idx=0)
      (position_embeddings): Embedding(512, 768)
      (token_type_embeddings): Embedding(2, 768)
      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
      (dropout): Dropout(p=0.1, inplace=False)
    )
    (encoder): BertEncoder(
      (layer): ModuleList(
        (0): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (pt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (en): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (1): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (pt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (en): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (2): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (pt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (en): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (3): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (pt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (en): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (4): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (pt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (en): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (5): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (pt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (en): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (6): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (pt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (en): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (7): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (pt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (en): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (8): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (pt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (en): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (9): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (pt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (en): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (10): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (pt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (en): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (11): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (pt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (en): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
      )
    )
    (pooler): BertPooler(
      (dense): Linear(in_features=768, out_features=768, bias=True)
      (activation): Tanh()
    )
    (prefix_tuning): PrefixTuningPool(
      (prefix_tunings): ModuleDict()
    )
  )
  (heads): ModuleDict(
    (pt): BertStyleMaskedLMHead(
      (0): Linear(in_features=768, out_features=768, bias=True)
      (1): Activation_Function_Class(
        (f): GELUActivation()
      )
      (2): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
      (3): Linear(in_features=768, out_features=119547, bias=True)
    )
    (disrpt): ClassificationHead(
      (0): Dropout(p=0.1, inplace=False)
      (1): Linear(in_features=768, out_features=768, bias=True)
      (2): Activation_Function_Class(
        (f): Tanh()
      )
      (3): Dropout(p=0.1, inplace=False)
      (4): Linear(in_features=768, out_features=55, bias=True)
    )
  )
)
{'train@por.rst.cstn_loss': 2.55271053314209, 'train@por.rst.cstn_accuracy@por.rst.cstn': 0.2774831243972999, 'train@por.rst.cstn_f1@por.rst.cstn': 0.013575674655595393, 'train@por.rst.cstn_precision@por.rst.cstn': 0.008671347637415621, 'train@por.rst.cstn_recall@por.rst.cstn': 0.03125, 'train@por.rst.cstn_loss@por.rst.cstn': 2.55271053314209, 'train@por.rst.cstn_runtime': 49.9784, 'train@por.rst.cstn_samples_per_second': 82.996, 'train@por.rst.cstn_steps_per_second': 2.601, 'epoch': 1.0}
{'loss': 3.207, 'learning_rate': 1.8333333333333333e-05, 'epoch': 1.0}
{'eval_loss': 2.666320323944092, 'eval_accuracy@por.rst.cstn': 0.28097731239092494, 'eval_f1@por.rst.cstn': 0.019940549913301957, 'eval_precision@por.rst.cstn': 0.012771696017769315, 'eval_recall@por.rst.cstn': 0.045454545454545456, 'eval_loss@por.rst.cstn': 2.6663200855255127, 'eval_runtime': 7.3716, 'eval_samples_per_second': 77.731, 'eval_steps_per_second': 2.442, 'epoch': 1.0}
{'train@por.rst.cstn_loss': 2.2785086631774902, 'train@por.rst.cstn_accuracy@por.rst.cstn': 0.35776277724204436, 'train@por.rst.cstn_f1@por.rst.cstn': 0.04768490549302118, 'train@por.rst.cstn_precision@por.rst.cstn': 0.06768302497220191, 'train@por.rst.cstn_recall@por.rst.cstn': 0.05874172804927555, 'train@por.rst.cstn_loss@por.rst.cstn': 2.278508424758911, 'train@por.rst.cstn_runtime': 49.9601, 'train@por.rst.cstn_samples_per_second': 83.026, 'train@por.rst.cstn_steps_per_second': 2.602, 'epoch': 2.0}
{'loss': 2.4409, 'learning_rate': 1.6666666666666667e-05, 'epoch': 2.0}
{'eval_loss': 2.383815050125122, 'eval_accuracy@por.rst.cstn': 0.32111692844677137, 'eval_f1@por.rst.cstn': 0.06279819471308833, 'eval_precision@por.rst.cstn': 0.09906045751633986, 'eval_recall@por.rst.cstn': 0.08091217083311944, 'eval_loss@por.rst.cstn': 2.383815050125122, 'eval_runtime': 7.3818, 'eval_samples_per_second': 77.624, 'eval_steps_per_second': 2.438, 'epoch': 2.0}
{'train@por.rst.cstn_loss': 2.0752756595611572, 'train@por.rst.cstn_accuracy@por.rst.cstn': 0.4177917068466731, 'train@por.rst.cstn_f1@por.rst.cstn': 0.06436252029220153, 'train@por.rst.cstn_precision@por.rst.cstn': 0.08153342844869907, 'train@por.rst.cstn_recall@por.rst.cstn': 0.0773914012481751, 'train@por.rst.cstn_loss@por.rst.cstn': 2.0752758979797363, 'train@por.rst.cstn_runtime': 49.8753, 'train@por.rst.cstn_samples_per_second': 83.167, 'train@por.rst.cstn_steps_per_second': 2.607, 'epoch': 3.0}
{'loss': 2.2244, 'learning_rate': 1.5000000000000002e-05, 'epoch': 3.0}
{'eval_loss': 2.195826530456543, 'eval_accuracy@por.rst.cstn': 0.3525305410122164, 'eval_f1@por.rst.cstn': 0.09105035142968892, 'eval_precision@por.rst.cstn': 0.0842401738886114, 'eval_recall@por.rst.cstn': 0.11366416850606574, 'eval_loss@por.rst.cstn': 2.195826530456543, 'eval_runtime': 7.3503, 'eval_samples_per_second': 77.956, 'eval_steps_per_second': 2.449, 'epoch': 3.0}
{'train@por.rst.cstn_loss': 1.8999532461166382, 'train@por.rst.cstn_accuracy@por.rst.cstn': 0.5113307618129219, 'train@por.rst.cstn_f1@por.rst.cstn': 0.08604665441128614, 'train@por.rst.cstn_precision@por.rst.cstn': 0.10731708387839087, 'train@por.rst.cstn_recall@por.rst.cstn': 0.09978573500428342, 'train@por.rst.cstn_loss@por.rst.cstn': 1.8999532461166382, 'train@por.rst.cstn_runtime': 49.9542, 'train@por.rst.cstn_samples_per_second': 83.036, 'train@por.rst.cstn_steps_per_second': 2.602, 'epoch': 4.0}
{'loss': 2.0452, 'learning_rate': 1.3333333333333333e-05, 'epoch': 4.0}
{'eval_loss': 2.0440115928649902, 'eval_accuracy@por.rst.cstn': 0.4205933682373473, 'eval_f1@por.rst.cstn': 0.11316711104363751, 'eval_precision@por.rst.cstn': 0.09990980387937927, 'eval_recall@por.rst.cstn': 0.13907953048269253, 'eval_loss@por.rst.cstn': 2.0440115928649902, 'eval_runtime': 7.3489, 'eval_samples_per_second': 77.971, 'eval_steps_per_second': 2.449, 'epoch': 4.0}
{'train@por.rst.cstn_loss': 1.7739888429641724, 'train@por.rst.cstn_accuracy@por.rst.cstn': 0.529893924783028, 'train@por.rst.cstn_f1@por.rst.cstn': 0.10739639019406835, 'train@por.rst.cstn_precision@por.rst.cstn': 0.10396223498090855, 'train@por.rst.cstn_recall@por.rst.cstn': 0.11881542416511427, 'train@por.rst.cstn_loss@por.rst.cstn': 1.7739887237548828, 'train@por.rst.cstn_runtime': 49.9683, 'train@por.rst.cstn_samples_per_second': 83.013, 'train@por.rst.cstn_steps_per_second': 2.602, 'epoch': 5.0}
{'loss': 1.8939, 'learning_rate': 1.1666666666666668e-05, 'epoch': 5.0}
{'eval_loss': 1.9258158206939697, 'eval_accuracy@por.rst.cstn': 0.4293193717277487, 'eval_f1@por.rst.cstn': 0.1330913106032723, 'eval_precision@por.rst.cstn': 0.12797550910784405, 'eval_recall@por.rst.cstn': 0.15820648158904868, 'eval_loss@por.rst.cstn': 1.9258159399032593, 'eval_runtime': 7.3582, 'eval_samples_per_second': 77.872, 'eval_steps_per_second': 2.446, 'epoch': 5.0}
{'train@por.rst.cstn_loss': 1.7010444402694702, 'train@por.rst.cstn_accuracy@por.rst.cstn': 0.5503857280617165, 'train@por.rst.cstn_f1@por.rst.cstn': 0.11231917045702541, 'train@por.rst.cstn_precision@por.rst.cstn': 0.13187698237031176, 'train@por.rst.cstn_recall@por.rst.cstn': 0.12850803837333746, 'train@por.rst.cstn_loss@por.rst.cstn': 1.7010443210601807, 'train@por.rst.cstn_runtime': 49.9398, 'train@por.rst.cstn_samples_per_second': 83.06, 'train@por.rst.cstn_steps_per_second': 2.603, 'epoch': 6.0}
{'loss': 1.8002, 'learning_rate': 1e-05, 'epoch': 6.0}
{'eval_loss': 1.8523520231246948, 'eval_accuracy@por.rst.cstn': 0.44851657940663175, 'eval_f1@por.rst.cstn': 0.14594130499164615, 'eval_precision@por.rst.cstn': 0.13333818790295152, 'eval_recall@por.rst.cstn': 0.17302977169837996, 'eval_loss@por.rst.cstn': 1.852352261543274, 'eval_runtime': 7.385, 'eval_samples_per_second': 77.59, 'eval_steps_per_second': 2.437, 'epoch': 6.0}
{'train@por.rst.cstn_loss': 1.6510199308395386, 'train@por.rst.cstn_accuracy@por.rst.cstn': 0.5571359691417551, 'train@por.rst.cstn_f1@por.rst.cstn': 0.11574455048506997, 'train@por.rst.cstn_precision@por.rst.cstn': 0.12237914571492095, 'train@por.rst.cstn_recall@por.rst.cstn': 0.13211113101190358, 'train@por.rst.cstn_loss@por.rst.cstn': 1.651019811630249, 'train@por.rst.cstn_runtime': 49.9691, 'train@por.rst.cstn_samples_per_second': 83.011, 'train@por.rst.cstn_steps_per_second': 2.602, 'epoch': 7.0}
{'loss': 1.7341, 'learning_rate': 8.333333333333334e-06, 'epoch': 7.0}
{'eval_loss': 1.7989985942840576, 'eval_accuracy@por.rst.cstn': 0.45898778359511344, 'eval_f1@por.rst.cstn': 0.15018055059735055, 'eval_precision@por.rst.cstn': 0.14029267700697526, 'eval_recall@por.rst.cstn': 0.1772731700493298, 'eval_loss@por.rst.cstn': 1.7989983558654785, 'eval_runtime': 7.3631, 'eval_samples_per_second': 77.82, 'eval_steps_per_second': 2.445, 'epoch': 7.0}
{'train@por.rst.cstn_loss': 1.6201591491699219, 'train@por.rst.cstn_accuracy@por.rst.cstn': 0.5624397299903569, 'train@por.rst.cstn_f1@por.rst.cstn': 0.12007651192799601, 'train@por.rst.cstn_precision@por.rst.cstn': 0.12481484719510988, 'train@por.rst.cstn_recall@por.rst.cstn': 0.13534224455635138, 'train@por.rst.cstn_loss@por.rst.cstn': 1.620159387588501, 'train@por.rst.cstn_runtime': 49.9042, 'train@por.rst.cstn_samples_per_second': 83.119, 'train@por.rst.cstn_steps_per_second': 2.605, 'epoch': 8.0}
{'loss': 1.6978, 'learning_rate': 6.666666666666667e-06, 'epoch': 8.0}
{'eval_loss': 1.7716089487075806, 'eval_accuracy@por.rst.cstn': 0.4642233856893543, 'eval_f1@por.rst.cstn': 0.15982370925900682, 'eval_precision@por.rst.cstn': 0.15468821660682128, 'eval_recall@por.rst.cstn': 0.18282550609365672, 'eval_loss@por.rst.cstn': 1.7716089487075806, 'eval_runtime': 7.3667, 'eval_samples_per_second': 77.783, 'eval_steps_per_second': 2.443, 'epoch': 8.0}
{'train@por.rst.cstn_loss': 1.5977883338928223, 'train@por.rst.cstn_accuracy@por.rst.cstn': 0.567261330761813, 'train@por.rst.cstn_f1@por.rst.cstn': 0.12392045162934533, 'train@por.rst.cstn_precision@por.rst.cstn': 0.13138340900769122, 'train@por.rst.cstn_recall@por.rst.cstn': 0.13760247079323937, 'train@por.rst.cstn_loss@por.rst.cstn': 1.5977885723114014, 'train@por.rst.cstn_runtime': 49.9206, 'train@por.rst.cstn_samples_per_second': 83.092, 'train@por.rst.cstn_steps_per_second': 2.604, 'epoch': 9.0}
{'loss': 1.67, 'learning_rate': 5e-06, 'epoch': 9.0}
{'eval_loss': 1.7460238933563232, 'eval_accuracy@por.rst.cstn': 0.4694589877835951, 'eval_f1@por.rst.cstn': 0.16386029992841136, 'eval_precision@por.rst.cstn': 0.17745524281311656, 'eval_recall@por.rst.cstn': 0.18527927271166297, 'eval_loss@por.rst.cstn': 1.7460238933563232, 'eval_runtime': 7.3428, 'eval_samples_per_second': 78.035, 'eval_steps_per_second': 2.451, 'epoch': 9.0}
{'train@por.rst.cstn_loss': 1.5808000564575195, 'train@por.rst.cstn_accuracy@por.rst.cstn': 0.5728061716489875, 'train@por.rst.cstn_f1@por.rst.cstn': 0.12848977661320687, 'train@por.rst.cstn_precision@por.rst.cstn': 0.13342167477257677, 'train@por.rst.cstn_recall@por.rst.cstn': 0.14254648320465851, 'train@por.rst.cstn_loss@por.rst.cstn': 1.58079993724823, 'train@por.rst.cstn_runtime': 49.911, 'train@por.rst.cstn_samples_per_second': 83.108, 'train@por.rst.cstn_steps_per_second': 2.605, 'epoch': 10.0}
{'loss': 1.6379, 'learning_rate': 3.3333333333333333e-06, 'epoch': 10.0}
{'eval_loss': 1.727315068244934, 'eval_accuracy@por.rst.cstn': 0.49214659685863876, 'eval_f1@por.rst.cstn': 0.17644983440891526, 'eval_precision@por.rst.cstn': 0.18484145980420197, 'eval_recall@por.rst.cstn': 0.19843717495319327, 'eval_loss@por.rst.cstn': 1.727315068244934, 'eval_runtime': 7.3602, 'eval_samples_per_second': 77.851, 'eval_steps_per_second': 2.446, 'epoch': 10.0}
{'train@por.rst.cstn_loss': 1.5716921091079712, 'train@por.rst.cstn_accuracy@por.rst.cstn': 0.5759402121504339, 'train@por.rst.cstn_f1@por.rst.cstn': 0.1306004822913704, 'train@por.rst.cstn_precision@por.rst.cstn': 0.1370712084370822, 'train@por.rst.cstn_recall@por.rst.cstn': 0.14387765473588293, 'train@por.rst.cstn_loss@por.rst.cstn': 1.5716921091079712, 'train@por.rst.cstn_runtime': 49.9468, 'train@por.rst.cstn_samples_per_second': 83.048, 'train@por.rst.cstn_steps_per_second': 2.603, 'epoch': 11.0}
{'loss': 1.6336, 'learning_rate': 1.6666666666666667e-06, 'epoch': 11.0}
{'eval_loss': 1.719401240348816, 'eval_accuracy@por.rst.cstn': 0.493891797556719, 'eval_f1@por.rst.cstn': 0.17710894229703167, 'eval_precision@por.rst.cstn': 0.18121020323253315, 'eval_recall@por.rst.cstn': 0.1992261578307969, 'eval_loss@por.rst.cstn': 1.7194011211395264, 'eval_runtime': 7.3582, 'eval_samples_per_second': 77.873, 'eval_steps_per_second': 2.446, 'epoch': 11.0}
{'train@por.rst.cstn_loss': 1.5686851739883423, 'train@por.rst.cstn_accuracy@por.rst.cstn': 0.5761812921890067, 'train@por.rst.cstn_f1@por.rst.cstn': 0.1312222905159652, 'train@por.rst.cstn_precision@por.rst.cstn': 0.137291493735549, 'train@por.rst.cstn_recall@por.rst.cstn': 0.14405081853712515, 'train@por.rst.cstn_loss@por.rst.cstn': 1.5686850547790527, 'train@por.rst.cstn_runtime': 49.9651, 'train@por.rst.cstn_samples_per_second': 83.018, 'train@por.rst.cstn_steps_per_second': 2.602, 'epoch': 12.0}
{'loss': 1.626, 'learning_rate': 0.0, 'epoch': 12.0}
{'eval_loss': 1.715519666671753, 'eval_accuracy@por.rst.cstn': 0.49214659685863876, 'eval_f1@por.rst.cstn': 0.17677596343345156, 'eval_precision@por.rst.cstn': 0.18224477127633512, 'eval_recall@por.rst.cstn': 0.19784874736247735, 'eval_loss@por.rst.cstn': 1.7155195474624634, 'eval_runtime': 7.3712, 'eval_samples_per_second': 77.735, 'eval_steps_per_second': 2.442, 'epoch': 12.0}
{'train_runtime': 1954.8829, 'train_samples_per_second': 25.462, 'train_steps_per_second': 0.798, 'train_loss': 1.967588141025641, 'epoch': 12.0}
***** train_log metrics *****
  epoch                    =       12.0
  train_loss               =     1.9676
  train_runtime            = 0:32:34.88
  train_samples_per_second =     25.462
  train_steps_per_second   =      0.798
{'train@eng.pdtb.pdtb_loss': 1.2404793500900269, 'train@eng.pdtb.pdtb_accuracy@eng.pdtb.pdtb': 0.6015255009107469, 'train@eng.pdtb.pdtb_f1@eng.pdtb.pdtb': 0.28218385180235517, 'train@eng.pdtb.pdtb_precision@eng.pdtb.pdtb': 0.3813874667528471, 'train@eng.pdtb.pdtb_recall@eng.pdtb.pdtb': 0.2741657227426398, 'train@eng.pdtb.pdtb_loss@eng.pdtb.pdtb': 1.2404792308807373, 'train@eng.pdtb.pdtb_runtime': 515.6585, 'train@eng.pdtb.pdtb_samples_per_second': 85.173, 'train@eng.pdtb.pdtb_steps_per_second': 2.663, 'epoch': 1.0}
{'loss': 1.7076, 'learning_rate': 1.8333333333333333e-05, 'epoch': 1.0}
{'eval_loss': 1.1641827821731567, 'eval_accuracy@eng.pdtb.pdtb': 0.6326164874551972, 'eval_f1@eng.pdtb.pdtb': 0.31666346277196916, 'eval_precision@eng.pdtb.pdtb': 0.3546653712492852, 'eval_recall@eng.pdtb.pdtb': 0.31722294274745855, 'eval_loss@eng.pdtb.pdtb': 1.1641826629638672, 'eval_runtime': 20.126, 'eval_samples_per_second': 83.176, 'eval_steps_per_second': 2.633, 'epoch': 1.0}
{'train@eng.pdtb.pdtb_loss': 1.0958199501037598, 'train@eng.pdtb.pdtb_accuracy@eng.pdtb.pdtb': 0.6383879781420765, 'train@eng.pdtb.pdtb_f1@eng.pdtb.pdtb': 0.36011878742506215, 'train@eng.pdtb.pdtb_precision@eng.pdtb.pdtb': 0.48968429299602184, 'train@eng.pdtb.pdtb_recall@eng.pdtb.pdtb': 0.3473757089538459, 'train@eng.pdtb.pdtb_loss@eng.pdtb.pdtb': 1.0958199501037598, 'train@eng.pdtb.pdtb_runtime': 515.4635, 'train@eng.pdtb.pdtb_samples_per_second': 85.205, 'train@eng.pdtb.pdtb_steps_per_second': 2.664, 'epoch': 2.0}
{'loss': 1.2143, 'learning_rate': 1.6666666666666667e-05, 'epoch': 2.0}
{'eval_loss': 1.0329077243804932, 'eval_accuracy@eng.pdtb.pdtb': 0.6684587813620072, 'eval_f1@eng.pdtb.pdtb': 0.4239402437461828, 'eval_precision@eng.pdtb.pdtb': 0.48004779794577435, 'eval_recall@eng.pdtb.pdtb': 0.4123043363118576, 'eval_loss@eng.pdtb.pdtb': 1.0329076051712036, 'eval_runtime': 20.1238, 'eval_samples_per_second': 83.185, 'eval_steps_per_second': 2.634, 'epoch': 2.0}
{'train@eng.pdtb.pdtb_loss': 1.0461113452911377, 'train@eng.pdtb.pdtb_accuracy@eng.pdtb.pdtb': 0.6533242258652094, 'train@eng.pdtb.pdtb_f1@eng.pdtb.pdtb': 0.43045189810629536, 'train@eng.pdtb.pdtb_precision@eng.pdtb.pdtb': 0.4711264460291154, 'train@eng.pdtb.pdtb_recall@eng.pdtb.pdtb': 0.4108188407581536, 'train@eng.pdtb.pdtb_loss@eng.pdtb.pdtb': 1.0461115837097168, 'train@eng.pdtb.pdtb_runtime': 515.3845, 'train@eng.pdtb.pdtb_samples_per_second': 85.218, 'train@eng.pdtb.pdtb_steps_per_second': 2.664, 'epoch': 3.0}
{'loss': 1.1215, 'learning_rate': 1.5000000000000002e-05, 'epoch': 3.0}
{'eval_loss': 0.9933444261550903, 'eval_accuracy@eng.pdtb.pdtb': 0.6827956989247311, 'eval_f1@eng.pdtb.pdtb': 0.467657869168966, 'eval_precision@eng.pdtb.pdtb': 0.5115141319077436, 'eval_recall@eng.pdtb.pdtb': 0.4527185167563415, 'eval_loss@eng.pdtb.pdtb': 0.9933443069458008, 'eval_runtime': 20.1541, 'eval_samples_per_second': 83.06, 'eval_steps_per_second': 2.63, 'epoch': 3.0}
{'train@eng.pdtb.pdtb_loss': 1.000800609588623, 'train@eng.pdtb.pdtb_accuracy@eng.pdtb.pdtb': 0.666188524590164, 'train@eng.pdtb.pdtb_f1@eng.pdtb.pdtb': 0.4475214156123884, 'train@eng.pdtb.pdtb_precision@eng.pdtb.pdtb': 0.4773811845894035, 'train@eng.pdtb.pdtb_recall@eng.pdtb.pdtb': 0.43480082458221164, 'train@eng.pdtb.pdtb_loss@eng.pdtb.pdtb': 1.000800609588623, 'train@eng.pdtb.pdtb_runtime': 516.0559, 'train@eng.pdtb.pdtb_samples_per_second': 85.107, 'train@eng.pdtb.pdtb_steps_per_second': 2.661, 'epoch': 4.0}
{'loss': 1.0758, 'learning_rate': 1.3333333333333333e-05, 'epoch': 4.0}
{'eval_loss': 0.9541218876838684, 'eval_accuracy@eng.pdtb.pdtb': 0.6798088410991637, 'eval_f1@eng.pdtb.pdtb': 0.48378771853030916, 'eval_precision@eng.pdtb.pdtb': 0.5446328868825905, 'eval_recall@eng.pdtb.pdtb': 0.4607130895944082, 'eval_loss@eng.pdtb.pdtb': 0.9541218876838684, 'eval_runtime': 20.1865, 'eval_samples_per_second': 82.927, 'eval_steps_per_second': 2.626, 'epoch': 4.0}
{'train@eng.pdtb.pdtb_loss': 0.9778839945793152, 'train@eng.pdtb.pdtb_accuracy@eng.pdtb.pdtb': 0.6721311475409836, 'train@eng.pdtb.pdtb_f1@eng.pdtb.pdtb': 0.45421732706810863, 'train@eng.pdtb.pdtb_precision@eng.pdtb.pdtb': 0.520675092793991, 'train@eng.pdtb.pdtb_recall@eng.pdtb.pdtb': 0.44628591545407376, 'train@eng.pdtb.pdtb_loss@eng.pdtb.pdtb': 0.9778839945793152, 'train@eng.pdtb.pdtb_runtime': 515.7521, 'train@eng.pdtb.pdtb_samples_per_second': 85.157, 'train@eng.pdtb.pdtb_steps_per_second': 2.662, 'epoch': 5.0}
{'loss': 1.0461, 'learning_rate': 1.1666666666666668e-05, 'epoch': 5.0}
{'eval_loss': 0.9351977109909058, 'eval_accuracy@eng.pdtb.pdtb': 0.6857825567502986, 'eval_f1@eng.pdtb.pdtb': 0.511541369253249, 'eval_precision@eng.pdtb.pdtb': 0.5448113278066671, 'eval_recall@eng.pdtb.pdtb': 0.4978631104829872, 'eval_loss@eng.pdtb.pdtb': 0.9351978302001953, 'eval_runtime': 20.1835, 'eval_samples_per_second': 82.939, 'eval_steps_per_second': 2.626, 'epoch': 5.0}
{'train@eng.pdtb.pdtb_loss': 0.9592639803886414, 'train@eng.pdtb.pdtb_accuracy@eng.pdtb.pdtb': 0.677891621129326, 'train@eng.pdtb.pdtb_f1@eng.pdtb.pdtb': 0.4610330014533827, 'train@eng.pdtb.pdtb_precision@eng.pdtb.pdtb': 0.5261151252895554, 'train@eng.pdtb.pdtb_recall@eng.pdtb.pdtb': 0.452645101711341, 'train@eng.pdtb.pdtb_loss@eng.pdtb.pdtb': 0.9592640995979309, 'train@eng.pdtb.pdtb_runtime': 515.6349, 'train@eng.pdtb.pdtb_samples_per_second': 85.177, 'train@eng.pdtb.pdtb_steps_per_second': 2.663, 'epoch': 6.0}
{'loss': 1.0248, 'learning_rate': 1e-05, 'epoch': 6.0}
{'eval_loss': 0.9287770986557007, 'eval_accuracy@eng.pdtb.pdtb': 0.6851851851851852, 'eval_f1@eng.pdtb.pdtb': 0.5185873851817158, 'eval_precision@eng.pdtb.pdtb': 0.5579737405386839, 'eval_recall@eng.pdtb.pdtb': 0.5043171816817451, 'eval_loss@eng.pdtb.pdtb': 0.9287771582603455, 'eval_runtime': 20.1269, 'eval_samples_per_second': 83.172, 'eval_steps_per_second': 2.633, 'epoch': 6.0}
{'train@eng.pdtb.pdtb_loss': 0.9477819204330444, 'train@eng.pdtb.pdtb_accuracy@eng.pdtb.pdtb': 0.6802140255009107, 'train@eng.pdtb.pdtb_f1@eng.pdtb.pdtb': 0.46502498663700387, 'train@eng.pdtb.pdtb_precision@eng.pdtb.pdtb': 0.5341430030455698, 'train@eng.pdtb.pdtb_recall@eng.pdtb.pdtb': 0.4539954139927423, 'train@eng.pdtb.pdtb_loss@eng.pdtb.pdtb': 0.9477819204330444, 'train@eng.pdtb.pdtb_runtime': 515.6034, 'train@eng.pdtb.pdtb_samples_per_second': 85.182, 'train@eng.pdtb.pdtb_steps_per_second': 2.663, 'epoch': 7.0}
{'loss': 1.0087, 'learning_rate': 8.333333333333334e-06, 'epoch': 7.0}
{'eval_loss': 0.9153923392295837, 'eval_accuracy@eng.pdtb.pdtb': 0.6905615292712067, 'eval_f1@eng.pdtb.pdtb': 0.5395190508232537, 'eval_precision@eng.pdtb.pdtb': 0.6301550031461736, 'eval_recall@eng.pdtb.pdtb': 0.5143691619198595, 'eval_loss@eng.pdtb.pdtb': 0.915392279624939, 'eval_runtime': 20.1366, 'eval_samples_per_second': 83.132, 'eval_steps_per_second': 2.632, 'epoch': 7.0}
{'train@eng.pdtb.pdtb_loss': 0.936357319355011, 'train@eng.pdtb.pdtb_accuracy@eng.pdtb.pdtb': 0.6843123861566485, 'train@eng.pdtb.pdtb_f1@eng.pdtb.pdtb': 0.47142786078077564, 'train@eng.pdtb.pdtb_precision@eng.pdtb.pdtb': 0.5156425085468382, 'train@eng.pdtb.pdtb_recall@eng.pdtb.pdtb': 0.4670712158982519, 'train@eng.pdtb.pdtb_loss@eng.pdtb.pdtb': 0.936357319355011, 'train@eng.pdtb.pdtb_runtime': 515.6305, 'train@eng.pdtb.pdtb_samples_per_second': 85.177, 'train@eng.pdtb.pdtb_steps_per_second': 2.663, 'epoch': 8.0}
{'loss': 0.9966, 'learning_rate': 6.666666666666667e-06, 'epoch': 8.0}
{'eval_loss': 0.9120944738388062, 'eval_accuracy@eng.pdtb.pdtb': 0.6911589008363201, 'eval_f1@eng.pdtb.pdtb': 0.5431080456991386, 'eval_precision@eng.pdtb.pdtb': 0.5831341687451302, 'eval_recall@eng.pdtb.pdtb': 0.5314192456443725, 'eval_loss@eng.pdtb.pdtb': 0.9120944738388062, 'eval_runtime': 20.1184, 'eval_samples_per_second': 83.208, 'eval_steps_per_second': 2.634, 'epoch': 8.0}
{'train@eng.pdtb.pdtb_loss': 0.9279935359954834, 'train@eng.pdtb.pdtb_accuracy@eng.pdtb.pdtb': 0.6868397085610201, 'train@eng.pdtb.pdtb_f1@eng.pdtb.pdtb': 0.4746625869094881, 'train@eng.pdtb.pdtb_precision@eng.pdtb.pdtb': 0.5353730932567256, 'train@eng.pdtb.pdtb_recall@eng.pdtb.pdtb': 0.4675290931419685, 'train@eng.pdtb.pdtb_loss@eng.pdtb.pdtb': 0.9279935359954834, 'train@eng.pdtb.pdtb_runtime': 515.5614, 'train@eng.pdtb.pdtb_samples_per_second': 85.189, 'train@eng.pdtb.pdtb_steps_per_second': 2.663, 'epoch': 9.0}
{'loss': 0.9866, 'learning_rate': 5e-06, 'epoch': 9.0}
{'eval_loss': 0.9084542393684387, 'eval_accuracy@eng.pdtb.pdtb': 0.6923536439665472, 'eval_f1@eng.pdtb.pdtb': 0.5458959935401627, 'eval_precision@eng.pdtb.pdtb': 0.5940655839062541, 'eval_recall@eng.pdtb.pdtb': 0.5291639906647116, 'eval_loss@eng.pdtb.pdtb': 0.9084542393684387, 'eval_runtime': 20.1325, 'eval_samples_per_second': 83.149, 'eval_steps_per_second': 2.633, 'epoch': 9.0}
{'train@eng.pdtb.pdtb_loss': 0.9238511323928833, 'train@eng.pdtb.pdtb_accuracy@eng.pdtb.pdtb': 0.6882969034608379, 'train@eng.pdtb.pdtb_f1@eng.pdtb.pdtb': 0.494074546321873, 'train@eng.pdtb.pdtb_precision@eng.pdtb.pdtb': 0.5814583358872326, 'train@eng.pdtb.pdtb_recall@eng.pdtb.pdtb': 0.4798649913293641, 'train@eng.pdtb.pdtb_loss@eng.pdtb.pdtb': 0.9238511323928833, 'train@eng.pdtb.pdtb_runtime': 516.3915, 'train@eng.pdtb.pdtb_samples_per_second': 85.052, 'train@eng.pdtb.pdtb_steps_per_second': 2.659, 'epoch': 10.0}
{'loss': 0.9807, 'learning_rate': 3.3333333333333333e-06, 'epoch': 10.0}
{'eval_loss': 0.9036896824836731, 'eval_accuracy@eng.pdtb.pdtb': 0.6941457586618877, 'eval_f1@eng.pdtb.pdtb': 0.5541472718970732, 'eval_precision@eng.pdtb.pdtb': 0.6046526753702166, 'eval_recall@eng.pdtb.pdtb': 0.5414732434863442, 'eval_loss@eng.pdtb.pdtb': 0.9036895632743835, 'eval_runtime': 20.1888, 'eval_samples_per_second': 82.917, 'eval_steps_per_second': 2.625, 'epoch': 10.0}
{'train@eng.pdtb.pdtb_loss': 0.9203803539276123, 'train@eng.pdtb.pdtb_accuracy@eng.pdtb.pdtb': 0.6891848816029144, 'train@eng.pdtb.pdtb_f1@eng.pdtb.pdtb': 0.4969293760360792, 'train@eng.pdtb.pdtb_precision@eng.pdtb.pdtb': 0.5674892785315812, 'train@eng.pdtb.pdtb_recall@eng.pdtb.pdtb': 0.485075175348204, 'train@eng.pdtb.pdtb_loss@eng.pdtb.pdtb': 0.9203803539276123, 'train@eng.pdtb.pdtb_runtime': 516.2432, 'train@eng.pdtb.pdtb_samples_per_second': 85.076, 'train@eng.pdtb.pdtb_steps_per_second': 2.66, 'epoch': 11.0}
{'loss': 0.9744, 'learning_rate': 1.6666666666666667e-06, 'epoch': 11.0}
{'eval_loss': 0.9038406610488892, 'eval_accuracy@eng.pdtb.pdtb': 0.6935483870967742, 'eval_f1@eng.pdtb.pdtb': 0.5591194642961089, 'eval_precision@eng.pdtb.pdtb': 0.5949483105434602, 'eval_recall@eng.pdtb.pdtb': 0.5510772853717902, 'eval_loss@eng.pdtb.pdtb': 0.9038406610488892, 'eval_runtime': 20.1825, 'eval_samples_per_second': 82.943, 'eval_steps_per_second': 2.626, 'epoch': 11.0}
{'train@eng.pdtb.pdtb_loss': 0.9189341068267822, 'train@eng.pdtb.pdtb_accuracy@eng.pdtb.pdtb': 0.6898224043715847, 'train@eng.pdtb.pdtb_f1@eng.pdtb.pdtb': 0.49669822949229453, 'train@eng.pdtb.pdtb_precision@eng.pdtb.pdtb': 0.5751211721310066, 'train@eng.pdtb.pdtb_recall@eng.pdtb.pdtb': 0.4827896153803414, 'train@eng.pdtb.pdtb_loss@eng.pdtb.pdtb': 0.9189339876174927, 'train@eng.pdtb.pdtb_runtime': 516.1019, 'train@eng.pdtb.pdtb_samples_per_second': 85.099, 'train@eng.pdtb.pdtb_steps_per_second': 2.66, 'epoch': 12.0}
{'loss': 0.9716, 'learning_rate': 0.0, 'epoch': 12.0}
{'eval_loss': 0.9023227691650391, 'eval_accuracy@eng.pdtb.pdtb': 0.6917562724014337, 'eval_f1@eng.pdtb.pdtb': 0.5512490242941284, 'eval_precision@eng.pdtb.pdtb': 0.5928789135589891, 'eval_recall@eng.pdtb.pdtb': 0.5394964939029945, 'eval_loss@eng.pdtb.pdtb': 0.9023227691650391, 'eval_runtime': 20.1545, 'eval_samples_per_second': 83.058, 'eval_steps_per_second': 2.63, 'epoch': 12.0}
{'train_runtime': 19597.4389, 'train_samples_per_second': 26.893, 'train_steps_per_second': 0.841, 'train_loss': 1.0923879928570346, 'epoch': 12.0}
***** train_logger metrics *****
  epoch                    =       12.0
  train_loss               =     1.9676
  train_runtime            = 0:32:34.88
  train_samples_per_second =     25.462
  train_steps_per_second   =      0.798
-------------------------------------------------------------------
Lang1:  rus.rst.rrt    Lang2:  eng.pdtb.pdtb
Saving run to:  runs/full_shot/FullShot=v4_rus.rst.rrt_eng.pdtb.pdtb_lrfunc=Adafactor_lr=2e-05_plm=bert-base-multilingual-cased
Running with params: BERT_MODEL=bert-base-multilingual-cased lr=2e-05
read 28822 examples
read 2855 examples
read 2843 examples
read 43920 examples
read 1674 examples
read 2257 examples
Total prediction labels:  45
BertAdapterModel(
  (shared_parameters): ModuleDict()
  (bert): BertModel(
    (shared_parameters): ModuleDict()
    (invertible_adapters): ModuleDict(
      (ru): NICECouplingBlock(
        (F): Sequential(
          (0): Linear(in_features=384, out_features=192, bias=True)
          (1): Activation_Function_Class(
            (f): NewGELUActivation()
          )
          (2): Linear(in_features=192, out_features=384, bias=True)
        )
        (G): Sequential(
          (0): Linear(in_features=384, out_features=192, bias=True)
          (1): Activation_Function_Class(
            (f): NewGELUActivation()
          )
          (2): Linear(in_features=192, out_features=384, bias=True)
        )
      )
      (en): NICECouplingBlock(
        (F): Sequential(
          (0): Linear(in_features=384, out_features=192, bias=True)
          (1): Activation_Function_Class(
            (f): ReLU()
          )
          (2): Linear(in_features=192, out_features=384, bias=True)
        )
        (G): Sequential(
          (0): Linear(in_features=384, out_features=192, bias=True)
          (1): Activation_Function_Class(
            (f): ReLU()
          )
          (2): Linear(in_features=192, out_features=384, bias=True)
        )
      )
    )
    (embeddings): BertEmbeddings(
      (word_embeddings): Embedding(119547, 768, padding_idx=0)
      (position_embeddings): Embedding(512, 768)
      (token_type_embeddings): Embedding(2, 768)
      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
      (dropout): Dropout(p=0.1, inplace=False)
    )
    (encoder): BertEncoder(
      (layer): ModuleList(
        (0): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (ru): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (en): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (1): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (ru): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (en): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (2): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (ru): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (en): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (3): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (ru): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (en): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (4): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (ru): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (en): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (5): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (ru): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (en): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (6): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (ru): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (en): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (7): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (ru): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (en): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (8): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (ru): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (en): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (9): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (ru): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (en): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (10): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (ru): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (en): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (11): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (ru): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (en): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
      )
    )
    (pooler): BertPooler(
      (dense): Linear(in_features=768, out_features=768, bias=True)
      (activation): Tanh()
    )
    (prefix_tuning): PrefixTuningPool(
      (prefix_tunings): ModuleDict()
    )
  )
  (heads): ModuleDict(
    (disrpt): ClassificationHead(
      (0): Dropout(p=0.1, inplace=False)
      (1): Linear(in_features=768, out_features=768, bias=True)
      (2): Activation_Function_Class(
        (f): Tanh()
      )
      (3): Dropout(p=0.1, inplace=False)
      (4): Linear(in_features=768, out_features=45, bias=True)
    )
  )
)
{'train@rus.rst.rrt_loss': 1.7410888671875, 'train@rus.rst.rrt_accuracy@rus.rst.rrt': 0.4902158073693706, 'train@rus.rst.rrt_f1@rus.rst.rrt': 0.17682083302824292, 'train@rus.rst.rrt_precision@rus.rst.rrt': 0.22802386669093433, 'train@rus.rst.rrt_recall@rus.rst.rrt': 0.19253472280530504, 'train@rus.rst.rrt_loss@rus.rst.rrt': 1.7410889863967896, 'train@rus.rst.rrt_runtime': 345.0431, 'train@rus.rst.rrt_samples_per_second': 83.532, 'train@rus.rst.rrt_steps_per_second': 2.611, 'epoch': 1.0}
{'loss': 2.2066, 'learning_rate': 1.8333333333333333e-05, 'epoch': 1.0}
{'eval_loss': 1.7770639657974243, 'eval_accuracy@rus.rst.rrt': 0.4714535901926445, 'eval_f1@rus.rst.rrt': 0.19434984527654803, 'eval_precision@rus.rst.rrt': 0.20473004770078201, 'eval_recall@rus.rst.rrt': 0.21254928854382327, 'eval_loss@rus.rst.rrt': 1.7770639657974243, 'eval_runtime': 34.6233, 'eval_samples_per_second': 82.459, 'eval_steps_per_second': 2.599, 'epoch': 1.0}
{'train@rus.rst.rrt_loss': 1.5165908336639404, 'train@rus.rst.rrt_accuracy@rus.rst.rrt': 0.5364305044757477, 'train@rus.rst.rrt_f1@rus.rst.rrt': 0.2174133498395916, 'train@rus.rst.rrt_precision@rus.rst.rrt': 0.2722989181768409, 'train@rus.rst.rrt_recall@rus.rst.rrt': 0.224897966218231, 'train@rus.rst.rrt_loss@rus.rst.rrt': 1.5165908336639404, 'train@rus.rst.rrt_runtime': 344.7572, 'train@rus.rst.rrt_samples_per_second': 83.601, 'train@rus.rst.rrt_steps_per_second': 2.613, 'epoch': 2.0}
{'loss': 1.6586, 'learning_rate': 1.6666666666666667e-05, 'epoch': 2.0}
{'eval_loss': 1.5800262689590454, 'eval_accuracy@rus.rst.rrt': 0.5103327495621717, 'eval_f1@rus.rst.rrt': 0.2365740034054081, 'eval_precision@rus.rst.rrt': 0.30709988422948115, 'eval_recall@rus.rst.rrt': 0.2460763942768426, 'eval_loss@rus.rst.rrt': 1.5800265073776245, 'eval_runtime': 34.5972, 'eval_samples_per_second': 82.521, 'eval_steps_per_second': 2.601, 'epoch': 2.0}
{'train@rus.rst.rrt_loss': 1.4335696697235107, 'train@rus.rst.rrt_accuracy@rus.rst.rrt': 0.5612032475192561, 'train@rus.rst.rrt_f1@rus.rst.rrt': 0.2689898754320501, 'train@rus.rst.rrt_precision@rus.rst.rrt': 0.38945975041608977, 'train@rus.rst.rrt_recall@rus.rst.rrt': 0.26614200974888863, 'train@rus.rst.rrt_loss@rus.rst.rrt': 1.4335696697235107, 'train@rus.rst.rrt_runtime': 345.2828, 'train@rus.rst.rrt_samples_per_second': 83.474, 'train@rus.rst.rrt_steps_per_second': 2.609, 'epoch': 3.0}
{'loss': 1.5242, 'learning_rate': 1.5000000000000002e-05, 'epoch': 3.0}
{'eval_loss': 1.5024781227111816, 'eval_accuracy@rus.rst.rrt': 0.533800350262697, 'eval_f1@rus.rst.rrt': 0.29474042774421055, 'eval_precision@rus.rst.rrt': 0.3430797837663822, 'eval_recall@rus.rst.rrt': 0.2925714656362026, 'eval_loss@rus.rst.rrt': 1.5024781227111816, 'eval_runtime': 34.5825, 'eval_samples_per_second': 82.556, 'eval_steps_per_second': 2.602, 'epoch': 3.0}
{'train@rus.rst.rrt_loss': 1.3781344890594482, 'train@rus.rst.rrt_accuracy@rus.rst.rrt': 0.5760877107764902, 'train@rus.rst.rrt_f1@rus.rst.rrt': 0.30109131774908815, 'train@rus.rst.rrt_precision@rus.rst.rrt': 0.44205903593151774, 'train@rus.rst.rrt_recall@rus.rst.rrt': 0.2858706027548278, 'train@rus.rst.rrt_loss@rus.rst.rrt': 1.3781344890594482, 'train@rus.rst.rrt_runtime': 345.054, 'train@rus.rst.rrt_samples_per_second': 83.529, 'train@rus.rst.rrt_steps_per_second': 2.611, 'epoch': 4.0}
{'loss': 1.4569, 'learning_rate': 1.3333333333333333e-05, 'epoch': 4.0}
{'eval_loss': 1.4506982564926147, 'eval_accuracy@rus.rst.rrt': 0.557968476357268, 'eval_f1@rus.rst.rrt': 0.3442213019214714, 'eval_precision@rus.rst.rrt': 0.49767202322203363, 'eval_recall@rus.rst.rrt': 0.326750568434902, 'eval_loss@rus.rst.rrt': 1.4506982564926147, 'eval_runtime': 34.5921, 'eval_samples_per_second': 82.533, 'eval_steps_per_second': 2.602, 'epoch': 4.0}
{'train@rus.rst.rrt_loss': 1.3424991369247437, 'train@rus.rst.rrt_accuracy@rus.rst.rrt': 0.5867046006522795, 'train@rus.rst.rrt_f1@rus.rst.rrt': 0.3236221657591835, 'train@rus.rst.rrt_precision@rus.rst.rrt': 0.4474428203282335, 'train@rus.rst.rrt_recall@rus.rst.rrt': 0.30541594930351584, 'train@rus.rst.rrt_loss@rus.rst.rrt': 1.3424994945526123, 'train@rus.rst.rrt_runtime': 345.8757, 'train@rus.rst.rrt_samples_per_second': 83.331, 'train@rus.rst.rrt_steps_per_second': 2.605, 'epoch': 5.0}
{'loss': 1.415, 'learning_rate': 1.1666666666666668e-05, 'epoch': 5.0}
{'eval_loss': 1.4232702255249023, 'eval_accuracy@rus.rst.rrt': 0.5635726795096322, 'eval_f1@rus.rst.rrt': 0.37508900788045735, 'eval_precision@rus.rst.rrt': 0.507183958868845, 'eval_recall@rus.rst.rrt': 0.35620030477860737, 'eval_loss@rus.rst.rrt': 1.4232702255249023, 'eval_runtime': 34.5624, 'eval_samples_per_second': 82.604, 'eval_steps_per_second': 2.604, 'epoch': 5.0}
{'train@rus.rst.rrt_loss': 1.3177545070648193, 'train@rus.rst.rrt_accuracy@rus.rst.rrt': 0.5929845257095274, 'train@rus.rst.rrt_f1@rus.rst.rrt': 0.3387835970858013, 'train@rus.rst.rrt_precision@rus.rst.rrt': 0.4470539474908736, 'train@rus.rst.rrt_recall@rus.rst.rrt': 0.3189061093518491, 'train@rus.rst.rrt_loss@rus.rst.rrt': 1.3177545070648193, 'train@rus.rst.rrt_runtime': 345.013, 'train@rus.rst.rrt_samples_per_second': 83.539, 'train@rus.rst.rrt_steps_per_second': 2.611, 'epoch': 6.0}
{'loss': 1.3856, 'learning_rate': 1e-05, 'epoch': 6.0}
{'eval_loss': 1.4010814428329468, 'eval_accuracy@rus.rst.rrt': 0.5677758318739055, 'eval_f1@rus.rst.rrt': 0.38329558358686466, 'eval_precision@rus.rst.rrt': 0.48966597406639534, 'eval_recall@rus.rst.rrt': 0.36508476454947264, 'eval_loss@rus.rst.rrt': 1.4010814428329468, 'eval_runtime': 34.6167, 'eval_samples_per_second': 82.475, 'eval_steps_per_second': 2.6, 'epoch': 6.0}
{'train@rus.rst.rrt_loss': 1.3015780448913574, 'train@rus.rst.rrt_accuracy@rus.rst.rrt': 0.5990909721740337, 'train@rus.rst.rrt_f1@rus.rst.rrt': 0.35074127190538756, 'train@rus.rst.rrt_precision@rus.rst.rrt': 0.44295888163882835, 'train@rus.rst.rrt_recall@rus.rst.rrt': 0.3316290236728808, 'train@rus.rst.rrt_loss@rus.rst.rrt': 1.3015780448913574, 'train@rus.rst.rrt_runtime': 344.7843, 'train@rus.rst.rrt_samples_per_second': 83.594, 'train@rus.rst.rrt_steps_per_second': 2.613, 'epoch': 7.0}
{'loss': 1.3635, 'learning_rate': 8.333333333333334e-06, 'epoch': 7.0}
{'eval_loss': 1.392221212387085, 'eval_accuracy@rus.rst.rrt': 0.5705779334500876, 'eval_f1@rus.rst.rrt': 0.3925416788120059, 'eval_precision@rus.rst.rrt': 0.4807541357370879, 'eval_recall@rus.rst.rrt': 0.3782139543115274, 'eval_loss@rus.rst.rrt': 1.3922213315963745, 'eval_runtime': 34.6023, 'eval_samples_per_second': 82.509, 'eval_steps_per_second': 2.601, 'epoch': 7.0}
{'train@rus.rst.rrt_loss': 1.2884976863861084, 'train@rus.rst.rrt_accuracy@rus.rst.rrt': 0.6015890639095136, 'train@rus.rst.rrt_f1@rus.rst.rrt': 0.35959696179555106, 'train@rus.rst.rrt_precision@rus.rst.rrt': 0.44548978657228433, 'train@rus.rst.rrt_recall@rus.rst.rrt': 0.342392609302167, 'train@rus.rst.rrt_loss@rus.rst.rrt': 1.2884979248046875, 'train@rus.rst.rrt_runtime': 344.6449, 'train@rus.rst.rrt_samples_per_second': 83.628, 'train@rus.rst.rrt_steps_per_second': 2.614, 'epoch': 8.0}
{'loss': 1.3452, 'learning_rate': 6.666666666666667e-06, 'epoch': 8.0}
{'eval_loss': 1.3809306621551514, 'eval_accuracy@rus.rst.rrt': 0.5775831873905429, 'eval_f1@rus.rst.rrt': 0.40496718423384254, 'eval_precision@rus.rst.rrt': 0.4624815945968076, 'eval_recall@rus.rst.rrt': 0.3946088393312246, 'eval_loss@rus.rst.rrt': 1.3809306621551514, 'eval_runtime': 34.5558, 'eval_samples_per_second': 82.62, 'eval_steps_per_second': 2.604, 'epoch': 8.0}
{'train@rus.rst.rrt_loss': 1.2754956483840942, 'train@rus.rst.rrt_accuracy@rus.rst.rrt': 0.6050239400457984, 'train@rus.rst.rrt_f1@rus.rst.rrt': 0.3628993913817892, 'train@rus.rst.rrt_precision@rus.rst.rrt': 0.4529328840815979, 'train@rus.rst.rrt_recall@rus.rst.rrt': 0.3442196525888011, 'train@rus.rst.rrt_loss@rus.rst.rrt': 1.2754956483840942, 'train@rus.rst.rrt_runtime': 345.0352, 'train@rus.rst.rrt_samples_per_second': 83.534, 'train@rus.rst.rrt_steps_per_second': 2.611, 'epoch': 9.0}
{'loss': 1.335, 'learning_rate': 5e-06, 'epoch': 9.0}
{'eval_loss': 1.3731818199157715, 'eval_accuracy@rus.rst.rrt': 0.5751313485113836, 'eval_f1@rus.rst.rrt': 0.4049688008606728, 'eval_precision@rus.rst.rrt': 0.4780235399799145, 'eval_recall@rus.rst.rrt': 0.39033537644225225, 'eval_loss@rus.rst.rrt': 1.3731818199157715, 'eval_runtime': 34.6202, 'eval_samples_per_second': 82.466, 'eval_steps_per_second': 2.6, 'epoch': 9.0}
{'train@rus.rst.rrt_loss': 1.269708514213562, 'train@rus.rst.rrt_accuracy@rus.rst.rrt': 0.606550551661925, 'train@rus.rst.rrt_f1@rus.rst.rrt': 0.36352516574357396, 'train@rus.rst.rrt_precision@rus.rst.rrt': 0.4626088878611662, 'train@rus.rst.rrt_recall@rus.rst.rrt': 0.3420900818392702, 'train@rus.rst.rrt_loss@rus.rst.rrt': 1.2697086334228516, 'train@rus.rst.rrt_runtime': 344.7067, 'train@rus.rst.rrt_samples_per_second': 83.613, 'train@rus.rst.rrt_steps_per_second': 2.614, 'epoch': 10.0}
{'loss': 1.3267, 'learning_rate': 3.3333333333333333e-06, 'epoch': 10.0}
{'eval_loss': 1.3714873790740967, 'eval_accuracy@rus.rst.rrt': 0.5758318739054291, 'eval_f1@rus.rst.rrt': 0.40890475972360746, 'eval_precision@rus.rst.rrt': 0.4824792549568079, 'eval_recall@rus.rst.rrt': 0.3922212809397115, 'eval_loss@rus.rst.rrt': 1.3714874982833862, 'eval_runtime': 34.6204, 'eval_samples_per_second': 82.466, 'eval_steps_per_second': 2.6, 'epoch': 10.0}
{'train@rus.rst.rrt_loss': 1.2656079530715942, 'train@rus.rst.rrt_accuracy@rus.rst.rrt': 0.6070015960030533, 'train@rus.rst.rrt_f1@rus.rst.rrt': 0.3661246271772171, 'train@rus.rst.rrt_precision@rus.rst.rrt': 0.4665737966139844, 'train@rus.rst.rrt_recall@rus.rst.rrt': 0.34729446817965043, 'train@rus.rst.rrt_loss@rus.rst.rrt': 1.2656079530715942, 'train@rus.rst.rrt_runtime': 344.8569, 'train@rus.rst.rrt_samples_per_second': 83.577, 'train@rus.rst.rrt_steps_per_second': 2.613, 'epoch': 11.0}
{'loss': 1.3202, 'learning_rate': 1.6666666666666667e-06, 'epoch': 11.0}
{'eval_loss': 1.3716148138046265, 'eval_accuracy@rus.rst.rrt': 0.5751313485113836, 'eval_f1@rus.rst.rrt': 0.41189828489396135, 'eval_precision@rus.rst.rrt': 0.4761640806236275, 'eval_recall@rus.rst.rrt': 0.3997374020153492, 'eval_loss@rus.rst.rrt': 1.3716148138046265, 'eval_runtime': 34.5352, 'eval_samples_per_second': 82.669, 'eval_steps_per_second': 2.606, 'epoch': 11.0}
{'train@rus.rst.rrt_loss': 1.2639609575271606, 'train@rus.rst.rrt_accuracy@rus.rst.rrt': 0.6079383804038582, 'train@rus.rst.rrt_f1@rus.rst.rrt': 0.3670221286945647, 'train@rus.rst.rrt_precision@rus.rst.rrt': 0.4606184808285295, 'train@rus.rst.rrt_recall@rus.rst.rrt': 0.3482261989085472, 'train@rus.rst.rrt_loss@rus.rst.rrt': 1.2639609575271606, 'train@rus.rst.rrt_runtime': 345.1624, 'train@rus.rst.rrt_samples_per_second': 83.503, 'train@rus.rst.rrt_steps_per_second': 2.61, 'epoch': 12.0}
{'loss': 1.3143, 'learning_rate': 0.0, 'epoch': 12.0}
{'eval_loss': 1.369461178779602, 'eval_accuracy@rus.rst.rrt': 0.5786339754816112, 'eval_f1@rus.rst.rrt': 0.41260450763389317, 'eval_precision@rus.rst.rrt': 0.4766660466019753, 'eval_recall@rus.rst.rrt': 0.4006976069922839, 'eval_loss@rus.rst.rrt': 1.3694610595703125, 'eval_runtime': 34.6016, 'eval_samples_per_second': 82.511, 'eval_steps_per_second': 2.601, 'epoch': 12.0}
{'train_runtime': 13327.8299, 'train_samples_per_second': 25.951, 'train_steps_per_second': 0.811, 'train_loss': 1.4709841283303564, 'epoch': 12.0}
***** train_log metrics *****
  epoch                    =       12.0
  train_loss               =      1.471
  train_runtime            = 3:42:07.82
  train_samples_per_second =     25.951
  train_steps_per_second   =      0.811
{'train@eng.pdtb.pdtb_loss': 1.1459696292877197, 'train@eng.pdtb.pdtb_accuracy@eng.pdtb.pdtb': 0.6275500910746813, 'train@eng.pdtb.pdtb_f1@eng.pdtb.pdtb': 0.36241005189417247, 'train@eng.pdtb.pdtb_precision@eng.pdtb.pdtb': 0.42613449828783606, 'train@eng.pdtb.pdtb_recall@eng.pdtb.pdtb': 0.3600972038165712, 'train@eng.pdtb.pdtb_loss@eng.pdtb.pdtb': 1.1459696292877197, 'train@eng.pdtb.pdtb_runtime': 515.9462, 'train@eng.pdtb.pdtb_samples_per_second': 85.125, 'train@eng.pdtb.pdtb_steps_per_second': 2.661, 'epoch': 1.0}
{'loss': 1.5443, 'learning_rate': 1.8333333333333333e-05, 'epoch': 1.0}
{'eval_loss': 1.0902769565582275, 'eval_accuracy@eng.pdtb.pdtb': 0.6523297491039427, 'eval_f1@eng.pdtb.pdtb': 0.4172293888260475, 'eval_precision@eng.pdtb.pdtb': 0.45972167761151084, 'eval_recall@eng.pdtb.pdtb': 0.4143931943301123, 'eval_loss@eng.pdtb.pdtb': 1.090277075767517, 'eval_runtime': 20.0549, 'eval_samples_per_second': 83.471, 'eval_steps_per_second': 2.643, 'epoch': 1.0}
{'train@eng.pdtb.pdtb_loss': 1.043747901916504, 'train@eng.pdtb.pdtb_accuracy@eng.pdtb.pdtb': 0.6550318761384335, 'train@eng.pdtb.pdtb_f1@eng.pdtb.pdtb': 0.42262168345385936, 'train@eng.pdtb.pdtb_precision@eng.pdtb.pdtb': 0.47034225840439975, 'train@eng.pdtb.pdtb_recall@eng.pdtb.pdtb': 0.417389260755807, 'train@eng.pdtb.pdtb_loss@eng.pdtb.pdtb': 1.0437480211257935, 'train@eng.pdtb.pdtb_runtime': 515.8331, 'train@eng.pdtb.pdtb_samples_per_second': 85.144, 'train@eng.pdtb.pdtb_steps_per_second': 2.662, 'epoch': 2.0}
{'loss': 1.1445, 'learning_rate': 1.6666666666666667e-05, 'epoch': 2.0}
{'eval_loss': 0.9997494220733643, 'eval_accuracy@eng.pdtb.pdtb': 0.6708482676224612, 'eval_f1@eng.pdtb.pdtb': 0.4726363783963222, 'eval_precision@eng.pdtb.pdtb': 0.5254054146751556, 'eval_recall@eng.pdtb.pdtb': 0.4686308187575564, 'eval_loss@eng.pdtb.pdtb': 0.9997493624687195, 'eval_runtime': 20.07, 'eval_samples_per_second': 83.408, 'eval_steps_per_second': 2.641, 'epoch': 2.0}
{'train@eng.pdtb.pdtb_loss': 1.0074208974838257, 'train@eng.pdtb.pdtb_accuracy@eng.pdtb.pdtb': 0.6659608378870674, 'train@eng.pdtb.pdtb_f1@eng.pdtb.pdtb': 0.4528806762854032, 'train@eng.pdtb.pdtb_precision@eng.pdtb.pdtb': 0.4702883163871374, 'train@eng.pdtb.pdtb_recall@eng.pdtb.pdtb': 0.44897222873055925, 'train@eng.pdtb.pdtb_loss@eng.pdtb.pdtb': 1.0074208974838257, 'train@eng.pdtb.pdtb_runtime': 516.0541, 'train@eng.pdtb.pdtb_samples_per_second': 85.107, 'train@eng.pdtb.pdtb_steps_per_second': 2.661, 'epoch': 3.0}
{'loss': 1.0793, 'learning_rate': 1.5000000000000002e-05, 'epoch': 3.0}
{'eval_loss': 0.9736633896827698, 'eval_accuracy@eng.pdtb.pdtb': 0.6821983273596177, 'eval_f1@eng.pdtb.pdtb': 0.51514553403129, 'eval_precision@eng.pdtb.pdtb': 0.5440723854919102, 'eval_recall@eng.pdtb.pdtb': 0.5061421582179523, 'eval_loss@eng.pdtb.pdtb': 0.9736633896827698, 'eval_runtime': 20.0306, 'eval_samples_per_second': 83.572, 'eval_steps_per_second': 2.646, 'epoch': 3.0}
{'train@eng.pdtb.pdtb_loss': 0.9720936417579651, 'train@eng.pdtb.pdtb_accuracy@eng.pdtb.pdtb': 0.6760018214936248, 'train@eng.pdtb.pdtb_f1@eng.pdtb.pdtb': 0.4622218251863906, 'train@eng.pdtb.pdtb_precision@eng.pdtb.pdtb': 0.4768768839101511, 'train@eng.pdtb.pdtb_recall@eng.pdtb.pdtb': 0.46134454631942484, 'train@eng.pdtb.pdtb_loss@eng.pdtb.pdtb': 0.9720937013626099, 'train@eng.pdtb.pdtb_runtime': 516.4551, 'train@eng.pdtb.pdtb_samples_per_second': 85.041, 'train@eng.pdtb.pdtb_steps_per_second': 2.659, 'epoch': 4.0}
{'loss': 1.0446, 'learning_rate': 1.3333333333333333e-05, 'epoch': 4.0}
{'eval_loss': 0.9437393546104431, 'eval_accuracy@eng.pdtb.pdtb': 0.6857825567502986, 'eval_f1@eng.pdtb.pdtb': 0.5202733205314447, 'eval_precision@eng.pdtb.pdtb': 0.536167359915927, 'eval_recall@eng.pdtb.pdtb': 0.5210156970003698, 'eval_loss@eng.pdtb.pdtb': 0.9437393546104431, 'eval_runtime': 20.1737, 'eval_samples_per_second': 82.979, 'eval_steps_per_second': 2.627, 'epoch': 4.0}
{'train@eng.pdtb.pdtb_loss': 0.9543706178665161, 'train@eng.pdtb.pdtb_accuracy@eng.pdtb.pdtb': 0.6806010928961749, 'train@eng.pdtb.pdtb_f1@eng.pdtb.pdtb': 0.46652250211946483, 'train@eng.pdtb.pdtb_precision@eng.pdtb.pdtb': 0.5141000373966124, 'train@eng.pdtb.pdtb_recall@eng.pdtb.pdtb': 0.46452622816014716, 'train@eng.pdtb.pdtb_loss@eng.pdtb.pdtb': 0.9543706178665161, 'train@eng.pdtb.pdtb_runtime': 516.6575, 'train@eng.pdtb.pdtb_samples_per_second': 85.008, 'train@eng.pdtb.pdtb_steps_per_second': 2.657, 'epoch': 5.0}
{'loss': 1.0163, 'learning_rate': 1.1666666666666668e-05, 'epoch': 5.0}
{'eval_loss': 0.9343339800834656, 'eval_accuracy@eng.pdtb.pdtb': 0.6821983273596177, 'eval_f1@eng.pdtb.pdtb': 0.5212345626971542, 'eval_precision@eng.pdtb.pdtb': 0.5421151030678903, 'eval_recall@eng.pdtb.pdtb': 0.5180067832748049, 'eval_loss@eng.pdtb.pdtb': 0.9343339800834656, 'eval_runtime': 20.0934, 'eval_samples_per_second': 83.311, 'eval_steps_per_second': 2.638, 'epoch': 5.0}
{'train@eng.pdtb.pdtb_loss': 0.9375377893447876, 'train@eng.pdtb.pdtb_accuracy@eng.pdtb.pdtb': 0.6839708561020036, 'train@eng.pdtb.pdtb_f1@eng.pdtb.pdtb': 0.470835454549645, 'train@eng.pdtb.pdtb_precision@eng.pdtb.pdtb': 0.520356259589061, 'train@eng.pdtb.pdtb_recall@eng.pdtb.pdtb': 0.46705037638806796, 'train@eng.pdtb.pdtb_loss@eng.pdtb.pdtb': 0.9375377893447876, 'train@eng.pdtb.pdtb_runtime': 516.8334, 'train@eng.pdtb.pdtb_samples_per_second': 84.979, 'train@eng.pdtb.pdtb_steps_per_second': 2.657, 'epoch': 6.0}
{'loss': 0.9986, 'learning_rate': 1e-05, 'epoch': 6.0}
{'eval_loss': 0.9284326434135437, 'eval_accuracy@eng.pdtb.pdtb': 0.6851851851851852, 'eval_f1@eng.pdtb.pdtb': 0.5220127123648025, 'eval_precision@eng.pdtb.pdtb': 0.5440918492737084, 'eval_recall@eng.pdtb.pdtb': 0.5178823081354254, 'eval_loss@eng.pdtb.pdtb': 0.9284326434135437, 'eval_runtime': 20.1656, 'eval_samples_per_second': 83.013, 'eval_steps_per_second': 2.628, 'epoch': 6.0}
{'train@eng.pdtb.pdtb_loss': 0.9278833270072937, 'train@eng.pdtb.pdtb_accuracy@eng.pdtb.pdtb': 0.6862249544626594, 'train@eng.pdtb.pdtb_f1@eng.pdtb.pdtb': 0.4751022364917736, 'train@eng.pdtb.pdtb_precision@eng.pdtb.pdtb': 0.5274978093707108, 'train@eng.pdtb.pdtb_recall@eng.pdtb.pdtb': 0.4675909275029473, 'train@eng.pdtb.pdtb_loss@eng.pdtb.pdtb': 0.9278833270072937, 'train@eng.pdtb.pdtb_runtime': 516.1218, 'train@eng.pdtb.pdtb_samples_per_second': 85.096, 'train@eng.pdtb.pdtb_steps_per_second': 2.66, 'epoch': 7.0}
{'loss': 0.9873, 'learning_rate': 8.333333333333334e-06, 'epoch': 7.0}
{'eval_loss': 0.9125797748565674, 'eval_accuracy@eng.pdtb.pdtb': 0.6857825567502986, 'eval_f1@eng.pdtb.pdtb': 0.5370033036130325, 'eval_precision@eng.pdtb.pdtb': 0.5778300297840551, 'eval_recall@eng.pdtb.pdtb': 0.5239607084387298, 'eval_loss@eng.pdtb.pdtb': 0.9125797748565674, 'eval_runtime': 20.0222, 'eval_samples_per_second': 83.607, 'eval_steps_per_second': 2.647, 'epoch': 7.0}
{'train@eng.pdtb.pdtb_loss': 0.9172294735908508, 'train@eng.pdtb.pdtb_accuracy@eng.pdtb.pdtb': 0.6895036429872495, 'train@eng.pdtb.pdtb_f1@eng.pdtb.pdtb': 0.47902419037492255, 'train@eng.pdtb.pdtb_precision@eng.pdtb.pdtb': 0.5093127346501651, 'train@eng.pdtb.pdtb_recall@eng.pdtb.pdtb': 0.4772069078627394, 'train@eng.pdtb.pdtb_loss@eng.pdtb.pdtb': 0.917229413986206, 'train@eng.pdtb.pdtb_runtime': 515.68, 'train@eng.pdtb.pdtb_samples_per_second': 85.169, 'train@eng.pdtb.pdtb_steps_per_second': 2.663, 'epoch': 8.0}
{'loss': 0.9765, 'learning_rate': 6.666666666666667e-06, 'epoch': 8.0}
{'eval_loss': 0.9104594588279724, 'eval_accuracy@eng.pdtb.pdtb': 0.6917562724014337, 'eval_f1@eng.pdtb.pdtb': 0.5475938577032682, 'eval_precision@eng.pdtb.pdtb': 0.5824206175356028, 'eval_recall@eng.pdtb.pdtb': 0.5392288533143137, 'eval_loss@eng.pdtb.pdtb': 0.9104595184326172, 'eval_runtime': 20.0786, 'eval_samples_per_second': 83.372, 'eval_steps_per_second': 2.64, 'epoch': 8.0}
{'train@eng.pdtb.pdtb_loss': 0.9099161028862, 'train@eng.pdtb.pdtb_accuracy@eng.pdtb.pdtb': 0.6918488160291439, 'train@eng.pdtb.pdtb_f1@eng.pdtb.pdtb': 0.4812504425426308, 'train@eng.pdtb.pdtb_precision@eng.pdtb.pdtb': 0.5315516158429078, 'train@eng.pdtb.pdtb_recall@eng.pdtb.pdtb': 0.4769103077386995, 'train@eng.pdtb.pdtb_loss@eng.pdtb.pdtb': 0.9099162220954895, 'train@eng.pdtb.pdtb_runtime': 515.6728, 'train@eng.pdtb.pdtb_samples_per_second': 85.17, 'train@eng.pdtb.pdtb_steps_per_second': 2.663, 'epoch': 9.0}
{'loss': 0.9665, 'learning_rate': 5e-06, 'epoch': 9.0}
{'eval_loss': 0.9081505537033081, 'eval_accuracy@eng.pdtb.pdtb': 0.6893667861409797, 'eval_f1@eng.pdtb.pdtb': 0.5434872320558193, 'eval_precision@eng.pdtb.pdtb': 0.5772551413930116, 'eval_recall@eng.pdtb.pdtb': 0.5359930057693423, 'eval_loss@eng.pdtb.pdtb': 0.9081505537033081, 'eval_runtime': 20.0819, 'eval_samples_per_second': 83.359, 'eval_steps_per_second': 2.639, 'epoch': 9.0}
{'train@eng.pdtb.pdtb_loss': 0.9070670008659363, 'train@eng.pdtb.pdtb_accuracy@eng.pdtb.pdtb': 0.6932377049180328, 'train@eng.pdtb.pdtb_f1@eng.pdtb.pdtb': 0.5014196612590197, 'train@eng.pdtb.pdtb_precision@eng.pdtb.pdtb': 0.5746072375607744, 'train@eng.pdtb.pdtb_recall@eng.pdtb.pdtb': 0.48882276797187424, 'train@eng.pdtb.pdtb_loss@eng.pdtb.pdtb': 0.9070670008659363, 'train@eng.pdtb.pdtb_runtime': 515.6078, 'train@eng.pdtb.pdtb_samples_per_second': 85.181, 'train@eng.pdtb.pdtb_steps_per_second': 2.663, 'epoch': 10.0}
{'loss': 0.9616, 'learning_rate': 3.3333333333333333e-06, 'epoch': 10.0}
{'eval_loss': 0.9041267037391663, 'eval_accuracy@eng.pdtb.pdtb': 0.6887694145758662, 'eval_f1@eng.pdtb.pdtb': 0.5462749932295958, 'eval_precision@eng.pdtb.pdtb': 0.5785331641049174, 'eval_recall@eng.pdtb.pdtb': 0.5355921049688723, 'eval_loss@eng.pdtb.pdtb': 0.9041265249252319, 'eval_runtime': 20.0515, 'eval_samples_per_second': 83.485, 'eval_steps_per_second': 2.643, 'epoch': 10.0}
{'train@eng.pdtb.pdtb_loss': 0.9028542041778564, 'train@eng.pdtb.pdtb_accuracy@eng.pdtb.pdtb': 0.6943533697632058, 'train@eng.pdtb.pdtb_f1@eng.pdtb.pdtb': 0.5054483451569903, 'train@eng.pdtb.pdtb_precision@eng.pdtb.pdtb': 0.5628809498771075, 'train@eng.pdtb.pdtb_recall@eng.pdtb.pdtb': 0.495412266056294, 'train@eng.pdtb.pdtb_loss@eng.pdtb.pdtb': 0.9028542041778564, 'train@eng.pdtb.pdtb_runtime': 516.0722, 'train@eng.pdtb.pdtb_samples_per_second': 85.104, 'train@eng.pdtb.pdtb_steps_per_second': 2.66, 'epoch': 11.0}
{'loss': 0.9578, 'learning_rate': 1.6666666666666667e-06, 'epoch': 11.0}
{'eval_loss': 0.9028830528259277, 'eval_accuracy@eng.pdtb.pdtb': 0.6899641577060932, 'eval_f1@eng.pdtb.pdtb': 0.5475415605434936, 'eval_precision@eng.pdtb.pdtb': 0.5767144816136154, 'eval_recall@eng.pdtb.pdtb': 0.5428982392015524, 'eval_loss@eng.pdtb.pdtb': 0.9028830528259277, 'eval_runtime': 20.1415, 'eval_samples_per_second': 83.112, 'eval_steps_per_second': 2.631, 'epoch': 11.0}
{'train@eng.pdtb.pdtb_loss': 0.9018704891204834, 'train@eng.pdtb.pdtb_accuracy@eng.pdtb.pdtb': 0.695332422586521, 'train@eng.pdtb.pdtb_f1@eng.pdtb.pdtb': 0.5050341241582796, 'train@eng.pdtb.pdtb_precision@eng.pdtb.pdtb': 0.5717179888185145, 'train@eng.pdtb.pdtb_recall@eng.pdtb.pdtb': 0.49269874882960313, 'train@eng.pdtb.pdtb_loss@eng.pdtb.pdtb': 0.901870608329773, 'train@eng.pdtb.pdtb_runtime': 515.9133, 'train@eng.pdtb.pdtb_samples_per_second': 85.131, 'train@eng.pdtb.pdtb_steps_per_second': 2.661, 'epoch': 12.0}
{'loss': 0.954, 'learning_rate': 0.0, 'epoch': 12.0}
{'eval_loss': 0.9009937047958374, 'eval_accuracy@eng.pdtb.pdtb': 0.6911589008363201, 'eval_f1@eng.pdtb.pdtb': 0.5497657545716973, 'eval_precision@eng.pdtb.pdtb': 0.5799518823571344, 'eval_recall@eng.pdtb.pdtb': 0.5451183002236729, 'eval_loss@eng.pdtb.pdtb': 0.9009937047958374, 'eval_runtime': 20.0894, 'eval_samples_per_second': 83.328, 'eval_steps_per_second': 2.638, 'epoch': 12.0}
{'train_runtime': 19555.9372, 'train_samples_per_second': 26.95, 'train_steps_per_second': 0.843, 'train_loss': 1.0525924115830412, 'epoch': 12.0}
***** train_logger metrics *****
  epoch                    =       12.0
  train_loss               =      1.471
  train_runtime            = 3:42:07.82
  train_samples_per_second =     25.951
  train_steps_per_second   =      0.811
-------------------------------------------------------------------
Lang1:  spa.rst.rststb    Lang2:  eng.pdtb.pdtb
Saving run to:  runs/full_shot/FullShot=v4_spa.rst.rststb_eng.pdtb.pdtb_lrfunc=Adafactor_lr=2e-05_plm=bert-base-multilingual-cased
Running with params: BERT_MODEL=bert-base-multilingual-cased lr=2e-05
read 2240 examples
read 383 examples
read 426 examples
read 43920 examples
read 1674 examples
read 2257 examples
Total prediction labels:  52
BertAdapterModel(
  (shared_parameters): ModuleDict()
  (bert): BertModel(
    (shared_parameters): ModuleDict()
    (invertible_adapters): ModuleDict(
      (es): NICECouplingBlock(
        (F): Sequential(
          (0): Linear(in_features=384, out_features=192, bias=True)
          (1): Activation_Function_Class(
            (f): NewGELUActivation()
          )
          (2): Linear(in_features=192, out_features=384, bias=True)
        )
        (G): Sequential(
          (0): Linear(in_features=384, out_features=192, bias=True)
          (1): Activation_Function_Class(
            (f): NewGELUActivation()
          )
          (2): Linear(in_features=192, out_features=384, bias=True)
        )
      )
      (en): NICECouplingBlock(
        (F): Sequential(
          (0): Linear(in_features=384, out_features=192, bias=True)
          (1): Activation_Function_Class(
            (f): ReLU()
          )
          (2): Linear(in_features=192, out_features=384, bias=True)
        )
        (G): Sequential(
          (0): Linear(in_features=384, out_features=192, bias=True)
          (1): Activation_Function_Class(
            (f): ReLU()
          )
          (2): Linear(in_features=192, out_features=384, bias=True)
        )
      )
    )
    (embeddings): BertEmbeddings(
      (word_embeddings): Embedding(119547, 768, padding_idx=0)
      (position_embeddings): Embedding(512, 768)
      (token_type_embeddings): Embedding(2, 768)
      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
      (dropout): Dropout(p=0.1, inplace=False)
    )
    (encoder): BertEncoder(
      (layer): ModuleList(
        (0): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (es): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (en): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (1): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (es): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (en): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (2): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (es): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (en): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (3): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (es): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (en): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (4): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (es): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (en): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (5): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (es): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (en): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (6): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (es): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (en): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (7): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (es): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (en): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (8): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (es): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (en): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (9): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (es): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (en): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (10): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (es): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (en): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (11): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (es): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (en): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
      )
    )
    (pooler): BertPooler(
      (dense): Linear(in_features=768, out_features=768, bias=True)
      (activation): Tanh()
    )
    (prefix_tuning): PrefixTuningPool(
      (prefix_tunings): ModuleDict()
    )
  )
  (heads): ModuleDict(
    (disrpt): ClassificationHead(
      (0): Dropout(p=0.1, inplace=False)
      (1): Linear(in_features=768, out_features=768, bias=True)
      (2): Activation_Function_Class(
        (f): Tanh()
      )
      (3): Dropout(p=0.1, inplace=False)
      (4): Linear(in_features=768, out_features=52, bias=True)
    )
  )
)
{'train@spa.rst.rststb_loss': 2.9795048236846924, 'train@spa.rst.rststb_accuracy@spa.rst.rststb': 0.203125, 'train@spa.rst.rststb_f1@spa.rst.rststb': 0.013175519476909089, 'train@spa.rst.rststb_precision@spa.rst.rststb': 0.03399910233393178, 'train@spa.rst.rststb_recall@spa.rst.rststb': 0.03616222707311123, 'train@spa.rst.rststb_loss@spa.rst.rststb': 2.9795048236846924, 'train@spa.rst.rststb_runtime': 27.057, 'train@spa.rst.rststb_samples_per_second': 82.788, 'train@spa.rst.rststb_steps_per_second': 2.587, 'epoch': 1.0}
{'loss': 3.5022, 'learning_rate': 1.8333333333333333e-05, 'epoch': 1.0}
{'eval_loss': 3.01961612701416, 'eval_accuracy@spa.rst.rststb': 0.206266318537859, 'eval_f1@spa.rst.rststb': 0.014869188782232262, 'eval_precision@spa.rst.rststb': 0.00896810080599387, 'eval_recall@spa.rst.rststb': 0.043478260869565216, 'eval_loss@spa.rst.rststb': 3.01961612701416, 'eval_runtime': 5.0448, 'eval_samples_per_second': 75.92, 'eval_steps_per_second': 2.379, 'epoch': 1.0}
{'train@spa.rst.rststb_loss': 2.641415596008301, 'train@spa.rst.rststb_accuracy@spa.rst.rststb': 0.25357142857142856, 'train@spa.rst.rststb_f1@spa.rst.rststb': 0.03392293526763981, 'train@spa.rst.rststb_precision@spa.rst.rststb': 0.04930361671387012, 'train@spa.rst.rststb_recall@spa.rst.rststb': 0.04927574036882181, 'train@spa.rst.rststb_loss@spa.rst.rststb': 2.641415596008301, 'train@spa.rst.rststb_runtime': 27.2076, 'train@spa.rst.rststb_samples_per_second': 82.33, 'train@spa.rst.rststb_steps_per_second': 2.573, 'epoch': 2.0}
{'loss': 2.8092, 'learning_rate': 1.6666666666666667e-05, 'epoch': 2.0}
{'eval_loss': 2.747436761856079, 'eval_accuracy@spa.rst.rststb': 0.21671018276762402, 'eval_f1@spa.rst.rststb': 0.026774855579203404, 'eval_precision@spa.rst.rststb': 0.060167059327479115, 'eval_recall@spa.rst.rststb': 0.047923687710444626, 'eval_loss@spa.rst.rststb': 2.747436761856079, 'eval_runtime': 5.0531, 'eval_samples_per_second': 75.795, 'eval_steps_per_second': 2.375, 'epoch': 2.0}
{'train@spa.rst.rststb_loss': 2.506392240524292, 'train@spa.rst.rststb_accuracy@spa.rst.rststb': 0.315625, 'train@spa.rst.rststb_f1@spa.rst.rststb': 0.048938543762466605, 'train@spa.rst.rststb_precision@spa.rst.rststb': 0.042790908802172255, 'train@spa.rst.rststb_recall@spa.rst.rststb': 0.06725905047789797, 'train@spa.rst.rststb_loss@spa.rst.rststb': 2.506392240524292, 'train@spa.rst.rststb_runtime': 27.1646, 'train@spa.rst.rststb_samples_per_second': 82.46, 'train@spa.rst.rststb_steps_per_second': 2.577, 'epoch': 3.0}
{'loss': 2.6077, 'learning_rate': 1.5000000000000002e-05, 'epoch': 3.0}
{'eval_loss': 2.6466336250305176, 'eval_accuracy@spa.rst.rststb': 0.27154046997389036, 'eval_f1@spa.rst.rststb': 0.04970669557081087, 'eval_precision@spa.rst.rststb': 0.049047051816557476, 'eval_recall@spa.rst.rststb': 0.0677121098863515, 'eval_loss@spa.rst.rststb': 2.6466336250305176, 'eval_runtime': 5.053, 'eval_samples_per_second': 75.797, 'eval_steps_per_second': 2.375, 'epoch': 3.0}
{'train@spa.rst.rststb_loss': 2.405198335647583, 'train@spa.rst.rststb_accuracy@spa.rst.rststb': 0.334375, 'train@spa.rst.rststb_f1@spa.rst.rststb': 0.06304541838969516, 'train@spa.rst.rststb_precision@spa.rst.rststb': 0.06110485314164355, 'train@spa.rst.rststb_recall@spa.rst.rststb': 0.07849511043073086, 'train@spa.rst.rststb_loss@spa.rst.rststb': 2.405198335647583, 'train@spa.rst.rststb_runtime': 27.1925, 'train@spa.rst.rststb_samples_per_second': 82.376, 'train@spa.rst.rststb_steps_per_second': 2.574, 'epoch': 4.0}
{'loss': 2.5017, 'learning_rate': 1.3333333333333333e-05, 'epoch': 4.0}
{'eval_loss': 2.5687201023101807, 'eval_accuracy@spa.rst.rststb': 0.3028720626631854, 'eval_f1@spa.rst.rststb': 0.06437660128555842, 'eval_precision@spa.rst.rststb': 0.06819233113847002, 'eval_recall@spa.rst.rststb': 0.08129207225634828, 'eval_loss@spa.rst.rststb': 2.5687201023101807, 'eval_runtime': 5.0494, 'eval_samples_per_second': 75.851, 'eval_steps_per_second': 2.377, 'epoch': 4.0}
{'train@spa.rst.rststb_loss': 2.31835675239563, 'train@spa.rst.rststb_accuracy@spa.rst.rststb': 0.3611607142857143, 'train@spa.rst.rststb_f1@spa.rst.rststb': 0.07591646388380983, 'train@spa.rst.rststb_precision@spa.rst.rststb': 0.08910987853212782, 'train@spa.rst.rststb_recall@spa.rst.rststb': 0.09390374385687136, 'train@spa.rst.rststb_loss@spa.rst.rststb': 2.31835675239563, 'train@spa.rst.rststb_runtime': 27.1973, 'train@spa.rst.rststb_samples_per_second': 82.361, 'train@spa.rst.rststb_steps_per_second': 2.574, 'epoch': 5.0}
{'loss': 2.4077, 'learning_rate': 1.1666666666666668e-05, 'epoch': 5.0}
{'eval_loss': 2.5065550804138184, 'eval_accuracy@spa.rst.rststb': 0.3368146214099217, 'eval_f1@spa.rst.rststb': 0.08761643743574861, 'eval_precision@spa.rst.rststb': 0.11517102894335965, 'eval_recall@spa.rst.rststb': 0.10353956884341559, 'eval_loss@spa.rst.rststb': 2.5065553188323975, 'eval_runtime': 5.0474, 'eval_samples_per_second': 75.881, 'eval_steps_per_second': 2.377, 'epoch': 5.0}
{'train@spa.rst.rststb_loss': 2.243762254714966, 'train@spa.rst.rststb_accuracy@spa.rst.rststb': 0.3834821428571429, 'train@spa.rst.rststb_f1@spa.rst.rststb': 0.0857436983082681, 'train@spa.rst.rststb_precision@spa.rst.rststb': 0.0845623335743162, 'train@spa.rst.rststb_recall@spa.rst.rststb': 0.10838027190956587, 'train@spa.rst.rststb_loss@spa.rst.rststb': 2.243762254714966, 'train@spa.rst.rststb_runtime': 27.1863, 'train@spa.rst.rststb_samples_per_second': 82.395, 'train@spa.rst.rststb_steps_per_second': 2.575, 'epoch': 6.0}
{'loss': 2.3306, 'learning_rate': 1e-05, 'epoch': 6.0}
{'eval_loss': 2.448604106903076, 'eval_accuracy@spa.rst.rststb': 0.3524804177545692, 'eval_f1@spa.rst.rststb': 0.09174242119160535, 'eval_precision@spa.rst.rststb': 0.11540706997815063, 'eval_recall@spa.rst.rststb': 0.11441033420123933, 'eval_loss@spa.rst.rststb': 2.448604106903076, 'eval_runtime': 5.0438, 'eval_samples_per_second': 75.935, 'eval_steps_per_second': 2.379, 'epoch': 6.0}
{'train@spa.rst.rststb_loss': 2.179128646850586, 'train@spa.rst.rststb_accuracy@spa.rst.rststb': 0.3964285714285714, 'train@spa.rst.rststb_f1@spa.rst.rststb': 0.09340015837327831, 'train@spa.rst.rststb_precision@spa.rst.rststb': 0.09680902285538526, 'train@spa.rst.rststb_recall@spa.rst.rststb': 0.11727848633086708, 'train@spa.rst.rststb_loss@spa.rst.rststb': 2.179128646850586, 'train@spa.rst.rststb_runtime': 27.1669, 'train@spa.rst.rststb_samples_per_second': 82.453, 'train@spa.rst.rststb_steps_per_second': 2.577, 'epoch': 7.0}
{'loss': 2.2659, 'learning_rate': 8.333333333333334e-06, 'epoch': 7.0}
{'eval_loss': 2.398951530456543, 'eval_accuracy@spa.rst.rststb': 0.3577023498694517, 'eval_f1@spa.rst.rststb': 0.10932288951649034, 'eval_precision@spa.rst.rststb': 0.10761759948660006, 'eval_recall@spa.rst.rststb': 0.13207062735767588, 'eval_loss@spa.rst.rststb': 2.398951530456543, 'eval_runtime': 5.0612, 'eval_samples_per_second': 75.673, 'eval_steps_per_second': 2.371, 'epoch': 7.0}
{'train@spa.rst.rststb_loss': 2.129451036453247, 'train@spa.rst.rststb_accuracy@spa.rst.rststb': 0.40625, 'train@spa.rst.rststb_f1@spa.rst.rststb': 0.0965590668126569, 'train@spa.rst.rststb_precision@spa.rst.rststb': 0.13239467961931234, 'train@spa.rst.rststb_recall@spa.rst.rststb': 0.12172435660707916, 'train@spa.rst.rststb_loss@spa.rst.rststb': 2.129450798034668, 'train@spa.rst.rststb_runtime': 27.1547, 'train@spa.rst.rststb_samples_per_second': 82.49, 'train@spa.rst.rststb_steps_per_second': 2.578, 'epoch': 8.0}
{'loss': 2.2103, 'learning_rate': 6.666666666666667e-06, 'epoch': 8.0}
{'eval_loss': 2.359881639480591, 'eval_accuracy@spa.rst.rststb': 0.3629242819843342, 'eval_f1@spa.rst.rststb': 0.10642103116331164, 'eval_precision@spa.rst.rststb': 0.09615989651155891, 'eval_recall@spa.rst.rststb': 0.13514792077536966, 'eval_loss@spa.rst.rststb': 2.359881639480591, 'eval_runtime': 5.0588, 'eval_samples_per_second': 75.709, 'eval_steps_per_second': 2.372, 'epoch': 8.0}
{'train@spa.rst.rststb_loss': 2.093311071395874, 'train@spa.rst.rststb_accuracy@spa.rst.rststb': 0.4089285714285714, 'train@spa.rst.rststb_f1@spa.rst.rststb': 0.09857152180784094, 'train@spa.rst.rststb_precision@spa.rst.rststb': 0.11608798525291604, 'train@spa.rst.rststb_recall@spa.rst.rststb': 0.12548140361834528, 'train@spa.rst.rststb_loss@spa.rst.rststb': 2.093311071395874, 'train@spa.rst.rststb_runtime': 27.1389, 'train@spa.rst.rststb_samples_per_second': 82.538, 'train@spa.rst.rststb_steps_per_second': 2.579, 'epoch': 9.0}
{'loss': 2.1665, 'learning_rate': 5e-06, 'epoch': 9.0}
{'eval_loss': 2.328944683074951, 'eval_accuracy@spa.rst.rststb': 0.3681462140992167, 'eval_f1@spa.rst.rststb': 0.10532357583584444, 'eval_precision@spa.rst.rststb': 0.09228817312062275, 'eval_recall@spa.rst.rststb': 0.1368835638687535, 'eval_loss@spa.rst.rststb': 2.3289449214935303, 'eval_runtime': 5.0766, 'eval_samples_per_second': 75.444, 'eval_steps_per_second': 2.364, 'epoch': 9.0}
{'train@spa.rst.rststb_loss': 2.0678353309631348, 'train@spa.rst.rststb_accuracy@spa.rst.rststb': 0.4142857142857143, 'train@spa.rst.rststb_f1@spa.rst.rststb': 0.1015817278274099, 'train@spa.rst.rststb_precision@spa.rst.rststb': 0.11655497901574545, 'train@spa.rst.rststb_recall@spa.rst.rststb': 0.1278948249877613, 'train@spa.rst.rststb_loss@spa.rst.rststb': 2.0678350925445557, 'train@spa.rst.rststb_runtime': 27.1656, 'train@spa.rst.rststb_samples_per_second': 82.457, 'train@spa.rst.rststb_steps_per_second': 2.577, 'epoch': 10.0}
{'loss': 2.1324, 'learning_rate': 3.3333333333333333e-06, 'epoch': 10.0}
{'eval_loss': 2.3108065128326416, 'eval_accuracy@spa.rst.rststb': 0.37597911227154046, 'eval_f1@spa.rst.rststb': 0.10949491647432151, 'eval_precision@spa.rst.rststb': 0.09781987177130026, 'eval_recall@spa.rst.rststb': 0.13883035166888327, 'eval_loss@spa.rst.rststb': 2.3108062744140625, 'eval_runtime': 5.0465, 'eval_samples_per_second': 75.894, 'eval_steps_per_second': 2.378, 'epoch': 10.0}
{'train@spa.rst.rststb_loss': 2.0530264377593994, 'train@spa.rst.rststb_accuracy@spa.rst.rststb': 0.415625, 'train@spa.rst.rststb_f1@spa.rst.rststb': 0.10175643670418029, 'train@spa.rst.rststb_precision@spa.rst.rststb': 0.11416969446187261, 'train@spa.rst.rststb_recall@spa.rst.rststb': 0.12822337110505405, 'train@spa.rst.rststb_loss@spa.rst.rststb': 2.0530264377593994, 'train@spa.rst.rststb_runtime': 27.1513, 'train@spa.rst.rststb_samples_per_second': 82.501, 'train@spa.rst.rststb_steps_per_second': 2.578, 'epoch': 11.0}
{'loss': 2.1132, 'learning_rate': 1.6666666666666667e-06, 'epoch': 11.0}
{'eval_loss': 2.297788381576538, 'eval_accuracy@spa.rst.rststb': 0.3785900783289817, 'eval_f1@spa.rst.rststb': 0.11017147514993794, 'eval_precision@spa.rst.rststb': 0.09823528664260456, 'eval_recall@spa.rst.rststb': 0.1394792809355932, 'eval_loss@spa.rst.rststb': 2.2977888584136963, 'eval_runtime': 5.0299, 'eval_samples_per_second': 76.145, 'eval_steps_per_second': 2.386, 'epoch': 11.0}
{'train@spa.rst.rststb_loss': 2.0483062267303467, 'train@spa.rst.rststb_accuracy@spa.rst.rststb': 0.41964285714285715, 'train@spa.rst.rststb_f1@spa.rst.rststb': 0.1033021537029503, 'train@spa.rst.rststb_precision@spa.rst.rststb': 0.11267971600013695, 'train@spa.rst.rststb_recall@spa.rst.rststb': 0.12959482727185678, 'train@spa.rst.rststb_loss@spa.rst.rststb': 2.0483059883117676, 'train@spa.rst.rststb_runtime': 27.1387, 'train@spa.rst.rststb_samples_per_second': 82.539, 'train@spa.rst.rststb_steps_per_second': 2.579, 'epoch': 12.0}
{'loss': 2.1011, 'learning_rate': 0.0, 'epoch': 12.0}
{'eval_loss': 2.2943248748779297, 'eval_accuracy@spa.rst.rststb': 0.381201044386423, 'eval_f1@spa.rst.rststb': 0.11020332596653226, 'eval_precision@spa.rst.rststb': 0.09823913823120498, 'eval_recall@spa.rst.rststb': 0.14002963866811935, 'eval_loss@spa.rst.rststb': 2.2943248748779297, 'eval_runtime': 5.0348, 'eval_samples_per_second': 76.071, 'eval_steps_per_second': 2.383, 'epoch': 12.0}
{'train_runtime': 1067.7837, 'train_samples_per_second': 25.174, 'train_steps_per_second': 0.787, 'train_loss': 2.429030064174107, 'epoch': 12.0}
***** train_log metrics *****
  epoch                    =       12.0
  train_loss               =      2.429
  train_runtime            = 0:17:47.78
  train_samples_per_second =     25.174
  train_steps_per_second   =      0.787
{'train@eng.pdtb.pdtb_loss': 1.2402317523956299, 'train@eng.pdtb.pdtb_accuracy@eng.pdtb.pdtb': 0.6022085610200364, 'train@eng.pdtb.pdtb_f1@eng.pdtb.pdtb': 0.264889974519998, 'train@eng.pdtb.pdtb_precision@eng.pdtb.pdtb': 0.3227712994286546, 'train@eng.pdtb.pdtb_recall@eng.pdtb.pdtb': 0.26100848757860756, 'train@eng.pdtb.pdtb_loss@eng.pdtb.pdtb': 1.2402318716049194, 'train@eng.pdtb.pdtb_runtime': 515.2602, 'train@eng.pdtb.pdtb_samples_per_second': 85.238, 'train@eng.pdtb.pdtb_steps_per_second': 2.665, 'epoch': 1.0}
{'loss': 1.7267, 'learning_rate': 1.8333333333333333e-05, 'epoch': 1.0}
{'eval_loss': 1.1624751091003418, 'eval_accuracy@eng.pdtb.pdtb': 0.6403823178016727, 'eval_f1@eng.pdtb.pdtb': 0.32268618953031386, 'eval_precision@eng.pdtb.pdtb': 0.3798150206730501, 'eval_recall@eng.pdtb.pdtb': 0.3200073710890736, 'eval_loss@eng.pdtb.pdtb': 1.1624751091003418, 'eval_runtime': 20.096, 'eval_samples_per_second': 83.3, 'eval_steps_per_second': 2.637, 'epoch': 1.0}
{'train@eng.pdtb.pdtb_loss': 1.0912328958511353, 'train@eng.pdtb.pdtb_accuracy@eng.pdtb.pdtb': 0.639207650273224, 'train@eng.pdtb.pdtb_f1@eng.pdtb.pdtb': 0.3532556157964888, 'train@eng.pdtb.pdtb_precision@eng.pdtb.pdtb': 0.4752918805241203, 'train@eng.pdtb.pdtb_recall@eng.pdtb.pdtb': 0.34032688174825415, 'train@eng.pdtb.pdtb_loss@eng.pdtb.pdtb': 1.0912330150604248, 'train@eng.pdtb.pdtb_runtime': 515.2172, 'train@eng.pdtb.pdtb_samples_per_second': 85.246, 'train@eng.pdtb.pdtb_steps_per_second': 2.665, 'epoch': 2.0}
{'loss': 1.2058, 'learning_rate': 1.6666666666666667e-05, 'epoch': 2.0}
{'eval_loss': 1.0238319635391235, 'eval_accuracy@eng.pdtb.pdtb': 0.6594982078853047, 'eval_f1@eng.pdtb.pdtb': 0.38948146051020877, 'eval_precision@eng.pdtb.pdtb': 0.4333472938531351, 'eval_recall@eng.pdtb.pdtb': 0.3826972060937155, 'eval_loss@eng.pdtb.pdtb': 1.0238319635391235, 'eval_runtime': 20.0867, 'eval_samples_per_second': 83.339, 'eval_steps_per_second': 2.639, 'epoch': 2.0}
{'train@eng.pdtb.pdtb_loss': 1.0433601140975952, 'train@eng.pdtb.pdtb_accuracy@eng.pdtb.pdtb': 0.6561930783242259, 'train@eng.pdtb.pdtb_f1@eng.pdtb.pdtb': 0.4360581250806249, 'train@eng.pdtb.pdtb_precision@eng.pdtb.pdtb': 0.4722294924925691, 'train@eng.pdtb.pdtb_recall@eng.pdtb.pdtb': 0.41917891825555526, 'train@eng.pdtb.pdtb_loss@eng.pdtb.pdtb': 1.0433601140975952, 'train@eng.pdtb.pdtb_runtime': 515.6717, 'train@eng.pdtb.pdtb_samples_per_second': 85.17, 'train@eng.pdtb.pdtb_steps_per_second': 2.663, 'epoch': 3.0}
{'loss': 1.1151, 'learning_rate': 1.5000000000000002e-05, 'epoch': 3.0}
{'eval_loss': 0.9878396391868591, 'eval_accuracy@eng.pdtb.pdtb': 0.6798088410991637, 'eval_f1@eng.pdtb.pdtb': 0.4723496536430224, 'eval_precision@eng.pdtb.pdtb': 0.524732193299856, 'eval_recall@eng.pdtb.pdtb': 0.45379877328649076, 'eval_loss@eng.pdtb.pdtb': 0.9878396391868591, 'eval_runtime': 20.1085, 'eval_samples_per_second': 83.248, 'eval_steps_per_second': 2.636, 'epoch': 3.0}
{'train@eng.pdtb.pdtb_loss': 0.997779905796051, 'train@eng.pdtb.pdtb_accuracy@eng.pdtb.pdtb': 0.6680327868852459, 'train@eng.pdtb.pdtb_f1@eng.pdtb.pdtb': 0.44918257969985226, 'train@eng.pdtb.pdtb_precision@eng.pdtb.pdtb': 0.47665330691792285, 'train@eng.pdtb.pdtb_recall@eng.pdtb.pdtb': 0.4411311927057695, 'train@eng.pdtb.pdtb_loss@eng.pdtb.pdtb': 0.9977797269821167, 'train@eng.pdtb.pdtb_runtime': 516.015, 'train@eng.pdtb.pdtb_samples_per_second': 85.114, 'train@eng.pdtb.pdtb_steps_per_second': 2.661, 'epoch': 4.0}
{'loss': 1.0707, 'learning_rate': 1.3333333333333333e-05, 'epoch': 4.0}
{'eval_loss': 0.9507975578308105, 'eval_accuracy@eng.pdtb.pdtb': 0.6845878136200717, 'eval_f1@eng.pdtb.pdtb': 0.5049064559427545, 'eval_precision@eng.pdtb.pdtb': 0.5470101953278822, 'eval_recall@eng.pdtb.pdtb': 0.4927375891195016, 'eval_loss@eng.pdtb.pdtb': 0.9507976174354553, 'eval_runtime': 20.1424, 'eval_samples_per_second': 83.108, 'eval_steps_per_second': 2.631, 'epoch': 4.0}
{'train@eng.pdtb.pdtb_loss': 0.9755154848098755, 'train@eng.pdtb.pdtb_accuracy@eng.pdtb.pdtb': 0.6752959927140255, 'train@eng.pdtb.pdtb_f1@eng.pdtb.pdtb': 0.4581864910943186, 'train@eng.pdtb.pdtb_precision@eng.pdtb.pdtb': 0.4802598298962611, 'train@eng.pdtb.pdtb_recall@eng.pdtb.pdtb': 0.4527919937137822, 'train@eng.pdtb.pdtb_loss@eng.pdtb.pdtb': 0.9755153059959412, 'train@eng.pdtb.pdtb_runtime': 515.7937, 'train@eng.pdtb.pdtb_samples_per_second': 85.15, 'train@eng.pdtb.pdtb_steps_per_second': 2.662, 'epoch': 5.0}
{'loss': 1.0403, 'learning_rate': 1.1666666666666668e-05, 'epoch': 5.0}
{'eval_loss': 0.9343855381011963, 'eval_accuracy@eng.pdtb.pdtb': 0.6833930704898447, 'eval_f1@eng.pdtb.pdtb': 0.5011877912131062, 'eval_precision@eng.pdtb.pdtb': 0.535536626771268, 'eval_recall@eng.pdtb.pdtb': 0.4905894603512995, 'eval_loss@eng.pdtb.pdtb': 0.9343855977058411, 'eval_runtime': 20.1355, 'eval_samples_per_second': 83.137, 'eval_steps_per_second': 2.632, 'epoch': 5.0}
{'train@eng.pdtb.pdtb_loss': 0.9565532207489014, 'train@eng.pdtb.pdtb_accuracy@eng.pdtb.pdtb': 0.6794398907103825, 'train@eng.pdtb.pdtb_f1@eng.pdtb.pdtb': 0.464175986736545, 'train@eng.pdtb.pdtb_precision@eng.pdtb.pdtb': 0.5067533661740353, 'train@eng.pdtb.pdtb_recall@eng.pdtb.pdtb': 0.4578193777159482, 'train@eng.pdtb.pdtb_loss@eng.pdtb.pdtb': 0.9565532207489014, 'train@eng.pdtb.pdtb_runtime': 515.5138, 'train@eng.pdtb.pdtb_samples_per_second': 85.197, 'train@eng.pdtb.pdtb_steps_per_second': 2.663, 'epoch': 6.0}
{'loss': 1.0182, 'learning_rate': 1e-05, 'epoch': 6.0}
{'eval_loss': 0.9291256666183472, 'eval_accuracy@eng.pdtb.pdtb': 0.6839904420549582, 'eval_f1@eng.pdtb.pdtb': 0.5101898337835029, 'eval_precision@eng.pdtb.pdtb': 0.5438843231691728, 'eval_recall@eng.pdtb.pdtb': 0.5019034425159081, 'eval_loss@eng.pdtb.pdtb': 0.9291256666183472, 'eval_runtime': 20.1073, 'eval_samples_per_second': 83.253, 'eval_steps_per_second': 2.636, 'epoch': 6.0}
{'train@eng.pdtb.pdtb_loss': 0.9447581171989441, 'train@eng.pdtb.pdtb_accuracy@eng.pdtb.pdtb': 0.6806921675774135, 'train@eng.pdtb.pdtb_f1@eng.pdtb.pdtb': 0.4666019130462345, 'train@eng.pdtb.pdtb_precision@eng.pdtb.pdtb': 0.5349395098721097, 'train@eng.pdtb.pdtb_recall@eng.pdtb.pdtb': 0.4579290731085965, 'train@eng.pdtb.pdtb_loss@eng.pdtb.pdtb': 0.9447581171989441, 'train@eng.pdtb.pdtb_runtime': 515.3062, 'train@eng.pdtb.pdtb_samples_per_second': 85.231, 'train@eng.pdtb.pdtb_steps_per_second': 2.664, 'epoch': 7.0}
{'loss': 1.0041, 'learning_rate': 8.333333333333334e-06, 'epoch': 7.0}
{'eval_loss': 0.9143204689025879, 'eval_accuracy@eng.pdtb.pdtb': 0.6905615292712067, 'eval_f1@eng.pdtb.pdtb': 0.5300095583866744, 'eval_precision@eng.pdtb.pdtb': 0.5719223514228288, 'eval_recall@eng.pdtb.pdtb': 0.5176142185037274, 'eval_loss@eng.pdtb.pdtb': 0.9143204689025879, 'eval_runtime': 20.0744, 'eval_samples_per_second': 83.39, 'eval_steps_per_second': 2.64, 'epoch': 7.0}
{'train@eng.pdtb.pdtb_loss': 0.9347125887870789, 'train@eng.pdtb.pdtb_accuracy@eng.pdtb.pdtb': 0.6855874316939891, 'train@eng.pdtb.pdtb_f1@eng.pdtb.pdtb': 0.4733179229955706, 'train@eng.pdtb.pdtb_precision@eng.pdtb.pdtb': 0.5245699722903806, 'train@eng.pdtb.pdtb_recall@eng.pdtb.pdtb': 0.4687304377091736, 'train@eng.pdtb.pdtb_loss@eng.pdtb.pdtb': 0.9347125887870789, 'train@eng.pdtb.pdtb_runtime': 515.3454, 'train@eng.pdtb.pdtb_samples_per_second': 85.224, 'train@eng.pdtb.pdtb_steps_per_second': 2.664, 'epoch': 8.0}
{'loss': 0.991, 'learning_rate': 6.666666666666667e-06, 'epoch': 8.0}
{'eval_loss': 0.9144502878189087, 'eval_accuracy@eng.pdtb.pdtb': 0.6863799283154122, 'eval_f1@eng.pdtb.pdtb': 0.5341091655191273, 'eval_precision@eng.pdtb.pdtb': 0.579813107973023, 'eval_recall@eng.pdtb.pdtb': 0.5242905171730814, 'eval_loss@eng.pdtb.pdtb': 0.9144503474235535, 'eval_runtime': 20.0953, 'eval_samples_per_second': 83.303, 'eval_steps_per_second': 2.637, 'epoch': 8.0}
{'train@eng.pdtb.pdtb_loss': 0.9259046912193298, 'train@eng.pdtb.pdtb_accuracy@eng.pdtb.pdtb': 0.6885018214936248, 'train@eng.pdtb.pdtb_f1@eng.pdtb.pdtb': 0.47695269641964877, 'train@eng.pdtb.pdtb_precision@eng.pdtb.pdtb': 0.5296424644289235, 'train@eng.pdtb.pdtb_recall@eng.pdtb.pdtb': 0.4711808795711651, 'train@eng.pdtb.pdtb_loss@eng.pdtb.pdtb': 0.9259046912193298, 'train@eng.pdtb.pdtb_runtime': 515.767, 'train@eng.pdtb.pdtb_samples_per_second': 85.155, 'train@eng.pdtb.pdtb_steps_per_second': 2.662, 'epoch': 9.0}
{'loss': 0.9809, 'learning_rate': 5e-06, 'epoch': 9.0}
{'eval_loss': 0.9105181097984314, 'eval_accuracy@eng.pdtb.pdtb': 0.6869772998805257, 'eval_f1@eng.pdtb.pdtb': 0.5308423388078108, 'eval_precision@eng.pdtb.pdtb': 0.5563472696641029, 'eval_recall@eng.pdtb.pdtb': 0.5250288838804736, 'eval_loss@eng.pdtb.pdtb': 0.9105181097984314, 'eval_runtime': 20.0998, 'eval_samples_per_second': 83.284, 'eval_steps_per_second': 2.637, 'epoch': 9.0}
{'train@eng.pdtb.pdtb_loss': 0.9219285249710083, 'train@eng.pdtb.pdtb_accuracy@eng.pdtb.pdtb': 0.6874772313296903, 'train@eng.pdtb.pdtb_f1@eng.pdtb.pdtb': 0.47560035314431187, 'train@eng.pdtb.pdtb_precision@eng.pdtb.pdtb': 0.5301928487252272, 'train@eng.pdtb.pdtb_recall@eng.pdtb.pdtb': 0.4703094250570474, 'train@eng.pdtb.pdtb_loss@eng.pdtb.pdtb': 0.9219286441802979, 'train@eng.pdtb.pdtb_runtime': 515.9867, 'train@eng.pdtb.pdtb_samples_per_second': 85.118, 'train@eng.pdtb.pdtb_steps_per_second': 2.661, 'epoch': 10.0}
{'loss': 0.977, 'learning_rate': 3.3333333333333333e-06, 'epoch': 10.0}
{'eval_loss': 0.9038180708885193, 'eval_accuracy@eng.pdtb.pdtb': 0.6917562724014337, 'eval_f1@eng.pdtb.pdtb': 0.5538236893095446, 'eval_precision@eng.pdtb.pdtb': 0.5955069996670992, 'eval_recall@eng.pdtb.pdtb': 0.543156932810575, 'eval_loss@eng.pdtb.pdtb': 0.9038180708885193, 'eval_runtime': 20.1405, 'eval_samples_per_second': 83.116, 'eval_steps_per_second': 2.632, 'epoch': 10.0}
{'train@eng.pdtb.pdtb_loss': 0.9178665280342102, 'train@eng.pdtb.pdtb_accuracy@eng.pdtb.pdtb': 0.6897085610200364, 'train@eng.pdtb.pdtb_f1@eng.pdtb.pdtb': 0.4793560892264173, 'train@eng.pdtb.pdtb_precision@eng.pdtb.pdtb': 0.5249271620139919, 'train@eng.pdtb.pdtb_recall@eng.pdtb.pdtb': 0.4750803460026665, 'train@eng.pdtb.pdtb_loss@eng.pdtb.pdtb': 0.9178665280342102, 'train@eng.pdtb.pdtb_runtime': 515.3042, 'train@eng.pdtb.pdtb_samples_per_second': 85.231, 'train@eng.pdtb.pdtb_steps_per_second': 2.664, 'epoch': 11.0}
{'loss': 0.97, 'learning_rate': 1.6666666666666667e-06, 'epoch': 11.0}
{'eval_loss': 0.9038141369819641, 'eval_accuracy@eng.pdtb.pdtb': 0.6899641577060932, 'eval_f1@eng.pdtb.pdtb': 0.5475336075699787, 'eval_precision@eng.pdtb.pdtb': 0.579805151610673, 'eval_recall@eng.pdtb.pdtb': 0.5417109345999631, 'eval_loss@eng.pdtb.pdtb': 0.9038141965866089, 'eval_runtime': 20.0883, 'eval_samples_per_second': 83.332, 'eval_steps_per_second': 2.638, 'epoch': 11.0}
{'train@eng.pdtb.pdtb_loss': 0.9164576530456543, 'train@eng.pdtb.pdtb_accuracy@eng.pdtb.pdtb': 0.6899817850637523, 'train@eng.pdtb.pdtb_f1@eng.pdtb.pdtb': 0.47846223472030597, 'train@eng.pdtb.pdtb_precision@eng.pdtb.pdtb': 0.5305628843666613, 'train@eng.pdtb.pdtb_recall@eng.pdtb.pdtb': 0.4732133209014332, 'train@eng.pdtb.pdtb_loss@eng.pdtb.pdtb': 0.9164576530456543, 'train@eng.pdtb.pdtb_runtime': 524.7279, 'train@eng.pdtb.pdtb_samples_per_second': 83.701, 'train@eng.pdtb.pdtb_steps_per_second': 2.617, 'epoch': 12.0}
{'loss': 0.9672, 'learning_rate': 0.0, 'epoch': 12.0}
{'eval_loss': 0.9021989107131958, 'eval_accuracy@eng.pdtb.pdtb': 0.6911589008363201, 'eval_f1@eng.pdtb.pdtb': 0.554561766168179, 'eval_precision@eng.pdtb.pdtb': 0.5918802559456637, 'eval_recall@eng.pdtb.pdtb': 0.5444044849422567, 'eval_loss@eng.pdtb.pdtb': 0.9021990299224854, 'eval_runtime': 20.0676, 'eval_samples_per_second': 83.418, 'eval_steps_per_second': 2.641, 'epoch': 12.0}
{'train_runtime': 19549.4727, 'train_samples_per_second': 26.959, 'train_steps_per_second': 0.843, 'train_loss': 1.0889324976362398, 'epoch': 12.0}
***** train_logger metrics *****
  epoch                    =       12.0
  train_loss               =      2.429
  train_runtime            = 0:17:47.78
  train_samples_per_second =     25.174
  train_steps_per_second   =      0.787
-------------------------------------------------------------------
Lang1:  spa.rst.sctb    Lang2:  eng.pdtb.pdtb
Saving run to:  runs/full_shot/FullShot=v4_spa.rst.sctb_eng.pdtb.pdtb_lrfunc=Adafactor_lr=2e-05_plm=bert-base-multilingual-cased
Running with params: BERT_MODEL=bert-base-multilingual-cased lr=2e-05
read 439 examples
read 94 examples
read 159 examples
read 43920 examples
read 1674 examples
read 2257 examples
Total prediction labels:  48
BertAdapterModel(
  (shared_parameters): ModuleDict()
  (bert): BertModel(
    (shared_parameters): ModuleDict()
    (invertible_adapters): ModuleDict(
      (es): NICECouplingBlock(
        (F): Sequential(
          (0): Linear(in_features=384, out_features=192, bias=True)
          (1): Activation_Function_Class(
            (f): NewGELUActivation()
          )
          (2): Linear(in_features=192, out_features=384, bias=True)
        )
        (G): Sequential(
          (0): Linear(in_features=384, out_features=192, bias=True)
          (1): Activation_Function_Class(
            (f): NewGELUActivation()
          )
          (2): Linear(in_features=192, out_features=384, bias=True)
        )
      )
      (en): NICECouplingBlock(
        (F): Sequential(
          (0): Linear(in_features=384, out_features=192, bias=True)
          (1): Activation_Function_Class(
            (f): ReLU()
          )
          (2): Linear(in_features=192, out_features=384, bias=True)
        )
        (G): Sequential(
          (0): Linear(in_features=384, out_features=192, bias=True)
          (1): Activation_Function_Class(
            (f): ReLU()
          )
          (2): Linear(in_features=192, out_features=384, bias=True)
        )
      )
    )
    (embeddings): BertEmbeddings(
      (word_embeddings): Embedding(119547, 768, padding_idx=0)
      (position_embeddings): Embedding(512, 768)
      (token_type_embeddings): Embedding(2, 768)
      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
      (dropout): Dropout(p=0.1, inplace=False)
    )
    (encoder): BertEncoder(
      (layer): ModuleList(
        (0): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (es): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (en): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (1): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (es): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (en): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (2): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (es): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (en): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (3): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (es): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (en): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (4): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (es): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (en): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (5): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (es): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (en): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (6): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (es): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (en): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (7): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (es): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (en): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (8): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (es): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (en): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (9): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (es): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (en): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (10): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (es): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (en): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (11): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (es): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (en): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
      )
    )
    (pooler): BertPooler(
      (dense): Linear(in_features=768, out_features=768, bias=True)
      (activation): Tanh()
    )
    (prefix_tuning): PrefixTuningPool(
      (prefix_tunings): ModuleDict()
    )
  )
  (heads): ModuleDict(
    (disrpt): ClassificationHead(
      (0): Dropout(p=0.1, inplace=False)
      (1): Linear(in_features=768, out_features=768, bias=True)
      (2): Activation_Function_Class(
        (f): Tanh()
      )
      (3): Dropout(p=0.1, inplace=False)
      (4): Linear(in_features=768, out_features=48, bias=True)
    )
  )
)
{'train@spa.rst.sctb_loss': 3.645022392272949, 'train@spa.rst.sctb_accuracy@spa.rst.sctb': 0.3439635535307517, 'train@spa.rst.sctb_f1@spa.rst.sctb': 0.022475471282107817, 'train@spa.rst.sctb_precision@spa.rst.sctb': 0.05623543123543123, 'train@spa.rst.sctb_recall@spa.rst.sctb': 0.042114695340501794, 'train@spa.rst.sctb_loss@spa.rst.sctb': 3.645021677017212, 'train@spa.rst.sctb_runtime': 5.6272, 'train@spa.rst.sctb_samples_per_second': 78.014, 'train@spa.rst.sctb_steps_per_second': 2.488, 'epoch': 1.0}
{'loss': 3.7956, 'learning_rate': 1.8333333333333333e-05, 'epoch': 1.0}
{'eval_loss': 3.6354944705963135, 'eval_accuracy@spa.rst.sctb': 0.35106382978723405, 'eval_f1@spa.rst.sctb': 0.031309297912713474, 'eval_precision@spa.rst.sctb': 0.021331609566903685, 'eval_recall@spa.rst.sctb': 0.058823529411764705, 'eval_loss@spa.rst.sctb': 3.6354944705963135, 'eval_runtime': 1.5352, 'eval_samples_per_second': 61.231, 'eval_steps_per_second': 1.954, 'epoch': 1.0}
{'train@spa.rst.sctb_loss': 3.3923239707946777, 'train@spa.rst.sctb_accuracy@spa.rst.sctb': 0.3439635535307517, 'train@spa.rst.sctb_f1@spa.rst.sctb': 0.022208251002933953, 'train@spa.rst.sctb_precision@spa.rst.sctb': 0.035168195718654434, 'train@spa.rst.sctb_recall@spa.rst.sctb': 0.042114695340501794, 'train@spa.rst.sctb_loss@spa.rst.sctb': 3.392324209213257, 'train@spa.rst.sctb_runtime': 5.6379, 'train@spa.rst.sctb_samples_per_second': 77.866, 'train@spa.rst.sctb_steps_per_second': 2.483, 'epoch': 2.0}
{'loss': 3.5341, 'learning_rate': 1.6666666666666667e-05, 'epoch': 2.0}
{'eval_loss': 3.383685827255249, 'eval_accuracy@spa.rst.sctb': 0.3617021276595745, 'eval_f1@spa.rst.sctb': 0.03669467787114846, 'eval_precision@spa.rst.sctb': 0.07969639468690703, 'eval_recall@spa.rst.sctb': 0.06191950464396284, 'eval_loss@spa.rst.sctb': 3.38368558883667, 'eval_runtime': 1.5716, 'eval_samples_per_second': 59.813, 'eval_steps_per_second': 1.909, 'epoch': 2.0}
{'train@spa.rst.sctb_loss': 3.131553888320923, 'train@spa.rst.sctb_accuracy@spa.rst.sctb': 0.3439635535307517, 'train@spa.rst.sctb_f1@spa.rst.sctb': 0.022208251002933953, 'train@spa.rst.sctb_precision@spa.rst.sctb': 0.035168195718654434, 'train@spa.rst.sctb_recall@spa.rst.sctb': 0.042114695340501794, 'train@spa.rst.sctb_loss@spa.rst.sctb': 3.13155460357666, 'train@spa.rst.sctb_runtime': 5.6665, 'train@spa.rst.sctb_samples_per_second': 77.473, 'train@spa.rst.sctb_steps_per_second': 2.471, 'epoch': 3.0}
{'loss': 3.2975, 'learning_rate': 1.5000000000000002e-05, 'epoch': 3.0}
{'eval_loss': 3.1277222633361816, 'eval_accuracy@spa.rst.sctb': 0.3617021276595745, 'eval_f1@spa.rst.sctb': 0.03669467787114846, 'eval_precision@spa.rst.sctb': 0.07969639468690703, 'eval_recall@spa.rst.sctb': 0.06191950464396284, 'eval_loss@spa.rst.sctb': 3.1277220249176025, 'eval_runtime': 1.5395, 'eval_samples_per_second': 61.058, 'eval_steps_per_second': 1.949, 'epoch': 3.0}
{'train@spa.rst.sctb_loss': 2.8863511085510254, 'train@spa.rst.sctb_accuracy@spa.rst.sctb': 0.3439635535307517, 'train@spa.rst.sctb_f1@spa.rst.sctb': 0.02214502822405558, 'train@spa.rst.sctb_precision@spa.rst.sctb': 0.055936073059360734, 'train@spa.rst.sctb_recall@spa.rst.sctb': 0.042114695340501794, 'train@spa.rst.sctb_loss@spa.rst.sctb': 2.8863511085510254, 'train@spa.rst.sctb_runtime': 5.6668, 'train@spa.rst.sctb_samples_per_second': 77.469, 'train@spa.rst.sctb_steps_per_second': 2.471, 'epoch': 4.0}
{'loss': 3.0403, 'learning_rate': 1.3333333333333333e-05, 'epoch': 4.0}
{'eval_loss': 2.8908278942108154, 'eval_accuracy@spa.rst.sctb': 0.3617021276595745, 'eval_f1@spa.rst.sctb': 0.03669467787114846, 'eval_precision@spa.rst.sctb': 0.07969639468690703, 'eval_recall@spa.rst.sctb': 0.06191950464396284, 'eval_loss@spa.rst.sctb': 2.890827178955078, 'eval_runtime': 1.547, 'eval_samples_per_second': 60.763, 'eval_steps_per_second': 1.939, 'epoch': 4.0}
{'train@spa.rst.sctb_loss': 2.6869394779205322, 'train@spa.rst.sctb_accuracy@spa.rst.sctb': 0.3439635535307517, 'train@spa.rst.sctb_f1@spa.rst.sctb': 0.02214502822405558, 'train@spa.rst.sctb_precision@spa.rst.sctb': 0.055936073059360734, 'train@spa.rst.sctb_recall@spa.rst.sctb': 0.042114695340501794, 'train@spa.rst.sctb_loss@spa.rst.sctb': 2.6869397163391113, 'train@spa.rst.sctb_runtime': 5.6574, 'train@spa.rst.sctb_samples_per_second': 77.598, 'train@spa.rst.sctb_steps_per_second': 2.475, 'epoch': 5.0}
{'loss': 2.8262, 'learning_rate': 1.1666666666666668e-05, 'epoch': 5.0}
{'eval_loss': 2.6955463886260986, 'eval_accuracy@spa.rst.sctb': 0.3617021276595745, 'eval_f1@spa.rst.sctb': 0.03669467787114846, 'eval_precision@spa.rst.sctb': 0.07969639468690703, 'eval_recall@spa.rst.sctb': 0.06191950464396284, 'eval_loss@spa.rst.sctb': 2.6955463886260986, 'eval_runtime': 1.5799, 'eval_samples_per_second': 59.497, 'eval_steps_per_second': 1.899, 'epoch': 5.0}
{'train@spa.rst.sctb_loss': 2.5444998741149902, 'train@spa.rst.sctb_accuracy@spa.rst.sctb': 0.3416856492027335, 'train@spa.rst.sctb_f1@spa.rst.sctb': 0.021222410865874366, 'train@spa.rst.sctb_precision@spa.rst.sctb': 0.014236902050113895, 'train@spa.rst.sctb_recall@spa.rst.sctb': 0.041666666666666664, 'train@spa.rst.sctb_loss@spa.rst.sctb': 2.5444998741149902, 'train@spa.rst.sctb_runtime': 5.68, 'train@spa.rst.sctb_samples_per_second': 77.288, 'train@spa.rst.sctb_steps_per_second': 2.465, 'epoch': 6.0}
{'loss': 2.6592, 'learning_rate': 1e-05, 'epoch': 6.0}
{'eval_loss': 2.557621955871582, 'eval_accuracy@spa.rst.sctb': 0.3617021276595745, 'eval_f1@spa.rst.sctb': 0.03669467787114846, 'eval_precision@spa.rst.sctb': 0.07969639468690703, 'eval_recall@spa.rst.sctb': 0.06191950464396284, 'eval_loss@spa.rst.sctb': 2.557621479034424, 'eval_runtime': 1.557, 'eval_samples_per_second': 60.371, 'eval_steps_per_second': 1.927, 'epoch': 6.0}
{'train@spa.rst.sctb_loss': 2.4499294757843018, 'train@spa.rst.sctb_accuracy@spa.rst.sctb': 0.3416856492027335, 'train@spa.rst.sctb_f1@spa.rst.sctb': 0.021222410865874366, 'train@spa.rst.sctb_precision@spa.rst.sctb': 0.014236902050113895, 'train@spa.rst.sctb_recall@spa.rst.sctb': 0.041666666666666664, 'train@spa.rst.sctb_loss@spa.rst.sctb': 2.4499289989471436, 'train@spa.rst.sctb_runtime': 5.6724, 'train@spa.rst.sctb_samples_per_second': 77.392, 'train@spa.rst.sctb_steps_per_second': 2.468, 'epoch': 7.0}
{'loss': 2.542, 'learning_rate': 8.333333333333334e-06, 'epoch': 7.0}
{'eval_loss': 2.46733021736145, 'eval_accuracy@spa.rst.sctb': 0.3617021276595745, 'eval_f1@spa.rst.sctb': 0.03669467787114846, 'eval_precision@spa.rst.sctb': 0.07969639468690703, 'eval_recall@spa.rst.sctb': 0.06191950464396284, 'eval_loss@spa.rst.sctb': 2.46733021736145, 'eval_runtime': 1.5441, 'eval_samples_per_second': 60.876, 'eval_steps_per_second': 1.943, 'epoch': 7.0}
{'train@spa.rst.sctb_loss': 2.392221450805664, 'train@spa.rst.sctb_accuracy@spa.rst.sctb': 0.3462414578587699, 'train@spa.rst.sctb_f1@spa.rst.sctb': 0.023085734425940613, 'train@spa.rst.sctb_precision@spa.rst.sctb': 0.035201149425287355, 'train@spa.rst.sctb_recall@spa.rst.sctb': 0.04256272401433692, 'train@spa.rst.sctb_loss@spa.rst.sctb': 2.392221450805664, 'train@spa.rst.sctb_runtime': 5.6791, 'train@spa.rst.sctb_samples_per_second': 77.301, 'train@spa.rst.sctb_steps_per_second': 2.465, 'epoch': 8.0}
{'loss': 2.4543, 'learning_rate': 6.666666666666667e-06, 'epoch': 8.0}
{'eval_loss': 2.4127237796783447, 'eval_accuracy@spa.rst.sctb': 0.3617021276595745, 'eval_f1@spa.rst.sctb': 0.03669467787114846, 'eval_precision@spa.rst.sctb': 0.07969639468690703, 'eval_recall@spa.rst.sctb': 0.06191950464396284, 'eval_loss@spa.rst.sctb': 2.4127230644226074, 'eval_runtime': 1.5435, 'eval_samples_per_second': 60.899, 'eval_steps_per_second': 1.944, 'epoch': 8.0}
{'train@spa.rst.sctb_loss': 2.3536088466644287, 'train@spa.rst.sctb_accuracy@spa.rst.sctb': 0.35079726651480636, 'train@spa.rst.sctb_f1@spa.rst.sctb': 0.02481495998136819, 'train@spa.rst.sctb_precision@spa.rst.sctb': 0.035334493426140756, 'train@spa.rst.sctb_recall@spa.rst.sctb': 0.04345878136200717, 'train@spa.rst.sctb_loss@spa.rst.sctb': 2.353609085083008, 'train@spa.rst.sctb_runtime': 5.6672, 'train@spa.rst.sctb_samples_per_second': 77.464, 'train@spa.rst.sctb_steps_per_second': 2.47, 'epoch': 9.0}
{'loss': 2.4198, 'learning_rate': 5e-06, 'epoch': 9.0}
{'eval_loss': 2.3768961429595947, 'eval_accuracy@spa.rst.sctb': 0.39361702127659576, 'eval_f1@spa.rst.sctb': 0.05202420310648119, 'eval_precision@spa.rst.sctb': 0.0803921568627451, 'eval_recall@spa.rst.sctb': 0.07120743034055728, 'eval_loss@spa.rst.sctb': 2.3768959045410156, 'eval_runtime': 1.5481, 'eval_samples_per_second': 60.718, 'eval_steps_per_second': 1.938, 'epoch': 9.0}
{'train@spa.rst.sctb_loss': 2.3290019035339355, 'train@spa.rst.sctb_accuracy@spa.rst.sctb': 0.35990888382687924, 'train@spa.rst.sctb_f1@spa.rst.sctb': 0.0284206221404289, 'train@spa.rst.sctb_precision@spa.rst.sctb': 0.031933544073831845, 'train@spa.rst.sctb_recall@spa.rst.sctb': 0.04542114695340502, 'train@spa.rst.sctb_loss@spa.rst.sctb': 2.3290016651153564, 'train@spa.rst.sctb_runtime': 5.6918, 'train@spa.rst.sctb_samples_per_second': 77.129, 'train@spa.rst.sctb_steps_per_second': 2.46, 'epoch': 10.0}
{'loss': 2.3789, 'learning_rate': 3.3333333333333333e-06, 'epoch': 10.0}
{'eval_loss': 2.3554937839508057, 'eval_accuracy@spa.rst.sctb': 0.39361702127659576, 'eval_f1@spa.rst.sctb': 0.05143040822886532, 'eval_precision@spa.rst.sctb': 0.06886979510905486, 'eval_recall@spa.rst.sctb': 0.07120743034055728, 'eval_loss@spa.rst.sctb': 2.3554937839508057, 'eval_runtime': 1.5751, 'eval_samples_per_second': 59.679, 'eval_steps_per_second': 1.905, 'epoch': 10.0}
{'train@spa.rst.sctb_loss': 2.316027879714966, 'train@spa.rst.sctb_accuracy@spa.rst.sctb': 0.36674259681093396, 'train@spa.rst.sctb_f1@spa.rst.sctb': 0.030397558472424785, 'train@spa.rst.sctb_precision@spa.rst.sctb': 0.03296257675819719, 'train@spa.rst.sctb_recall@spa.rst.sctb': 0.04676523297491039, 'train@spa.rst.sctb_loss@spa.rst.sctb': 2.316028118133545, 'train@spa.rst.sctb_runtime': 5.6693, 'train@spa.rst.sctb_samples_per_second': 77.434, 'train@spa.rst.sctb_steps_per_second': 2.469, 'epoch': 11.0}
{'loss': 2.3623, 'learning_rate': 1.6666666666666667e-06, 'epoch': 11.0}
{'eval_loss': 2.343489408493042, 'eval_accuracy@spa.rst.sctb': 0.40425531914893614, 'eval_f1@spa.rst.sctb': 0.05497737556561085, 'eval_precision@spa.rst.sctb': 0.06432917994784121, 'eval_recall@spa.rst.sctb': 0.07430340557275542, 'eval_loss@spa.rst.sctb': 2.343489408493042, 'eval_runtime': 4.935, 'eval_samples_per_second': 19.047, 'eval_steps_per_second': 0.608, 'epoch': 11.0}
{'train@spa.rst.sctb_loss': 2.311765670776367, 'train@spa.rst.sctb_accuracy@spa.rst.sctb': 0.36674259681093396, 'train@spa.rst.sctb_f1@spa.rst.sctb': 0.030397558472424785, 'train@spa.rst.sctb_precision@spa.rst.sctb': 0.03296257675819719, 'train@spa.rst.sctb_recall@spa.rst.sctb': 0.04676523297491039, 'train@spa.rst.sctb_loss@spa.rst.sctb': 2.311765670776367, 'train@spa.rst.sctb_runtime': 5.6945, 'train@spa.rst.sctb_samples_per_second': 77.092, 'train@spa.rst.sctb_steps_per_second': 2.459, 'epoch': 12.0}
{'loss': 2.3528, 'learning_rate': 0.0, 'epoch': 12.0}
{'eval_loss': 2.3398187160491943, 'eval_accuracy@spa.rst.sctb': 0.40425531914893614, 'eval_f1@spa.rst.sctb': 0.05497737556561085, 'eval_precision@spa.rst.sctb': 0.06432917994784121, 'eval_recall@spa.rst.sctb': 0.07430340557275542, 'eval_loss@spa.rst.sctb': 2.3398184776306152, 'eval_runtime': 1.5457, 'eval_samples_per_second': 60.815, 'eval_steps_per_second': 1.941, 'epoch': 12.0}
{'train_runtime': 223.6612, 'train_samples_per_second': 23.553, 'train_steps_per_second': 0.751, 'train_loss': 2.8052531424022855, 'epoch': 12.0}
***** train_log metrics *****
  epoch                    =       12.0
  train_loss               =     2.8053
  train_runtime            = 0:03:43.66
  train_samples_per_second =     23.553
  train_steps_per_second   =      0.751
{'train@eng.pdtb.pdtb_loss': 1.2425845861434937, 'train@eng.pdtb.pdtb_accuracy@eng.pdtb.pdtb': 0.6036429872495446, 'train@eng.pdtb.pdtb_f1@eng.pdtb.pdtb': 0.281866314976715, 'train@eng.pdtb.pdtb_precision@eng.pdtb.pdtb': 0.3501274521516614, 'train@eng.pdtb.pdtb_recall@eng.pdtb.pdtb': 0.2702852713192064, 'train@eng.pdtb.pdtb_loss@eng.pdtb.pdtb': 1.2425845861434937, 'train@eng.pdtb.pdtb_runtime': 515.4103, 'train@eng.pdtb.pdtb_samples_per_second': 85.214, 'train@eng.pdtb.pdtb_steps_per_second': 2.664, 'epoch': 1.0}
{'loss': 1.8263, 'learning_rate': 1.8333333333333333e-05, 'epoch': 1.0}
{'eval_loss': 1.1659587621688843, 'eval_accuracy@eng.pdtb.pdtb': 0.6326164874551972, 'eval_f1@eng.pdtb.pdtb': 0.32732727619608515, 'eval_precision@eng.pdtb.pdtb': 0.4101449485839931, 'eval_recall@eng.pdtb.pdtb': 0.3185262694615674, 'eval_loss@eng.pdtb.pdtb': 1.1659587621688843, 'eval_runtime': 20.0707, 'eval_samples_per_second': 83.405, 'eval_steps_per_second': 2.641, 'epoch': 1.0}
{'train@eng.pdtb.pdtb_loss': 1.08859121799469, 'train@eng.pdtb.pdtb_accuracy@eng.pdtb.pdtb': 0.6422131147540984, 'train@eng.pdtb.pdtb_f1@eng.pdtb.pdtb': 0.37427116430520846, 'train@eng.pdtb.pdtb_precision@eng.pdtb.pdtb': 0.4976255904449227, 'train@eng.pdtb.pdtb_recall@eng.pdtb.pdtb': 0.36222647788050255, 'train@eng.pdtb.pdtb_loss@eng.pdtb.pdtb': 1.08859121799469, 'train@eng.pdtb.pdtb_runtime': 515.6037, 'train@eng.pdtb.pdtb_samples_per_second': 85.182, 'train@eng.pdtb.pdtb_steps_per_second': 2.663, 'epoch': 2.0}
{'loss': 1.2038, 'learning_rate': 1.6666666666666667e-05, 'epoch': 2.0}
{'eval_loss': 1.0245460271835327, 'eval_accuracy@eng.pdtb.pdtb': 0.6666666666666666, 'eval_f1@eng.pdtb.pdtb': 0.41608384980262175, 'eval_precision@eng.pdtb.pdtb': 0.4932514020518264, 'eval_recall@eng.pdtb.pdtb': 0.40169184279922565, 'eval_loss@eng.pdtb.pdtb': 1.0245460271835327, 'eval_runtime': 20.0978, 'eval_samples_per_second': 83.293, 'eval_steps_per_second': 2.637, 'epoch': 2.0}
{'train@eng.pdtb.pdtb_loss': 1.0421494245529175, 'train@eng.pdtb.pdtb_accuracy@eng.pdtb.pdtb': 0.6539389799635701, 'train@eng.pdtb.pdtb_f1@eng.pdtb.pdtb': 0.42484688267262943, 'train@eng.pdtb.pdtb_precision@eng.pdtb.pdtb': 0.46615985175179003, 'train@eng.pdtb.pdtb_recall@eng.pdtb.pdtb': 0.4059654423235578, 'train@eng.pdtb.pdtb_loss@eng.pdtb.pdtb': 1.042149543762207, 'train@eng.pdtb.pdtb_runtime': 515.6211, 'train@eng.pdtb.pdtb_samples_per_second': 85.179, 'train@eng.pdtb.pdtb_steps_per_second': 2.663, 'epoch': 3.0}
{'loss': 1.113, 'learning_rate': 1.5000000000000002e-05, 'epoch': 3.0}
{'eval_loss': 0.9871286749839783, 'eval_accuracy@eng.pdtb.pdtb': 0.6726403823178017, 'eval_f1@eng.pdtb.pdtb': 0.4627415592741427, 'eval_precision@eng.pdtb.pdtb': 0.5146279299442986, 'eval_recall@eng.pdtb.pdtb': 0.4423889732169826, 'eval_loss@eng.pdtb.pdtb': 0.9871286749839783, 'eval_runtime': 20.0919, 'eval_samples_per_second': 83.317, 'eval_steps_per_second': 2.638, 'epoch': 3.0}
{'train@eng.pdtb.pdtb_loss': 0.9955452680587769, 'train@eng.pdtb.pdtb_accuracy@eng.pdtb.pdtb': 0.6682149362477231, 'train@eng.pdtb.pdtb_f1@eng.pdtb.pdtb': 0.44875789255830273, 'train@eng.pdtb.pdtb_precision@eng.pdtb.pdtb': 0.5187972423970496, 'train@eng.pdtb.pdtb_recall@eng.pdtb.pdtb': 0.43825180923798895, 'train@eng.pdtb.pdtb_loss@eng.pdtb.pdtb': 0.9955454468727112, 'train@eng.pdtb.pdtb_runtime': 516.1168, 'train@eng.pdtb.pdtb_samples_per_second': 85.097, 'train@eng.pdtb.pdtb_steps_per_second': 2.66, 'epoch': 4.0}
{'loss': 1.0666, 'learning_rate': 1.3333333333333333e-05, 'epoch': 4.0}
{'eval_loss': 0.9479125738143921, 'eval_accuracy@eng.pdtb.pdtb': 0.6798088410991637, 'eval_f1@eng.pdtb.pdtb': 0.49931464096838474, 'eval_precision@eng.pdtb.pdtb': 0.55326255712401, 'eval_recall@eng.pdtb.pdtb': 0.4778742837748631, 'eval_loss@eng.pdtb.pdtb': 0.9479126334190369, 'eval_runtime': 20.0751, 'eval_samples_per_second': 83.387, 'eval_steps_per_second': 2.64, 'epoch': 4.0}
{'train@eng.pdtb.pdtb_loss': 0.973698616027832, 'train@eng.pdtb.pdtb_accuracy@eng.pdtb.pdtb': 0.6742714025500911, 'train@eng.pdtb.pdtb_f1@eng.pdtb.pdtb': 0.4560793721805119, 'train@eng.pdtb.pdtb_precision@eng.pdtb.pdtb': 0.4996399359197971, 'train@eng.pdtb.pdtb_recall@eng.pdtb.pdtb': 0.44967121334823174, 'train@eng.pdtb.pdtb_loss@eng.pdtb.pdtb': 0.973698616027832, 'train@eng.pdtb.pdtb_runtime': 514.9943, 'train@eng.pdtb.pdtb_samples_per_second': 85.282, 'train@eng.pdtb.pdtb_steps_per_second': 2.666, 'epoch': 5.0}
{'loss': 1.0371, 'learning_rate': 1.1666666666666668e-05, 'epoch': 5.0}
{'eval_loss': 0.9308363199234009, 'eval_accuracy@eng.pdtb.pdtb': 0.6821983273596177, 'eval_f1@eng.pdtb.pdtb': 0.5152496939553027, 'eval_precision@eng.pdtb.pdtb': 0.550986903305199, 'eval_recall@eng.pdtb.pdtb': 0.50376964129413, 'eval_loss@eng.pdtb.pdtb': 0.9308362007141113, 'eval_runtime': 20.0457, 'eval_samples_per_second': 83.509, 'eval_steps_per_second': 2.644, 'epoch': 5.0}
{'train@eng.pdtb.pdtb_loss': 0.9548943638801575, 'train@eng.pdtb.pdtb_accuracy@eng.pdtb.pdtb': 0.6803734061930783, 'train@eng.pdtb.pdtb_f1@eng.pdtb.pdtb': 0.4639377893334215, 'train@eng.pdtb.pdtb_precision@eng.pdtb.pdtb': 0.509104556513188, 'train@eng.pdtb.pdtb_recall@eng.pdtb.pdtb': 0.4597570062938411, 'train@eng.pdtb.pdtb_loss@eng.pdtb.pdtb': 0.954894483089447, 'train@eng.pdtb.pdtb_runtime': 514.9261, 'train@eng.pdtb.pdtb_samples_per_second': 85.294, 'train@eng.pdtb.pdtb_steps_per_second': 2.666, 'epoch': 6.0}
{'loss': 1.0149, 'learning_rate': 1e-05, 'epoch': 6.0}
{'eval_loss': 0.9259950518608093, 'eval_accuracy@eng.pdtb.pdtb': 0.6816009557945042, 'eval_f1@eng.pdtb.pdtb': 0.5305189111156491, 'eval_precision@eng.pdtb.pdtb': 0.5538945144005127, 'eval_recall@eng.pdtb.pdtb': 0.5250287344744949, 'eval_loss@eng.pdtb.pdtb': 0.9259951114654541, 'eval_runtime': 20.0707, 'eval_samples_per_second': 83.405, 'eval_steps_per_second': 2.641, 'epoch': 6.0}
{'train@eng.pdtb.pdtb_loss': 0.9442269206047058, 'train@eng.pdtb.pdtb_accuracy@eng.pdtb.pdtb': 0.6810564663023679, 'train@eng.pdtb.pdtb_f1@eng.pdtb.pdtb': 0.46674690523238443, 'train@eng.pdtb.pdtb_precision@eng.pdtb.pdtb': 0.531131579923367, 'train@eng.pdtb.pdtb_recall@eng.pdtb.pdtb': 0.45927716247576594, 'train@eng.pdtb.pdtb_loss@eng.pdtb.pdtb': 0.9442269206047058, 'train@eng.pdtb.pdtb_runtime': 515.0785, 'train@eng.pdtb.pdtb_samples_per_second': 85.269, 'train@eng.pdtb.pdtb_steps_per_second': 2.666, 'epoch': 7.0}
{'loss': 1.0024, 'learning_rate': 8.333333333333334e-06, 'epoch': 7.0}
{'eval_loss': 0.9101747870445251, 'eval_accuracy@eng.pdtb.pdtb': 0.6816009557945042, 'eval_f1@eng.pdtb.pdtb': 0.5301330380271396, 'eval_precision@eng.pdtb.pdtb': 0.5649202063839088, 'eval_recall@eng.pdtb.pdtb': 0.5181758318273662, 'eval_loss@eng.pdtb.pdtb': 0.9101747870445251, 'eval_runtime': 20.082, 'eval_samples_per_second': 83.358, 'eval_steps_per_second': 2.639, 'epoch': 7.0}
{'train@eng.pdtb.pdtb_loss': 0.9326833486557007, 'train@eng.pdtb.pdtb_accuracy@eng.pdtb.pdtb': 0.6846539162112932, 'train@eng.pdtb.pdtb_f1@eng.pdtb.pdtb': 0.47168555604913076, 'train@eng.pdtb.pdtb_precision@eng.pdtb.pdtb': 0.51686588365503, 'train@eng.pdtb.pdtb_recall@eng.pdtb.pdtb': 0.47021722485164213, 'train@eng.pdtb.pdtb_loss@eng.pdtb.pdtb': 0.9326832890510559, 'train@eng.pdtb.pdtb_runtime': 515.2183, 'train@eng.pdtb.pdtb_samples_per_second': 85.245, 'train@eng.pdtb.pdtb_steps_per_second': 2.665, 'epoch': 8.0}
{'loss': 0.9898, 'learning_rate': 6.666666666666667e-06, 'epoch': 8.0}
{'eval_loss': 0.9080819487571716, 'eval_accuracy@eng.pdtb.pdtb': 0.6869772998805257, 'eval_f1@eng.pdtb.pdtb': 0.5527115695598699, 'eval_precision@eng.pdtb.pdtb': 0.6188925066964427, 'eval_recall@eng.pdtb.pdtb': 0.5378470192337639, 'eval_loss@eng.pdtb.pdtb': 0.9080819487571716, 'eval_runtime': 20.0784, 'eval_samples_per_second': 83.373, 'eval_steps_per_second': 2.64, 'epoch': 8.0}
{'train@eng.pdtb.pdtb_loss': 0.924396812915802, 'train@eng.pdtb.pdtb_accuracy@eng.pdtb.pdtb': 0.6876366120218579, 'train@eng.pdtb.pdtb_f1@eng.pdtb.pdtb': 0.4781076339503092, 'train@eng.pdtb.pdtb_precision@eng.pdtb.pdtb': 0.5659416925071354, 'train@eng.pdtb.pdtb_recall@eng.pdtb.pdtb': 0.4730400425295331, 'train@eng.pdtb.pdtb_loss@eng.pdtb.pdtb': 0.924396812915802, 'train@eng.pdtb.pdtb_runtime': 514.9355, 'train@eng.pdtb.pdtb_samples_per_second': 85.292, 'train@eng.pdtb.pdtb_steps_per_second': 2.666, 'epoch': 9.0}
{'loss': 0.9786, 'learning_rate': 5e-06, 'epoch': 9.0}
{'eval_loss': 0.9059356451034546, 'eval_accuracy@eng.pdtb.pdtb': 0.6845878136200717, 'eval_f1@eng.pdtb.pdtb': 0.5385119247810073, 'eval_precision@eng.pdtb.pdtb': 0.5659891125451996, 'eval_recall@eng.pdtb.pdtb': 0.5289267842345452, 'eval_loss@eng.pdtb.pdtb': 0.9059356451034546, 'eval_runtime': 20.0756, 'eval_samples_per_second': 83.385, 'eval_steps_per_second': 2.64, 'epoch': 9.0}
{'train@eng.pdtb.pdtb_loss': 0.9210423231124878, 'train@eng.pdtb.pdtb_accuracy@eng.pdtb.pdtb': 0.6888205828779599, 'train@eng.pdtb.pdtb_f1@eng.pdtb.pdtb': 0.47842948009301567, 'train@eng.pdtb.pdtb_precision@eng.pdtb.pdtb': 0.5729792214740559, 'train@eng.pdtb.pdtb_recall@eng.pdtb.pdtb': 0.47286329458463994, 'train@eng.pdtb.pdtb_loss@eng.pdtb.pdtb': 0.921042263507843, 'train@eng.pdtb.pdtb_runtime': 515.0899, 'train@eng.pdtb.pdtb_samples_per_second': 85.267, 'train@eng.pdtb.pdtb_steps_per_second': 2.666, 'epoch': 10.0}
{'loss': 0.9745, 'learning_rate': 3.3333333333333333e-06, 'epoch': 10.0}
{'eval_loss': 0.9013543725013733, 'eval_accuracy@eng.pdtb.pdtb': 0.6857825567502986, 'eval_f1@eng.pdtb.pdtb': 0.5479021986017246, 'eval_precision@eng.pdtb.pdtb': 0.6184182625727924, 'eval_recall@eng.pdtb.pdtb': 0.5316100601709175, 'eval_loss@eng.pdtb.pdtb': 0.9013544321060181, 'eval_runtime': 20.0589, 'eval_samples_per_second': 83.454, 'eval_steps_per_second': 2.642, 'epoch': 10.0}
{'train@eng.pdtb.pdtb_loss': 0.9164085388183594, 'train@eng.pdtb.pdtb_accuracy@eng.pdtb.pdtb': 0.6893897996357012, 'train@eng.pdtb.pdtb_f1@eng.pdtb.pdtb': 0.48343292656382136, 'train@eng.pdtb.pdtb_precision@eng.pdtb.pdtb': 0.5680982556677663, 'train@eng.pdtb.pdtb_recall@eng.pdtb.pdtb': 0.478559502986054, 'train@eng.pdtb.pdtb_loss@eng.pdtb.pdtb': 0.9164084792137146, 'train@eng.pdtb.pdtb_runtime': 515.2382, 'train@eng.pdtb.pdtb_samples_per_second': 85.242, 'train@eng.pdtb.pdtb_steps_per_second': 2.665, 'epoch': 11.0}
{'loss': 0.9684, 'learning_rate': 1.6666666666666667e-06, 'epoch': 11.0}
{'eval_loss': 0.9006840586662292, 'eval_accuracy@eng.pdtb.pdtb': 0.6917562724014337, 'eval_f1@eng.pdtb.pdtb': 0.5603659555353742, 'eval_precision@eng.pdtb.pdtb': 0.5963843539607004, 'eval_recall@eng.pdtb.pdtb': 0.5484199294869041, 'eval_loss@eng.pdtb.pdtb': 0.9006840586662292, 'eval_runtime': 20.0783, 'eval_samples_per_second': 83.374, 'eval_steps_per_second': 2.64, 'epoch': 11.0}
{'train@eng.pdtb.pdtb_loss': 0.9152104258537292, 'train@eng.pdtb.pdtb_accuracy@eng.pdtb.pdtb': 0.6906420765027322, 'train@eng.pdtb.pdtb_f1@eng.pdtb.pdtb': 0.48375160389269867, 'train@eng.pdtb.pdtb_precision@eng.pdtb.pdtb': 0.5716464042728756, 'train@eng.pdtb.pdtb_recall@eng.pdtb.pdtb': 0.47709774596265214, 'train@eng.pdtb.pdtb_loss@eng.pdtb.pdtb': 0.9152104258537292, 'train@eng.pdtb.pdtb_runtime': 515.1481, 'train@eng.pdtb.pdtb_samples_per_second': 85.257, 'train@eng.pdtb.pdtb_steps_per_second': 2.665, 'epoch': 12.0}
{'loss': 0.9649, 'learning_rate': 0.0, 'epoch': 12.0}
{'eval_loss': 0.8988729119300842, 'eval_accuracy@eng.pdtb.pdtb': 0.6869772998805257, 'eval_f1@eng.pdtb.pdtb': 0.554749933939291, 'eval_precision@eng.pdtb.pdtb': 0.6161479401020987, 'eval_recall@eng.pdtb.pdtb': 0.5415756300874442, 'eval_loss@eng.pdtb.pdtb': 0.8988728523254395, 'eval_runtime': 20.1014, 'eval_samples_per_second': 83.278, 'eval_steps_per_second': 2.637, 'epoch': 12.0}
{'train_runtime': 19530.3875, 'train_samples_per_second': 26.986, 'train_steps_per_second': 0.844, 'train_loss': 1.0950217509796447, 'epoch': 12.0}
***** train_logger metrics *****
  epoch                    =       12.0
  train_loss               =     2.8053
  train_runtime            = 0:03:43.66
  train_samples_per_second =     23.553
  train_steps_per_second   =      0.751
-------------------------------------------------------------------
Lang1:  tur.pdtb.tdb    Lang2:  eng.pdtb.pdtb
Saving run to:  runs/full_shot/FullShot=v4_tur.pdtb.tdb_eng.pdtb.pdtb_lrfunc=Adafactor_lr=2e-05_plm=bert-base-multilingual-cased
Running with params: BERT_MODEL=bert-base-multilingual-cased lr=2e-05
read 2451 examples
read 312 examples
read 422 examples
read 43920 examples
read 1674 examples
read 2257 examples
Total prediction labels:  25
BertAdapterModel(
  (shared_parameters): ModuleDict()
  (bert): BertModel(
    (shared_parameters): ModuleDict()
    (invertible_adapters): ModuleDict(
      (tr): NICECouplingBlock(
        (F): Sequential(
          (0): Linear(in_features=384, out_features=192, bias=True)
          (1): Activation_Function_Class(
            (f): NewGELUActivation()
          )
          (2): Linear(in_features=192, out_features=384, bias=True)
        )
        (G): Sequential(
          (0): Linear(in_features=384, out_features=192, bias=True)
          (1): Activation_Function_Class(
            (f): NewGELUActivation()
          )
          (2): Linear(in_features=192, out_features=384, bias=True)
        )
      )
      (en): NICECouplingBlock(
        (F): Sequential(
          (0): Linear(in_features=384, out_features=192, bias=True)
          (1): Activation_Function_Class(
            (f): ReLU()
          )
          (2): Linear(in_features=192, out_features=384, bias=True)
        )
        (G): Sequential(
          (0): Linear(in_features=384, out_features=192, bias=True)
          (1): Activation_Function_Class(
            (f): ReLU()
          )
          (2): Linear(in_features=192, out_features=384, bias=True)
        )
      )
    )
    (embeddings): BertEmbeddings(
      (word_embeddings): Embedding(119547, 768, padding_idx=0)
      (position_embeddings): Embedding(512, 768)
      (token_type_embeddings): Embedding(2, 768)
      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
      (dropout): Dropout(p=0.1, inplace=False)
    )
    (encoder): BertEncoder(
      (layer): ModuleList(
        (0): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (tr): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (en): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (1): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (tr): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (en): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (2): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (tr): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (en): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (3): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (tr): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (en): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (4): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (tr): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (en): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (5): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (tr): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (en): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (6): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (tr): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (en): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (7): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (tr): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (en): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (8): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (tr): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (en): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (9): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (tr): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (en): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (10): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (tr): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (en): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (11): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (tr): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (en): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
      )
    )
    (pooler): BertPooler(
      (dense): Linear(in_features=768, out_features=768, bias=True)
      (activation): Tanh()
    )
    (prefix_tuning): PrefixTuningPool(
      (prefix_tunings): ModuleDict()
    )
  )
  (heads): ModuleDict(
    (tr): BertStyleMaskedLMHead(
      (0): Linear(in_features=768, out_features=768, bias=True)
      (1): Activation_Function_Class(
        (f): GELUActivation()
      )
      (2): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
      (3): Linear(in_features=768, out_features=119547, bias=True)
    )
    (disrpt): ClassificationHead(
      (0): Dropout(p=0.1, inplace=False)
      (1): Linear(in_features=768, out_features=768, bias=True)
      (2): Activation_Function_Class(
        (f): Tanh()
      )
      (3): Dropout(p=0.1, inplace=False)
      (4): Linear(in_features=768, out_features=25, bias=True)
    )
  )
)
{'train@tur.pdtb.tdb_loss': 2.573967456817627, 'train@tur.pdtb.tdb_accuracy@tur.pdtb.tdb': 0.25050999592003265, 'train@tur.pdtb.tdb_f1@tur.pdtb.tdb': 0.01741967515426626, 'train@tur.pdtb.tdb_precision@tur.pdtb.tdb': 0.010891738953044898, 'train@tur.pdtb.tdb_recall@tur.pdtb.tdb': 0.043478260869565216, 'train@tur.pdtb.tdb_loss@tur.pdtb.tdb': 2.573967456817627, 'train@tur.pdtb.tdb_runtime': 29.447, 'train@tur.pdtb.tdb_samples_per_second': 83.234, 'train@tur.pdtb.tdb_steps_per_second': 2.615, 'epoch': 1.0}
{'loss': 2.8571, 'learning_rate': 1.8333333333333333e-05, 'epoch': 1.0}
{'eval_loss': 2.5069639682769775, 'eval_accuracy@tur.pdtb.tdb': 0.266025641025641, 'eval_f1@tur.pdtb.tdb': 0.019102416570771, 'eval_precision@tur.pdtb.tdb': 0.012092074592074592, 'eval_recall@tur.pdtb.tdb': 0.045454545454545456, 'eval_loss@tur.pdtb.tdb': 2.5069639682769775, 'eval_runtime': 4.0085, 'eval_samples_per_second': 77.835, 'eval_steps_per_second': 2.495, 'epoch': 1.0}
{'train@tur.pdtb.tdb_loss': 2.3619775772094727, 'train@tur.pdtb.tdb_accuracy@tur.pdtb.tdb': 0.25050999592003265, 'train@tur.pdtb.tdb_f1@tur.pdtb.tdb': 0.01741967515426626, 'train@tur.pdtb.tdb_precision@tur.pdtb.tdb': 0.010891738953044898, 'train@tur.pdtb.tdb_recall@tur.pdtb.tdb': 0.043478260869565216, 'train@tur.pdtb.tdb_loss@tur.pdtb.tdb': 2.3619775772094727, 'train@tur.pdtb.tdb_runtime': 29.4299, 'train@tur.pdtb.tdb_samples_per_second': 83.283, 'train@tur.pdtb.tdb_steps_per_second': 2.616, 'epoch': 2.0}
{'loss': 2.4685, 'learning_rate': 1.6666666666666667e-05, 'epoch': 2.0}
{'eval_loss': 2.283029556274414, 'eval_accuracy@tur.pdtb.tdb': 0.266025641025641, 'eval_f1@tur.pdtb.tdb': 0.019102416570771, 'eval_precision@tur.pdtb.tdb': 0.012092074592074592, 'eval_recall@tur.pdtb.tdb': 0.045454545454545456, 'eval_loss@tur.pdtb.tdb': 2.283029317855835, 'eval_runtime': 3.9982, 'eval_samples_per_second': 78.036, 'eval_steps_per_second': 2.501, 'epoch': 2.0}
{'train@tur.pdtb.tdb_loss': 2.298520088195801, 'train@tur.pdtb.tdb_accuracy@tur.pdtb.tdb': 0.2562219502243982, 'train@tur.pdtb.tdb_f1@tur.pdtb.tdb': 0.023113463187736945, 'train@tur.pdtb.tdb_precision@tur.pdtb.tdb': 0.049729423348662635, 'train@tur.pdtb.tdb_recall@tur.pdtb.tdb': 0.046382237585587974, 'train@tur.pdtb.tdb_loss@tur.pdtb.tdb': 2.29852032661438, 'train@tur.pdtb.tdb_runtime': 29.4054, 'train@tur.pdtb.tdb_samples_per_second': 83.352, 'train@tur.pdtb.tdb_steps_per_second': 2.619, 'epoch': 3.0}
{'loss': 2.3539, 'learning_rate': 1.5000000000000002e-05, 'epoch': 3.0}
{'eval_loss': 2.2423150539398193, 'eval_accuracy@tur.pdtb.tdb': 0.2692307692307692, 'eval_f1@tur.pdtb.tdb': 0.023441075826594885, 'eval_precision@tur.pdtb.tdb': 0.02520757930593996, 'eval_recall@tur.pdtb.tdb': 0.047432152853839595, 'eval_loss@tur.pdtb.tdb': 2.2423152923583984, 'eval_runtime': 3.9793, 'eval_samples_per_second': 78.405, 'eval_steps_per_second': 2.513, 'epoch': 3.0}
{'train@tur.pdtb.tdb_loss': 2.2342681884765625, 'train@tur.pdtb.tdb_accuracy@tur.pdtb.tdb': 0.30273357813137497, 'train@tur.pdtb.tdb_f1@tur.pdtb.tdb': 0.0554758166779305, 'train@tur.pdtb.tdb_precision@tur.pdtb.tdb': 0.08828108877703002, 'train@tur.pdtb.tdb_recall@tur.pdtb.tdb': 0.07235337539632762, 'train@tur.pdtb.tdb_loss@tur.pdtb.tdb': 2.2342681884765625, 'train@tur.pdtb.tdb_runtime': 29.4325, 'train@tur.pdtb.tdb_samples_per_second': 83.275, 'train@tur.pdtb.tdb_steps_per_second': 2.616, 'epoch': 4.0}
{'loss': 2.2961, 'learning_rate': 1.3333333333333333e-05, 'epoch': 4.0}
{'eval_loss': 2.2046189308166504, 'eval_accuracy@tur.pdtb.tdb': 0.2692307692307692, 'eval_f1@tur.pdtb.tdb': 0.041300515127330774, 'eval_precision@tur.pdtb.tdb': 0.07103896103896103, 'eval_recall@tur.pdtb.tdb': 0.056828025112929434, 'eval_loss@tur.pdtb.tdb': 2.2046186923980713, 'eval_runtime': 3.9827, 'eval_samples_per_second': 78.339, 'eval_steps_per_second': 2.511, 'epoch': 4.0}
{'train@tur.pdtb.tdb_loss': 2.189509153366089, 'train@tur.pdtb.tdb_accuracy@tur.pdtb.tdb': 0.32109343125254997, 'train@tur.pdtb.tdb_f1@tur.pdtb.tdb': 0.081942630436433, 'train@tur.pdtb.tdb_precision@tur.pdtb.tdb': 0.09807714179866496, 'train@tur.pdtb.tdb_recall@tur.pdtb.tdb': 0.10062483550854495, 'train@tur.pdtb.tdb_loss@tur.pdtb.tdb': 2.189509153366089, 'train@tur.pdtb.tdb_runtime': 29.4154, 'train@tur.pdtb.tdb_samples_per_second': 83.324, 'train@tur.pdtb.tdb_steps_per_second': 2.618, 'epoch': 5.0}
{'loss': 2.2524, 'learning_rate': 1.1666666666666668e-05, 'epoch': 5.0}
{'eval_loss': 2.1702628135681152, 'eval_accuracy@tur.pdtb.tdb': 0.2916666666666667, 'eval_f1@tur.pdtb.tdb': 0.07114795612819329, 'eval_precision@tur.pdtb.tdb': 0.0634017382108985, 'eval_recall@tur.pdtb.tdb': 0.09881057205649692, 'eval_loss@tur.pdtb.tdb': 2.1702628135681152, 'eval_runtime': 3.9841, 'eval_samples_per_second': 78.311, 'eval_steps_per_second': 2.51, 'epoch': 5.0}
{'train@tur.pdtb.tdb_loss': 2.1468589305877686, 'train@tur.pdtb.tdb_accuracy@tur.pdtb.tdb': 0.33414932680538556, 'train@tur.pdtb.tdb_f1@tur.pdtb.tdb': 0.0876609015661309, 'train@tur.pdtb.tdb_precision@tur.pdtb.tdb': 0.10428165353537945, 'train@tur.pdtb.tdb_recall@tur.pdtb.tdb': 0.11008134745697301, 'train@tur.pdtb.tdb_loss@tur.pdtb.tdb': 2.1468589305877686, 'train@tur.pdtb.tdb_runtime': 29.4043, 'train@tur.pdtb.tdb_samples_per_second': 83.355, 'train@tur.pdtb.tdb_steps_per_second': 2.619, 'epoch': 6.0}
{'loss': 2.2129, 'learning_rate': 1e-05, 'epoch': 6.0}
{'eval_loss': 2.1430299282073975, 'eval_accuracy@tur.pdtb.tdb': 0.30448717948717946, 'eval_f1@tur.pdtb.tdb': 0.07395820947381326, 'eval_precision@tur.pdtb.tdb': 0.06407191532550384, 'eval_recall@tur.pdtb.tdb': 0.10408389350628897, 'eval_loss@tur.pdtb.tdb': 2.1430299282073975, 'eval_runtime': 3.994, 'eval_samples_per_second': 78.117, 'eval_steps_per_second': 2.504, 'epoch': 6.0}
{'train@tur.pdtb.tdb_loss': 2.1174280643463135, 'train@tur.pdtb.tdb_accuracy@tur.pdtb.tdb': 0.3402692778457772, 'train@tur.pdtb.tdb_f1@tur.pdtb.tdb': 0.0913756575262517, 'train@tur.pdtb.tdb_precision@tur.pdtb.tdb': 0.10088088229282491, 'train@tur.pdtb.tdb_recall@tur.pdtb.tdb': 0.11425017651512882, 'train@tur.pdtb.tdb_loss@tur.pdtb.tdb': 2.1174283027648926, 'train@tur.pdtb.tdb_runtime': 29.4918, 'train@tur.pdtb.tdb_samples_per_second': 83.108, 'train@tur.pdtb.tdb_steps_per_second': 2.611, 'epoch': 7.0}
{'loss': 2.1755, 'learning_rate': 8.333333333333334e-06, 'epoch': 7.0}
{'eval_loss': 2.123792886734009, 'eval_accuracy@tur.pdtb.tdb': 0.32051282051282054, 'eval_f1@tur.pdtb.tdb': 0.07842847058533332, 'eval_precision@tur.pdtb.tdb': 0.07270732759845291, 'eval_recall@tur.pdtb.tdb': 0.11120888265143035, 'eval_loss@tur.pdtb.tdb': 2.123793125152588, 'eval_runtime': 3.986, 'eval_samples_per_second': 78.273, 'eval_steps_per_second': 2.509, 'epoch': 7.0}
{'train@tur.pdtb.tdb_loss': 2.1001744270324707, 'train@tur.pdtb.tdb_accuracy@tur.pdtb.tdb': 0.3459812321501428, 'train@tur.pdtb.tdb_f1@tur.pdtb.tdb': 0.09405755024958509, 'train@tur.pdtb.tdb_precision@tur.pdtb.tdb': 0.10581360270028468, 'train@tur.pdtb.tdb_recall@tur.pdtb.tdb': 0.11583952790272652, 'train@tur.pdtb.tdb_loss@tur.pdtb.tdb': 2.1001741886138916, 'train@tur.pdtb.tdb_runtime': 29.503, 'train@tur.pdtb.tdb_samples_per_second': 83.076, 'train@tur.pdtb.tdb_steps_per_second': 2.61, 'epoch': 8.0}
{'loss': 2.1501, 'learning_rate': 6.666666666666667e-06, 'epoch': 8.0}
{'eval_loss': 2.1101832389831543, 'eval_accuracy@tur.pdtb.tdb': 0.32371794871794873, 'eval_f1@tur.pdtb.tdb': 0.08354885854885855, 'eval_precision@tur.pdtb.tdb': 0.08022386878186251, 'eval_recall@tur.pdtb.tdb': 0.112986278213684, 'eval_loss@tur.pdtb.tdb': 2.1101832389831543, 'eval_runtime': 3.991, 'eval_samples_per_second': 78.176, 'eval_steps_per_second': 2.506, 'epoch': 8.0}
{'train@tur.pdtb.tdb_loss': 2.0795319080352783, 'train@tur.pdtb.tdb_accuracy@tur.pdtb.tdb': 0.34394124847001223, 'train@tur.pdtb.tdb_f1@tur.pdtb.tdb': 0.09315809350528599, 'train@tur.pdtb.tdb_precision@tur.pdtb.tdb': 0.10142219034516076, 'train@tur.pdtb.tdb_recall@tur.pdtb.tdb': 0.11589090478444511, 'train@tur.pdtb.tdb_loss@tur.pdtb.tdb': 2.0795319080352783, 'train@tur.pdtb.tdb_runtime': 29.4678, 'train@tur.pdtb.tdb_samples_per_second': 83.176, 'train@tur.pdtb.tdb_steps_per_second': 2.613, 'epoch': 9.0}
{'loss': 2.1324, 'learning_rate': 5e-06, 'epoch': 9.0}
{'eval_loss': 2.0942423343658447, 'eval_accuracy@tur.pdtb.tdb': 0.32371794871794873, 'eval_f1@tur.pdtb.tdb': 0.08006521156149939, 'eval_precision@tur.pdtb.tdb': 0.07324773933074534, 'eval_recall@tur.pdtb.tdb': 0.1133350343169158, 'eval_loss@tur.pdtb.tdb': 2.094242572784424, 'eval_runtime': 3.9879, 'eval_samples_per_second': 78.236, 'eval_steps_per_second': 2.508, 'epoch': 9.0}
{'train@tur.pdtb.tdb_loss': 2.0680315494537354, 'train@tur.pdtb.tdb_accuracy@tur.pdtb.tdb': 0.3496532027743778, 'train@tur.pdtb.tdb_f1@tur.pdtb.tdb': 0.09677579221988358, 'train@tur.pdtb.tdb_precision@tur.pdtb.tdb': 0.12152565316101228, 'train@tur.pdtb.tdb_recall@tur.pdtb.tdb': 0.11837254110032643, 'train@tur.pdtb.tdb_loss@tur.pdtb.tdb': 2.0680313110351562, 'train@tur.pdtb.tdb_runtime': 29.4118, 'train@tur.pdtb.tdb_samples_per_second': 83.334, 'train@tur.pdtb.tdb_steps_per_second': 2.618, 'epoch': 10.0}
{'loss': 2.1177, 'learning_rate': 3.3333333333333333e-06, 'epoch': 10.0}
{'eval_loss': 2.0851476192474365, 'eval_accuracy@tur.pdtb.tdb': 0.32371794871794873, 'eval_f1@tur.pdtb.tdb': 0.08081003229023806, 'eval_precision@tur.pdtb.tdb': 0.07255483750699061, 'eval_recall@tur.pdtb.tdb': 0.1133350343169158, 'eval_loss@tur.pdtb.tdb': 2.0851471424102783, 'eval_runtime': 4.0058, 'eval_samples_per_second': 77.886, 'eval_steps_per_second': 2.496, 'epoch': 10.0}
{'train@tur.pdtb.tdb_loss': 2.062840700149536, 'train@tur.pdtb.tdb_accuracy@tur.pdtb.tdb': 0.35046919624643, 'train@tur.pdtb.tdb_f1@tur.pdtb.tdb': 0.09714987361273018, 'train@tur.pdtb.tdb_precision@tur.pdtb.tdb': 0.12156066430725054, 'train@tur.pdtb.tdb_recall@tur.pdtb.tdb': 0.11845637207687389, 'train@tur.pdtb.tdb_loss@tur.pdtb.tdb': 2.062840700149536, 'train@tur.pdtb.tdb_runtime': 29.4881, 'train@tur.pdtb.tdb_samples_per_second': 83.118, 'train@tur.pdtb.tdb_steps_per_second': 2.611, 'epoch': 11.0}
{'loss': 2.1171, 'learning_rate': 1.6666666666666667e-06, 'epoch': 11.0}
{'eval_loss': 2.082310438156128, 'eval_accuracy@tur.pdtb.tdb': 0.3301282051282051, 'eval_f1@tur.pdtb.tdb': 0.08326597837075822, 'eval_precision@tur.pdtb.tdb': 0.07566158896535836, 'eval_recall@tur.pdtb.tdb': 0.11514530570550044, 'eval_loss@tur.pdtb.tdb': 2.082310438156128, 'eval_runtime': 3.9966, 'eval_samples_per_second': 78.067, 'eval_steps_per_second': 2.502, 'epoch': 11.0}
{'train@tur.pdtb.tdb_loss': 2.0601847171783447, 'train@tur.pdtb.tdb_accuracy@tur.pdtb.tdb': 0.35454916360669114, 'train@tur.pdtb.tdb_f1@tur.pdtb.tdb': 0.09930026703693029, 'train@tur.pdtb.tdb_precision@tur.pdtb.tdb': 0.116129193056128, 'train@tur.pdtb.tdb_recall@tur.pdtb.tdb': 0.12022053466893269, 'train@tur.pdtb.tdb_loss@tur.pdtb.tdb': 2.060184955596924, 'train@tur.pdtb.tdb_runtime': 29.4953, 'train@tur.pdtb.tdb_samples_per_second': 83.098, 'train@tur.pdtb.tdb_steps_per_second': 2.611, 'epoch': 12.0}
{'loss': 2.1051, 'learning_rate': 0.0, 'epoch': 12.0}
{'eval_loss': 2.0800602436065674, 'eval_accuracy@tur.pdtb.tdb': 0.3301282051282051, 'eval_f1@tur.pdtb.tdb': 0.08437116754122828, 'eval_precision@tur.pdtb.tdb': 0.07855542964184963, 'eval_recall@tur.pdtb.tdb': 0.1155858028720322, 'eval_loss@tur.pdtb.tdb': 2.0800602436065674, 'eval_runtime': 4.0073, 'eval_samples_per_second': 77.858, 'eval_steps_per_second': 2.495, 'epoch': 12.0}
{'train_runtime': 1147.8428, 'train_samples_per_second': 25.624, 'train_steps_per_second': 0.805, 'train_loss': 2.269896321482473, 'epoch': 12.0}
***** train_log metrics *****
  epoch                    =       12.0
  train_loss               =     2.2699
  train_runtime            = 0:19:07.84
  train_samples_per_second =     25.624
  train_steps_per_second   =      0.805
{'train@eng.pdtb.pdtb_loss': 1.2452126741409302, 'train@eng.pdtb.pdtb_accuracy@eng.pdtb.pdtb': 0.5976775956284153, 'train@eng.pdtb.pdtb_f1@eng.pdtb.pdtb': 0.26888282214607484, 'train@eng.pdtb.pdtb_precision@eng.pdtb.pdtb': 0.3575581308429854, 'train@eng.pdtb.pdtb_recall@eng.pdtb.pdtb': 0.26245356757236477, 'train@eng.pdtb.pdtb_loss@eng.pdtb.pdtb': 1.2452126741409302, 'train@eng.pdtb.pdtb_runtime': 514.8256, 'train@eng.pdtb.pdtb_samples_per_second': 85.31, 'train@eng.pdtb.pdtb_steps_per_second': 2.667, 'epoch': 1.0}
{'loss': 1.6629, 'learning_rate': 1.8333333333333333e-05, 'epoch': 1.0}
{'eval_loss': 1.1752817630767822, 'eval_accuracy@eng.pdtb.pdtb': 0.6212664277180406, 'eval_f1@eng.pdtb.pdtb': 0.31272882527305024, 'eval_precision@eng.pdtb.pdtb': 0.3591669305380239, 'eval_recall@eng.pdtb.pdtb': 0.308284820710507, 'eval_loss@eng.pdtb.pdtb': 1.1752818822860718, 'eval_runtime': 19.9286, 'eval_samples_per_second': 84.0, 'eval_steps_per_second': 2.659, 'epoch': 1.0}
{'train@eng.pdtb.pdtb_loss': 1.0935490131378174, 'train@eng.pdtb.pdtb_accuracy@eng.pdtb.pdtb': 0.6401183970856102, 'train@eng.pdtb.pdtb_f1@eng.pdtb.pdtb': 0.3810196222797882, 'train@eng.pdtb.pdtb_precision@eng.pdtb.pdtb': 0.4747429865294406, 'train@eng.pdtb.pdtb_recall@eng.pdtb.pdtb': 0.36359182833980236, 'train@eng.pdtb.pdtb_loss@eng.pdtb.pdtb': 1.0935488939285278, 'train@eng.pdtb.pdtb_runtime': 515.1459, 'train@eng.pdtb.pdtb_samples_per_second': 85.257, 'train@eng.pdtb.pdtb_steps_per_second': 2.665, 'epoch': 2.0}
{'loss': 1.2131, 'learning_rate': 1.6666666666666667e-05, 'epoch': 2.0}
{'eval_loss': 1.0350825786590576, 'eval_accuracy@eng.pdtb.pdtb': 0.6624850657108722, 'eval_f1@eng.pdtb.pdtb': 0.42693103585545283, 'eval_precision@eng.pdtb.pdtb': 0.49110885324920145, 'eval_recall@eng.pdtb.pdtb': 0.40968556669714984, 'eval_loss@eng.pdtb.pdtb': 1.0350825786590576, 'eval_runtime': 19.8934, 'eval_samples_per_second': 84.149, 'eval_steps_per_second': 2.664, 'epoch': 2.0}
{'train@eng.pdtb.pdtb_loss': 1.0452593564987183, 'train@eng.pdtb.pdtb_accuracy@eng.pdtb.pdtb': 0.6537795992714025, 'train@eng.pdtb.pdtb_f1@eng.pdtb.pdtb': 0.4348899626187935, 'train@eng.pdtb.pdtb_precision@eng.pdtb.pdtb': 0.46980547840451303, 'train@eng.pdtb.pdtb_recall@eng.pdtb.pdtb': 0.41856720482913373, 'train@eng.pdtb.pdtb_loss@eng.pdtb.pdtb': 1.0452592372894287, 'train@eng.pdtb.pdtb_runtime': 514.9728, 'train@eng.pdtb.pdtb_samples_per_second': 85.286, 'train@eng.pdtb.pdtb_steps_per_second': 2.666, 'epoch': 3.0}
{'loss': 1.1202, 'learning_rate': 1.5000000000000002e-05, 'epoch': 3.0}
{'eval_loss': 0.9935949444770813, 'eval_accuracy@eng.pdtb.pdtb': 0.6768219832735962, 'eval_f1@eng.pdtb.pdtb': 0.4968344735288272, 'eval_precision@eng.pdtb.pdtb': 0.550232603090344, 'eval_recall@eng.pdtb.pdtb': 0.48023798911894666, 'eval_loss@eng.pdtb.pdtb': 0.9935948848724365, 'eval_runtime': 19.9014, 'eval_samples_per_second': 84.115, 'eval_steps_per_second': 2.663, 'epoch': 3.0}
{'train@eng.pdtb.pdtb_loss': 0.9979812502861023, 'train@eng.pdtb.pdtb_accuracy@eng.pdtb.pdtb': 0.6675774134790529, 'train@eng.pdtb.pdtb_f1@eng.pdtb.pdtb': 0.45020522369409816, 'train@eng.pdtb.pdtb_precision@eng.pdtb.pdtb': 0.5189059979213237, 'train@eng.pdtb.pdtb_recall@eng.pdtb.pdtb': 0.4421152220145991, 'train@eng.pdtb.pdtb_loss@eng.pdtb.pdtb': 0.9979813098907471, 'train@eng.pdtb.pdtb_runtime': 514.6581, 'train@eng.pdtb.pdtb_samples_per_second': 85.338, 'train@eng.pdtb.pdtb_steps_per_second': 2.668, 'epoch': 4.0}
{'loss': 1.0738, 'learning_rate': 1.3333333333333333e-05, 'epoch': 4.0}
{'eval_loss': 0.951004147529602, 'eval_accuracy@eng.pdtb.pdtb': 0.6869772998805257, 'eval_f1@eng.pdtb.pdtb': 0.5203524847607796, 'eval_precision@eng.pdtb.pdtb': 0.6065722588514415, 'eval_recall@eng.pdtb.pdtb': 0.495555416206418, 'eval_loss@eng.pdtb.pdtb': 0.951004147529602, 'eval_runtime': 19.8905, 'eval_samples_per_second': 84.161, 'eval_steps_per_second': 2.665, 'epoch': 4.0}
{'train@eng.pdtb.pdtb_loss': 0.975703775882721, 'train@eng.pdtb.pdtb_accuracy@eng.pdtb.pdtb': 0.6744535519125683, 'train@eng.pdtb.pdtb_f1@eng.pdtb.pdtb': 0.4586670818416943, 'train@eng.pdtb.pdtb_precision@eng.pdtb.pdtb': 0.5208504352847291, 'train@eng.pdtb.pdtb_recall@eng.pdtb.pdtb': 0.45339820766699906, 'train@eng.pdtb.pdtb_loss@eng.pdtb.pdtb': 0.9757037162780762, 'train@eng.pdtb.pdtb_runtime': 514.9698, 'train@eng.pdtb.pdtb_samples_per_second': 85.287, 'train@eng.pdtb.pdtb_steps_per_second': 2.666, 'epoch': 5.0}
{'loss': 1.0442, 'learning_rate': 1.1666666666666668e-05, 'epoch': 5.0}
{'eval_loss': 0.9347754120826721, 'eval_accuracy@eng.pdtb.pdtb': 0.6911589008363201, 'eval_f1@eng.pdtb.pdtb': 0.5275920708712933, 'eval_precision@eng.pdtb.pdtb': 0.6057789494820154, 'eval_recall@eng.pdtb.pdtb': 0.5088507368671794, 'eval_loss@eng.pdtb.pdtb': 0.9347754120826721, 'eval_runtime': 19.8749, 'eval_samples_per_second': 84.227, 'eval_steps_per_second': 2.667, 'epoch': 5.0}
{'train@eng.pdtb.pdtb_loss': 0.9579277038574219, 'train@eng.pdtb.pdtb_accuracy@eng.pdtb.pdtb': 0.6800546448087431, 'train@eng.pdtb.pdtb_f1@eng.pdtb.pdtb': 0.46730559010670164, 'train@eng.pdtb.pdtb_precision@eng.pdtb.pdtb': 0.5222688483472916, 'train@eng.pdtb.pdtb_recall@eng.pdtb.pdtb': 0.46161513148212835, 'train@eng.pdtb.pdtb_loss@eng.pdtb.pdtb': 0.9579277038574219, 'train@eng.pdtb.pdtb_runtime': 514.7314, 'train@eng.pdtb.pdtb_samples_per_second': 85.326, 'train@eng.pdtb.pdtb_steps_per_second': 2.667, 'epoch': 6.0}
{'loss': 1.0214, 'learning_rate': 1e-05, 'epoch': 6.0}
{'eval_loss': 0.9282745718955994, 'eval_accuracy@eng.pdtb.pdtb': 0.6905615292712067, 'eval_f1@eng.pdtb.pdtb': 0.5376306221420908, 'eval_precision@eng.pdtb.pdtb': 0.598902918997816, 'eval_recall@eng.pdtb.pdtb': 0.5241531989182027, 'eval_loss@eng.pdtb.pdtb': 0.9282745122909546, 'eval_runtime': 19.8782, 'eval_samples_per_second': 84.213, 'eval_steps_per_second': 2.666, 'epoch': 6.0}
{'train@eng.pdtb.pdtb_loss': 0.9455438256263733, 'train@eng.pdtb.pdtb_accuracy@eng.pdtb.pdtb': 0.681511839708561, 'train@eng.pdtb.pdtb_f1@eng.pdtb.pdtb': 0.4699894414835659, 'train@eng.pdtb.pdtb_precision@eng.pdtb.pdtb': 0.53093434472494, 'train@eng.pdtb.pdtb_recall@eng.pdtb.pdtb': 0.4601352088808211, 'train@eng.pdtb.pdtb_loss@eng.pdtb.pdtb': 0.9455439448356628, 'train@eng.pdtb.pdtb_runtime': 515.0386, 'train@eng.pdtb.pdtb_samples_per_second': 85.275, 'train@eng.pdtb.pdtb_steps_per_second': 2.666, 'epoch': 7.0}
{'loss': 1.0082, 'learning_rate': 8.333333333333334e-06, 'epoch': 7.0}
{'eval_loss': 0.9144333004951477, 'eval_accuracy@eng.pdtb.pdtb': 0.6917562724014337, 'eval_f1@eng.pdtb.pdtb': 0.5379191019277527, 'eval_precision@eng.pdtb.pdtb': 0.615320578418163, 'eval_recall@eng.pdtb.pdtb': 0.5188120058360391, 'eval_loss@eng.pdtb.pdtb': 0.9144333004951477, 'eval_runtime': 19.8873, 'eval_samples_per_second': 84.174, 'eval_steps_per_second': 2.665, 'epoch': 7.0}
{'train@eng.pdtb.pdtb_loss': 0.9336465001106262, 'train@eng.pdtb.pdtb_accuracy@eng.pdtb.pdtb': 0.6851092896174863, 'train@eng.pdtb.pdtb_f1@eng.pdtb.pdtb': 0.47602023833643353, 'train@eng.pdtb.pdtb_precision@eng.pdtb.pdtb': 0.5222809834518276, 'train@eng.pdtb.pdtb_recall@eng.pdtb.pdtb': 0.47192979710209754, 'train@eng.pdtb.pdtb_loss@eng.pdtb.pdtb': 0.9336463809013367, 'train@eng.pdtb.pdtb_runtime': 514.3525, 'train@eng.pdtb.pdtb_samples_per_second': 85.389, 'train@eng.pdtb.pdtb_steps_per_second': 2.669, 'epoch': 8.0}
{'loss': 0.9936, 'learning_rate': 6.666666666666667e-06, 'epoch': 8.0}
{'eval_loss': 0.9105275869369507, 'eval_accuracy@eng.pdtb.pdtb': 0.6965352449223416, 'eval_f1@eng.pdtb.pdtb': 0.545488225539542, 'eval_precision@eng.pdtb.pdtb': 0.5872484144324193, 'eval_recall@eng.pdtb.pdtb': 0.5325186003533742, 'eval_loss@eng.pdtb.pdtb': 0.9105275869369507, 'eval_runtime': 19.9436, 'eval_samples_per_second': 83.937, 'eval_steps_per_second': 2.657, 'epoch': 8.0}
{'train@eng.pdtb.pdtb_loss': 0.9258949160575867, 'train@eng.pdtb.pdtb_accuracy@eng.pdtb.pdtb': 0.6882969034608379, 'train@eng.pdtb.pdtb_f1@eng.pdtb.pdtb': 0.4788924360986076, 'train@eng.pdtb.pdtb_precision@eng.pdtb.pdtb': 0.5272458577114534, 'train@eng.pdtb.pdtb_recall@eng.pdtb.pdtb': 0.4726557793700813, 'train@eng.pdtb.pdtb_loss@eng.pdtb.pdtb': 0.9258949160575867, 'train@eng.pdtb.pdtb_runtime': 515.3463, 'train@eng.pdtb.pdtb_samples_per_second': 85.224, 'train@eng.pdtb.pdtb_steps_per_second': 2.664, 'epoch': 9.0}
{'loss': 0.9835, 'learning_rate': 5e-06, 'epoch': 9.0}
{'eval_loss': 0.9077911972999573, 'eval_accuracy@eng.pdtb.pdtb': 0.6941457586618877, 'eval_f1@eng.pdtb.pdtb': 0.5436161168350353, 'eval_precision@eng.pdtb.pdtb': 0.6059418532346876, 'eval_recall@eng.pdtb.pdtb': 0.5289571592194628, 'eval_loss@eng.pdtb.pdtb': 0.907791018486023, 'eval_runtime': 19.9237, 'eval_samples_per_second': 84.02, 'eval_steps_per_second': 2.66, 'epoch': 9.0}
{'train@eng.pdtb.pdtb_loss': 0.921886146068573, 'train@eng.pdtb.pdtb_accuracy@eng.pdtb.pdtb': 0.6882741347905282, 'train@eng.pdtb.pdtb_f1@eng.pdtb.pdtb': 0.478124909131552, 'train@eng.pdtb.pdtb_precision@eng.pdtb.pdtb': 0.5265182790067494, 'train@eng.pdtb.pdtb_recall@eng.pdtb.pdtb': 0.4717736908481017, 'train@eng.pdtb.pdtb_loss@eng.pdtb.pdtb': 0.921886146068573, 'train@eng.pdtb.pdtb_runtime': 514.9207, 'train@eng.pdtb.pdtb_samples_per_second': 85.295, 'train@eng.pdtb.pdtb_steps_per_second': 2.666, 'epoch': 10.0}
{'loss': 0.9804, 'learning_rate': 3.3333333333333333e-06, 'epoch': 10.0}
{'eval_loss': 0.9021143913269043, 'eval_accuracy@eng.pdtb.pdtb': 0.6953405017921147, 'eval_f1@eng.pdtb.pdtb': 0.5460875255563411, 'eval_precision@eng.pdtb.pdtb': 0.6224163812078147, 'eval_recall@eng.pdtb.pdtb': 0.5284381376803126, 'eval_loss@eng.pdtb.pdtb': 0.9021143913269043, 'eval_runtime': 19.8465, 'eval_samples_per_second': 84.348, 'eval_steps_per_second': 2.671, 'epoch': 10.0}
{'train@eng.pdtb.pdtb_loss': 0.9181951880455017, 'train@eng.pdtb.pdtb_accuracy@eng.pdtb.pdtb': 0.6892987249544626, 'train@eng.pdtb.pdtb_f1@eng.pdtb.pdtb': 0.48058458079115374, 'train@eng.pdtb.pdtb_precision@eng.pdtb.pdtb': 0.5255640465596183, 'train@eng.pdtb.pdtb_recall@eng.pdtb.pdtb': 0.47593418411504, 'train@eng.pdtb.pdtb_loss@eng.pdtb.pdtb': 0.9181953072547913, 'train@eng.pdtb.pdtb_runtime': 514.5594, 'train@eng.pdtb.pdtb_samples_per_second': 85.355, 'train@eng.pdtb.pdtb_steps_per_second': 2.668, 'epoch': 11.0}
{'loss': 0.9743, 'learning_rate': 1.6666666666666667e-06, 'epoch': 11.0}
{'eval_loss': 0.9015852808952332, 'eval_accuracy@eng.pdtb.pdtb': 0.7013142174432497, 'eval_f1@eng.pdtb.pdtb': 0.5504768606757858, 'eval_precision@eng.pdtb.pdtb': 0.5858199292149189, 'eval_recall@eng.pdtb.pdtb': 0.537858887327377, 'eval_loss@eng.pdtb.pdtb': 0.9015852212905884, 'eval_runtime': 19.8663, 'eval_samples_per_second': 84.263, 'eval_steps_per_second': 2.668, 'epoch': 11.0}
{'train@eng.pdtb.pdtb_loss': 0.9167703986167908, 'train@eng.pdtb.pdtb_accuracy@eng.pdtb.pdtb': 0.6902094717668488, 'train@eng.pdtb.pdtb_f1@eng.pdtb.pdtb': 0.4809852137012931, 'train@eng.pdtb.pdtb_precision@eng.pdtb.pdtb': 0.5292565172924137, 'train@eng.pdtb.pdtb_recall@eng.pdtb.pdtb': 0.47457615875695913, 'train@eng.pdtb.pdtb_loss@eng.pdtb.pdtb': 0.9167704582214355, 'train@eng.pdtb.pdtb_runtime': 514.5155, 'train@eng.pdtb.pdtb_samples_per_second': 85.362, 'train@eng.pdtb.pdtb_steps_per_second': 2.669, 'epoch': 12.0}
{'loss': 0.9695, 'learning_rate': 0.0, 'epoch': 12.0}
{'eval_loss': 0.8999822735786438, 'eval_accuracy@eng.pdtb.pdtb': 0.6971326164874552, 'eval_f1@eng.pdtb.pdtb': 0.5470682898195173, 'eval_precision@eng.pdtb.pdtb': 0.610241533156572, 'eval_recall@eng.pdtb.pdtb': 0.5321207202990494, 'eval_loss@eng.pdtb.pdtb': 0.8999823331832886, 'eval_runtime': 19.8646, 'eval_samples_per_second': 84.271, 'eval_steps_per_second': 2.668, 'epoch': 12.0}
{'train_runtime': 19580.2931, 'train_samples_per_second': 26.917, 'train_steps_per_second': 0.841, 'train_loss': 1.0870815942981458, 'epoch': 12.0}
***** train_logger metrics *****
  epoch                    =       12.0
  train_loss               =     2.2699
  train_runtime            = 0:19:07.84
  train_samples_per_second =     25.624
  train_steps_per_second   =      0.805
-------------------------------------------------------------------
Lang1:  zho.rst.sctb    Lang2:  eng.pdtb.pdtb
Saving run to:  runs/full_shot/FullShot=v4_zho.rst.sctb_eng.pdtb.pdtb_lrfunc=Adafactor_lr=2e-05_plm=bert-base-multilingual-cased
Running with params: BERT_MODEL=bert-base-multilingual-cased lr=2e-05
read 439 examples
read 94 examples
read 159 examples
read 43920 examples
read 1674 examples
read 2257 examples
Total prediction labels:  49
BertAdapterModel(
  (shared_parameters): ModuleDict()
  (bert): BertModel(
    (shared_parameters): ModuleDict()
    (invertible_adapters): ModuleDict(
      (zh): NICECouplingBlock(
        (F): Sequential(
          (0): Linear(in_features=384, out_features=192, bias=True)
          (1): Activation_Function_Class(
            (f): ReLU()
          )
          (2): Linear(in_features=192, out_features=384, bias=True)
        )
        (G): Sequential(
          (0): Linear(in_features=384, out_features=192, bias=True)
          (1): Activation_Function_Class(
            (f): ReLU()
          )
          (2): Linear(in_features=192, out_features=384, bias=True)
        )
      )
      (en): NICECouplingBlock(
        (F): Sequential(
          (0): Linear(in_features=384, out_features=192, bias=True)
          (1): Activation_Function_Class(
            (f): ReLU()
          )
          (2): Linear(in_features=192, out_features=384, bias=True)
        )
        (G): Sequential(
          (0): Linear(in_features=384, out_features=192, bias=True)
          (1): Activation_Function_Class(
            (f): ReLU()
          )
          (2): Linear(in_features=192, out_features=384, bias=True)
        )
      )
    )
    (embeddings): BertEmbeddings(
      (word_embeddings): Embedding(119547, 768, padding_idx=0)
      (position_embeddings): Embedding(512, 768)
      (token_type_embeddings): Embedding(2, 768)
      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
      (dropout): Dropout(p=0.1, inplace=False)
    )
    (encoder): BertEncoder(
      (layer): ModuleList(
        (0): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (zh): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (en): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (1): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (zh): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (en): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (2): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (zh): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (en): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (3): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (zh): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (en): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (4): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (zh): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (en): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (5): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (zh): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (en): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (6): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (zh): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (en): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (7): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (zh): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (en): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (8): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (zh): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (en): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (9): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (zh): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (en): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (10): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (zh): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (en): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (11): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (zh): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (en): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
      )
    )
    (pooler): BertPooler(
      (dense): Linear(in_features=768, out_features=768, bias=True)
      (activation): Tanh()
    )
    (prefix_tuning): PrefixTuningPool(
      (prefix_tunings): ModuleDict()
    )
  )
  (heads): ModuleDict(
    (disrpt): ClassificationHead(
      (0): Dropout(p=0.1, inplace=False)
      (1): Linear(in_features=768, out_features=768, bias=True)
      (2): Activation_Function_Class(
        (f): Tanh()
      )
      (3): Dropout(p=0.1, inplace=False)
      (4): Linear(in_features=768, out_features=49, bias=True)
    )
  )
)
{'train@zho.rst.sctb_loss': 3.6967151165008545, 'train@zho.rst.sctb_accuracy@zho.rst.sctb': 0.3325740318906606, 'train@zho.rst.sctb_f1@zho.rst.sctb': 0.019197896120973047, 'train@zho.rst.sctb_precision@zho.rst.sctb': 0.012791308918871562, 'train@zho.rst.sctb_recall@zho.rst.sctb': 0.038461538461538464, 'train@zho.rst.sctb_loss@zho.rst.sctb': 3.6967153549194336, 'train@zho.rst.sctb_runtime': 5.5311, 'train@zho.rst.sctb_samples_per_second': 79.369, 'train@zho.rst.sctb_steps_per_second': 2.531, 'epoch': 1.0}
{'loss': 3.8349, 'learning_rate': 1.8333333333333333e-05, 'epoch': 1.0}
{'eval_loss': 3.6957526206970215, 'eval_accuracy@zho.rst.sctb': 0.3404255319148936, 'eval_f1@zho.rst.sctb': 0.026733500417710943, 'eval_precision@zho.rst.sctb': 0.01791713325867861, 'eval_recall@zho.rst.sctb': 0.05263157894736842, 'eval_loss@zho.rst.sctb': 3.6957523822784424, 'eval_runtime': 1.519, 'eval_samples_per_second': 61.883, 'eval_steps_per_second': 1.975, 'epoch': 1.0}
{'train@zho.rst.sctb_loss': 3.47660756111145, 'train@zho.rst.sctb_accuracy@zho.rst.sctb': 0.3325740318906606, 'train@zho.rst.sctb_f1@zho.rst.sctb': 0.019197896120973047, 'train@zho.rst.sctb_precision@zho.rst.sctb': 0.012791308918871562, 'train@zho.rst.sctb_recall@zho.rst.sctb': 0.038461538461538464, 'train@zho.rst.sctb_loss@zho.rst.sctb': 3.47660756111145, 'train@zho.rst.sctb_runtime': 5.5511, 'train@zho.rst.sctb_samples_per_second': 79.084, 'train@zho.rst.sctb_steps_per_second': 2.522, 'epoch': 2.0}
{'loss': 3.6013, 'learning_rate': 1.6666666666666667e-05, 'epoch': 2.0}
{'eval_loss': 3.4849817752838135, 'eval_accuracy@zho.rst.sctb': 0.3404255319148936, 'eval_f1@zho.rst.sctb': 0.026733500417710943, 'eval_precision@zho.rst.sctb': 0.01791713325867861, 'eval_recall@zho.rst.sctb': 0.05263157894736842, 'eval_loss@zho.rst.sctb': 3.484982490539551, 'eval_runtime': 1.5397, 'eval_samples_per_second': 61.053, 'eval_steps_per_second': 1.948, 'epoch': 2.0}
{'train@zho.rst.sctb_loss': 3.2686409950256348, 'train@zho.rst.sctb_accuracy@zho.rst.sctb': 0.3325740318906606, 'train@zho.rst.sctb_f1@zho.rst.sctb': 0.019197896120973047, 'train@zho.rst.sctb_precision@zho.rst.sctb': 0.012791308918871562, 'train@zho.rst.sctb_recall@zho.rst.sctb': 0.038461538461538464, 'train@zho.rst.sctb_loss@zho.rst.sctb': 3.2686402797698975, 'train@zho.rst.sctb_runtime': 5.586, 'train@zho.rst.sctb_samples_per_second': 78.59, 'train@zho.rst.sctb_steps_per_second': 2.506, 'epoch': 3.0}
{'loss': 3.4039, 'learning_rate': 1.5000000000000002e-05, 'epoch': 3.0}
{'eval_loss': 3.285731792449951, 'eval_accuracy@zho.rst.sctb': 0.3404255319148936, 'eval_f1@zho.rst.sctb': 0.026733500417710943, 'eval_precision@zho.rst.sctb': 0.01791713325867861, 'eval_recall@zho.rst.sctb': 0.05263157894736842, 'eval_loss@zho.rst.sctb': 3.2857322692871094, 'eval_runtime': 1.5295, 'eval_samples_per_second': 61.459, 'eval_steps_per_second': 1.961, 'epoch': 3.0}
{'train@zho.rst.sctb_loss': 3.082702398300171, 'train@zho.rst.sctb_accuracy@zho.rst.sctb': 0.3325740318906606, 'train@zho.rst.sctb_f1@zho.rst.sctb': 0.019197896120973047, 'train@zho.rst.sctb_precision@zho.rst.sctb': 0.012791308918871562, 'train@zho.rst.sctb_recall@zho.rst.sctb': 0.038461538461538464, 'train@zho.rst.sctb_loss@zho.rst.sctb': 3.0827033519744873, 'train@zho.rst.sctb_runtime': 5.5486, 'train@zho.rst.sctb_samples_per_second': 79.119, 'train@zho.rst.sctb_steps_per_second': 2.523, 'epoch': 4.0}
{'loss': 3.2067, 'learning_rate': 1.3333333333333333e-05, 'epoch': 4.0}
{'eval_loss': 3.1092402935028076, 'eval_accuracy@zho.rst.sctb': 0.3404255319148936, 'eval_f1@zho.rst.sctb': 0.026733500417710943, 'eval_precision@zho.rst.sctb': 0.01791713325867861, 'eval_recall@zho.rst.sctb': 0.05263157894736842, 'eval_loss@zho.rst.sctb': 3.109239339828491, 'eval_runtime': 1.5324, 'eval_samples_per_second': 61.341, 'eval_steps_per_second': 1.958, 'epoch': 4.0}
{'train@zho.rst.sctb_loss': 2.921382188796997, 'train@zho.rst.sctb_accuracy@zho.rst.sctb': 0.3325740318906606, 'train@zho.rst.sctb_f1@zho.rst.sctb': 0.019197896120973047, 'train@zho.rst.sctb_precision@zho.rst.sctb': 0.012791308918871562, 'train@zho.rst.sctb_recall@zho.rst.sctb': 0.038461538461538464, 'train@zho.rst.sctb_loss@zho.rst.sctb': 2.921382188796997, 'train@zho.rst.sctb_runtime': 5.5455, 'train@zho.rst.sctb_samples_per_second': 79.164, 'train@zho.rst.sctb_steps_per_second': 2.525, 'epoch': 5.0}
{'loss': 3.0435, 'learning_rate': 1.1666666666666668e-05, 'epoch': 5.0}
{'eval_loss': 2.9583756923675537, 'eval_accuracy@zho.rst.sctb': 0.3404255319148936, 'eval_f1@zho.rst.sctb': 0.026733500417710943, 'eval_precision@zho.rst.sctb': 0.01791713325867861, 'eval_recall@zho.rst.sctb': 0.05263157894736842, 'eval_loss@zho.rst.sctb': 2.9583752155303955, 'eval_runtime': 1.5644, 'eval_samples_per_second': 60.087, 'eval_steps_per_second': 1.918, 'epoch': 5.0}
{'train@zho.rst.sctb_loss': 2.7840235233306885, 'train@zho.rst.sctb_accuracy@zho.rst.sctb': 0.3325740318906606, 'train@zho.rst.sctb_f1@zho.rst.sctb': 0.019197896120973047, 'train@zho.rst.sctb_precision@zho.rst.sctb': 0.012791308918871562, 'train@zho.rst.sctb_recall@zho.rst.sctb': 0.038461538461538464, 'train@zho.rst.sctb_loss@zho.rst.sctb': 2.7840235233306885, 'train@zho.rst.sctb_runtime': 5.5772, 'train@zho.rst.sctb_samples_per_second': 78.714, 'train@zho.rst.sctb_steps_per_second': 2.51, 'epoch': 6.0}
{'loss': 2.9001, 'learning_rate': 1e-05, 'epoch': 6.0}
{'eval_loss': 2.8333373069763184, 'eval_accuracy@zho.rst.sctb': 0.3404255319148936, 'eval_f1@zho.rst.sctb': 0.026733500417710943, 'eval_precision@zho.rst.sctb': 0.01791713325867861, 'eval_recall@zho.rst.sctb': 0.05263157894736842, 'eval_loss@zho.rst.sctb': 2.8333375453948975, 'eval_runtime': 1.5412, 'eval_samples_per_second': 60.99, 'eval_steps_per_second': 1.946, 'epoch': 6.0}
{'train@zho.rst.sctb_loss': 2.6758768558502197, 'train@zho.rst.sctb_accuracy@zho.rst.sctb': 0.3325740318906606, 'train@zho.rst.sctb_f1@zho.rst.sctb': 0.019197896120973047, 'train@zho.rst.sctb_precision@zho.rst.sctb': 0.012791308918871562, 'train@zho.rst.sctb_recall@zho.rst.sctb': 0.038461538461538464, 'train@zho.rst.sctb_loss@zho.rst.sctb': 2.675877094268799, 'train@zho.rst.sctb_runtime': 5.5812, 'train@zho.rst.sctb_samples_per_second': 78.656, 'train@zho.rst.sctb_steps_per_second': 2.508, 'epoch': 7.0}
{'loss': 2.7724, 'learning_rate': 8.333333333333334e-06, 'epoch': 7.0}
{'eval_loss': 2.7371771335601807, 'eval_accuracy@zho.rst.sctb': 0.3404255319148936, 'eval_f1@zho.rst.sctb': 0.026733500417710943, 'eval_precision@zho.rst.sctb': 0.01791713325867861, 'eval_recall@zho.rst.sctb': 0.05263157894736842, 'eval_loss@zho.rst.sctb': 2.7371766567230225, 'eval_runtime': 1.5403, 'eval_samples_per_second': 61.027, 'eval_steps_per_second': 1.948, 'epoch': 7.0}
{'train@zho.rst.sctb_loss': 2.598184585571289, 'train@zho.rst.sctb_accuracy@zho.rst.sctb': 0.3325740318906606, 'train@zho.rst.sctb_f1@zho.rst.sctb': 0.019197896120973047, 'train@zho.rst.sctb_precision@zho.rst.sctb': 0.012791308918871562, 'train@zho.rst.sctb_recall@zho.rst.sctb': 0.038461538461538464, 'train@zho.rst.sctb_loss@zho.rst.sctb': 2.598184823989868, 'train@zho.rst.sctb_runtime': 5.5712, 'train@zho.rst.sctb_samples_per_second': 78.798, 'train@zho.rst.sctb_steps_per_second': 2.513, 'epoch': 8.0}
{'loss': 2.6775, 'learning_rate': 6.666666666666667e-06, 'epoch': 8.0}
{'eval_loss': 2.6705594062805176, 'eval_accuracy@zho.rst.sctb': 0.3404255319148936, 'eval_f1@zho.rst.sctb': 0.026733500417710943, 'eval_precision@zho.rst.sctb': 0.01791713325867861, 'eval_recall@zho.rst.sctb': 0.05263157894736842, 'eval_loss@zho.rst.sctb': 2.6705594062805176, 'eval_runtime': 1.5241, 'eval_samples_per_second': 61.677, 'eval_steps_per_second': 1.968, 'epoch': 8.0}
{'train@zho.rst.sctb_loss': 2.5434300899505615, 'train@zho.rst.sctb_accuracy@zho.rst.sctb': 0.3325740318906606, 'train@zho.rst.sctb_f1@zho.rst.sctb': 0.019197896120973047, 'train@zho.rst.sctb_precision@zho.rst.sctb': 0.012791308918871562, 'train@zho.rst.sctb_recall@zho.rst.sctb': 0.038461538461538464, 'train@zho.rst.sctb_loss@zho.rst.sctb': 2.5434300899505615, 'train@zho.rst.sctb_runtime': 5.5709, 'train@zho.rst.sctb_samples_per_second': 78.802, 'train@zho.rst.sctb_steps_per_second': 2.513, 'epoch': 9.0}
{'loss': 2.6099, 'learning_rate': 5e-06, 'epoch': 9.0}
{'eval_loss': 2.6240551471710205, 'eval_accuracy@zho.rst.sctb': 0.32978723404255317, 'eval_f1@zho.rst.sctb': 0.026105263157894736, 'eval_precision@zho.rst.sctb': 0.017543859649122806, 'eval_recall@zho.rst.sctb': 0.05098684210526316, 'eval_loss@zho.rst.sctb': 2.6240546703338623, 'eval_runtime': 1.5475, 'eval_samples_per_second': 60.742, 'eval_steps_per_second': 1.939, 'epoch': 9.0}
{'train@zho.rst.sctb_loss': 2.5071842670440674, 'train@zho.rst.sctb_accuracy@zho.rst.sctb': 0.3325740318906606, 'train@zho.rst.sctb_f1@zho.rst.sctb': 0.019197896120973047, 'train@zho.rst.sctb_precision@zho.rst.sctb': 0.012791308918871562, 'train@zho.rst.sctb_recall@zho.rst.sctb': 0.038461538461538464, 'train@zho.rst.sctb_loss@zho.rst.sctb': 2.5071849822998047, 'train@zho.rst.sctb_runtime': 5.548, 'train@zho.rst.sctb_samples_per_second': 79.127, 'train@zho.rst.sctb_steps_per_second': 2.523, 'epoch': 10.0}
{'loss': 2.5634, 'learning_rate': 3.3333333333333333e-06, 'epoch': 10.0}
{'eval_loss': 2.5943994522094727, 'eval_accuracy@zho.rst.sctb': 0.32978723404255317, 'eval_f1@zho.rst.sctb': 0.026105263157894736, 'eval_precision@zho.rst.sctb': 0.017543859649122806, 'eval_recall@zho.rst.sctb': 0.05098684210526316, 'eval_loss@zho.rst.sctb': 2.594399929046631, 'eval_runtime': 1.5412, 'eval_samples_per_second': 60.993, 'eval_steps_per_second': 1.947, 'epoch': 10.0}
{'train@zho.rst.sctb_loss': 2.4879097938537598, 'train@zho.rst.sctb_accuracy@zho.rst.sctb': 0.3325740318906606, 'train@zho.rst.sctb_f1@zho.rst.sctb': 0.019197896120973047, 'train@zho.rst.sctb_precision@zho.rst.sctb': 0.012791308918871562, 'train@zho.rst.sctb_recall@zho.rst.sctb': 0.038461538461538464, 'train@zho.rst.sctb_loss@zho.rst.sctb': 2.4879097938537598, 'train@zho.rst.sctb_runtime': 5.5758, 'train@zho.rst.sctb_samples_per_second': 78.733, 'train@zho.rst.sctb_steps_per_second': 2.511, 'epoch': 11.0}
{'loss': 2.5296, 'learning_rate': 1.6666666666666667e-06, 'epoch': 11.0}
{'eval_loss': 2.578681230545044, 'eval_accuracy@zho.rst.sctb': 0.32978723404255317, 'eval_f1@zho.rst.sctb': 0.026105263157894736, 'eval_precision@zho.rst.sctb': 0.017543859649122806, 'eval_recall@zho.rst.sctb': 0.05098684210526316, 'eval_loss@zho.rst.sctb': 2.578681468963623, 'eval_runtime': 1.5201, 'eval_samples_per_second': 61.838, 'eval_steps_per_second': 1.974, 'epoch': 11.0}
{'train@zho.rst.sctb_loss': 2.4815256595611572, 'train@zho.rst.sctb_accuracy@zho.rst.sctb': 0.3348519362186788, 'train@zho.rst.sctb_f1@zho.rst.sctb': 0.020032051282051284, 'train@zho.rst.sctb_precision@zho.rst.sctb': 0.05128205128205128, 'train@zho.rst.sctb_recall@zho.rst.sctb': 0.038866396761133605, 'train@zho.rst.sctb_loss@zho.rst.sctb': 2.481525421142578, 'train@zho.rst.sctb_runtime': 5.5437, 'train@zho.rst.sctb_samples_per_second': 79.189, 'train@zho.rst.sctb_steps_per_second': 2.525, 'epoch': 12.0}
{'loss': 2.5113, 'learning_rate': 0.0, 'epoch': 12.0}
{'eval_loss': 2.57350754737854, 'eval_accuracy@zho.rst.sctb': 0.32978723404255317, 'eval_f1@zho.rst.sctb': 0.026105263157894736, 'eval_precision@zho.rst.sctb': 0.017543859649122806, 'eval_recall@zho.rst.sctb': 0.05098684210526316, 'eval_loss@zho.rst.sctb': 2.5735080242156982, 'eval_runtime': 1.5644, 'eval_samples_per_second': 60.089, 'eval_steps_per_second': 1.918, 'epoch': 12.0}
{'train_runtime': 216.1369, 'train_samples_per_second': 24.373, 'train_steps_per_second': 0.777, 'train_loss': 2.9712087313334146, 'epoch': 12.0}
***** train_log metrics *****
  epoch                    =       12.0
  train_loss               =     2.9712
  train_runtime            = 0:03:36.13
  train_samples_per_second =     24.373
  train_steps_per_second   =      0.777
{'train@eng.pdtb.pdtb_loss': 1.2867003679275513, 'train@eng.pdtb.pdtb_accuracy@eng.pdtb.pdtb': 0.5880009107468124, 'train@eng.pdtb.pdtb_f1@eng.pdtb.pdtb': 0.24982521236354086, 'train@eng.pdtb.pdtb_precision@eng.pdtb.pdtb': 0.36007115999476175, 'train@eng.pdtb.pdtb_recall@eng.pdtb.pdtb': 0.24463632295117846, 'train@eng.pdtb.pdtb_loss@eng.pdtb.pdtb': 1.2867004871368408, 'train@eng.pdtb.pdtb_runtime': 515.1, 'train@eng.pdtb.pdtb_samples_per_second': 85.265, 'train@eng.pdtb.pdtb_steps_per_second': 2.666, 'epoch': 1.0}
{'loss': 1.8795, 'learning_rate': 1.8333333333333333e-05, 'epoch': 1.0}
{'eval_loss': 1.2129169702529907, 'eval_accuracy@eng.pdtb.pdtb': 0.6218637992831542, 'eval_f1@eng.pdtb.pdtb': 0.29704890927892996, 'eval_precision@eng.pdtb.pdtb': 0.38414983985639445, 'eval_recall@eng.pdtb.pdtb': 0.29277509250324624, 'eval_loss@eng.pdtb.pdtb': 1.2129170894622803, 'eval_runtime': 20.0997, 'eval_samples_per_second': 83.285, 'eval_steps_per_second': 2.637, 'epoch': 1.0}
{'train@eng.pdtb.pdtb_loss': 1.1049035787582397, 'train@eng.pdtb.pdtb_accuracy@eng.pdtb.pdtb': 0.6361111111111111, 'train@eng.pdtb.pdtb_f1@eng.pdtb.pdtb': 0.36024837003519, 'train@eng.pdtb.pdtb_precision@eng.pdtb.pdtb': 0.46529284512864705, 'train@eng.pdtb.pdtb_recall@eng.pdtb.pdtb': 0.3467414654010129, 'train@eng.pdtb.pdtb_loss@eng.pdtb.pdtb': 1.1049036979675293, 'train@eng.pdtb.pdtb_runtime': 515.2692, 'train@eng.pdtb.pdtb_samples_per_second': 85.237, 'train@eng.pdtb.pdtb_steps_per_second': 2.665, 'epoch': 2.0}
{'loss': 1.2318, 'learning_rate': 1.6666666666666667e-05, 'epoch': 2.0}
{'eval_loss': 1.032259464263916, 'eval_accuracy@eng.pdtb.pdtb': 0.6642771804062126, 'eval_f1@eng.pdtb.pdtb': 0.4119357509064173, 'eval_precision@eng.pdtb.pdtb': 0.48961529561694117, 'eval_recall@eng.pdtb.pdtb': 0.39716747525985363, 'eval_loss@eng.pdtb.pdtb': 1.032259464263916, 'eval_runtime': 20.0809, 'eval_samples_per_second': 83.363, 'eval_steps_per_second': 2.639, 'epoch': 2.0}
{'train@eng.pdtb.pdtb_loss': 1.0543086528778076, 'train@eng.pdtb.pdtb_accuracy@eng.pdtb.pdtb': 0.6497040072859745, 'train@eng.pdtb.pdtb_f1@eng.pdtb.pdtb': 0.4320276340281478, 'train@eng.pdtb.pdtb_precision@eng.pdtb.pdtb': 0.4674145283188719, 'train@eng.pdtb.pdtb_recall@eng.pdtb.pdtb': 0.4203507604842124, 'train@eng.pdtb.pdtb_loss@eng.pdtb.pdtb': 1.0543086528778076, 'train@eng.pdtb.pdtb_runtime': 515.2485, 'train@eng.pdtb.pdtb_samples_per_second': 85.24, 'train@eng.pdtb.pdtb_steps_per_second': 2.665, 'epoch': 3.0}
{'loss': 1.1248, 'learning_rate': 1.5000000000000002e-05, 'epoch': 3.0}
{'eval_loss': 0.992111086845398, 'eval_accuracy@eng.pdtb.pdtb': 0.6851851851851852, 'eval_f1@eng.pdtb.pdtb': 0.49156909231417434, 'eval_precision@eng.pdtb.pdtb': 0.5539241264467163, 'eval_recall@eng.pdtb.pdtb': 0.4679756597654543, 'eval_loss@eng.pdtb.pdtb': 0.992111086845398, 'eval_runtime': 20.0574, 'eval_samples_per_second': 83.46, 'eval_steps_per_second': 2.642, 'epoch': 3.0}
{'train@eng.pdtb.pdtb_loss': 1.0058908462524414, 'train@eng.pdtb.pdtb_accuracy@eng.pdtb.pdtb': 0.6631602914389799, 'train@eng.pdtb.pdtb_f1@eng.pdtb.pdtb': 0.44374427369322017, 'train@eng.pdtb.pdtb_precision@eng.pdtb.pdtb': 0.46961327658055124, 'train@eng.pdtb.pdtb_recall@eng.pdtb.pdtb': 0.43883849899721245, 'train@eng.pdtb.pdtb_loss@eng.pdtb.pdtb': 1.0058908462524414, 'train@eng.pdtb.pdtb_runtime': 514.9478, 'train@eng.pdtb.pdtb_samples_per_second': 85.29, 'train@eng.pdtb.pdtb_steps_per_second': 2.666, 'epoch': 4.0}
{'loss': 1.0777, 'learning_rate': 1.3333333333333333e-05, 'epoch': 4.0}
{'eval_loss': 0.9514530301094055, 'eval_accuracy@eng.pdtb.pdtb': 0.6804062126642771, 'eval_f1@eng.pdtb.pdtb': 0.5020114661336954, 'eval_precision@eng.pdtb.pdtb': 0.536400454505359, 'eval_recall@eng.pdtb.pdtb': 0.4984164062298203, 'eval_loss@eng.pdtb.pdtb': 0.9514529705047607, 'eval_runtime': 20.0476, 'eval_samples_per_second': 83.501, 'eval_steps_per_second': 2.644, 'epoch': 4.0}
{'train@eng.pdtb.pdtb_loss': 0.9823417663574219, 'train@eng.pdtb.pdtb_accuracy@eng.pdtb.pdtb': 0.671311475409836, 'train@eng.pdtb.pdtb_f1@eng.pdtb.pdtb': 0.45489108633213227, 'train@eng.pdtb.pdtb_precision@eng.pdtb.pdtb': 0.4744678931224061, 'train@eng.pdtb.pdtb_recall@eng.pdtb.pdtb': 0.4528195152229306, 'train@eng.pdtb.pdtb_loss@eng.pdtb.pdtb': 0.9823418259620667, 'train@eng.pdtb.pdtb_runtime': 514.5321, 'train@eng.pdtb.pdtb_samples_per_second': 85.359, 'train@eng.pdtb.pdtb_steps_per_second': 2.668, 'epoch': 5.0}
{'loss': 1.0454, 'learning_rate': 1.1666666666666668e-05, 'epoch': 5.0}
{'eval_loss': 0.9345448613166809, 'eval_accuracy@eng.pdtb.pdtb': 0.6887694145758662, 'eval_f1@eng.pdtb.pdtb': 0.5190371096301003, 'eval_precision@eng.pdtb.pdtb': 0.5392685156713303, 'eval_recall@eng.pdtb.pdtb': 0.5203674348911295, 'eval_loss@eng.pdtb.pdtb': 0.9345448017120361, 'eval_runtime': 20.0223, 'eval_samples_per_second': 83.607, 'eval_steps_per_second': 2.647, 'epoch': 5.0}
{'train@eng.pdtb.pdtb_loss': 0.9627819657325745, 'train@eng.pdtb.pdtb_accuracy@eng.pdtb.pdtb': 0.6763205828779599, 'train@eng.pdtb.pdtb_f1@eng.pdtb.pdtb': 0.4618347847594626, 'train@eng.pdtb.pdtb_precision@eng.pdtb.pdtb': 0.5218635433221308, 'train@eng.pdtb.pdtb_recall@eng.pdtb.pdtb': 0.4594790151106711, 'train@eng.pdtb.pdtb_loss@eng.pdtb.pdtb': 0.9627818465232849, 'train@eng.pdtb.pdtb_runtime': 515.0151, 'train@eng.pdtb.pdtb_samples_per_second': 85.279, 'train@eng.pdtb.pdtb_steps_per_second': 2.666, 'epoch': 6.0}
{'loss': 1.0228, 'learning_rate': 1e-05, 'epoch': 6.0}
{'eval_loss': 0.926426887512207, 'eval_accuracy@eng.pdtb.pdtb': 0.6839904420549582, 'eval_f1@eng.pdtb.pdtb': 0.53628492102316, 'eval_precision@eng.pdtb.pdtb': 0.5636668302300836, 'eval_recall@eng.pdtb.pdtb': 0.5301306171974118, 'eval_loss@eng.pdtb.pdtb': 0.9264268279075623, 'eval_runtime': 20.0676, 'eval_samples_per_second': 83.418, 'eval_steps_per_second': 2.641, 'epoch': 6.0}
{'train@eng.pdtb.pdtb_loss': 0.9507191777229309, 'train@eng.pdtb.pdtb_accuracy@eng.pdtb.pdtb': 0.6778688524590164, 'train@eng.pdtb.pdtb_f1@eng.pdtb.pdtb': 0.46523445858853835, 'train@eng.pdtb.pdtb_precision@eng.pdtb.pdtb': 0.5292609375957052, 'train@eng.pdtb.pdtb_recall@eng.pdtb.pdtb': 0.45948762362358425, 'train@eng.pdtb.pdtb_loss@eng.pdtb.pdtb': 0.9507192373275757, 'train@eng.pdtb.pdtb_runtime': 514.7634, 'train@eng.pdtb.pdtb_samples_per_second': 85.321, 'train@eng.pdtb.pdtb_steps_per_second': 2.667, 'epoch': 7.0}
{'loss': 1.008, 'learning_rate': 8.333333333333334e-06, 'epoch': 7.0}
{'eval_loss': 0.9115971922874451, 'eval_accuracy@eng.pdtb.pdtb': 0.6965352449223416, 'eval_f1@eng.pdtb.pdtb': 0.5490961478652853, 'eval_precision@eng.pdtb.pdtb': 0.5811699906548705, 'eval_recall@eng.pdtb.pdtb': 0.5396393901284412, 'eval_loss@eng.pdtb.pdtb': 0.9115973114967346, 'eval_runtime': 20.0888, 'eval_samples_per_second': 83.33, 'eval_steps_per_second': 2.638, 'epoch': 7.0}
{'train@eng.pdtb.pdtb_loss': 0.9390449523925781, 'train@eng.pdtb.pdtb_accuracy@eng.pdtb.pdtb': 0.6818078324225865, 'train@eng.pdtb.pdtb_f1@eng.pdtb.pdtb': 0.4706293489883142, 'train@eng.pdtb.pdtb_precision@eng.pdtb.pdtb': 0.5169938184328363, 'train@eng.pdtb.pdtb_recall@eng.pdtb.pdtb': 0.46946704074163004, 'train@eng.pdtb.pdtb_loss@eng.pdtb.pdtb': 0.9390449523925781, 'train@eng.pdtb.pdtb_runtime': 515.3296, 'train@eng.pdtb.pdtb_samples_per_second': 85.227, 'train@eng.pdtb.pdtb_steps_per_second': 2.664, 'epoch': 8.0}
{'loss': 0.9977, 'learning_rate': 6.666666666666667e-06, 'epoch': 8.0}
{'eval_loss': 0.9091402888298035, 'eval_accuracy@eng.pdtb.pdtb': 0.6899641577060932, 'eval_f1@eng.pdtb.pdtb': 0.5560844831385164, 'eval_precision@eng.pdtb.pdtb': 0.5899360767549033, 'eval_recall@eng.pdtb.pdtb': 0.550642334414177, 'eval_loss@eng.pdtb.pdtb': 0.9091402888298035, 'eval_runtime': 20.0527, 'eval_samples_per_second': 83.48, 'eval_steps_per_second': 2.643, 'epoch': 8.0}
{'train@eng.pdtb.pdtb_loss': 0.9308079481124878, 'train@eng.pdtb.pdtb_accuracy@eng.pdtb.pdtb': 0.6846083788706739, 'train@eng.pdtb.pdtb_f1@eng.pdtb.pdtb': 0.47322100809841067, 'train@eng.pdtb.pdtb_precision@eng.pdtb.pdtb': 0.5303973618277995, 'train@eng.pdtb.pdtb_recall@eng.pdtb.pdtb': 0.47009899939465094, 'train@eng.pdtb.pdtb_loss@eng.pdtb.pdtb': 0.930807888507843, 'train@eng.pdtb.pdtb_runtime': 515.4207, 'train@eng.pdtb.pdtb_samples_per_second': 85.212, 'train@eng.pdtb.pdtb_steps_per_second': 2.664, 'epoch': 9.0}
{'loss': 0.9854, 'learning_rate': 5e-06, 'epoch': 9.0}
{'eval_loss': 0.9063758850097656, 'eval_accuracy@eng.pdtb.pdtb': 0.6929510155316607, 'eval_f1@eng.pdtb.pdtb': 0.5627876637992708, 'eval_precision@eng.pdtb.pdtb': 0.6262666548498328, 'eval_recall@eng.pdtb.pdtb': 0.5505613724058155, 'eval_loss@eng.pdtb.pdtb': 0.9063758850097656, 'eval_runtime': 20.0805, 'eval_samples_per_second': 83.365, 'eval_steps_per_second': 2.639, 'epoch': 9.0}
{'train@eng.pdtb.pdtb_loss': 0.9267898201942444, 'train@eng.pdtb.pdtb_accuracy@eng.pdtb.pdtb': 0.6853142076502732, 'train@eng.pdtb.pdtb_f1@eng.pdtb.pdtb': 0.4732170843907401, 'train@eng.pdtb.pdtb_precision@eng.pdtb.pdtb': 0.5336823949079875, 'train@eng.pdtb.pdtb_recall@eng.pdtb.pdtb': 0.468863536066095, 'train@eng.pdtb.pdtb_loss@eng.pdtb.pdtb': 0.9267897605895996, 'train@eng.pdtb.pdtb_runtime': 515.452, 'train@eng.pdtb.pdtb_samples_per_second': 85.207, 'train@eng.pdtb.pdtb_steps_per_second': 2.664, 'epoch': 10.0}
{'loss': 0.9786, 'learning_rate': 3.3333333333333333e-06, 'epoch': 10.0}
{'eval_loss': 0.9005570411682129, 'eval_accuracy@eng.pdtb.pdtb': 0.6887694145758662, 'eval_f1@eng.pdtb.pdtb': 0.5581609506618176, 'eval_precision@eng.pdtb.pdtb': 0.6252868894738102, 'eval_recall@eng.pdtb.pdtb': 0.5451057960574155, 'eval_loss@eng.pdtb.pdtb': 0.9005569815635681, 'eval_runtime': 20.0252, 'eval_samples_per_second': 83.595, 'eval_steps_per_second': 2.647, 'epoch': 10.0}
{'train@eng.pdtb.pdtb_loss': 0.9225040078163147, 'train@eng.pdtb.pdtb_accuracy@eng.pdtb.pdtb': 0.6864526411657559, 'train@eng.pdtb.pdtb_f1@eng.pdtb.pdtb': 0.4753246329469286, 'train@eng.pdtb.pdtb_precision@eng.pdtb.pdtb': 0.5201327127908659, 'train@eng.pdtb.pdtb_recall@eng.pdtb.pdtb': 0.4734661692565842, 'train@eng.pdtb.pdtb_loss@eng.pdtb.pdtb': 0.9225040078163147, 'train@eng.pdtb.pdtb_runtime': 514.6758, 'train@eng.pdtb.pdtb_samples_per_second': 85.335, 'train@eng.pdtb.pdtb_steps_per_second': 2.668, 'epoch': 11.0}
{'loss': 0.9736, 'learning_rate': 1.6666666666666667e-06, 'epoch': 11.0}
{'eval_loss': 0.9009092450141907, 'eval_accuracy@eng.pdtb.pdtb': 0.6923536439665472, 'eval_f1@eng.pdtb.pdtb': 0.5578118223608788, 'eval_precision@eng.pdtb.pdtb': 0.5867106708530837, 'eval_recall@eng.pdtb.pdtb': 0.5540003363517373, 'eval_loss@eng.pdtb.pdtb': 0.9009092450141907, 'eval_runtime': 20.06, 'eval_samples_per_second': 83.45, 'eval_steps_per_second': 2.642, 'epoch': 11.0}
{'train@eng.pdtb.pdtb_loss': 0.9212152361869812, 'train@eng.pdtb.pdtb_accuracy@eng.pdtb.pdtb': 0.6866803278688525, 'train@eng.pdtb.pdtb_f1@eng.pdtb.pdtb': 0.4752186627829711, 'train@eng.pdtb.pdtb_precision@eng.pdtb.pdtb': 0.531665937399141, 'train@eng.pdtb.pdtb_recall@eng.pdtb.pdtb': 0.47216210832066313, 'train@eng.pdtb.pdtb_loss@eng.pdtb.pdtb': 0.9212152361869812, 'train@eng.pdtb.pdtb_runtime': 515.36, 'train@eng.pdtb.pdtb_samples_per_second': 85.222, 'train@eng.pdtb.pdtb_steps_per_second': 2.664, 'epoch': 12.0}
{'loss': 0.9706, 'learning_rate': 0.0, 'epoch': 12.0}
{'eval_loss': 0.8991585969924927, 'eval_accuracy@eng.pdtb.pdtb': 0.6935483870967742, 'eval_f1@eng.pdtb.pdtb': 0.5659701953674554, 'eval_precision@eng.pdtb.pdtb': 0.6263618082639298, 'eval_recall@eng.pdtb.pdtb': 0.5529871314277662, 'eval_loss@eng.pdtb.pdtb': 0.8991585969924927, 'eval_runtime': 20.0806, 'eval_samples_per_second': 83.364, 'eval_steps_per_second': 2.639, 'epoch': 12.0}
{'train_runtime': 19519.936, 'train_samples_per_second': 27.0, 'train_steps_per_second': 0.844, 'train_loss': 1.1079905584733096, 'epoch': 12.0}
***** train_logger metrics *****
  epoch                    =       12.0
  train_loss               =     2.9712
  train_runtime            = 0:03:36.13
  train_samples_per_second =     24.373
  train_steps_per_second   =      0.777

-------------------------------------------------------------------
Lang1:  deu.rst.pcc    Lang2:  fas.rst.prstc
Saving run to:  runs/full_shot/FullShot=v4_deu.rst.pcc_fas.rst.prstc_lrfunc=Adafactor_lr=2e-05_plm=bert-base-multilingual-cased
Running with params: BERT_MODEL=bert-base-multilingual-cased lr=2e-05
read 2164 examples
read 241 examples
read 260 examples
read 4100 examples
read 499 examples
read 592 examples
Total prediction labels:  36
BertAdapterModel(
  (shared_parameters): ModuleDict()
  (bert): BertModel(
    (shared_parameters): ModuleDict()
    (invertible_adapters): ModuleDict(
      (de): NICECouplingBlock(
        (F): Sequential(
          (0): Linear(in_features=384, out_features=192, bias=True)
          (1): Activation_Function_Class(
            (f): NewGELUActivation()
          )
          (2): Linear(in_features=192, out_features=384, bias=True)
        )
        (G): Sequential(
          (0): Linear(in_features=384, out_features=192, bias=True)
          (1): Activation_Function_Class(
            (f): NewGELUActivation()
          )
          (2): Linear(in_features=192, out_features=384, bias=True)
        )
      )
      (ar): NICECouplingBlock(
        (F): Sequential(
          (0): Linear(in_features=384, out_features=192, bias=True)
          (1): Activation_Function_Class(
            (f): ReLU()
          )
          (2): Linear(in_features=192, out_features=384, bias=True)
        )
        (G): Sequential(
          (0): Linear(in_features=384, out_features=192, bias=True)
          (1): Activation_Function_Class(
            (f): ReLU()
          )
          (2): Linear(in_features=192, out_features=384, bias=True)
        )
      )
    )
    (embeddings): BertEmbeddings(
      (word_embeddings): Embedding(119547, 768, padding_idx=0)
      (position_embeddings): Embedding(512, 768)
      (token_type_embeddings): Embedding(2, 768)
      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
      (dropout): Dropout(p=0.1, inplace=False)
    )
    (encoder): BertEncoder(
      (layer): ModuleList(
        (0): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (de): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (ar): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (1): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (de): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (ar): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (2): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (de): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (ar): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (3): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (de): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (ar): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (4): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (de): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (ar): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (5): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (de): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (ar): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (6): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (de): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (ar): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (7): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (de): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (ar): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (8): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (de): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (ar): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (9): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (de): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (ar): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (10): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (de): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (ar): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (11): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (de): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (ar): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
      )
    )
    (pooler): BertPooler(
      (dense): Linear(in_features=768, out_features=768, bias=True)
      (activation): Tanh()
    )
    (prefix_tuning): PrefixTuningPool(
      (prefix_tunings): ModuleDict()
    )
  )
  (heads): ModuleDict(
    (disrpt): ClassificationHead(
      (0): Dropout(p=0.1, inplace=False)
      (1): Linear(in_features=768, out_features=768, bias=True)
      (2): Activation_Function_Class(
        (f): Tanh()
      )
      (3): Dropout(p=0.1, inplace=False)
      (4): Linear(in_features=768, out_features=36, bias=True)
    )
  )
)
{'train@deu.rst.pcc_loss': 3.1977553367614746, 'train@deu.rst.pcc_accuracy@deu.rst.pcc': 0.10027726432532348, 'train@deu.rst.pcc_f1@deu.rst.pcc': 0.014571875089890766, 'train@deu.rst.pcc_precision@deu.rst.pcc': 0.03612466927406343, 'train@deu.rst.pcc_recall@deu.rst.pcc': 0.04142389869701657, 'train@deu.rst.pcc_loss@deu.rst.pcc': 3.1977553367614746, 'train@deu.rst.pcc_runtime': 27.4212, 'train@deu.rst.pcc_samples_per_second': 78.917, 'train@deu.rst.pcc_steps_per_second': 2.48, 'epoch': 1.0}
{'loss': 3.3953, 'learning_rate': 1.8333333333333333e-05, 'epoch': 1.0}
{'eval_loss': 3.2147812843322754, 'eval_accuracy@deu.rst.pcc': 0.06224066390041494, 'eval_f1@deu.rst.pcc': 0.015104325312214438, 'eval_precision@deu.rst.pcc': 0.02464800943265363, 'eval_recall@deu.rst.pcc': 0.03850605413105413, 'eval_loss@deu.rst.pcc': 3.2147812843322754, 'eval_runtime': 3.3385, 'eval_samples_per_second': 72.188, 'eval_steps_per_second': 2.396, 'epoch': 1.0}
{'train@deu.rst.pcc_loss': 2.9572343826293945, 'train@deu.rst.pcc_accuracy@deu.rst.pcc': 0.12430683918669132, 'train@deu.rst.pcc_f1@deu.rst.pcc': 0.031392790728495316, 'train@deu.rst.pcc_precision@deu.rst.pcc': 0.07507204929380765, 'train@deu.rst.pcc_recall@deu.rst.pcc': 0.05204333790244083, 'train@deu.rst.pcc_loss@deu.rst.pcc': 2.9572339057922363, 'train@deu.rst.pcc_runtime': 27.1296, 'train@deu.rst.pcc_samples_per_second': 79.765, 'train@deu.rst.pcc_steps_per_second': 2.506, 'epoch': 2.0}
{'loss': 3.0746, 'learning_rate': 1.6666666666666667e-05, 'epoch': 2.0}
{'eval_loss': 2.995147943496704, 'eval_accuracy@deu.rst.pcc': 0.1037344398340249, 'eval_f1@deu.rst.pcc': 0.0277583255524432, 'eval_precision@deu.rst.pcc': 0.029605263157894735, 'eval_recall@deu.rst.pcc': 0.04801332926332926, 'eval_loss@deu.rst.pcc': 2.995147943496704, 'eval_runtime': 3.4266, 'eval_samples_per_second': 70.332, 'eval_steps_per_second': 2.335, 'epoch': 2.0}
{'train@deu.rst.pcc_loss': 2.881049633026123, 'train@deu.rst.pcc_accuracy@deu.rst.pcc': 0.14695009242144177, 'train@deu.rst.pcc_f1@deu.rst.pcc': 0.05528552995596874, 'train@deu.rst.pcc_precision@deu.rst.pcc': 0.062346973703082474, 'train@deu.rst.pcc_recall@deu.rst.pcc': 0.07194788599971597, 'train@deu.rst.pcc_loss@deu.rst.pcc': 2.881049633026123, 'train@deu.rst.pcc_runtime': 27.0012, 'train@deu.rst.pcc_samples_per_second': 80.145, 'train@deu.rst.pcc_steps_per_second': 2.518, 'epoch': 3.0}
{'loss': 2.9469, 'learning_rate': 1.5000000000000002e-05, 'epoch': 3.0}
{'eval_loss': 2.9347071647644043, 'eval_accuracy@deu.rst.pcc': 0.13692946058091288, 'eval_f1@deu.rst.pcc': 0.050681909657933066, 'eval_precision@deu.rst.pcc': 0.04534460568943327, 'eval_recall@deu.rst.pcc': 0.08243030118030119, 'eval_loss@deu.rst.pcc': 2.934706449508667, 'eval_runtime': 3.3625, 'eval_samples_per_second': 71.673, 'eval_steps_per_second': 2.379, 'epoch': 3.0}
{'train@deu.rst.pcc_loss': 2.828303337097168, 'train@deu.rst.pcc_accuracy@deu.rst.pcc': 0.18345656192236598, 'train@deu.rst.pcc_f1@deu.rst.pcc': 0.06360583025341207, 'train@deu.rst.pcc_precision@deu.rst.pcc': 0.08060609766026196, 'train@deu.rst.pcc_recall@deu.rst.pcc': 0.10328287338798234, 'train@deu.rst.pcc_loss@deu.rst.pcc': 2.828303337097168, 'train@deu.rst.pcc_runtime': 27.0916, 'train@deu.rst.pcc_samples_per_second': 79.877, 'train@deu.rst.pcc_steps_per_second': 2.51, 'epoch': 4.0}
{'loss': 2.8853, 'learning_rate': 1.3333333333333333e-05, 'epoch': 4.0}
{'eval_loss': 2.8956358432769775, 'eval_accuracy@deu.rst.pcc': 0.14107883817427386, 'eval_f1@deu.rst.pcc': 0.05016017757162602, 'eval_precision@deu.rst.pcc': 0.034763765228563955, 'eval_recall@deu.rst.pcc': 0.1020426332926333, 'eval_loss@deu.rst.pcc': 2.8956358432769775, 'eval_runtime': 3.4938, 'eval_samples_per_second': 68.98, 'eval_steps_per_second': 2.29, 'epoch': 4.0}
{'train@deu.rst.pcc_loss': 2.78743839263916, 'train@deu.rst.pcc_accuracy@deu.rst.pcc': 0.1922365988909427, 'train@deu.rst.pcc_f1@deu.rst.pcc': 0.06591387846158989, 'train@deu.rst.pcc_precision@deu.rst.pcc': 0.09019890512697708, 'train@deu.rst.pcc_recall@deu.rst.pcc': 0.11248366777714511, 'train@deu.rst.pcc_loss@deu.rst.pcc': 2.78743839263916, 'train@deu.rst.pcc_runtime': 27.0213, 'train@deu.rst.pcc_samples_per_second': 80.085, 'train@deu.rst.pcc_steps_per_second': 2.517, 'epoch': 5.0}
{'loss': 2.841, 'learning_rate': 1.1666666666666668e-05, 'epoch': 5.0}
{'eval_loss': 2.8634190559387207, 'eval_accuracy@deu.rst.pcc': 0.14107883817427386, 'eval_f1@deu.rst.pcc': 0.04459575798803445, 'eval_precision@deu.rst.pcc': 0.02888993169560372, 'eval_recall@deu.rst.pcc': 0.1049361518111518, 'eval_loss@deu.rst.pcc': 2.8634190559387207, 'eval_runtime': 3.3491, 'eval_samples_per_second': 71.961, 'eval_steps_per_second': 2.389, 'epoch': 5.0}
{'train@deu.rst.pcc_loss': 2.7513458728790283, 'train@deu.rst.pcc_accuracy@deu.rst.pcc': 0.1977818853974122, 'train@deu.rst.pcc_f1@deu.rst.pcc': 0.06935769515668841, 'train@deu.rst.pcc_precision@deu.rst.pcc': 0.08755202587496153, 'train@deu.rst.pcc_recall@deu.rst.pcc': 0.11734777562639322, 'train@deu.rst.pcc_loss@deu.rst.pcc': 2.7513458728790283, 'train@deu.rst.pcc_runtime': 27.0224, 'train@deu.rst.pcc_samples_per_second': 80.082, 'train@deu.rst.pcc_steps_per_second': 2.516, 'epoch': 6.0}
{'loss': 2.8062, 'learning_rate': 1e-05, 'epoch': 6.0}
{'eval_loss': 2.834076166152954, 'eval_accuracy@deu.rst.pcc': 0.14107883817427386, 'eval_f1@deu.rst.pcc': 0.04573591429650344, 'eval_precision@deu.rst.pcc': 0.06957444934595472, 'eval_recall@deu.rst.pcc': 0.10576287138787138, 'eval_loss@deu.rst.pcc': 2.834076404571533, 'eval_runtime': 3.3455, 'eval_samples_per_second': 72.037, 'eval_steps_per_second': 2.391, 'epoch': 6.0}
{'train@deu.rst.pcc_loss': 2.7254130840301514, 'train@deu.rst.pcc_accuracy@deu.rst.pcc': 0.2010166358595194, 'train@deu.rst.pcc_f1@deu.rst.pcc': 0.0718155558144634, 'train@deu.rst.pcc_precision@deu.rst.pcc': 0.08707807836863811, 'train@deu.rst.pcc_recall@deu.rst.pcc': 0.11942376670553334, 'train@deu.rst.pcc_loss@deu.rst.pcc': 2.7254130840301514, 'train@deu.rst.pcc_runtime': 27.0799, 'train@deu.rst.pcc_samples_per_second': 79.912, 'train@deu.rst.pcc_steps_per_second': 2.511, 'epoch': 7.0}
{'loss': 2.7719, 'learning_rate': 8.333333333333334e-06, 'epoch': 7.0}
{'eval_loss': 2.8132102489471436, 'eval_accuracy@deu.rst.pcc': 0.14107883817427386, 'eval_f1@deu.rst.pcc': 0.0462654085997929, 'eval_precision@deu.rst.pcc': 0.07000694186262664, 'eval_recall@deu.rst.pcc': 0.10576287138787138, 'eval_loss@deu.rst.pcc': 2.8132104873657227, 'eval_runtime': 3.3984, 'eval_samples_per_second': 70.916, 'eval_steps_per_second': 2.354, 'epoch': 7.0}
{'train@deu.rst.pcc_loss': 2.704073905944824, 'train@deu.rst.pcc_accuracy@deu.rst.pcc': 0.2033271719038817, 'train@deu.rst.pcc_f1@deu.rst.pcc': 0.07432512303232292, 'train@deu.rst.pcc_precision@deu.rst.pcc': 0.08626798839716383, 'train@deu.rst.pcc_recall@deu.rst.pcc': 0.12138244331171871, 'train@deu.rst.pcc_loss@deu.rst.pcc': 2.704073905944824, 'train@deu.rst.pcc_runtime': 27.1701, 'train@deu.rst.pcc_samples_per_second': 79.646, 'train@deu.rst.pcc_steps_per_second': 2.503, 'epoch': 8.0}
{'loss': 2.7517, 'learning_rate': 6.666666666666667e-06, 'epoch': 8.0}
{'eval_loss': 2.7968883514404297, 'eval_accuracy@deu.rst.pcc': 0.14937759336099585, 'eval_f1@deu.rst.pcc': 0.05486064717268787, 'eval_precision@deu.rst.pcc': 0.10937945127429712, 'eval_recall@deu.rst.pcc': 0.11051968864468864, 'eval_loss@deu.rst.pcc': 2.7968883514404297, 'eval_runtime': 3.3524, 'eval_samples_per_second': 71.888, 'eval_steps_per_second': 2.386, 'epoch': 8.0}
{'train@deu.rst.pcc_loss': 2.6873042583465576, 'train@deu.rst.pcc_accuracy@deu.rst.pcc': 0.21025878003696857, 'train@deu.rst.pcc_f1@deu.rst.pcc': 0.08037695785072138, 'train@deu.rst.pcc_precision@deu.rst.pcc': 0.10846449464139285, 'train@deu.rst.pcc_recall@deu.rst.pcc': 0.1259381509727654, 'train@deu.rst.pcc_loss@deu.rst.pcc': 2.6873044967651367, 'train@deu.rst.pcc_runtime': 27.0982, 'train@deu.rst.pcc_samples_per_second': 79.858, 'train@deu.rst.pcc_steps_per_second': 2.509, 'epoch': 9.0}
{'loss': 2.7362, 'learning_rate': 5e-06, 'epoch': 9.0}
{'eval_loss': 2.785759210586548, 'eval_accuracy@deu.rst.pcc': 0.15352697095435686, 'eval_f1@deu.rst.pcc': 0.058382413988822233, 'eval_precision@deu.rst.pcc': 0.09910606548264776, 'eval_recall@deu.rst.pcc': 0.11366122303622304, 'eval_loss@deu.rst.pcc': 2.785759449005127, 'eval_runtime': 3.3556, 'eval_samples_per_second': 71.82, 'eval_steps_per_second': 2.384, 'epoch': 9.0}
{'train@deu.rst.pcc_loss': 2.6763968467712402, 'train@deu.rst.pcc_accuracy@deu.rst.pcc': 0.2121072088724584, 'train@deu.rst.pcc_f1@deu.rst.pcc': 0.08289065752643741, 'train@deu.rst.pcc_precision@deu.rst.pcc': 0.11351906531643331, 'train@deu.rst.pcc_recall@deu.rst.pcc': 0.12804270331164908, 'train@deu.rst.pcc_loss@deu.rst.pcc': 2.6763970851898193, 'train@deu.rst.pcc_runtime': 27.0452, 'train@deu.rst.pcc_samples_per_second': 80.014, 'train@deu.rst.pcc_steps_per_second': 2.514, 'epoch': 10.0}
{'loss': 2.7202, 'learning_rate': 3.3333333333333333e-06, 'epoch': 10.0}
{'eval_loss': 2.7746670246124268, 'eval_accuracy@deu.rst.pcc': 0.14937759336099585, 'eval_f1@deu.rst.pcc': 0.054806583901411486, 'eval_precision@deu.rst.pcc': 0.09599966993259675, 'eval_recall@deu.rst.pcc': 0.11051968864468864, 'eval_loss@deu.rst.pcc': 2.7746670246124268, 'eval_runtime': 3.3761, 'eval_samples_per_second': 71.384, 'eval_steps_per_second': 2.37, 'epoch': 10.0}
{'train@deu.rst.pcc_loss': 2.6694939136505127, 'train@deu.rst.pcc_accuracy@deu.rst.pcc': 0.21256931608133087, 'train@deu.rst.pcc_f1@deu.rst.pcc': 0.08370163387318265, 'train@deu.rst.pcc_precision@deu.rst.pcc': 0.09859307409698603, 'train@deu.rst.pcc_recall@deu.rst.pcc': 0.12885708222188946, 'train@deu.rst.pcc_loss@deu.rst.pcc': 2.6694934368133545, 'train@deu.rst.pcc_runtime': 27.122, 'train@deu.rst.pcc_samples_per_second': 79.788, 'train@deu.rst.pcc_steps_per_second': 2.507, 'epoch': 11.0}
{'loss': 2.7074, 'learning_rate': 1.6666666666666667e-06, 'epoch': 11.0}
{'eval_loss': 2.769493341445923, 'eval_accuracy@deu.rst.pcc': 0.16182572614107885, 'eval_f1@deu.rst.pcc': 0.05873025337980361, 'eval_precision@deu.rst.pcc': 0.09902709678571749, 'eval_recall@deu.rst.pcc': 0.11581069393569393, 'eval_loss@deu.rst.pcc': 2.7694931030273438, 'eval_runtime': 3.3851, 'eval_samples_per_second': 71.194, 'eval_steps_per_second': 2.363, 'epoch': 11.0}
{'train@deu.rst.pcc_loss': 2.6672232151031494, 'train@deu.rst.pcc_accuracy@deu.rst.pcc': 0.21626617375231053, 'train@deu.rst.pcc_f1@deu.rst.pcc': 0.08615913381427916, 'train@deu.rst.pcc_precision@deu.rst.pcc': 0.10586324652359004, 'train@deu.rst.pcc_recall@deu.rst.pcc': 0.13114716351021213, 'train@deu.rst.pcc_loss@deu.rst.pcc': 2.6672234535217285, 'train@deu.rst.pcc_runtime': 27.102, 'train@deu.rst.pcc_samples_per_second': 79.847, 'train@deu.rst.pcc_steps_per_second': 2.509, 'epoch': 12.0}
{'loss': 2.7, 'learning_rate': 0.0, 'epoch': 12.0}
{'eval_loss': 2.7671704292297363, 'eval_accuracy@deu.rst.pcc': 0.15352697095435686, 'eval_f1@deu.rst.pcc': 0.05531412038905115, 'eval_precision@deu.rst.pcc': 0.09624847581237034, 'eval_recall@deu.rst.pcc': 0.11200778388278387, 'eval_loss@deu.rst.pcc': 2.7671706676483154, 'eval_runtime': 3.424, 'eval_samples_per_second': 70.386, 'eval_steps_per_second': 2.336, 'epoch': 12.0}
{'train_runtime': 1048.5379, 'train_samples_per_second': 24.766, 'train_steps_per_second': 0.778, 'train_loss': 2.8613858316458907, 'epoch': 12.0}
***** train_log metrics *****
  epoch                    =       12.0
  train_loss               =     2.8614
  train_runtime            = 0:17:28.53
  train_samples_per_second =     24.766
  train_steps_per_second   =      0.778
{'train@fas.rst.prstc_loss': 2.405208110809326, 'train@fas.rst.prstc_accuracy@fas.rst.prstc': 0.23780487804878048, 'train@fas.rst.prstc_f1@fas.rst.prstc': 0.02260214430599826, 'train@fas.rst.prstc_precision@fas.rst.prstc': 0.013988522238163558, 'train@fas.rst.prstc_recall@fas.rst.prstc': 0.058823529411764705, 'train@fas.rst.prstc_loss@fas.rst.prstc': 2.4052083492279053, 'train@fas.rst.prstc_runtime': 49.9511, 'train@fas.rst.prstc_samples_per_second': 82.08, 'train@fas.rst.prstc_steps_per_second': 2.583, 'epoch': 1.0}
{'loss': 2.7454, 'learning_rate': 1.8333333333333333e-05, 'epoch': 1.0}
{'eval_loss': 2.320941686630249, 'eval_accuracy@fas.rst.prstc': 0.24448897795591182, 'eval_f1@fas.rst.prstc': 0.026194310252281264, 'eval_precision@fas.rst.prstc': 0.016299265197060788, 'eval_recall@fas.rst.prstc': 0.06666666666666667, 'eval_loss@fas.rst.prstc': 2.320941686630249, 'eval_runtime': 6.4463, 'eval_samples_per_second': 77.409, 'eval_steps_per_second': 2.482, 'epoch': 1.0}
{'train@fas.rst.prstc_loss': 2.351874589920044, 'train@fas.rst.prstc_accuracy@fas.rst.prstc': 0.28317073170731705, 'train@fas.rst.prstc_f1@fas.rst.prstc': 0.04471035618692234, 'train@fas.rst.prstc_precision@fas.rst.prstc': 0.03347607722584891, 'train@fas.rst.prstc_recall@fas.rst.prstc': 0.0732742851641475, 'train@fas.rst.prstc_loss@fas.rst.prstc': 2.351874589920044, 'train@fas.rst.prstc_runtime': 49.8946, 'train@fas.rst.prstc_samples_per_second': 82.173, 'train@fas.rst.prstc_steps_per_second': 2.585, 'epoch': 2.0}
{'loss': 2.3984, 'learning_rate': 1.6666666666666667e-05, 'epoch': 2.0}
{'eval_loss': 2.262939214706421, 'eval_accuracy@fas.rst.prstc': 0.3106212424849699, 'eval_f1@fas.rst.prstc': 0.05475625823451911, 'eval_precision@fas.rst.prstc': 0.04477807971014493, 'eval_recall@fas.rst.prstc': 0.08616298408172963, 'eval_loss@fas.rst.prstc': 2.262938976287842, 'eval_runtime': 6.475, 'eval_samples_per_second': 77.066, 'eval_steps_per_second': 2.471, 'epoch': 2.0}
{'train@fas.rst.prstc_loss': 2.329920530319214, 'train@fas.rst.prstc_accuracy@fas.rst.prstc': 0.2602439024390244, 'train@fas.rst.prstc_f1@fas.rst.prstc': 0.03543018056619346, 'train@fas.rst.prstc_precision@fas.rst.prstc': 0.03314473032983525, 'train@fas.rst.prstc_recall@fas.rst.prstc': 0.0655147994822588, 'train@fas.rst.prstc_loss@fas.rst.prstc': 2.329920768737793, 'train@fas.rst.prstc_runtime': 49.8522, 'train@fas.rst.prstc_samples_per_second': 82.243, 'train@fas.rst.prstc_steps_per_second': 2.588, 'epoch': 3.0}
{'loss': 2.3561, 'learning_rate': 1.5000000000000002e-05, 'epoch': 3.0}
{'eval_loss': 2.2433295249938965, 'eval_accuracy@fas.rst.prstc': 0.2725450901803607, 'eval_f1@fas.rst.prstc': 0.04122196187144538, 'eval_precision@fas.rst.prstc': 0.042902295856343996, 'eval_recall@fas.rst.prstc': 0.07484913281064386, 'eval_loss@fas.rst.prstc': 2.2433297634124756, 'eval_runtime': 6.4867, 'eval_samples_per_second': 76.927, 'eval_steps_per_second': 2.467, 'epoch': 3.0}
{'train@fas.rst.prstc_loss': 2.299527168273926, 'train@fas.rst.prstc_accuracy@fas.rst.prstc': 0.25341463414634147, 'train@fas.rst.prstc_f1@fas.rst.prstc': 0.0324318196846749, 'train@fas.rst.prstc_precision@fas.rst.prstc': 0.032635555429318036, 'train@fas.rst.prstc_recall@fas.rst.prstc': 0.0634851256378165, 'train@fas.rst.prstc_loss@fas.rst.prstc': 2.299527406692505, 'train@fas.rst.prstc_runtime': 49.8137, 'train@fas.rst.prstc_samples_per_second': 82.307, 'train@fas.rst.prstc_steps_per_second': 2.59, 'epoch': 4.0}
{'loss': 2.336, 'learning_rate': 1.3333333333333333e-05, 'epoch': 4.0}
{'eval_loss': 2.217583179473877, 'eval_accuracy@fas.rst.prstc': 0.2685370741482966, 'eval_f1@fas.rst.prstc': 0.03961262105592002, 'eval_precision@fas.rst.prstc': 0.04132292827945002, 'eval_recall@fas.rst.prstc': 0.07368971252078878, 'eval_loss@fas.rst.prstc': 2.217583179473877, 'eval_runtime': 6.4899, 'eval_samples_per_second': 76.889, 'eval_steps_per_second': 2.465, 'epoch': 4.0}
{'train@fas.rst.prstc_loss': 2.2306902408599854, 'train@fas.rst.prstc_accuracy@fas.rst.prstc': 0.3104878048780488, 'train@fas.rst.prstc_f1@fas.rst.prstc': 0.051865883072424394, 'train@fas.rst.prstc_precision@fas.rst.prstc': 0.038964738581074666, 'train@fas.rst.prstc_recall@fas.rst.prstc': 0.08243252783928628, 'train@fas.rst.prstc_loss@fas.rst.prstc': 2.2306902408599854, 'train@fas.rst.prstc_runtime': 49.7879, 'train@fas.rst.prstc_samples_per_second': 82.349, 'train@fas.rst.prstc_steps_per_second': 2.591, 'epoch': 5.0}
{'loss': 2.2902, 'learning_rate': 1.1666666666666668e-05, 'epoch': 5.0}
{'eval_loss': 2.136683464050293, 'eval_accuracy@fas.rst.prstc': 0.3246492985971944, 'eval_f1@fas.rst.prstc': 0.05975507994858032, 'eval_precision@fas.rst.prstc': 0.04529196221467711, 'eval_recall@fas.rst.prstc': 0.09121881682109766, 'eval_loss@fas.rst.prstc': 2.136683464050293, 'eval_runtime': 6.8156, 'eval_samples_per_second': 73.214, 'eval_steps_per_second': 2.348, 'epoch': 5.0}
{'train@fas.rst.prstc_loss': 2.186950445175171, 'train@fas.rst.prstc_accuracy@fas.rst.prstc': 0.32634146341463416, 'train@fas.rst.prstc_f1@fas.rst.prstc': 0.05534897230825863, 'train@fas.rst.prstc_precision@fas.rst.prstc': 0.043047761085126535, 'train@fas.rst.prstc_recall@fas.rst.prstc': 0.08712685728956066, 'train@fas.rst.prstc_loss@fas.rst.prstc': 2.18695068359375, 'train@fas.rst.prstc_runtime': 49.9068, 'train@fas.rst.prstc_samples_per_second': 82.153, 'train@fas.rst.prstc_steps_per_second': 2.585, 'epoch': 6.0}
{'loss': 2.2309, 'learning_rate': 1e-05, 'epoch': 6.0}
{'eval_loss': 2.0899999141693115, 'eval_accuracy@fas.rst.prstc': 0.3486973947895792, 'eval_f1@fas.rst.prstc': 0.06500512330603472, 'eval_precision@fas.rst.prstc': 0.05066799790804925, 'eval_recall@fas.rst.prstc': 0.09814207650273224, 'eval_loss@fas.rst.prstc': 2.0899999141693115, 'eval_runtime': 6.4614, 'eval_samples_per_second': 77.227, 'eval_steps_per_second': 2.476, 'epoch': 6.0}
{'train@fas.rst.prstc_loss': 2.170971393585205, 'train@fas.rst.prstc_accuracy@fas.rst.prstc': 0.3380487804878049, 'train@fas.rst.prstc_f1@fas.rst.prstc': 0.05741546751952009, 'train@fas.rst.prstc_precision@fas.rst.prstc': 0.045101121560788604, 'train@fas.rst.prstc_recall@fas.rst.prstc': 0.09045516297080752, 'train@fas.rst.prstc_loss@fas.rst.prstc': 2.170971393585205, 'train@fas.rst.prstc_runtime': 49.9776, 'train@fas.rst.prstc_samples_per_second': 82.037, 'train@fas.rst.prstc_steps_per_second': 2.581, 'epoch': 7.0}
{'loss': 2.1975, 'learning_rate': 8.333333333333334e-06, 'epoch': 7.0}
{'eval_loss': 2.070350170135498, 'eval_accuracy@fas.rst.prstc': 0.3667334669338677, 'eval_f1@fas.rst.prstc': 0.06921353484493174, 'eval_precision@fas.rst.prstc': 0.0551967320991793, 'eval_recall@fas.rst.prstc': 0.10325968163459254, 'eval_loss@fas.rst.prstc': 2.070350408554077, 'eval_runtime': 6.4002, 'eval_samples_per_second': 77.966, 'eval_steps_per_second': 2.5, 'epoch': 7.0}
{'train@fas.rst.prstc_loss': 2.145979404449463, 'train@fas.rst.prstc_accuracy@fas.rst.prstc': 0.34512195121951217, 'train@fas.rst.prstc_f1@fas.rst.prstc': 0.05800083240248954, 'train@fas.rst.prstc_precision@fas.rst.prstc': 0.0445995229442828, 'train@fas.rst.prstc_recall@fas.rst.prstc': 0.09224158403132121, 'train@fas.rst.prstc_loss@fas.rst.prstc': 2.145979404449463, 'train@fas.rst.prstc_runtime': 49.9586, 'train@fas.rst.prstc_samples_per_second': 82.068, 'train@fas.rst.prstc_steps_per_second': 2.582, 'epoch': 8.0}
{'loss': 2.1758, 'learning_rate': 6.666666666666667e-06, 'epoch': 8.0}
{'eval_loss': 2.055924654006958, 'eval_accuracy@fas.rst.prstc': 0.36472945891783565, 'eval_f1@fas.rst.prstc': 0.06752052545155994, 'eval_precision@fas.rst.prstc': 0.052029795158286776, 'eval_recall@fas.rst.prstc': 0.10264670943216916, 'eval_loss@fas.rst.prstc': 2.055924654006958, 'eval_runtime': 6.5283, 'eval_samples_per_second': 76.436, 'eval_steps_per_second': 2.451, 'epoch': 8.0}
{'train@fas.rst.prstc_loss': 2.1277048587799072, 'train@fas.rst.prstc_accuracy@fas.rst.prstc': 0.34804878048780485, 'train@fas.rst.prstc_f1@fas.rst.prstc': 0.058430611183625525, 'train@fas.rst.prstc_precision@fas.rst.prstc': 0.10264799511433044, 'train@fas.rst.prstc_recall@fas.rst.prstc': 0.09304955617846732, 'train@fas.rst.prstc_loss@fas.rst.prstc': 2.1277050971984863, 'train@fas.rst.prstc_runtime': 49.8553, 'train@fas.rst.prstc_samples_per_second': 82.238, 'train@fas.rst.prstc_steps_per_second': 2.587, 'epoch': 9.0}
{'loss': 2.1558, 'learning_rate': 5e-06, 'epoch': 9.0}
{'eval_loss': 2.0407752990722656, 'eval_accuracy@fas.rst.prstc': 0.37074148296593185, 'eval_f1@fas.rst.prstc': 0.06801501293003692, 'eval_precision@fas.rst.prstc': 0.051619616359030035, 'eval_recall@fas.rst.prstc': 0.10431931575196009, 'eval_loss@fas.rst.prstc': 2.0407752990722656, 'eval_runtime': 6.4231, 'eval_samples_per_second': 77.689, 'eval_steps_per_second': 2.491, 'epoch': 9.0}
{'train@fas.rst.prstc_loss': 2.1116693019866943, 'train@fas.rst.prstc_accuracy@fas.rst.prstc': 0.3539024390243902, 'train@fas.rst.prstc_f1@fas.rst.prstc': 0.06080207043486131, 'train@fas.rst.prstc_precision@fas.rst.prstc': 0.15097979314135504, 'train@fas.rst.prstc_recall@fas.rst.prstc': 0.09512696523614868, 'train@fas.rst.prstc_loss@fas.rst.prstc': 2.1116690635681152, 'train@fas.rst.prstc_runtime': 49.7531, 'train@fas.rst.prstc_samples_per_second': 82.407, 'train@fas.rst.prstc_steps_per_second': 2.593, 'epoch': 10.0}
{'loss': 2.1454, 'learning_rate': 3.3333333333333333e-06, 'epoch': 10.0}
{'eval_loss': 2.027939558029175, 'eval_accuracy@fas.rst.prstc': 0.37675350701402804, 'eval_f1@fas.rst.prstc': 0.06919755753115613, 'eval_precision@fas.rst.prstc': 0.05274007038712922, 'eval_recall@fas.rst.prstc': 0.10605844618674269, 'eval_loss@fas.rst.prstc': 2.027939796447754, 'eval_runtime': 6.4607, 'eval_samples_per_second': 77.236, 'eval_steps_per_second': 2.477, 'epoch': 10.0}
{'train@fas.rst.prstc_loss': 2.10351824760437, 'train@fas.rst.prstc_accuracy@fas.rst.prstc': 0.354390243902439, 'train@fas.rst.prstc_f1@fas.rst.prstc': 0.06079533745320487, 'train@fas.rst.prstc_precision@fas.rst.prstc': 0.15212290235185622, 'train@fas.rst.prstc_recall@fas.rst.prstc': 0.09525779766273754, 'train@fas.rst.prstc_loss@fas.rst.prstc': 2.103517770767212, 'train@fas.rst.prstc_runtime': 49.8102, 'train@fas.rst.prstc_samples_per_second': 82.312, 'train@fas.rst.prstc_steps_per_second': 2.59, 'epoch': 11.0}
{'loss': 2.1336, 'learning_rate': 1.6666666666666667e-06, 'epoch': 11.0}
{'eval_loss': 2.0236682891845703, 'eval_accuracy@fas.rst.prstc': 0.37074148296593185, 'eval_f1@fas.rst.prstc': 0.07090204607713399, 'eval_precision@fas.rst.prstc': 0.11751009263447075, 'eval_recall@fas.rst.prstc': 0.10546072951444899, 'eval_loss@fas.rst.prstc': 2.023668050765991, 'eval_runtime': 7.8935, 'eval_samples_per_second': 63.216, 'eval_steps_per_second': 2.027, 'epoch': 11.0}
{'train@fas.rst.prstc_loss': 2.1022515296936035, 'train@fas.rst.prstc_accuracy@fas.rst.prstc': 0.35317073170731705, 'train@fas.rst.prstc_f1@fas.rst.prstc': 0.06098068151974017, 'train@fas.rst.prstc_precision@fas.rst.prstc': 0.1041029962879239, 'train@fas.rst.prstc_recall@fas.rst.prstc': 0.09504087282581118, 'train@fas.rst.prstc_loss@fas.rst.prstc': 2.1022515296936035, 'train@fas.rst.prstc_runtime': 49.7853, 'train@fas.rst.prstc_samples_per_second': 82.354, 'train@fas.rst.prstc_steps_per_second': 2.591, 'epoch': 12.0}
{'loss': 2.1372, 'learning_rate': 0.0, 'epoch': 12.0}
{'eval_loss': 2.0244991779327393, 'eval_accuracy@fas.rst.prstc': 0.374749498997996, 'eval_f1@fas.rst.prstc': 0.0744926013483933, 'eval_precision@fas.rst.prstc': 0.11758343050696873, 'eval_recall@fas.rst.prstc': 0.10772830150929713, 'eval_loss@fas.rst.prstc': 2.0244991779327393, 'eval_runtime': 6.4604, 'eval_samples_per_second': 77.239, 'eval_steps_per_second': 2.477, 'epoch': 12.0}
{'train_runtime': 1937.8021, 'train_samples_per_second': 25.39, 'train_steps_per_second': 0.799, 'train_loss': 2.275173473111727, 'epoch': 12.0}
***** train_logger metrics *****
  epoch                    =       12.0
  train_loss               =     2.8614
  train_runtime            = 0:17:28.53
  train_samples_per_second =     24.766
  train_steps_per_second   =      0.778
-------------------------------------------------------------------
Lang1:  eng.pdtb.pdtb    Lang2:  fas.rst.prstc
Saving run to:  runs/full_shot/FullShot=v4_eng.pdtb.pdtb_fas.rst.prstc_lrfunc=Adafactor_lr=2e-05_plm=bert-base-multilingual-cased
Running with params: BERT_MODEL=bert-base-multilingual-cased lr=2e-05
read 43920 examples
read 1674 examples
read 2257 examples
read 4100 examples
read 499 examples
read 592 examples
Total prediction labels:  40
BertAdapterModel(
  (shared_parameters): ModuleDict()
  (bert): BertModel(
    (shared_parameters): ModuleDict()
    (invertible_adapters): ModuleDict(
      (en): NICECouplingBlock(
        (F): Sequential(
          (0): Linear(in_features=384, out_features=192, bias=True)
          (1): Activation_Function_Class(
            (f): ReLU()
          )
          (2): Linear(in_features=192, out_features=384, bias=True)
        )
        (G): Sequential(
          (0): Linear(in_features=384, out_features=192, bias=True)
          (1): Activation_Function_Class(
            (f): ReLU()
          )
          (2): Linear(in_features=192, out_features=384, bias=True)
        )
      )
      (ar): NICECouplingBlock(
        (F): Sequential(
          (0): Linear(in_features=384, out_features=192, bias=True)
          (1): Activation_Function_Class(
            (f): ReLU()
          )
          (2): Linear(in_features=192, out_features=384, bias=True)
        )
        (G): Sequential(
          (0): Linear(in_features=384, out_features=192, bias=True)
          (1): Activation_Function_Class(
            (f): ReLU()
          )
          (2): Linear(in_features=192, out_features=384, bias=True)
        )
      )
    )
    (embeddings): BertEmbeddings(
      (word_embeddings): Embedding(119547, 768, padding_idx=0)
      (position_embeddings): Embedding(512, 768)
      (token_type_embeddings): Embedding(2, 768)
      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
      (dropout): Dropout(p=0.1, inplace=False)
    )
    (encoder): BertEncoder(
      (layer): ModuleList(
        (0): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (en): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (ar): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (1): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (en): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (ar): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (2): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (en): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (ar): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (3): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (en): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (ar): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (4): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (en): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (ar): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (5): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (en): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (ar): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (6): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (en): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (ar): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (7): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (en): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (ar): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (8): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (en): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (ar): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (9): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (en): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (ar): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (10): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (en): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (ar): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (11): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (en): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (ar): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
      )
    )
    (pooler): BertPooler(
      (dense): Linear(in_features=768, out_features=768, bias=True)
      (activation): Tanh()
    )
    (prefix_tuning): PrefixTuningPool(
      (prefix_tunings): ModuleDict()
    )
  )
  (heads): ModuleDict(
    (disrpt): ClassificationHead(
      (0): Dropout(p=0.1, inplace=False)
      (1): Linear(in_features=768, out_features=768, bias=True)
      (2): Activation_Function_Class(
        (f): Tanh()
      )
      (3): Dropout(p=0.1, inplace=False)
      (4): Linear(in_features=768, out_features=40, bias=True)
    )
  )
)
{'train@eng.pdtb.pdtb_loss': 1.2564761638641357, 'train@eng.pdtb.pdtb_accuracy@eng.pdtb.pdtb': 0.5997950819672131, 'train@eng.pdtb.pdtb_f1@eng.pdtb.pdtb': 0.2616976665247401, 'train@eng.pdtb.pdtb_precision@eng.pdtb.pdtb': 0.3655457518244028, 'train@eng.pdtb.pdtb_recall@eng.pdtb.pdtb': 0.25708848977965554, 'train@eng.pdtb.pdtb_loss@eng.pdtb.pdtb': 1.2564760446548462, 'train@eng.pdtb.pdtb_runtime': 533.0007, 'train@eng.pdtb.pdtb_samples_per_second': 82.401, 'train@eng.pdtb.pdtb_steps_per_second': 2.576, 'epoch': 1.0}
{'loss': 1.843, 'learning_rate': 1.8333333333333333e-05, 'epoch': 1.0}
{'eval_loss': 1.1722300052642822, 'eval_accuracy@eng.pdtb.pdtb': 0.6320191158900836, 'eval_f1@eng.pdtb.pdtb': 0.31042856087621246, 'eval_precision@eng.pdtb.pdtb': 0.3816629262630031, 'eval_recall@eng.pdtb.pdtb': 0.3080168235109718, 'eval_loss@eng.pdtb.pdtb': 1.1722302436828613, 'eval_runtime': 20.6832, 'eval_samples_per_second': 80.935, 'eval_steps_per_second': 2.562, 'epoch': 1.0}
{'train@eng.pdtb.pdtb_loss': 1.095133900642395, 'train@eng.pdtb.pdtb_accuracy@eng.pdtb.pdtb': 0.6411657559198543, 'train@eng.pdtb.pdtb_f1@eng.pdtb.pdtb': 0.3488775742464251, 'train@eng.pdtb.pdtb_precision@eng.pdtb.pdtb': 0.45319640088440893, 'train@eng.pdtb.pdtb_recall@eng.pdtb.pdtb': 0.3388390911196078, 'train@eng.pdtb.pdtb_loss@eng.pdtb.pdtb': 1.0951340198516846, 'train@eng.pdtb.pdtb_runtime': 530.3073, 'train@eng.pdtb.pdtb_samples_per_second': 82.82, 'train@eng.pdtb.pdtb_steps_per_second': 2.589, 'epoch': 2.0}
{'loss': 1.2134, 'learning_rate': 1.6666666666666667e-05, 'epoch': 2.0}
{'eval_loss': 1.0246126651763916, 'eval_accuracy@eng.pdtb.pdtb': 0.6702508960573477, 'eval_f1@eng.pdtb.pdtb': 0.3985862555488081, 'eval_precision@eng.pdtb.pdtb': 0.44365514545253343, 'eval_recall@eng.pdtb.pdtb': 0.3912918540138763, 'eval_loss@eng.pdtb.pdtb': 1.0246126651763916, 'eval_runtime': 20.7631, 'eval_samples_per_second': 80.624, 'eval_steps_per_second': 2.553, 'epoch': 2.0}
{'train@eng.pdtb.pdtb_loss': 1.0461921691894531, 'train@eng.pdtb.pdtb_accuracy@eng.pdtb.pdtb': 0.6539389799635701, 'train@eng.pdtb.pdtb_f1@eng.pdtb.pdtb': 0.42976532949820906, 'train@eng.pdtb.pdtb_precision@eng.pdtb.pdtb': 0.4713427015192912, 'train@eng.pdtb.pdtb_recall@eng.pdtb.pdtb': 0.40988475846427636, 'train@eng.pdtb.pdtb_loss@eng.pdtb.pdtb': 1.0461921691894531, 'train@eng.pdtb.pdtb_runtime': 530.2176, 'train@eng.pdtb.pdtb_samples_per_second': 82.834, 'train@eng.pdtb.pdtb_steps_per_second': 2.59, 'epoch': 3.0}
{'loss': 1.1187, 'learning_rate': 1.5000000000000002e-05, 'epoch': 3.0}
{'eval_loss': 0.9890613555908203, 'eval_accuracy@eng.pdtb.pdtb': 0.6821983273596177, 'eval_f1@eng.pdtb.pdtb': 0.4681000286340729, 'eval_precision@eng.pdtb.pdtb': 0.5193029940727062, 'eval_recall@eng.pdtb.pdtb': 0.451031371016638, 'eval_loss@eng.pdtb.pdtb': 0.9890612959861755, 'eval_runtime': 20.6728, 'eval_samples_per_second': 80.976, 'eval_steps_per_second': 2.564, 'epoch': 3.0}
{'train@eng.pdtb.pdtb_loss': 0.9988186359405518, 'train@eng.pdtb.pdtb_accuracy@eng.pdtb.pdtb': 0.6672586520947177, 'train@eng.pdtb.pdtb_f1@eng.pdtb.pdtb': 0.44862986579834224, 'train@eng.pdtb.pdtb_precision@eng.pdtb.pdtb': 0.476546789940983, 'train@eng.pdtb.pdtb_recall@eng.pdtb.pdtb': 0.4398265164721855, 'train@eng.pdtb.pdtb_loss@eng.pdtb.pdtb': 0.9988184571266174, 'train@eng.pdtb.pdtb_runtime': 530.1843, 'train@eng.pdtb.pdtb_samples_per_second': 82.839, 'train@eng.pdtb.pdtb_steps_per_second': 2.59, 'epoch': 4.0}
{'loss': 1.0726, 'learning_rate': 1.3333333333333333e-05, 'epoch': 4.0}
{'eval_loss': 0.9474700093269348, 'eval_accuracy@eng.pdtb.pdtb': 0.6792114695340502, 'eval_f1@eng.pdtb.pdtb': 0.4919492155772881, 'eval_precision@eng.pdtb.pdtb': 0.5323676101336945, 'eval_recall@eng.pdtb.pdtb': 0.47525810936662494, 'eval_loss@eng.pdtb.pdtb': 0.9474700093269348, 'eval_runtime': 20.6841, 'eval_samples_per_second': 80.932, 'eval_steps_per_second': 2.562, 'epoch': 4.0}
{'train@eng.pdtb.pdtb_loss': 0.9769942164421082, 'train@eng.pdtb.pdtb_accuracy@eng.pdtb.pdtb': 0.6726092896174863, 'train@eng.pdtb.pdtb_f1@eng.pdtb.pdtb': 0.45455577919390683, 'train@eng.pdtb.pdtb_precision@eng.pdtb.pdtb': 0.5194339059091293, 'train@eng.pdtb.pdtb_recall@eng.pdtb.pdtb': 0.4502963988044462, 'train@eng.pdtb.pdtb_loss@eng.pdtb.pdtb': 0.9769942164421082, 'train@eng.pdtb.pdtb_runtime': 530.3741, 'train@eng.pdtb.pdtb_samples_per_second': 82.809, 'train@eng.pdtb.pdtb_steps_per_second': 2.589, 'epoch': 5.0}
{'loss': 1.0405, 'learning_rate': 1.1666666666666668e-05, 'epoch': 5.0}
{'eval_loss': 0.932245135307312, 'eval_accuracy@eng.pdtb.pdtb': 0.6827956989247311, 'eval_f1@eng.pdtb.pdtb': 0.5186718855678262, 'eval_precision@eng.pdtb.pdtb': 0.5436356026575601, 'eval_recall@eng.pdtb.pdtb': 0.514032275763518, 'eval_loss@eng.pdtb.pdtb': 0.932245135307312, 'eval_runtime': 20.7434, 'eval_samples_per_second': 80.7, 'eval_steps_per_second': 2.555, 'epoch': 5.0}
{'train@eng.pdtb.pdtb_loss': 0.9566686153411865, 'train@eng.pdtb.pdtb_accuracy@eng.pdtb.pdtb': 0.6793032786885246, 'train@eng.pdtb.pdtb_f1@eng.pdtb.pdtb': 0.4656965050196234, 'train@eng.pdtb.pdtb_precision@eng.pdtb.pdtb': 0.5272718653714946, 'train@eng.pdtb.pdtb_recall@eng.pdtb.pdtb': 0.46127898717583987, 'train@eng.pdtb.pdtb_loss@eng.pdtb.pdtb': 0.9566685557365417, 'train@eng.pdtb.pdtb_runtime': 530.5826, 'train@eng.pdtb.pdtb_samples_per_second': 82.777, 'train@eng.pdtb.pdtb_steps_per_second': 2.588, 'epoch': 6.0}
{'loss': 1.0182, 'learning_rate': 1e-05, 'epoch': 6.0}
{'eval_loss': 0.9236809015274048, 'eval_accuracy@eng.pdtb.pdtb': 0.6816009557945042, 'eval_f1@eng.pdtb.pdtb': 0.5241681334696899, 'eval_precision@eng.pdtb.pdtb': 0.549913297255246, 'eval_recall@eng.pdtb.pdtb': 0.5178764754582874, 'eval_loss@eng.pdtb.pdtb': 0.9236809611320496, 'eval_runtime': 20.6853, 'eval_samples_per_second': 80.927, 'eval_steps_per_second': 2.562, 'epoch': 6.0}
{'train@eng.pdtb.pdtb_loss': 0.9456487894058228, 'train@eng.pdtb.pdtb_accuracy@eng.pdtb.pdtb': 0.6796448087431693, 'train@eng.pdtb.pdtb_f1@eng.pdtb.pdtb': 0.4671772001862738, 'train@eng.pdtb.pdtb_precision@eng.pdtb.pdtb': 0.5331427711262254, 'train@eng.pdtb.pdtb_recall@eng.pdtb.pdtb': 0.4596622002396443, 'train@eng.pdtb.pdtb_loss@eng.pdtb.pdtb': 0.9456488490104675, 'train@eng.pdtb.pdtb_runtime': 530.5701, 'train@eng.pdtb.pdtb_samples_per_second': 82.779, 'train@eng.pdtb.pdtb_steps_per_second': 2.588, 'epoch': 7.0}
{'loss': 1.0046, 'learning_rate': 8.333333333333334e-06, 'epoch': 7.0}
{'eval_loss': 0.909687340259552, 'eval_accuracy@eng.pdtb.pdtb': 0.6869772998805257, 'eval_f1@eng.pdtb.pdtb': 0.5294060159420532, 'eval_precision@eng.pdtb.pdtb': 0.5656749480961676, 'eval_recall@eng.pdtb.pdtb': 0.5141031296367977, 'eval_loss@eng.pdtb.pdtb': 0.909687340259552, 'eval_runtime': 20.7099, 'eval_samples_per_second': 80.831, 'eval_steps_per_second': 2.559, 'epoch': 7.0}
{'train@eng.pdtb.pdtb_loss': 0.9343177080154419, 'train@eng.pdtb.pdtb_accuracy@eng.pdtb.pdtb': 0.6844034608378871, 'train@eng.pdtb.pdtb_f1@eng.pdtb.pdtb': 0.4708164461511315, 'train@eng.pdtb.pdtb_precision@eng.pdtb.pdtb': 0.5048371375137175, 'train@eng.pdtb.pdtb_recall@eng.pdtb.pdtb': 0.46932473999551166, 'train@eng.pdtb.pdtb_loss@eng.pdtb.pdtb': 0.9343178272247314, 'train@eng.pdtb.pdtb_runtime': 529.3725, 'train@eng.pdtb.pdtb_samples_per_second': 82.966, 'train@eng.pdtb.pdtb_steps_per_second': 2.594, 'epoch': 8.0}
{'loss': 0.993, 'learning_rate': 6.666666666666667e-06, 'epoch': 8.0}
{'eval_loss': 0.9075481295585632, 'eval_accuracy@eng.pdtb.pdtb': 0.6935483870967742, 'eval_f1@eng.pdtb.pdtb': 0.5329757808478466, 'eval_precision@eng.pdtb.pdtb': 0.5626927037728426, 'eval_recall@eng.pdtb.pdtb': 0.5246207520986746, 'eval_loss@eng.pdtb.pdtb': 0.9075483679771423, 'eval_runtime': 20.7082, 'eval_samples_per_second': 80.838, 'eval_steps_per_second': 2.559, 'epoch': 8.0}
{'train@eng.pdtb.pdtb_loss': 0.9260206818580627, 'train@eng.pdtb.pdtb_accuracy@eng.pdtb.pdtb': 0.6876366120218579, 'train@eng.pdtb.pdtb_f1@eng.pdtb.pdtb': 0.47519720251226993, 'train@eng.pdtb.pdtb_precision@eng.pdtb.pdtb': 0.5176159450011418, 'train@eng.pdtb.pdtb_recall@eng.pdtb.pdtb': 0.4723877183865079, 'train@eng.pdtb.pdtb_loss@eng.pdtb.pdtb': 0.9260207414627075, 'train@eng.pdtb.pdtb_runtime': 527.2985, 'train@eng.pdtb.pdtb_samples_per_second': 83.292, 'train@eng.pdtb.pdtb_steps_per_second': 2.604, 'epoch': 9.0}
{'loss': 0.9812, 'learning_rate': 5e-06, 'epoch': 9.0}
{'eval_loss': 0.905498206615448, 'eval_accuracy@eng.pdtb.pdtb': 0.6887694145758662, 'eval_f1@eng.pdtb.pdtb': 0.5398649897036487, 'eval_precision@eng.pdtb.pdtb': 0.5707881196295262, 'eval_recall@eng.pdtb.pdtb': 0.5281752442991824, 'eval_loss@eng.pdtb.pdtb': 0.905498206615448, 'eval_runtime': 20.426, 'eval_samples_per_second': 81.954, 'eval_steps_per_second': 2.595, 'epoch': 9.0}
{'train@eng.pdtb.pdtb_loss': 0.9222122430801392, 'train@eng.pdtb.pdtb_accuracy@eng.pdtb.pdtb': 0.688615664845173, 'train@eng.pdtb.pdtb_f1@eng.pdtb.pdtb': 0.47530487116058395, 'train@eng.pdtb.pdtb_precision@eng.pdtb.pdtb': 0.5258506511155171, 'train@eng.pdtb.pdtb_recall@eng.pdtb.pdtb': 0.47173194147850456, 'train@eng.pdtb.pdtb_loss@eng.pdtb.pdtb': 0.9222120642662048, 'train@eng.pdtb.pdtb_runtime': 524.615, 'train@eng.pdtb.pdtb_samples_per_second': 83.719, 'train@eng.pdtb.pdtb_steps_per_second': 2.617, 'epoch': 10.0}
{'loss': 0.9756, 'learning_rate': 3.3333333333333333e-06, 'epoch': 10.0}
{'eval_loss': 0.89938884973526, 'eval_accuracy@eng.pdtb.pdtb': 0.6881720430107527, 'eval_f1@eng.pdtb.pdtb': 0.5493551460266622, 'eval_precision@eng.pdtb.pdtb': 0.6174854215732218, 'eval_recall@eng.pdtb.pdtb': 0.5340859972779631, 'eval_loss@eng.pdtb.pdtb': 0.8993889093399048, 'eval_runtime': 20.3315, 'eval_samples_per_second': 82.335, 'eval_steps_per_second': 2.607, 'epoch': 10.0}
{'train@eng.pdtb.pdtb_loss': 0.9180994629859924, 'train@eng.pdtb.pdtb_accuracy@eng.pdtb.pdtb': 0.6891165755919855, 'train@eng.pdtb.pdtb_f1@eng.pdtb.pdtb': 0.4762873896622926, 'train@eng.pdtb.pdtb_precision@eng.pdtb.pdtb': 0.5049663769164485, 'train@eng.pdtb.pdtb_recall@eng.pdtb.pdtb': 0.4749833842928041, 'train@eng.pdtb.pdtb_loss@eng.pdtb.pdtb': 0.9180994033813477, 'train@eng.pdtb.pdtb_runtime': 524.5665, 'train@eng.pdtb.pdtb_samples_per_second': 83.726, 'train@eng.pdtb.pdtb_steps_per_second': 2.617, 'epoch': 11.0}
{'loss': 0.9701, 'learning_rate': 1.6666666666666667e-06, 'epoch': 11.0}
{'eval_loss': 0.899306058883667, 'eval_accuracy@eng.pdtb.pdtb': 0.6953405017921147, 'eval_f1@eng.pdtb.pdtb': 0.5601731681832554, 'eval_precision@eng.pdtb.pdtb': 0.5955648870453367, 'eval_recall@eng.pdtb.pdtb': 0.550024168087498, 'eval_loss@eng.pdtb.pdtb': 0.8993059992790222, 'eval_runtime': 20.3413, 'eval_samples_per_second': 82.296, 'eval_steps_per_second': 2.606, 'epoch': 11.0}
{'train@eng.pdtb.pdtb_loss': 0.9167644381523132, 'train@eng.pdtb.pdtb_accuracy@eng.pdtb.pdtb': 0.6897313296903461, 'train@eng.pdtb.pdtb_f1@eng.pdtb.pdtb': 0.4768992164568749, 'train@eng.pdtb.pdtb_precision@eng.pdtb.pdtb': 0.5157264469877679, 'train@eng.pdtb.pdtb_recall@eng.pdtb.pdtb': 0.4740891356105777, 'train@eng.pdtb.pdtb_loss@eng.pdtb.pdtb': 0.9167644381523132, 'train@eng.pdtb.pdtb_runtime': 524.7187, 'train@eng.pdtb.pdtb_samples_per_second': 83.702, 'train@eng.pdtb.pdtb_steps_per_second': 2.617, 'epoch': 12.0}
{'loss': 0.9652, 'learning_rate': 0.0, 'epoch': 12.0}
{'eval_loss': 0.8973062038421631, 'eval_accuracy@eng.pdtb.pdtb': 0.6911589008363201, 'eval_f1@eng.pdtb.pdtb': 0.558093081664042, 'eval_precision@eng.pdtb.pdtb': 0.5982862491794232, 'eval_recall@eng.pdtb.pdtb': 0.5439435783447205, 'eval_loss@eng.pdtb.pdtb': 0.8973062038421631, 'eval_runtime': 20.3615, 'eval_samples_per_second': 82.214, 'eval_steps_per_second': 2.603, 'epoch': 12.0}
{'train_runtime': 19978.8455, 'train_samples_per_second': 26.38, 'train_steps_per_second': 0.825, 'train_loss': 1.0996783673980335, 'epoch': 12.0}
***** train_log metrics *****
  epoch                    =       12.0
  train_loss               =     1.0997
  train_runtime            = 5:32:58.84
  train_samples_per_second =      26.38
  train_steps_per_second   =      0.825
{'train@fas.rst.prstc_loss': 2.3846545219421387, 'train@fas.rst.prstc_accuracy@fas.rst.prstc': 0.24609756097560975, 'train@fas.rst.prstc_f1@fas.rst.prstc': 0.031622927421536585, 'train@fas.rst.prstc_precision@fas.rst.prstc': 0.029799486213188783, 'train@fas.rst.prstc_recall@fas.rst.prstc': 0.06168437042029032, 'train@fas.rst.prstc_loss@fas.rst.prstc': 2.3846540451049805, 'train@fas.rst.prstc_runtime': 49.1908, 'train@fas.rst.prstc_samples_per_second': 83.349, 'train@fas.rst.prstc_steps_per_second': 2.622, 'epoch': 1.0}
{'loss': 2.7409, 'learning_rate': 1.8333333333333333e-05, 'epoch': 1.0}
{'eval_loss': 2.2800776958465576, 'eval_accuracy@fas.rst.prstc': 0.2545090180360721, 'eval_f1@fas.rst.prstc': 0.04236595392304735, 'eval_precision@fas.rst.prstc': 0.10121951219512196, 'eval_recall@fas.rst.prstc': 0.07218066549124057, 'eval_loss@fas.rst.prstc': 2.2800776958465576, 'eval_runtime': 6.3349, 'eval_samples_per_second': 78.77, 'eval_steps_per_second': 2.526, 'epoch': 1.0}
{'train@fas.rst.prstc_loss': 2.2608261108398438, 'train@fas.rst.prstc_accuracy@fas.rst.prstc': 0.33170731707317075, 'train@fas.rst.prstc_f1@fas.rst.prstc': 0.07841913297300632, 'train@fas.rst.prstc_precision@fas.rst.prstc': 0.06982338479175056, 'train@fas.rst.prstc_recall@fas.rst.prstc': 0.1025605754655188, 'train@fas.rst.prstc_loss@fas.rst.prstc': 2.260826349258423, 'train@fas.rst.prstc_runtime': 49.238, 'train@fas.rst.prstc_samples_per_second': 83.269, 'train@fas.rst.prstc_steps_per_second': 2.62, 'epoch': 2.0}
{'loss': 2.366, 'learning_rate': 1.6666666666666667e-05, 'epoch': 2.0}
{'eval_loss': 2.1653530597686768, 'eval_accuracy@fas.rst.prstc': 0.33867735470941884, 'eval_f1@fas.rst.prstc': 0.08670710650672482, 'eval_precision@fas.rst.prstc': 0.0810416285438196, 'eval_recall@fas.rst.prstc': 0.11105700816545999, 'eval_loss@fas.rst.prstc': 2.1653530597686768, 'eval_runtime': 6.3508, 'eval_samples_per_second': 78.573, 'eval_steps_per_second': 2.519, 'epoch': 2.0}
{'train@fas.rst.prstc_loss': 2.164173126220703, 'train@fas.rst.prstc_accuracy@fas.rst.prstc': 0.3729268292682927, 'train@fas.rst.prstc_f1@fas.rst.prstc': 0.08776089597532313, 'train@fas.rst.prstc_precision@fas.rst.prstc': 0.07842741031755175, 'train@fas.rst.prstc_recall@fas.rst.prstc': 0.11410463201776931, 'train@fas.rst.prstc_loss@fas.rst.prstc': 2.1641733646392822, 'train@fas.rst.prstc_runtime': 49.2019, 'train@fas.rst.prstc_samples_per_second': 83.33, 'train@fas.rst.prstc_steps_per_second': 2.622, 'epoch': 3.0}
{'loss': 2.2786, 'learning_rate': 1.5000000000000002e-05, 'epoch': 3.0}
{'eval_loss': 2.07684063911438, 'eval_accuracy@fas.rst.prstc': 0.3867735470941884, 'eval_f1@fas.rst.prstc': 0.0998991807896105, 'eval_precision@fas.rst.prstc': 0.09812423994242177, 'eval_recall@fas.rst.prstc': 0.1256125345437721, 'eval_loss@fas.rst.prstc': 2.07684063911438, 'eval_runtime': 6.3104, 'eval_samples_per_second': 79.076, 'eval_steps_per_second': 2.535, 'epoch': 3.0}
{'train@fas.rst.prstc_loss': 2.068889617919922, 'train@fas.rst.prstc_accuracy@fas.rst.prstc': 0.38634146341463416, 'train@fas.rst.prstc_f1@fas.rst.prstc': 0.09135237226280726, 'train@fas.rst.prstc_precision@fas.rst.prstc': 0.1256416011515443, 'train@fas.rst.prstc_recall@fas.rst.prstc': 0.11957729931397472, 'train@fas.rst.prstc_loss@fas.rst.prstc': 2.068889856338501, 'train@fas.rst.prstc_runtime': 49.1797, 'train@fas.rst.prstc_samples_per_second': 83.368, 'train@fas.rst.prstc_steps_per_second': 2.623, 'epoch': 4.0}
{'loss': 2.1827, 'learning_rate': 1.3333333333333333e-05, 'epoch': 4.0}
{'eval_loss': 1.9981952905654907, 'eval_accuracy@fas.rst.prstc': 0.38877755511022044, 'eval_f1@fas.rst.prstc': 0.09900446836355528, 'eval_precision@fas.rst.prstc': 0.09292161828690175, 'eval_recall@fas.rst.prstc': 0.12625876880369133, 'eval_loss@fas.rst.prstc': 1.9981952905654907, 'eval_runtime': 6.286, 'eval_samples_per_second': 79.382, 'eval_steps_per_second': 2.545, 'epoch': 4.0}
{'train@fas.rst.prstc_loss': 2.0223388671875, 'train@fas.rst.prstc_accuracy@fas.rst.prstc': 0.39439024390243904, 'train@fas.rst.prstc_f1@fas.rst.prstc': 0.09717927169105929, 'train@fas.rst.prstc_precision@fas.rst.prstc': 0.11929906244116137, 'train@fas.rst.prstc_recall@fas.rst.prstc': 0.12307203908231734, 'train@fas.rst.prstc_loss@fas.rst.prstc': 2.022339105606079, 'train@fas.rst.prstc_runtime': 49.1549, 'train@fas.rst.prstc_samples_per_second': 83.41, 'train@fas.rst.prstc_steps_per_second': 2.624, 'epoch': 5.0}
{'loss': 2.1157, 'learning_rate': 1.1666666666666668e-05, 'epoch': 5.0}
{'eval_loss': 1.9462326765060425, 'eval_accuracy@fas.rst.prstc': 0.39478957915831664, 'eval_f1@fas.rst.prstc': 0.10184568773780392, 'eval_precision@fas.rst.prstc': 0.09794415389791113, 'eval_recall@fas.rst.prstc': 0.1280311612959698, 'eval_loss@fas.rst.prstc': 1.9462326765060425, 'eval_runtime': 6.3272, 'eval_samples_per_second': 78.866, 'eval_steps_per_second': 2.529, 'epoch': 5.0}
{'train@fas.rst.prstc_loss': 1.9899712800979614, 'train@fas.rst.prstc_accuracy@fas.rst.prstc': 0.40219512195121954, 'train@fas.rst.prstc_f1@fas.rst.prstc': 0.10733487857121549, 'train@fas.rst.prstc_precision@fas.rst.prstc': 0.12036662832650336, 'train@fas.rst.prstc_recall@fas.rst.prstc': 0.13061535465489843, 'train@fas.rst.prstc_loss@fas.rst.prstc': 1.9899711608886719, 'train@fas.rst.prstc_runtime': 49.1749, 'train@fas.rst.prstc_samples_per_second': 83.376, 'train@fas.rst.prstc_steps_per_second': 2.623, 'epoch': 6.0}
{'loss': 2.0636, 'learning_rate': 1e-05, 'epoch': 6.0}
{'eval_loss': 1.9166032075881958, 'eval_accuracy@fas.rst.prstc': 0.4028056112224449, 'eval_f1@fas.rst.prstc': 0.10750773479032474, 'eval_precision@fas.rst.prstc': 0.10770148044818431, 'eval_recall@fas.rst.prstc': 0.13375033815611817, 'eval_loss@fas.rst.prstc': 1.9166033267974854, 'eval_runtime': 6.2998, 'eval_samples_per_second': 79.209, 'eval_steps_per_second': 2.54, 'epoch': 6.0}
{'train@fas.rst.prstc_loss': 1.970235824584961, 'train@fas.rst.prstc_accuracy@fas.rst.prstc': 0.4068292682926829, 'train@fas.rst.prstc_f1@fas.rst.prstc': 0.11060437482257891, 'train@fas.rst.prstc_precision@fas.rst.prstc': 0.17846167024991852, 'train@fas.rst.prstc_recall@fas.rst.prstc': 0.13234546255619617, 'train@fas.rst.prstc_loss@fas.rst.prstc': 1.9702359437942505, 'train@fas.rst.prstc_runtime': 49.2057, 'train@fas.rst.prstc_samples_per_second': 83.324, 'train@fas.rst.prstc_steps_per_second': 2.622, 'epoch': 7.0}
{'loss': 2.0331, 'learning_rate': 8.333333333333334e-06, 'epoch': 7.0}
{'eval_loss': 1.8885977268218994, 'eval_accuracy@fas.rst.prstc': 0.41082164328657317, 'eval_f1@fas.rst.prstc': 0.11140749910511874, 'eval_precision@fas.rst.prstc': 0.11452705030655493, 'eval_recall@fas.rst.prstc': 0.13610711931855785, 'eval_loss@fas.rst.prstc': 1.8885976076126099, 'eval_runtime': 6.3359, 'eval_samples_per_second': 78.757, 'eval_steps_per_second': 2.525, 'epoch': 7.0}
{'train@fas.rst.prstc_loss': 1.9513355493545532, 'train@fas.rst.prstc_accuracy@fas.rst.prstc': 0.41073170731707315, 'train@fas.rst.prstc_f1@fas.rst.prstc': 0.1186067865347083, 'train@fas.rst.prstc_precision@fas.rst.prstc': 0.14913556640169628, 'train@fas.rst.prstc_recall@fas.rst.prstc': 0.1383911194985932, 'train@fas.rst.prstc_loss@fas.rst.prstc': 1.9513354301452637, 'train@fas.rst.prstc_runtime': 49.1693, 'train@fas.rst.prstc_samples_per_second': 83.385, 'train@fas.rst.prstc_steps_per_second': 2.624, 'epoch': 8.0}
{'loss': 2.0217, 'learning_rate': 6.666666666666667e-06, 'epoch': 8.0}
{'eval_loss': 1.87775456905365, 'eval_accuracy@fas.rst.prstc': 0.4228456913827655, 'eval_f1@fas.rst.prstc': 0.12853269040249607, 'eval_precision@fas.rst.prstc': 0.15008622640975583, 'eval_recall@fas.rst.prstc': 0.15000332818165865, 'eval_loss@fas.rst.prstc': 1.8777544498443604, 'eval_runtime': 6.3234, 'eval_samples_per_second': 78.914, 'eval_steps_per_second': 2.53, 'epoch': 8.0}
{'train@fas.rst.prstc_loss': 1.9420496225357056, 'train@fas.rst.prstc_accuracy@fas.rst.prstc': 0.41341463414634144, 'train@fas.rst.prstc_f1@fas.rst.prstc': 0.12281198828610768, 'train@fas.rst.prstc_precision@fas.rst.prstc': 0.14891374723990444, 'train@fas.rst.prstc_recall@fas.rst.prstc': 0.14043960521144197, 'train@fas.rst.prstc_loss@fas.rst.prstc': 1.9420497417449951, 'train@fas.rst.prstc_runtime': 49.2678, 'train@fas.rst.prstc_samples_per_second': 83.219, 'train@fas.rst.prstc_steps_per_second': 2.618, 'epoch': 9.0}
{'loss': 2.0, 'learning_rate': 5e-06, 'epoch': 9.0}
{'eval_loss': 1.8764643669128418, 'eval_accuracy@fas.rst.prstc': 0.42084168336673344, 'eval_f1@fas.rst.prstc': 0.127541678590752, 'eval_precision@fas.rst.prstc': 0.1388929021901158, 'eval_recall@fas.rst.prstc': 0.14790691972932118, 'eval_loss@fas.rst.prstc': 1.8764642477035522, 'eval_runtime': 6.3253, 'eval_samples_per_second': 78.89, 'eval_steps_per_second': 2.53, 'epoch': 9.0}
{'train@fas.rst.prstc_loss': 1.9318796396255493, 'train@fas.rst.prstc_accuracy@fas.rst.prstc': 0.41585365853658535, 'train@fas.rst.prstc_f1@fas.rst.prstc': 0.12735438730260204, 'train@fas.rst.prstc_precision@fas.rst.prstc': 0.15198490182691482, 'train@fas.rst.prstc_recall@fas.rst.prstc': 0.14383899066982686, 'train@fas.rst.prstc_loss@fas.rst.prstc': 1.9318797588348389, 'train@fas.rst.prstc_runtime': 49.1734, 'train@fas.rst.prstc_samples_per_second': 83.378, 'train@fas.rst.prstc_steps_per_second': 2.623, 'epoch': 10.0}
{'loss': 1.9921, 'learning_rate': 3.3333333333333333e-06, 'epoch': 10.0}
{'eval_loss': 1.8626855611801147, 'eval_accuracy@fas.rst.prstc': 0.43286573146292584, 'eval_f1@fas.rst.prstc': 0.1385089913234935, 'eval_precision@fas.rst.prstc': 0.15493826260552798, 'eval_recall@fas.rst.prstc': 0.15615512447318855, 'eval_loss@fas.rst.prstc': 1.8626857995986938, 'eval_runtime': 10.6393, 'eval_samples_per_second': 46.901, 'eval_steps_per_second': 1.504, 'epoch': 10.0}
{'train@fas.rst.prstc_loss': 1.9260891675949097, 'train@fas.rst.prstc_accuracy@fas.rst.prstc': 0.41780487804878047, 'train@fas.rst.prstc_f1@fas.rst.prstc': 0.12942265894299787, 'train@fas.rst.prstc_precision@fas.rst.prstc': 0.15199479583743578, 'train@fas.rst.prstc_recall@fas.rst.prstc': 0.14544268122601234, 'train@fas.rst.prstc_loss@fas.rst.prstc': 1.9260890483856201, 'train@fas.rst.prstc_runtime': 49.2402, 'train@fas.rst.prstc_samples_per_second': 83.265, 'train@fas.rst.prstc_steps_per_second': 2.62, 'epoch': 11.0}
{'loss': 1.9946, 'learning_rate': 1.6666666666666667e-06, 'epoch': 11.0}
{'eval_loss': 1.8579661846160889, 'eval_accuracy@fas.rst.prstc': 0.4288577154308617, 'eval_f1@fas.rst.prstc': 0.13963186611355977, 'eval_precision@fas.rst.prstc': 0.15277204658152074, 'eval_recall@fas.rst.prstc': 0.15636763206274532, 'eval_loss@fas.rst.prstc': 1.8579663038253784, 'eval_runtime': 6.3181, 'eval_samples_per_second': 78.979, 'eval_steps_per_second': 2.532, 'epoch': 11.0}
{'train@fas.rst.prstc_loss': 1.9257380962371826, 'train@fas.rst.prstc_accuracy@fas.rst.prstc': 0.4192682926829268, 'train@fas.rst.prstc_f1@fas.rst.prstc': 0.1308773163326243, 'train@fas.rst.prstc_precision@fas.rst.prstc': 0.15547943303725284, 'train@fas.rst.prstc_recall@fas.rst.prstc': 0.14627588946516912, 'train@fas.rst.prstc_loss@fas.rst.prstc': 1.9257384538650513, 'train@fas.rst.prstc_runtime': 49.2384, 'train@fas.rst.prstc_samples_per_second': 83.268, 'train@fas.rst.prstc_steps_per_second': 2.62, 'epoch': 12.0}
{'loss': 1.9719, 'learning_rate': 0.0, 'epoch': 12.0}
{'eval_loss': 1.859387755393982, 'eval_accuracy@fas.rst.prstc': 0.4288577154308617, 'eval_f1@fas.rst.prstc': 0.13611437539390234, 'eval_precision@fas.rst.prstc': 0.1487756196174046, 'eval_recall@fas.rst.prstc': 0.15398266012559425, 'eval_loss@fas.rst.prstc': 1.8593876361846924, 'eval_runtime': 7.5014, 'eval_samples_per_second': 66.521, 'eval_steps_per_second': 2.133, 'epoch': 12.0}
{'train_runtime': 1907.4971, 'train_samples_per_second': 25.793, 'train_steps_per_second': 0.812, 'train_loss': 2.146745913404519, 'epoch': 12.0}
***** train_logger metrics *****
  epoch                    =       12.0
  train_loss               =     1.0997
  train_runtime            = 5:32:58.84
  train_samples_per_second =      26.38
  train_steps_per_second   =      0.825
-------------------------------------------------------------------
Lang1:  eng.rst.gum    Lang2:  fas.rst.prstc
Saving run to:  runs/full_shot/FullShot=v4_eng.rst.gum_fas.rst.prstc_lrfunc=Adafactor_lr=2e-05_plm=bert-base-multilingual-cased
Running with params: BERT_MODEL=bert-base-multilingual-cased lr=2e-05
read 13897 examples
read 2149 examples
read 2091 examples
read 4100 examples
read 499 examples
read 592 examples
Total prediction labels:  32
BertAdapterModel(
  (shared_parameters): ModuleDict()
  (bert): BertModel(
    (shared_parameters): ModuleDict()
    (invertible_adapters): ModuleDict(
      (en): NICECouplingBlock(
        (F): Sequential(
          (0): Linear(in_features=384, out_features=192, bias=True)
          (1): Activation_Function_Class(
            (f): ReLU()
          )
          (2): Linear(in_features=192, out_features=384, bias=True)
        )
        (G): Sequential(
          (0): Linear(in_features=384, out_features=192, bias=True)
          (1): Activation_Function_Class(
            (f): ReLU()
          )
          (2): Linear(in_features=192, out_features=384, bias=True)
        )
      )
      (ar): NICECouplingBlock(
        (F): Sequential(
          (0): Linear(in_features=384, out_features=192, bias=True)
          (1): Activation_Function_Class(
            (f): ReLU()
          )
          (2): Linear(in_features=192, out_features=384, bias=True)
        )
        (G): Sequential(
          (0): Linear(in_features=384, out_features=192, bias=True)
          (1): Activation_Function_Class(
            (f): ReLU()
          )
          (2): Linear(in_features=192, out_features=384, bias=True)
        )
      )
    )
    (embeddings): BertEmbeddings(
      (word_embeddings): Embedding(119547, 768, padding_idx=0)
      (position_embeddings): Embedding(512, 768)
      (token_type_embeddings): Embedding(2, 768)
      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
      (dropout): Dropout(p=0.1, inplace=False)
    )
    (encoder): BertEncoder(
      (layer): ModuleList(
        (0): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (en): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (ar): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (1): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (en): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (ar): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (2): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (en): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (ar): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (3): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (en): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (ar): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (4): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (en): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (ar): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (5): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (en): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (ar): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (6): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (en): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (ar): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (7): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (en): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (ar): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (8): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (en): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (ar): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (9): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (en): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (ar): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (10): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (en): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (ar): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (11): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (en): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (ar): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
      )
    )
    (pooler): BertPooler(
      (dense): Linear(in_features=768, out_features=768, bias=True)
      (activation): Tanh()
    )
    (prefix_tuning): PrefixTuningPool(
      (prefix_tunings): ModuleDict()
    )
  )
  (heads): ModuleDict(
    (disrpt): ClassificationHead(
      (0): Dropout(p=0.1, inplace=False)
      (1): Linear(in_features=768, out_features=768, bias=True)
      (2): Activation_Function_Class(
        (f): Tanh()
      )
      (3): Dropout(p=0.1, inplace=False)
      (4): Linear(in_features=768, out_features=32, bias=True)
    )
  )
)
{'train@eng.rst.gum_loss': 2.5265605449676514, 'train@eng.rst.gum_accuracy@eng.rst.gum': 0.2406994315319853, 'train@eng.rst.gum_f1@eng.rst.gum': 0.03304299187749916, 'train@eng.rst.gum_precision@eng.rst.gum': 0.03898126217682319, 'train@eng.rst.gum_recall@eng.rst.gum': 0.05389192149883092, 'train@eng.rst.gum_loss@eng.rst.gum': 2.5265605449676514, 'train@eng.rst.gum_runtime': 166.049, 'train@eng.rst.gum_samples_per_second': 83.692, 'train@eng.rst.gum_steps_per_second': 2.62, 'epoch': 1.0}
{'loss': 2.7838, 'learning_rate': 1.8333333333333333e-05, 'epoch': 1.0}
{'eval_loss': 2.6047070026397705, 'eval_accuracy@eng.rst.gum': 0.24057701256398326, 'eval_f1@eng.rst.gum': 0.0361721879455163, 'eval_precision@eng.rst.gum': 0.03657808977139079, 'eval_recall@eng.rst.gum': 0.05826616337272871, 'eval_loss@eng.rst.gum': 2.6047070026397705, 'eval_runtime': 25.9518, 'eval_samples_per_second': 82.807, 'eval_steps_per_second': 2.62, 'epoch': 1.0}
{'train@eng.rst.gum_loss': 2.103957176208496, 'train@eng.rst.gum_accuracy@eng.rst.gum': 0.37151903288479526, 'train@eng.rst.gum_f1@eng.rst.gum': 0.13186086707382105, 'train@eng.rst.gum_precision@eng.rst.gum': 0.24910240087630833, 'train@eng.rst.gum_recall@eng.rst.gum': 0.14757011021944078, 'train@eng.rst.gum_loss@eng.rst.gum': 2.103957176208496, 'train@eng.rst.gum_runtime': 166.0773, 'train@eng.rst.gum_samples_per_second': 83.678, 'train@eng.rst.gum_steps_per_second': 2.619, 'epoch': 2.0}
{'loss': 2.3802, 'learning_rate': 1.6666666666666667e-05, 'epoch': 2.0}
{'eval_loss': 2.2051191329956055, 'eval_accuracy@eng.rst.gum': 0.35272219637040486, 'eval_f1@eng.rst.gum': 0.1245305197580536, 'eval_precision@eng.rst.gum': 0.22018062642452177, 'eval_recall@eng.rst.gum': 0.1485468895535144, 'eval_loss@eng.rst.gum': 2.2051191329956055, 'eval_runtime': 25.992, 'eval_samples_per_second': 82.679, 'eval_steps_per_second': 2.616, 'epoch': 2.0}
{'train@eng.rst.gum_loss': 1.8117761611938477, 'train@eng.rst.gum_accuracy@eng.rst.gum': 0.4798157875800532, 'train@eng.rst.gum_f1@eng.rst.gum': 0.27599698444978715, 'train@eng.rst.gum_precision@eng.rst.gum': 0.37787801513602476, 'train@eng.rst.gum_recall@eng.rst.gum': 0.28366016652078757, 'train@eng.rst.gum_loss@eng.rst.gum': 1.811776041984558, 'train@eng.rst.gum_runtime': 166.043, 'train@eng.rst.gum_samples_per_second': 83.695, 'train@eng.rst.gum_steps_per_second': 2.62, 'epoch': 3.0}
{'loss': 2.0215, 'learning_rate': 1.5000000000000002e-05, 'epoch': 3.0}
{'eval_loss': 1.9371323585510254, 'eval_accuracy@eng.rst.gum': 0.4527687296416938, 'eval_f1@eng.rst.gum': 0.26827261517103335, 'eval_precision@eng.rst.gum': 0.3277742698242102, 'eval_recall@eng.rst.gum': 0.2791700067357872, 'eval_loss@eng.rst.gum': 1.9371323585510254, 'eval_runtime': 25.9577, 'eval_samples_per_second': 82.789, 'eval_steps_per_second': 2.62, 'epoch': 3.0}
{'train@eng.rst.gum_loss': 1.6758887767791748, 'train@eng.rst.gum_accuracy@eng.rst.gum': 0.509102684032525, 'train@eng.rst.gum_f1@eng.rst.gum': 0.31262387533067904, 'train@eng.rst.gum_precision@eng.rst.gum': 0.40203345077109676, 'train@eng.rst.gum_recall@eng.rst.gum': 0.320581558474239, 'train@eng.rst.gum_loss@eng.rst.gum': 1.6758887767791748, 'train@eng.rst.gum_runtime': 166.0418, 'train@eng.rst.gum_samples_per_second': 83.696, 'train@eng.rst.gum_steps_per_second': 2.62, 'epoch': 4.0}
{'loss': 1.8173, 'learning_rate': 1.3333333333333333e-05, 'epoch': 4.0}
{'eval_loss': 1.830041766166687, 'eval_accuracy@eng.rst.gum': 0.47417403443462075, 'eval_f1@eng.rst.gum': 0.2968989195523315, 'eval_precision@eng.rst.gum': 0.33642254512035125, 'eval_recall@eng.rst.gum': 0.30828769591437377, 'eval_loss@eng.rst.gum': 1.8300416469573975, 'eval_runtime': 25.9595, 'eval_samples_per_second': 82.783, 'eval_steps_per_second': 2.619, 'epoch': 4.0}
{'train@eng.rst.gum_loss': 1.5961453914642334, 'train@eng.rst.gum_accuracy@eng.rst.gum': 0.5247175649420738, 'train@eng.rst.gum_f1@eng.rst.gum': 0.3458118510340821, 'train@eng.rst.gum_precision@eng.rst.gum': 0.4880108125603171, 'train@eng.rst.gum_recall@eng.rst.gum': 0.3543497516388366, 'train@eng.rst.gum_loss@eng.rst.gum': 1.596145510673523, 'train@eng.rst.gum_runtime': 166.088, 'train@eng.rst.gum_samples_per_second': 83.673, 'train@eng.rst.gum_steps_per_second': 2.619, 'epoch': 5.0}
{'loss': 1.7083, 'learning_rate': 1.1666666666666668e-05, 'epoch': 5.0}
{'eval_loss': 1.7675174474716187, 'eval_accuracy@eng.rst.gum': 0.4872033503955328, 'eval_f1@eng.rst.gum': 0.3196746717536057, 'eval_precision@eng.rst.gum': 0.33629534476228234, 'eval_recall@eng.rst.gum': 0.3392032340683054, 'eval_loss@eng.rst.gum': 1.767517328262329, 'eval_runtime': 26.012, 'eval_samples_per_second': 82.616, 'eval_steps_per_second': 2.614, 'epoch': 5.0}
{'train@eng.rst.gum_loss': 1.5424695014953613, 'train@eng.rst.gum_accuracy@eng.rst.gum': 0.5409081096639563, 'train@eng.rst.gum_f1@eng.rst.gum': 0.38692433286633293, 'train@eng.rst.gum_precision@eng.rst.gum': 0.5456446469806584, 'train@eng.rst.gum_recall@eng.rst.gum': 0.3868489105953741, 'train@eng.rst.gum_loss@eng.rst.gum': 1.5424697399139404, 'train@eng.rst.gum_runtime': 167.082, 'train@eng.rst.gum_samples_per_second': 83.175, 'train@eng.rst.gum_steps_per_second': 2.604, 'epoch': 6.0}
{'loss': 1.6454, 'learning_rate': 1e-05, 'epoch': 6.0}
{'eval_loss': 1.7277135848999023, 'eval_accuracy@eng.rst.gum': 0.498371335504886, 'eval_f1@eng.rst.gum': 0.3587437619199491, 'eval_precision@eng.rst.gum': 0.4145482310719665, 'eval_recall@eng.rst.gum': 0.36968645195736255, 'eval_loss@eng.rst.gum': 1.7277135848999023, 'eval_runtime': 25.914, 'eval_samples_per_second': 82.928, 'eval_steps_per_second': 2.624, 'epoch': 6.0}
{'train@eng.rst.gum_loss': 1.5059503316879272, 'train@eng.rst.gum_accuracy@eng.rst.gum': 0.5478880333885011, 'train@eng.rst.gum_f1@eng.rst.gum': 0.3966110388145773, 'train@eng.rst.gum_precision@eng.rst.gum': 0.5529842251797917, 'train@eng.rst.gum_recall@eng.rst.gum': 0.3990365113347597, 'train@eng.rst.gum_loss@eng.rst.gum': 1.5059504508972168, 'train@eng.rst.gum_runtime': 166.0272, 'train@eng.rst.gum_samples_per_second': 83.703, 'train@eng.rst.gum_steps_per_second': 2.62, 'epoch': 7.0}
{'loss': 1.6028, 'learning_rate': 8.333333333333334e-06, 'epoch': 7.0}
{'eval_loss': 1.7058141231536865, 'eval_accuracy@eng.rst.gum': 0.5002326663564448, 'eval_f1@eng.rst.gum': 0.3694804170225878, 'eval_precision@eng.rst.gum': 0.45446581010517384, 'eval_recall@eng.rst.gum': 0.3814680418143076, 'eval_loss@eng.rst.gum': 1.705814003944397, 'eval_runtime': 25.9361, 'eval_samples_per_second': 82.857, 'eval_steps_per_second': 2.622, 'epoch': 7.0}
{'train@eng.rst.gum_loss': 1.4810850620269775, 'train@eng.rst.gum_accuracy@eng.rst.gum': 0.5545801252068792, 'train@eng.rst.gum_f1@eng.rst.gum': 0.40673966203769163, 'train@eng.rst.gum_precision@eng.rst.gum': 0.5440260242526508, 'train@eng.rst.gum_recall@eng.rst.gum': 0.4069967908140316, 'train@eng.rst.gum_loss@eng.rst.gum': 1.481084942817688, 'train@eng.rst.gum_runtime': 166.0705, 'train@eng.rst.gum_samples_per_second': 83.681, 'train@eng.rst.gum_steps_per_second': 2.619, 'epoch': 8.0}
{'loss': 1.5673, 'learning_rate': 6.666666666666667e-06, 'epoch': 8.0}
{'eval_loss': 1.6907317638397217, 'eval_accuracy@eng.rst.gum': 0.5109353187529083, 'eval_f1@eng.rst.gum': 0.38032397371014154, 'eval_precision@eng.rst.gum': 0.47076719247539756, 'eval_recall@eng.rst.gum': 0.3935333171626407, 'eval_loss@eng.rst.gum': 1.6907317638397217, 'eval_runtime': 25.9559, 'eval_samples_per_second': 82.794, 'eval_steps_per_second': 2.62, 'epoch': 8.0}
{'train@eng.rst.gum_loss': 1.458627700805664, 'train@eng.rst.gum_accuracy@eng.rst.gum': 0.5625674606030079, 'train@eng.rst.gum_f1@eng.rst.gum': 0.42501145604708024, 'train@eng.rst.gum_precision@eng.rst.gum': 0.5365449711373241, 'train@eng.rst.gum_recall@eng.rst.gum': 0.42347024955448204, 'train@eng.rst.gum_loss@eng.rst.gum': 1.4586278200149536, 'train@eng.rst.gum_runtime': 166.1498, 'train@eng.rst.gum_samples_per_second': 83.641, 'train@eng.rst.gum_steps_per_second': 2.618, 'epoch': 9.0}
{'loss': 1.5434, 'learning_rate': 5e-06, 'epoch': 9.0}
{'eval_loss': 1.6707037687301636, 'eval_accuracy@eng.rst.gum': 0.5062819916240112, 'eval_f1@eng.rst.gum': 0.3816214553072675, 'eval_precision@eng.rst.gum': 0.4635997018588221, 'eval_recall@eng.rst.gum': 0.39607157022177936, 'eval_loss@eng.rst.gum': 1.6707037687301636, 'eval_runtime': 25.9808, 'eval_samples_per_second': 82.715, 'eval_steps_per_second': 2.617, 'epoch': 9.0}
{'train@eng.rst.gum_loss': 1.4447203874588013, 'train@eng.rst.gum_accuracy@eng.rst.gum': 0.5666690652658847, 'train@eng.rst.gum_f1@eng.rst.gum': 0.4316999729641751, 'train@eng.rst.gum_precision@eng.rst.gum': 0.5490303724048811, 'train@eng.rst.gum_recall@eng.rst.gum': 0.425488212138821, 'train@eng.rst.gum_loss@eng.rst.gum': 1.4447205066680908, 'train@eng.rst.gum_runtime': 166.0197, 'train@eng.rst.gum_samples_per_second': 83.707, 'train@eng.rst.gum_steps_per_second': 2.62, 'epoch': 10.0}
{'loss': 1.529, 'learning_rate': 3.3333333333333333e-06, 'epoch': 10.0}
{'eval_loss': 1.6622886657714844, 'eval_accuracy@eng.rst.gum': 0.5100046533271289, 'eval_f1@eng.rst.gum': 0.38328098519571757, 'eval_precision@eng.rst.gum': 0.46110151164892477, 'eval_recall@eng.rst.gum': 0.3947728563553041, 'eval_loss@eng.rst.gum': 1.662288784980774, 'eval_runtime': 25.9532, 'eval_samples_per_second': 82.803, 'eval_steps_per_second': 2.62, 'epoch': 10.0}
{'train@eng.rst.gum_loss': 1.4373232126235962, 'train@eng.rst.gum_accuracy@eng.rst.gum': 0.5681801827732604, 'train@eng.rst.gum_f1@eng.rst.gum': 0.43692749389869806, 'train@eng.rst.gum_precision@eng.rst.gum': 0.5390463584638, 'train@eng.rst.gum_recall@eng.rst.gum': 0.4329379352384491, 'train@eng.rst.gum_loss@eng.rst.gum': 1.4373230934143066, 'train@eng.rst.gum_runtime': 166.2085, 'train@eng.rst.gum_samples_per_second': 83.612, 'train@eng.rst.gum_steps_per_second': 2.617, 'epoch': 11.0}
{'loss': 1.5197, 'learning_rate': 1.6666666666666667e-06, 'epoch': 11.0}
{'eval_loss': 1.6570141315460205, 'eval_accuracy@eng.rst.gum': 0.5095393206142392, 'eval_f1@eng.rst.gum': 0.38574416284574026, 'eval_precision@eng.rst.gum': 0.44660909265787674, 'eval_recall@eng.rst.gum': 0.3993887348391206, 'eval_loss@eng.rst.gum': 1.6570141315460205, 'eval_runtime': 25.9747, 'eval_samples_per_second': 82.734, 'eval_steps_per_second': 2.618, 'epoch': 11.0}
{'train@eng.rst.gum_loss': 1.4344868659973145, 'train@eng.rst.gum_accuracy@eng.rst.gum': 0.5689717205152192, 'train@eng.rst.gum_f1@eng.rst.gum': 0.4368157175420472, 'train@eng.rst.gum_precision@eng.rst.gum': 0.5418001255849255, 'train@eng.rst.gum_recall@eng.rst.gum': 0.4319074855773016, 'train@eng.rst.gum_loss@eng.rst.gum': 1.434486985206604, 'train@eng.rst.gum_runtime': 166.0611, 'train@eng.rst.gum_samples_per_second': 83.686, 'train@eng.rst.gum_steps_per_second': 2.62, 'epoch': 12.0}
{'loss': 1.5072, 'learning_rate': 0.0, 'epoch': 12.0}
{'eval_loss': 1.6559114456176758, 'eval_accuracy@eng.rst.gum': 0.5118659841786878, 'eval_f1@eng.rst.gum': 0.38723528515227396, 'eval_precision@eng.rst.gum': 0.44615743811882963, 'eval_recall@eng.rst.gum': 0.4000433150709622, 'eval_loss@eng.rst.gum': 1.6559112071990967, 'eval_runtime': 25.9611, 'eval_samples_per_second': 82.778, 'eval_steps_per_second': 2.619, 'epoch': 12.0}
{'train_runtime': 6486.4249, 'train_samples_per_second': 25.71, 'train_steps_per_second': 0.805, 'train_loss': 1.8021618488647928, 'epoch': 12.0}
***** train_log metrics *****
  epoch                    =       12.0
  train_loss               =     1.8022
  train_runtime            = 1:48:06.42
  train_samples_per_second =      25.71
  train_steps_per_second   =      0.805
{'train@fas.rst.prstc_loss': 2.357128381729126, 'train@fas.rst.prstc_accuracy@fas.rst.prstc': 0.24390243902439024, 'train@fas.rst.prstc_f1@fas.rst.prstc': 0.03152391507828717, 'train@fas.rst.prstc_precision@fas.rst.prstc': 0.028663344745787953, 'train@fas.rst.prstc_recall@fas.rst.prstc': 0.06116898259576608, 'train@fas.rst.prstc_loss@fas.rst.prstc': 2.357128381729126, 'train@fas.rst.prstc_runtime': 49.1415, 'train@fas.rst.prstc_samples_per_second': 83.432, 'train@fas.rst.prstc_steps_per_second': 2.625, 'epoch': 1.0}
{'loss': 2.6212, 'learning_rate': 1.8333333333333333e-05, 'epoch': 1.0}
{'eval_loss': 2.281423330307007, 'eval_accuracy@fas.rst.prstc': 0.250501002004008, 'eval_f1@fas.rst.prstc': 0.03753676359115855, 'eval_precision@fas.rst.prstc': 0.032910115400703724, 'eval_recall@fas.rst.prstc': 0.06880494179139938, 'eval_loss@fas.rst.prstc': 2.281423330307007, 'eval_runtime': 6.2593, 'eval_samples_per_second': 79.721, 'eval_steps_per_second': 2.556, 'epoch': 1.0}
{'train@fas.rst.prstc_loss': 2.295412063598633, 'train@fas.rst.prstc_accuracy@fas.rst.prstc': 0.27951219512195125, 'train@fas.rst.prstc_f1@fas.rst.prstc': 0.04523636059394526, 'train@fas.rst.prstc_precision@fas.rst.prstc': 0.03269854557166873, 'train@fas.rst.prstc_recall@fas.rst.prstc': 0.07351924948921194, 'train@fas.rst.prstc_loss@fas.rst.prstc': 2.2954118251800537, 'train@fas.rst.prstc_runtime': 49.1472, 'train@fas.rst.prstc_samples_per_second': 83.423, 'train@fas.rst.prstc_steps_per_second': 2.625, 'epoch': 2.0}
{'loss': 2.3609, 'learning_rate': 1.6666666666666667e-05, 'epoch': 2.0}
{'eval_loss': 2.2174227237701416, 'eval_accuracy@fas.rst.prstc': 0.2745490981963928, 'eval_f1@fas.rst.prstc': 0.049538442597722374, 'eval_precision@fas.rst.prstc': 0.03655001767408979, 'eval_recall@fas.rst.prstc': 0.07685911142789262, 'eval_loss@fas.rst.prstc': 2.2174224853515625, 'eval_runtime': 6.2681, 'eval_samples_per_second': 79.61, 'eval_steps_per_second': 2.553, 'epoch': 2.0}
{'train@fas.rst.prstc_loss': 2.226560115814209, 'train@fas.rst.prstc_accuracy@fas.rst.prstc': 0.2897560975609756, 'train@fas.rst.prstc_f1@fas.rst.prstc': 0.048043579892119215, 'train@fas.rst.prstc_precision@fas.rst.prstc': 0.07677297604504459, 'train@fas.rst.prstc_recall@fas.rst.prstc': 0.07555690422160251, 'train@fas.rst.prstc_loss@fas.rst.prstc': 2.226560354232788, 'train@fas.rst.prstc_runtime': 49.1723, 'train@fas.rst.prstc_samples_per_second': 83.38, 'train@fas.rst.prstc_steps_per_second': 2.623, 'epoch': 3.0}
{'loss': 2.2999, 'learning_rate': 1.5000000000000002e-05, 'epoch': 3.0}
{'eval_loss': 2.150387763977051, 'eval_accuracy@fas.rst.prstc': 0.30060120240480964, 'eval_f1@fas.rst.prstc': 0.05189874507979983, 'eval_precision@fas.rst.prstc': 0.042027796764638865, 'eval_recall@fas.rst.prstc': 0.08326443335709195, 'eval_loss@fas.rst.prstc': 2.1503875255584717, 'eval_runtime': 6.2483, 'eval_samples_per_second': 79.862, 'eval_steps_per_second': 2.561, 'epoch': 3.0}
{'train@fas.rst.prstc_loss': 2.112097978591919, 'train@fas.rst.prstc_accuracy@fas.rst.prstc': 0.35560975609756096, 'train@fas.rst.prstc_f1@fas.rst.prstc': 0.08479129732092705, 'train@fas.rst.prstc_precision@fas.rst.prstc': 0.1166061025566567, 'train@fas.rst.prstc_recall@fas.rst.prstc': 0.11100061360884628, 'train@fas.rst.prstc_loss@fas.rst.prstc': 2.112097978591919, 'train@fas.rst.prstc_runtime': 49.1479, 'train@fas.rst.prstc_samples_per_second': 83.422, 'train@fas.rst.prstc_steps_per_second': 2.625, 'epoch': 4.0}
{'loss': 2.2225, 'learning_rate': 1.3333333333333333e-05, 'epoch': 4.0}
{'eval_loss': 2.040540933609009, 'eval_accuracy@fas.rst.prstc': 0.33867735470941884, 'eval_f1@fas.rst.prstc': 0.08344138952834605, 'eval_precision@fas.rst.prstc': 0.07604529995834343, 'eval_recall@fas.rst.prstc': 0.10968276000050017, 'eval_loss@fas.rst.prstc': 2.040541172027588, 'eval_runtime': 6.2661, 'eval_samples_per_second': 79.635, 'eval_steps_per_second': 2.553, 'epoch': 4.0}
{'train@fas.rst.prstc_loss': 2.0108859539031982, 'train@fas.rst.prstc_accuracy@fas.rst.prstc': 0.3985365853658537, 'train@fas.rst.prstc_f1@fas.rst.prstc': 0.12649824666818874, 'train@fas.rst.prstc_precision@fas.rst.prstc': 0.16209420951855177, 'train@fas.rst.prstc_recall@fas.rst.prstc': 0.14051910751189062, 'train@fas.rst.prstc_loss@fas.rst.prstc': 2.01088547706604, 'train@fas.rst.prstc_runtime': 49.2272, 'train@fas.rst.prstc_samples_per_second': 83.287, 'train@fas.rst.prstc_steps_per_second': 2.621, 'epoch': 5.0}
{'loss': 2.1294, 'learning_rate': 1.1666666666666668e-05, 'epoch': 5.0}
{'eval_loss': 1.9386118650436401, 'eval_accuracy@fas.rst.prstc': 0.3847695390781563, 'eval_f1@fas.rst.prstc': 0.1271451355661882, 'eval_precision@fas.rst.prstc': 0.2242942627335691, 'eval_recall@fas.rst.prstc': 0.13817609364163738, 'eval_loss@fas.rst.prstc': 1.9386117458343506, 'eval_runtime': 6.2739, 'eval_samples_per_second': 79.536, 'eval_steps_per_second': 2.55, 'epoch': 5.0}
{'train@fas.rst.prstc_loss': 1.9360432624816895, 'train@fas.rst.prstc_accuracy@fas.rst.prstc': 0.4192682926829268, 'train@fas.rst.prstc_f1@fas.rst.prstc': 0.1519272929960465, 'train@fas.rst.prstc_precision@fas.rst.prstc': 0.16802902142150652, 'train@fas.rst.prstc_recall@fas.rst.prstc': 0.168433006921732, 'train@fas.rst.prstc_loss@fas.rst.prstc': 1.9360432624816895, 'train@fas.rst.prstc_runtime': 49.1283, 'train@fas.rst.prstc_samples_per_second': 83.455, 'train@fas.rst.prstc_steps_per_second': 2.626, 'epoch': 6.0}
{'loss': 2.0402, 'learning_rate': 1e-05, 'epoch': 6.0}
{'eval_loss': 1.8650519847869873, 'eval_accuracy@fas.rst.prstc': 0.4188376753507014, 'eval_f1@fas.rst.prstc': 0.16777269277990126, 'eval_precision@fas.rst.prstc': 0.20623224728487885, 'eval_recall@fas.rst.prstc': 0.17770720732419348, 'eval_loss@fas.rst.prstc': 1.8650518655776978, 'eval_runtime': 6.2595, 'eval_samples_per_second': 79.719, 'eval_steps_per_second': 2.556, 'epoch': 6.0}
{'train@fas.rst.prstc_loss': 1.897147297859192, 'train@fas.rst.prstc_accuracy@fas.rst.prstc': 0.43, 'train@fas.rst.prstc_f1@fas.rst.prstc': 0.167756285907979, 'train@fas.rst.prstc_precision@fas.rst.prstc': 0.1937200077430785, 'train@fas.rst.prstc_recall@fas.rst.prstc': 0.18428324935462836, 'train@fas.rst.prstc_loss@fas.rst.prstc': 1.8971474170684814, 'train@fas.rst.prstc_runtime': 49.1159, 'train@fas.rst.prstc_samples_per_second': 83.476, 'train@fas.rst.prstc_steps_per_second': 2.626, 'epoch': 7.0}
{'loss': 1.9958, 'learning_rate': 8.333333333333334e-06, 'epoch': 7.0}
{'eval_loss': 1.821123480796814, 'eval_accuracy@fas.rst.prstc': 0.43887775551102204, 'eval_f1@fas.rst.prstc': 0.17686809303354376, 'eval_precision@fas.rst.prstc': 0.18326942589594744, 'eval_recall@fas.rst.prstc': 0.19272800091956413, 'eval_loss@fas.rst.prstc': 1.821123480796814, 'eval_runtime': 6.2608, 'eval_samples_per_second': 79.702, 'eval_steps_per_second': 2.556, 'epoch': 7.0}
{'train@fas.rst.prstc_loss': 1.8698067665100098, 'train@fas.rst.prstc_accuracy@fas.rst.prstc': 0.43609756097560975, 'train@fas.rst.prstc_f1@fas.rst.prstc': 0.1845103167505414, 'train@fas.rst.prstc_precision@fas.rst.prstc': 0.24629249671977838, 'train@fas.rst.prstc_recall@fas.rst.prstc': 0.19735950195967203, 'train@fas.rst.prstc_loss@fas.rst.prstc': 1.8698067665100098, 'train@fas.rst.prstc_runtime': 49.1559, 'train@fas.rst.prstc_samples_per_second': 83.408, 'train@fas.rst.prstc_steps_per_second': 2.624, 'epoch': 8.0}
{'loss': 1.9526, 'learning_rate': 6.666666666666667e-06, 'epoch': 8.0}
{'eval_loss': 1.7953433990478516, 'eval_accuracy@fas.rst.prstc': 0.44889779559118237, 'eval_f1@fas.rst.prstc': 0.18273579633720452, 'eval_precision@fas.rst.prstc': 0.19228549872899717, 'eval_recall@fas.rst.prstc': 0.20180509612084807, 'eval_loss@fas.rst.prstc': 1.795343279838562, 'eval_runtime': 6.2723, 'eval_samples_per_second': 79.556, 'eval_steps_per_second': 2.551, 'epoch': 8.0}
{'train@fas.rst.prstc_loss': 1.853188157081604, 'train@fas.rst.prstc_accuracy@fas.rst.prstc': 0.4404878048780488, 'train@fas.rst.prstc_f1@fas.rst.prstc': 0.19140138596877224, 'train@fas.rst.prstc_precision@fas.rst.prstc': 0.24073317855270623, 'train@fas.rst.prstc_recall@fas.rst.prstc': 0.20536275264712073, 'train@fas.rst.prstc_loss@fas.rst.prstc': 1.853188157081604, 'train@fas.rst.prstc_runtime': 49.1841, 'train@fas.rst.prstc_samples_per_second': 83.36, 'train@fas.rst.prstc_steps_per_second': 2.623, 'epoch': 9.0}
{'loss': 1.9245, 'learning_rate': 5e-06, 'epoch': 9.0}
{'eval_loss': 1.7823456525802612, 'eval_accuracy@fas.rst.prstc': 0.45691382765531063, 'eval_f1@fas.rst.prstc': 0.21275186415971187, 'eval_precision@fas.rst.prstc': 0.21994789711531998, 'eval_recall@fas.rst.prstc': 0.2279628411239245, 'eval_loss@fas.rst.prstc': 1.7823457717895508, 'eval_runtime': 6.2648, 'eval_samples_per_second': 79.651, 'eval_steps_per_second': 2.554, 'epoch': 9.0}
{'train@fas.rst.prstc_loss': 1.83793306350708, 'train@fas.rst.prstc_accuracy@fas.rst.prstc': 0.4473170731707317, 'train@fas.rst.prstc_f1@fas.rst.prstc': 0.2026922039266012, 'train@fas.rst.prstc_precision@fas.rst.prstc': 0.2460148892910496, 'train@fas.rst.prstc_recall@fas.rst.prstc': 0.21573343981144855, 'train@fas.rst.prstc_loss@fas.rst.prstc': 1.83793306350708, 'train@fas.rst.prstc_runtime': 49.1981, 'train@fas.rst.prstc_samples_per_second': 83.337, 'train@fas.rst.prstc_steps_per_second': 2.622, 'epoch': 10.0}
{'loss': 1.9196, 'learning_rate': 3.3333333333333333e-06, 'epoch': 10.0}
{'eval_loss': 1.7653064727783203, 'eval_accuracy@fas.rst.prstc': 0.46292585170340683, 'eval_f1@fas.rst.prstc': 0.21764365387575865, 'eval_precision@fas.rst.prstc': 0.2240412243662163, 'eval_recall@fas.rst.prstc': 0.23344247065202162, 'eval_loss@fas.rst.prstc': 1.7653063535690308, 'eval_runtime': 6.2617, 'eval_samples_per_second': 79.691, 'eval_steps_per_second': 2.555, 'epoch': 10.0}
{'train@fas.rst.prstc_loss': 1.8313055038452148, 'train@fas.rst.prstc_accuracy@fas.rst.prstc': 0.44926829268292684, 'train@fas.rst.prstc_f1@fas.rst.prstc': 0.2061052421835909, 'train@fas.rst.prstc_precision@fas.rst.prstc': 0.25630547176808594, 'train@fas.rst.prstc_recall@fas.rst.prstc': 0.2194243328402256, 'train@fas.rst.prstc_loss@fas.rst.prstc': 1.8313055038452148, 'train@fas.rst.prstc_runtime': 49.2178, 'train@fas.rst.prstc_samples_per_second': 83.303, 'train@fas.rst.prstc_steps_per_second': 2.621, 'epoch': 11.0}
{'loss': 1.9109, 'learning_rate': 1.6666666666666667e-06, 'epoch': 11.0}
{'eval_loss': 1.7576656341552734, 'eval_accuracy@fas.rst.prstc': 0.46292585170340683, 'eval_f1@fas.rst.prstc': 0.21724096575560903, 'eval_precision@fas.rst.prstc': 0.2208898589956898, 'eval_recall@fas.rst.prstc': 0.23347573270951744, 'eval_loss@fas.rst.prstc': 1.7576656341552734, 'eval_runtime': 6.2718, 'eval_samples_per_second': 79.562, 'eval_steps_per_second': 2.551, 'epoch': 11.0}
{'train@fas.rst.prstc_loss': 1.8300150632858276, 'train@fas.rst.prstc_accuracy@fas.rst.prstc': 0.4482926829268293, 'train@fas.rst.prstc_f1@fas.rst.prstc': 0.2037514964482774, 'train@fas.rst.prstc_precision@fas.rst.prstc': 0.25989530690309753, 'train@fas.rst.prstc_recall@fas.rst.prstc': 0.21793525651043938, 'train@fas.rst.prstc_loss@fas.rst.prstc': 1.8300151824951172, 'train@fas.rst.prstc_runtime': 49.1091, 'train@fas.rst.prstc_samples_per_second': 83.488, 'train@fas.rst.prstc_steps_per_second': 2.627, 'epoch': 12.0}
{'loss': 1.897, 'learning_rate': 0.0, 'epoch': 12.0}
{'eval_loss': 1.7588461637496948, 'eval_accuracy@fas.rst.prstc': 0.4709418837675351, 'eval_f1@fas.rst.prstc': 0.2194508116116854, 'eval_precision@fas.rst.prstc': 0.22567054438703887, 'eval_recall@fas.rst.prstc': 0.23679121662371838, 'eval_loss@fas.rst.prstc': 1.7588461637496948, 'eval_runtime': 6.2696, 'eval_samples_per_second': 79.591, 'eval_steps_per_second': 2.552, 'epoch': 12.0}
{'train_runtime': 1900.7152, 'train_samples_per_second': 25.885, 'train_steps_per_second': 0.814, 'train_loss': 2.1062173079458626, 'epoch': 12.0}
***** train_logger metrics *****
  epoch                    =       12.0
  train_loss               =     1.8022
  train_runtime            = 1:48:06.42
  train_samples_per_second =      25.71
  train_steps_per_second   =      0.805
-------------------------------------------------------------------
Lang1:  eng.rst.rstdt    Lang2:  fas.rst.prstc
Saving run to:  runs/full_shot/FullShot=v4_eng.rst.rstdt_fas.rst.prstc_lrfunc=Adafactor_lr=2e-05_plm=bert-base-multilingual-cased
Running with params: BERT_MODEL=bert-base-multilingual-cased lr=2e-05
read 16002 examples
read 1621 examples
read 2155 examples
read 4100 examples
read 499 examples
read 592 examples
Total prediction labels:  20
BertAdapterModel(
  (shared_parameters): ModuleDict()
  (bert): BertModel(
    (shared_parameters): ModuleDict()
    (invertible_adapters): ModuleDict(
      (en): NICECouplingBlock(
        (F): Sequential(
          (0): Linear(in_features=384, out_features=192, bias=True)
          (1): Activation_Function_Class(
            (f): ReLU()
          )
          (2): Linear(in_features=192, out_features=384, bias=True)
        )
        (G): Sequential(
          (0): Linear(in_features=384, out_features=192, bias=True)
          (1): Activation_Function_Class(
            (f): ReLU()
          )
          (2): Linear(in_features=192, out_features=384, bias=True)
        )
      )
      (ar): NICECouplingBlock(
        (F): Sequential(
          (0): Linear(in_features=384, out_features=192, bias=True)
          (1): Activation_Function_Class(
            (f): ReLU()
          )
          (2): Linear(in_features=192, out_features=384, bias=True)
        )
        (G): Sequential(
          (0): Linear(in_features=384, out_features=192, bias=True)
          (1): Activation_Function_Class(
            (f): ReLU()
          )
          (2): Linear(in_features=192, out_features=384, bias=True)
        )
      )
    )
    (embeddings): BertEmbeddings(
      (word_embeddings): Embedding(119547, 768, padding_idx=0)
      (position_embeddings): Embedding(512, 768)
      (token_type_embeddings): Embedding(2, 768)
      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
      (dropout): Dropout(p=0.1, inplace=False)
    )
    (encoder): BertEncoder(
      (layer): ModuleList(
        (0): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (en): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (ar): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (1): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (en): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (ar): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (2): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (en): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (ar): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (3): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (en): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (ar): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (4): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (en): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (ar): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (5): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (en): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (ar): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (6): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (en): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (ar): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (7): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (en): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (ar): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (8): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (en): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (ar): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (9): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (en): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (ar): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (10): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (en): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (ar): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (11): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (en): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (ar): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
      )
    )
    (pooler): BertPooler(
      (dense): Linear(in_features=768, out_features=768, bias=True)
      (activation): Tanh()
    )
    (prefix_tuning): PrefixTuningPool(
      (prefix_tunings): ModuleDict()
    )
  )
  (heads): ModuleDict(
    (disrpt): ClassificationHead(
      (0): Dropout(p=0.1, inplace=False)
      (1): Linear(in_features=768, out_features=768, bias=True)
      (2): Activation_Function_Class(
        (f): Tanh()
      )
      (3): Dropout(p=0.1, inplace=False)
      (4): Linear(in_features=768, out_features=20, bias=True)
    )
  )
)
{'train@eng.rst.rstdt_loss': 1.7035924196243286, 'train@eng.rst.rstdt_accuracy@eng.rst.rstdt': 0.5174978127734033, 'train@eng.rst.rstdt_f1@eng.rst.rstdt': 0.0924452003941135, 'train@eng.rst.rstdt_precision@eng.rst.rstdt': 0.08797854736992117, 'train@eng.rst.rstdt_recall@eng.rst.rstdt': 0.11567595137828393, 'train@eng.rst.rstdt_loss@eng.rst.rstdt': 1.703592300415039, 'train@eng.rst.rstdt_runtime': 191.1539, 'train@eng.rst.rstdt_samples_per_second': 83.713, 'train@eng.rst.rstdt_steps_per_second': 2.621, 'epoch': 1.0}
{'loss': 2.0616, 'learning_rate': 1.8333333333333333e-05, 'epoch': 1.0}
{'eval_loss': 1.6836344003677368, 'eval_accuracy@eng.rst.rstdt': 0.5299198025909932, 'eval_f1@eng.rst.rstdt': 0.09566518010552683, 'eval_precision@eng.rst.rstdt': 0.089678705509866, 'eval_recall@eng.rst.rstdt': 0.1164365266814832, 'eval_loss@eng.rst.rstdt': 1.6836344003677368, 'eval_runtime': 19.5951, 'eval_samples_per_second': 82.725, 'eval_steps_per_second': 2.603, 'epoch': 1.0}
{'train@eng.rst.rstdt_loss': 1.423830509185791, 'train@eng.rst.rstdt_accuracy@eng.rst.rstdt': 0.5979877515310587, 'train@eng.rst.rstdt_f1@eng.rst.rstdt': 0.19345738535759743, 'train@eng.rst.rstdt_precision@eng.rst.rstdt': 0.25510741329878367, 'train@eng.rst.rstdt_recall@eng.rst.rstdt': 0.2052798912587139, 'train@eng.rst.rstdt_loss@eng.rst.rstdt': 1.423830509185791, 'train@eng.rst.rstdt_runtime': 191.0701, 'train@eng.rst.rstdt_samples_per_second': 83.749, 'train@eng.rst.rstdt_steps_per_second': 2.622, 'epoch': 2.0}
{'loss': 1.5915, 'learning_rate': 1.6666666666666667e-05, 'epoch': 2.0}
{'eval_loss': 1.4244345426559448, 'eval_accuracy@eng.rst.rstdt': 0.6039481801357187, 'eval_f1@eng.rst.rstdt': 0.1955988958452141, 'eval_precision@eng.rst.rstdt': 0.2172373279594113, 'eval_recall@eng.rst.rstdt': 0.20733258695673643, 'eval_loss@eng.rst.rstdt': 1.4244341850280762, 'eval_runtime': 19.5709, 'eval_samples_per_second': 82.827, 'eval_steps_per_second': 2.606, 'epoch': 2.0}
{'train@eng.rst.rstdt_loss': 1.3146662712097168, 'train@eng.rst.rstdt_accuracy@eng.rst.rstdt': 0.6304211973503312, 'train@eng.rst.rstdt_f1@eng.rst.rstdt': 0.27824743307796107, 'train@eng.rst.rstdt_precision@eng.rst.rstdt': 0.4115894740878682, 'train@eng.rst.rstdt_recall@eng.rst.rstdt': 0.2667446261373867, 'train@eng.rst.rstdt_loss@eng.rst.rstdt': 1.3146662712097168, 'train@eng.rst.rstdt_runtime': 191.0496, 'train@eng.rst.rstdt_samples_per_second': 83.758, 'train@eng.rst.rstdt_steps_per_second': 2.622, 'epoch': 3.0}
{'loss': 1.425, 'learning_rate': 1.5000000000000002e-05, 'epoch': 3.0}
{'eval_loss': 1.3351061344146729, 'eval_accuracy@eng.rst.rstdt': 0.6255397902529303, 'eval_f1@eng.rst.rstdt': 0.26416045414683637, 'eval_precision@eng.rst.rstdt': 0.329924575915215, 'eval_recall@eng.rst.rstdt': 0.26061495151149555, 'eval_loss@eng.rst.rstdt': 1.3351061344146729, 'eval_runtime': 19.5708, 'eval_samples_per_second': 82.827, 'eval_steps_per_second': 2.606, 'epoch': 3.0}
{'train@eng.rst.rstdt_loss': 1.2426339387893677, 'train@eng.rst.rstdt_accuracy@eng.rst.rstdt': 0.6463567054118236, 'train@eng.rst.rstdt_f1@eng.rst.rstdt': 0.33278328010717, 'train@eng.rst.rstdt_precision@eng.rst.rstdt': 0.44768408498956086, 'train@eng.rst.rstdt_recall@eng.rst.rstdt': 0.3154190864365613, 'train@eng.rst.rstdt_loss@eng.rst.rstdt': 1.2426338195800781, 'train@eng.rst.rstdt_runtime': 191.2313, 'train@eng.rst.rstdt_samples_per_second': 83.679, 'train@eng.rst.rstdt_steps_per_second': 2.62, 'epoch': 4.0}
{'loss': 1.3306, 'learning_rate': 1.3333333333333333e-05, 'epoch': 4.0}
{'eval_loss': 1.2900437116622925, 'eval_accuracy@eng.rst.rstdt': 0.6329426280074029, 'eval_f1@eng.rst.rstdt': 0.30693417178618093, 'eval_precision@eng.rst.rstdt': 0.3640694925958024, 'eval_recall@eng.rst.rstdt': 0.30579051696142123, 'eval_loss@eng.rst.rstdt': 1.2900437116622925, 'eval_runtime': 19.5858, 'eval_samples_per_second': 82.764, 'eval_steps_per_second': 2.604, 'epoch': 4.0}
{'train@eng.rst.rstdt_loss': 1.1943224668502808, 'train@eng.rst.rstdt_accuracy@eng.rst.rstdt': 0.6574178227721534, 'train@eng.rst.rstdt_f1@eng.rst.rstdt': 0.3552272828833187, 'train@eng.rst.rstdt_precision@eng.rst.rstdt': 0.46846539485173405, 'train@eng.rst.rstdt_recall@eng.rst.rstdt': 0.332289396992293, 'train@eng.rst.rstdt_loss@eng.rst.rstdt': 1.1943224668502808, 'train@eng.rst.rstdt_runtime': 191.1671, 'train@eng.rst.rstdt_samples_per_second': 83.707, 'train@eng.rst.rstdt_steps_per_second': 2.621, 'epoch': 5.0}
{'loss': 1.2712, 'learning_rate': 1.1666666666666668e-05, 'epoch': 5.0}
{'eval_loss': 1.2518634796142578, 'eval_accuracy@eng.rst.rstdt': 0.634793337446021, 'eval_f1@eng.rst.rstdt': 0.3198326246919717, 'eval_precision@eng.rst.rstdt': 0.38998387586895306, 'eval_recall@eng.rst.rstdt': 0.31352170508952343, 'eval_loss@eng.rst.rstdt': 1.2518635988235474, 'eval_runtime': 19.6226, 'eval_samples_per_second': 82.609, 'eval_steps_per_second': 2.599, 'epoch': 5.0}
{'train@eng.rst.rstdt_loss': 1.1618155241012573, 'train@eng.rst.rstdt_accuracy@eng.rst.rstdt': 0.6624796900387452, 'train@eng.rst.rstdt_f1@eng.rst.rstdt': 0.37007087970572466, 'train@eng.rst.rstdt_precision@eng.rst.rstdt': 0.5236613388061468, 'train@eng.rst.rstdt_recall@eng.rst.rstdt': 0.3424312285679962, 'train@eng.rst.rstdt_loss@eng.rst.rstdt': 1.1618155241012573, 'train@eng.rst.rstdt_runtime': 191.1448, 'train@eng.rst.rstdt_samples_per_second': 83.717, 'train@eng.rst.rstdt_steps_per_second': 2.621, 'epoch': 6.0}
{'loss': 1.2288, 'learning_rate': 1e-05, 'epoch': 6.0}
{'eval_loss': 1.2239400148391724, 'eval_accuracy@eng.rst.rstdt': 0.6452806909315237, 'eval_f1@eng.rst.rstdt': 0.33522742863737975, 'eval_precision@eng.rst.rstdt': 0.4099084842045945, 'eval_recall@eng.rst.rstdt': 0.32593582610283817, 'eval_loss@eng.rst.rstdt': 1.2239400148391724, 'eval_runtime': 19.6077, 'eval_samples_per_second': 82.672, 'eval_steps_per_second': 2.601, 'epoch': 6.0}
{'train@eng.rst.rstdt_loss': 1.1401174068450928, 'train@eng.rst.rstdt_accuracy@eng.rst.rstdt': 0.6653543307086615, 'train@eng.rst.rstdt_f1@eng.rst.rstdt': 0.3836865787164292, 'train@eng.rst.rstdt_precision@eng.rst.rstdt': 0.5815369349005453, 'train@eng.rst.rstdt_recall@eng.rst.rstdt': 0.3514737326528061, 'train@eng.rst.rstdt_loss@eng.rst.rstdt': 1.1401174068450928, 'train@eng.rst.rstdt_runtime': 191.2924, 'train@eng.rst.rstdt_samples_per_second': 83.652, 'train@eng.rst.rstdt_steps_per_second': 2.619, 'epoch': 7.0}
{'loss': 1.2044, 'learning_rate': 8.333333333333334e-06, 'epoch': 7.0}
{'eval_loss': 1.210151195526123, 'eval_accuracy@eng.rst.rstdt': 0.6434299814929056, 'eval_f1@eng.rst.rstdt': 0.3436847530782676, 'eval_precision@eng.rst.rstdt': 0.4741204568794861, 'eval_recall@eng.rst.rstdt': 0.3344687907138577, 'eval_loss@eng.rst.rstdt': 1.210151195526123, 'eval_runtime': 19.6, 'eval_samples_per_second': 82.704, 'eval_steps_per_second': 2.602, 'epoch': 7.0}
{'train@eng.rst.rstdt_loss': 1.122582197189331, 'train@eng.rst.rstdt_accuracy@eng.rst.rstdt': 0.6685414323209599, 'train@eng.rst.rstdt_f1@eng.rst.rstdt': 0.39747068244361233, 'train@eng.rst.rstdt_precision@eng.rst.rstdt': 0.5714144195674666, 'train@eng.rst.rstdt_recall@eng.rst.rstdt': 0.3641417605637943, 'train@eng.rst.rstdt_loss@eng.rst.rstdt': 1.122582197189331, 'train@eng.rst.rstdt_runtime': 191.2505, 'train@eng.rst.rstdt_samples_per_second': 83.67, 'train@eng.rst.rstdt_steps_per_second': 2.62, 'epoch': 8.0}
{'loss': 1.1823, 'learning_rate': 6.666666666666667e-06, 'epoch': 8.0}
{'eval_loss': 1.203701138496399, 'eval_accuracy@eng.rst.rstdt': 0.6440468846391116, 'eval_f1@eng.rst.rstdt': 0.3588805996843714, 'eval_precision@eng.rst.rstdt': 0.5223988878518456, 'eval_recall@eng.rst.rstdt': 0.3450137769637633, 'eval_loss@eng.rst.rstdt': 1.203701138496399, 'eval_runtime': 19.5795, 'eval_samples_per_second': 82.791, 'eval_steps_per_second': 2.605, 'epoch': 8.0}
{'train@eng.rst.rstdt_loss': 1.1106051206588745, 'train@eng.rst.rstdt_accuracy@eng.rst.rstdt': 0.6721659792525935, 'train@eng.rst.rstdt_f1@eng.rst.rstdt': 0.4077074809930068, 'train@eng.rst.rstdt_precision@eng.rst.rstdt': 0.6038279741026304, 'train@eng.rst.rstdt_recall@eng.rst.rstdt': 0.3703312796088336, 'train@eng.rst.rstdt_loss@eng.rst.rstdt': 1.1106051206588745, 'train@eng.rst.rstdt_runtime': 191.2101, 'train@eng.rst.rstdt_samples_per_second': 83.688, 'train@eng.rst.rstdt_steps_per_second': 2.62, 'epoch': 9.0}
{'loss': 1.1665, 'learning_rate': 5e-06, 'epoch': 9.0}
{'eval_loss': 1.1938153505325317, 'eval_accuracy@eng.rst.rstdt': 0.6452806909315237, 'eval_f1@eng.rst.rstdt': 0.3611147293755669, 'eval_precision@eng.rst.rstdt': 0.5244701541862229, 'eval_recall@eng.rst.rstdt': 0.3455351498729771, 'eval_loss@eng.rst.rstdt': 1.1938153505325317, 'eval_runtime': 19.5931, 'eval_samples_per_second': 82.733, 'eval_steps_per_second': 2.603, 'epoch': 9.0}
{'train@eng.rst.rstdt_loss': 1.1044822931289673, 'train@eng.rst.rstdt_accuracy@eng.rst.rstdt': 0.6704786901637295, 'train@eng.rst.rstdt_f1@eng.rst.rstdt': 0.41675278779971736, 'train@eng.rst.rstdt_precision@eng.rst.rstdt': 0.6000575182285722, 'train@eng.rst.rstdt_recall@eng.rst.rstdt': 0.38037914138462803, 'train@eng.rst.rstdt_loss@eng.rst.rstdt': 1.1044822931289673, 'train@eng.rst.rstdt_runtime': 191.2856, 'train@eng.rst.rstdt_samples_per_second': 83.655, 'train@eng.rst.rstdt_steps_per_second': 2.619, 'epoch': 10.0}
{'loss': 1.1569, 'learning_rate': 3.3333333333333333e-06, 'epoch': 10.0}
{'eval_loss': 1.1973602771759033, 'eval_accuracy@eng.rst.rstdt': 0.6391116594694632, 'eval_f1@eng.rst.rstdt': 0.36852050096895833, 'eval_precision@eng.rst.rstdt': 0.5266439526158505, 'eval_recall@eng.rst.rstdt': 0.35352558730178485, 'eval_loss@eng.rst.rstdt': 1.1973602771759033, 'eval_runtime': 19.5884, 'eval_samples_per_second': 82.753, 'eval_steps_per_second': 2.604, 'epoch': 10.0}
{'train@eng.rst.rstdt_loss': 1.0984796285629272, 'train@eng.rst.rstdt_accuracy@eng.rst.rstdt': 0.6729783777027871, 'train@eng.rst.rstdt_f1@eng.rst.rstdt': 0.416350960196619, 'train@eng.rst.rstdt_precision@eng.rst.rstdt': 0.6133528741361924, 'train@eng.rst.rstdt_recall@eng.rst.rstdt': 0.3779220784917031, 'train@eng.rst.rstdt_loss@eng.rst.rstdt': 1.0984795093536377, 'train@eng.rst.rstdt_runtime': 191.1873, 'train@eng.rst.rstdt_samples_per_second': 83.698, 'train@eng.rst.rstdt_steps_per_second': 2.62, 'epoch': 11.0}
{'loss': 1.1517, 'learning_rate': 1.6666666666666667e-06, 'epoch': 11.0}
{'eval_loss': 1.1899783611297607, 'eval_accuracy@eng.rst.rstdt': 0.6452806909315237, 'eval_f1@eng.rst.rstdt': 0.3691401346337537, 'eval_precision@eng.rst.rstdt': 0.5292351920691847, 'eval_recall@eng.rst.rstdt': 0.3520853100330289, 'eval_loss@eng.rst.rstdt': 1.1899782419204712, 'eval_runtime': 19.5834, 'eval_samples_per_second': 82.774, 'eval_steps_per_second': 2.604, 'epoch': 11.0}
{'train@eng.rst.rstdt_loss': 1.09697687625885, 'train@eng.rst.rstdt_accuracy@eng.rst.rstdt': 0.6743532058492688, 'train@eng.rst.rstdt_f1@eng.rst.rstdt': 0.41755324519010817, 'train@eng.rst.rstdt_precision@eng.rst.rstdt': 0.615463127170161, 'train@eng.rst.rstdt_recall@eng.rst.rstdt': 0.3782864719083909, 'train@eng.rst.rstdt_loss@eng.rst.rstdt': 1.0969769954681396, 'train@eng.rst.rstdt_runtime': 191.2915, 'train@eng.rst.rstdt_samples_per_second': 83.652, 'train@eng.rst.rstdt_steps_per_second': 2.619, 'epoch': 12.0}
{'loss': 1.1464, 'learning_rate': 0.0, 'epoch': 12.0}
{'eval_loss': 1.1882110834121704, 'eval_accuracy@eng.rst.rstdt': 0.6458975940777298, 'eval_f1@eng.rst.rstdt': 0.3714224318835775, 'eval_precision@eng.rst.rstdt': 0.5301946645229032, 'eval_recall@eng.rst.rstdt': 0.35303610804744967, 'eval_loss@eng.rst.rstdt': 1.1882110834121704, 'eval_runtime': 19.6245, 'eval_samples_per_second': 82.601, 'eval_steps_per_second': 2.599, 'epoch': 12.0}
{'train_runtime': 7344.4816, 'train_samples_per_second': 26.145, 'train_steps_per_second': 0.819, 'train_loss': 1.3264011933814663, 'epoch': 12.0}
***** train_log metrics *****
  epoch                    =       12.0
  train_loss               =     1.3264
  train_runtime            = 2:02:24.48
  train_samples_per_second =     26.145
  train_steps_per_second   =      0.819
{'train@fas.rst.prstc_loss': 2.28102707862854, 'train@fas.rst.prstc_accuracy@fas.rst.prstc': 0.2751219512195122, 'train@fas.rst.prstc_f1@fas.rst.prstc': 0.04460032743113723, 'train@fas.rst.prstc_precision@fas.rst.prstc': 0.0661194305085254, 'train@fas.rst.prstc_recall@fas.rst.prstc': 0.07160484226665019, 'train@fas.rst.prstc_loss@fas.rst.prstc': 2.281027317047119, 'train@fas.rst.prstc_runtime': 49.113, 'train@fas.rst.prstc_samples_per_second': 83.481, 'train@fas.rst.prstc_steps_per_second': 2.627, 'epoch': 1.0}
{'loss': 2.3943, 'learning_rate': 1.8333333333333333e-05, 'epoch': 1.0}
{'eval_loss': 2.2055141925811768, 'eval_accuracy@fas.rst.prstc': 0.2905811623246493, 'eval_f1@fas.rst.prstc': 0.049244924492449244, 'eval_precision@fas.rst.prstc': 0.03926202694997149, 'eval_recall@fas.rst.prstc': 0.0803991446899501, 'eval_loss@fas.rst.prstc': 2.205514430999756, 'eval_runtime': 6.1899, 'eval_samples_per_second': 80.615, 'eval_steps_per_second': 2.585, 'epoch': 1.0}
{'train@fas.rst.prstc_loss': 2.1527328491210938, 'train@fas.rst.prstc_accuracy@fas.rst.prstc': 0.34780487804878046, 'train@fas.rst.prstc_f1@fas.rst.prstc': 0.093450058945621, 'train@fas.rst.prstc_precision@fas.rst.prstc': 0.12089474836171102, 'train@fas.rst.prstc_recall@fas.rst.prstc': 0.1128842298212433, 'train@fas.rst.prstc_loss@fas.rst.prstc': 2.1527328491210938, 'train@fas.rst.prstc_runtime': 49.1541, 'train@fas.rst.prstc_samples_per_second': 83.411, 'train@fas.rst.prstc_steps_per_second': 2.624, 'epoch': 2.0}
{'loss': 2.2715, 'learning_rate': 1.6666666666666667e-05, 'epoch': 2.0}
{'eval_loss': 2.065681219100952, 'eval_accuracy@fas.rst.prstc': 0.3667334669338677, 'eval_f1@fas.rst.prstc': 0.09701543708966236, 'eval_precision@fas.rst.prstc': 0.1538701454490928, 'eval_recall@fas.rst.prstc': 0.11836056195370821, 'eval_loss@fas.rst.prstc': 2.065680980682373, 'eval_runtime': 6.2152, 'eval_samples_per_second': 80.287, 'eval_steps_per_second': 2.574, 'epoch': 2.0}
{'train@fas.rst.prstc_loss': 2.03731107711792, 'train@fas.rst.prstc_accuracy@fas.rst.prstc': 0.38121951219512196, 'train@fas.rst.prstc_f1@fas.rst.prstc': 0.12209161322178357, 'train@fas.rst.prstc_precision@fas.rst.prstc': 0.1440148596301246, 'train@fas.rst.prstc_recall@fas.rst.prstc': 0.14497486148555552, 'train@fas.rst.prstc_loss@fas.rst.prstc': 2.03731107711792, 'train@fas.rst.prstc_runtime': 49.1564, 'train@fas.rst.prstc_samples_per_second': 83.407, 'train@fas.rst.prstc_steps_per_second': 2.624, 'epoch': 3.0}
{'loss': 2.1486, 'learning_rate': 1.5000000000000002e-05, 'epoch': 3.0}
{'eval_loss': 1.9388573169708252, 'eval_accuracy@fas.rst.prstc': 0.4028056112224449, 'eval_f1@fas.rst.prstc': 0.13570229922632668, 'eval_precision@fas.rst.prstc': 0.13535659651690593, 'eval_recall@fas.rst.prstc': 0.15629024896524993, 'eval_loss@fas.rst.prstc': 1.9388571977615356, 'eval_runtime': 6.2162, 'eval_samples_per_second': 80.274, 'eval_steps_per_second': 2.574, 'epoch': 3.0}
{'train@fas.rst.prstc_loss': 1.968650221824646, 'train@fas.rst.prstc_accuracy@fas.rst.prstc': 0.3973170731707317, 'train@fas.rst.prstc_f1@fas.rst.prstc': 0.12586062201017365, 'train@fas.rst.prstc_precision@fas.rst.prstc': 0.1598871634651467, 'train@fas.rst.prstc_recall@fas.rst.prstc': 0.15697279846125886, 'train@fas.rst.prstc_loss@fas.rst.prstc': 1.968650460243225, 'train@fas.rst.prstc_runtime': 49.1682, 'train@fas.rst.prstc_samples_per_second': 83.387, 'train@fas.rst.prstc_steps_per_second': 2.624, 'epoch': 4.0}
{'loss': 2.0626, 'learning_rate': 1.3333333333333333e-05, 'epoch': 4.0}
{'eval_loss': 1.8703341484069824, 'eval_accuracy@fas.rst.prstc': 0.4288577154308617, 'eval_f1@fas.rst.prstc': 0.14866878774038175, 'eval_precision@fas.rst.prstc': 0.19465841149456825, 'eval_recall@fas.rst.prstc': 0.17914487831967293, 'eval_loss@fas.rst.prstc': 1.870334267616272, 'eval_runtime': 6.2303, 'eval_samples_per_second': 80.092, 'eval_steps_per_second': 2.568, 'epoch': 4.0}
{'train@fas.rst.prstc_loss': 1.932235598564148, 'train@fas.rst.prstc_accuracy@fas.rst.prstc': 0.41341463414634144, 'train@fas.rst.prstc_f1@fas.rst.prstc': 0.138626137832911, 'train@fas.rst.prstc_precision@fas.rst.prstc': 0.27220189952085644, 'train@fas.rst.prstc_recall@fas.rst.prstc': 0.16449391136205915, 'train@fas.rst.prstc_loss@fas.rst.prstc': 1.932235598564148, 'train@fas.rst.prstc_runtime': 49.1051, 'train@fas.rst.prstc_samples_per_second': 83.494, 'train@fas.rst.prstc_steps_per_second': 2.627, 'epoch': 5.0}
{'loss': 1.9967, 'learning_rate': 1.1666666666666668e-05, 'epoch': 5.0}
{'eval_loss': 1.850596308708191, 'eval_accuracy@fas.rst.prstc': 0.4288577154308617, 'eval_f1@fas.rst.prstc': 0.1575491998962559, 'eval_precision@fas.rst.prstc': 0.1969992397089171, 'eval_recall@fas.rst.prstc': 0.1823307856149136, 'eval_loss@fas.rst.prstc': 1.8505961894989014, 'eval_runtime': 6.2152, 'eval_samples_per_second': 80.287, 'eval_steps_per_second': 2.574, 'epoch': 5.0}
{'train@fas.rst.prstc_loss': 1.8892693519592285, 'train@fas.rst.prstc_accuracy@fas.rst.prstc': 0.4275609756097561, 'train@fas.rst.prstc_f1@fas.rst.prstc': 0.15887775236587534, 'train@fas.rst.prstc_precision@fas.rst.prstc': 0.21583705962614663, 'train@fas.rst.prstc_recall@fas.rst.prstc': 0.18161325001595474, 'train@fas.rst.prstc_loss@fas.rst.prstc': 1.889269471168518, 'train@fas.rst.prstc_runtime': 49.1016, 'train@fas.rst.prstc_samples_per_second': 83.5, 'train@fas.rst.prstc_steps_per_second': 2.627, 'epoch': 6.0}
{'loss': 1.9606, 'learning_rate': 1e-05, 'epoch': 6.0}
{'eval_loss': 1.8178434371948242, 'eval_accuracy@fas.rst.prstc': 0.43687374749499, 'eval_f1@fas.rst.prstc': 0.17203108431341446, 'eval_precision@fas.rst.prstc': 0.19676398113278545, 'eval_recall@fas.rst.prstc': 0.19837198866468395, 'eval_loss@fas.rst.prstc': 1.8178433179855347, 'eval_runtime': 6.2275, 'eval_samples_per_second': 80.128, 'eval_steps_per_second': 2.569, 'epoch': 6.0}
{'train@fas.rst.prstc_loss': 1.8650951385498047, 'train@fas.rst.prstc_accuracy@fas.rst.prstc': 0.43585365853658536, 'train@fas.rst.prstc_f1@fas.rst.prstc': 0.1677839821303364, 'train@fas.rst.prstc_precision@fas.rst.prstc': 0.26952187688321017, 'train@fas.rst.prstc_recall@fas.rst.prstc': 0.18851375234237008, 'train@fas.rst.prstc_loss@fas.rst.prstc': 1.8650951385498047, 'train@fas.rst.prstc_runtime': 49.109, 'train@fas.rst.prstc_samples_per_second': 83.488, 'train@fas.rst.prstc_steps_per_second': 2.627, 'epoch': 7.0}
{'loss': 1.9228, 'learning_rate': 8.333333333333334e-06, 'epoch': 7.0}
{'eval_loss': 1.7988100051879883, 'eval_accuracy@fas.rst.prstc': 0.44488977955911824, 'eval_f1@fas.rst.prstc': 0.17912323011701894, 'eval_precision@fas.rst.prstc': 0.19837441706511913, 'eval_recall@fas.rst.prstc': 0.20301673648833413, 'eval_loss@fas.rst.prstc': 1.798810362815857, 'eval_runtime': 6.1984, 'eval_samples_per_second': 80.505, 'eval_steps_per_second': 2.581, 'epoch': 7.0}
{'train@fas.rst.prstc_loss': 1.843005895614624, 'train@fas.rst.prstc_accuracy@fas.rst.prstc': 0.445609756097561, 'train@fas.rst.prstc_f1@fas.rst.prstc': 0.17894869613153405, 'train@fas.rst.prstc_precision@fas.rst.prstc': 0.27874245417912297, 'train@fas.rst.prstc_recall@fas.rst.prstc': 0.1991618764437388, 'train@fas.rst.prstc_loss@fas.rst.prstc': 1.843005895614624, 'train@fas.rst.prstc_runtime': 49.1085, 'train@fas.rst.prstc_samples_per_second': 83.489, 'train@fas.rst.prstc_steps_per_second': 2.627, 'epoch': 8.0}
{'loss': 1.9146, 'learning_rate': 6.666666666666667e-06, 'epoch': 8.0}
{'eval_loss': 1.7817167043685913, 'eval_accuracy@fas.rst.prstc': 0.45090180360721444, 'eval_f1@fas.rst.prstc': 0.18153648159231464, 'eval_precision@fas.rst.prstc': 0.1853714923209047, 'eval_recall@fas.rst.prstc': 0.20804515544405172, 'eval_loss@fas.rst.prstc': 1.7817165851593018, 'eval_runtime': 6.2037, 'eval_samples_per_second': 80.436, 'eval_steps_per_second': 2.579, 'epoch': 8.0}
{'train@fas.rst.prstc_loss': 1.8307092189788818, 'train@fas.rst.prstc_accuracy@fas.rst.prstc': 0.44853658536585367, 'train@fas.rst.prstc_f1@fas.rst.prstc': 0.18255197589743968, 'train@fas.rst.prstc_precision@fas.rst.prstc': 0.27412440469004457, 'train@fas.rst.prstc_recall@fas.rst.prstc': 0.20397121413442335, 'train@fas.rst.prstc_loss@fas.rst.prstc': 1.8307090997695923, 'train@fas.rst.prstc_runtime': 49.1196, 'train@fas.rst.prstc_samples_per_second': 83.47, 'train@fas.rst.prstc_steps_per_second': 2.626, 'epoch': 9.0}
{'loss': 1.887, 'learning_rate': 5e-06, 'epoch': 9.0}
{'eval_loss': 1.773978352546692, 'eval_accuracy@fas.rst.prstc': 0.46292585170340683, 'eval_f1@fas.rst.prstc': 0.19709401199323828, 'eval_precision@fas.rst.prstc': 0.23758007797334865, 'eval_recall@fas.rst.prstc': 0.21895007925898394, 'eval_loss@fas.rst.prstc': 1.7739781141281128, 'eval_runtime': 6.1992, 'eval_samples_per_second': 80.495, 'eval_steps_per_second': 2.581, 'epoch': 9.0}
{'train@fas.rst.prstc_loss': 1.8219472169876099, 'train@fas.rst.prstc_accuracy@fas.rst.prstc': 0.4504878048780488, 'train@fas.rst.prstc_f1@fas.rst.prstc': 0.1868587330100226, 'train@fas.rst.prstc_precision@fas.rst.prstc': 0.32749540888431367, 'train@fas.rst.prstc_recall@fas.rst.prstc': 0.20658354318788044, 'train@fas.rst.prstc_loss@fas.rst.prstc': 1.8219472169876099, 'train@fas.rst.prstc_runtime': 49.1718, 'train@fas.rst.prstc_samples_per_second': 83.381, 'train@fas.rst.prstc_steps_per_second': 2.623, 'epoch': 10.0}
{'loss': 1.8871, 'learning_rate': 3.3333333333333333e-06, 'epoch': 10.0}
{'eval_loss': 1.7668499946594238, 'eval_accuracy@fas.rst.prstc': 0.46693386773547096, 'eval_f1@fas.rst.prstc': 0.20357332184742002, 'eval_precision@fas.rst.prstc': 0.24477926240426243, 'eval_recall@fas.rst.prstc': 0.2236410243847117, 'eval_loss@fas.rst.prstc': 1.7668499946594238, 'eval_runtime': 6.2175, 'eval_samples_per_second': 80.258, 'eval_steps_per_second': 2.573, 'epoch': 10.0}
{'train@fas.rst.prstc_loss': 1.8160496950149536, 'train@fas.rst.prstc_accuracy@fas.rst.prstc': 0.45390243902439026, 'train@fas.rst.prstc_f1@fas.rst.prstc': 0.19193660531237686, 'train@fas.rst.prstc_precision@fas.rst.prstc': 0.33538294347207503, 'train@fas.rst.prstc_recall@fas.rst.prstc': 0.209895458590949, 'train@fas.rst.prstc_loss@fas.rst.prstc': 1.8160498142242432, 'train@fas.rst.prstc_runtime': 49.3775, 'train@fas.rst.prstc_samples_per_second': 83.034, 'train@fas.rst.prstc_steps_per_second': 2.613, 'epoch': 11.0}
{'loss': 1.8732, 'learning_rate': 1.6666666666666667e-06, 'epoch': 11.0}
{'eval_loss': 1.7628891468048096, 'eval_accuracy@fas.rst.prstc': 0.4649298597194389, 'eval_f1@fas.rst.prstc': 0.2043057477977549, 'eval_precision@fas.rst.prstc': 0.24688914758144523, 'eval_recall@fas.rst.prstc': 0.2241910058042583, 'eval_loss@fas.rst.prstc': 1.7628891468048096, 'eval_runtime': 6.2233, 'eval_samples_per_second': 80.183, 'eval_steps_per_second': 2.571, 'epoch': 11.0}
{'train@fas.rst.prstc_loss': 1.814460277557373, 'train@fas.rst.prstc_accuracy@fas.rst.prstc': 0.45414634146341465, 'train@fas.rst.prstc_f1@fas.rst.prstc': 0.1909493905486191, 'train@fas.rst.prstc_precision@fas.rst.prstc': 0.32931405805992, 'train@fas.rst.prstc_recall@fas.rst.prstc': 0.20930106623830472, 'train@fas.rst.prstc_loss@fas.rst.prstc': 1.814460277557373, 'train@fas.rst.prstc_runtime': 49.1124, 'train@fas.rst.prstc_samples_per_second': 83.482, 'train@fas.rst.prstc_steps_per_second': 2.627, 'epoch': 12.0}
{'loss': 1.8657, 'learning_rate': 0.0, 'epoch': 12.0}
{'eval_loss': 1.762257695198059, 'eval_accuracy@fas.rst.prstc': 0.46292585170340683, 'eval_f1@fas.rst.prstc': 0.20169050130726418, 'eval_precision@fas.rst.prstc': 0.2385748292855986, 'eval_recall@fas.rst.prstc': 0.22346884551688062, 'eval_loss@fas.rst.prstc': 1.7622579336166382, 'eval_runtime': 6.2312, 'eval_samples_per_second': 80.081, 'eval_steps_per_second': 2.568, 'epoch': 12.0}
{'train_runtime': 1899.899, 'train_samples_per_second': 25.896, 'train_steps_per_second': 0.815, 'train_loss': 2.0153841812173217, 'epoch': 12.0}
***** train_logger metrics *****
  epoch                    =       12.0
  train_loss               =     1.3264
  train_runtime            = 2:02:24.48
  train_samples_per_second =     26.145
  train_steps_per_second   =      0.819
-------------------------------------------------------------------
Lang1:  eng.sdrt.stac    Lang2:  fas.rst.prstc
Saving run to:  runs/full_shot/FullShot=v4_eng.sdrt.stac_fas.rst.prstc_lrfunc=Adafactor_lr=2e-05_plm=bert-base-multilingual-cased
Running with params: BERT_MODEL=bert-base-multilingual-cased lr=2e-05
read 9580 examples
read 1145 examples
read 1510 examples
read 4100 examples
read 499 examples
read 592 examples
Total prediction labels:  33
BertAdapterModel(
  (shared_parameters): ModuleDict()
  (bert): BertModel(
    (shared_parameters): ModuleDict()
    (invertible_adapters): ModuleDict(
      (en): NICECouplingBlock(
        (F): Sequential(
          (0): Linear(in_features=384, out_features=192, bias=True)
          (1): Activation_Function_Class(
            (f): ReLU()
          )
          (2): Linear(in_features=192, out_features=384, bias=True)
        )
        (G): Sequential(
          (0): Linear(in_features=384, out_features=192, bias=True)
          (1): Activation_Function_Class(
            (f): ReLU()
          )
          (2): Linear(in_features=192, out_features=384, bias=True)
        )
      )
      (ar): NICECouplingBlock(
        (F): Sequential(
          (0): Linear(in_features=384, out_features=192, bias=True)
          (1): Activation_Function_Class(
            (f): ReLU()
          )
          (2): Linear(in_features=192, out_features=384, bias=True)
        )
        (G): Sequential(
          (0): Linear(in_features=384, out_features=192, bias=True)
          (1): Activation_Function_Class(
            (f): ReLU()
          )
          (2): Linear(in_features=192, out_features=384, bias=True)
        )
      )
    )
    (embeddings): BertEmbeddings(
      (word_embeddings): Embedding(119547, 768, padding_idx=0)
      (position_embeddings): Embedding(512, 768)
      (token_type_embeddings): Embedding(2, 768)
      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
      (dropout): Dropout(p=0.1, inplace=False)
    )
    (encoder): BertEncoder(
      (layer): ModuleList(
        (0): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (en): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (ar): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (1): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (en): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (ar): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (2): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (en): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (ar): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (3): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (en): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (ar): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (4): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (en): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (ar): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (5): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (en): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (ar): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (6): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (en): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (ar): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (7): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (en): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (ar): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (8): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (en): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (ar): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (9): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (en): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (ar): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (10): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (en): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (ar): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (11): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (en): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (ar): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
      )
    )
    (pooler): BertPooler(
      (dense): Linear(in_features=768, out_features=768, bias=True)
      (activation): Tanh()
    )
    (prefix_tuning): PrefixTuningPool(
      (prefix_tunings): ModuleDict()
    )
  )
  (heads): ModuleDict(
    (disrpt): ClassificationHead(
      (0): Dropout(p=0.1, inplace=False)
      (1): Linear(in_features=768, out_features=768, bias=True)
      (2): Activation_Function_Class(
        (f): Tanh()
      )
      (3): Dropout(p=0.1, inplace=False)
      (4): Linear(in_features=768, out_features=33, bias=True)
    )
  )
)
{'train@eng.sdrt.stac_loss': 2.0785529613494873, 'train@eng.sdrt.stac_accuracy@eng.sdrt.stac': 0.35511482254697285, 'train@eng.sdrt.stac_f1@eng.sdrt.stac': 0.06697731124755832, 'train@eng.sdrt.stac_precision@eng.sdrt.stac': 0.07267951292046818, 'train@eng.sdrt.stac_recall@eng.sdrt.stac': 0.11106172146204316, 'train@eng.sdrt.stac_loss@eng.sdrt.stac': 2.078552722930908, 'train@eng.sdrt.stac_runtime': 114.5987, 'train@eng.sdrt.stac_samples_per_second': 83.596, 'train@eng.sdrt.stac_steps_per_second': 2.618, 'epoch': 1.0}
{'loss': 2.4867, 'learning_rate': 1.8333333333333333e-05, 'epoch': 1.0}
{'eval_loss': 2.035874843597412, 'eval_accuracy@eng.sdrt.stac': 0.36331877729257644, 'eval_f1@eng.sdrt.stac': 0.0672284172075607, 'eval_precision@eng.sdrt.stac': 0.0570833191358866, 'eval_recall@eng.sdrt.stac': 0.11319966184272426, 'eval_loss@eng.sdrt.stac': 2.035874843597412, 'eval_runtime': 14.0015, 'eval_samples_per_second': 81.777, 'eval_steps_per_second': 2.571, 'epoch': 1.0}
{'train@eng.sdrt.stac_loss': 1.8788657188415527, 'train@eng.sdrt.stac_accuracy@eng.sdrt.stac': 0.4293319415448852, 'train@eng.sdrt.stac_f1@eng.sdrt.stac': 0.13549012464510918, 'train@eng.sdrt.stac_precision@eng.sdrt.stac': 0.13248038936175183, 'train@eng.sdrt.stac_recall@eng.sdrt.stac': 0.172260653510247, 'train@eng.sdrt.stac_loss@eng.sdrt.stac': 1.8788654804229736, 'train@eng.sdrt.stac_runtime': 114.5135, 'train@eng.sdrt.stac_samples_per_second': 83.658, 'train@eng.sdrt.stac_steps_per_second': 2.62, 'epoch': 2.0}
{'loss': 2.0145, 'learning_rate': 1.6666666666666667e-05, 'epoch': 2.0}
{'eval_loss': 1.8371806144714355, 'eval_accuracy@eng.sdrt.stac': 0.42096069868995634, 'eval_f1@eng.sdrt.stac': 0.12433874762734702, 'eval_precision@eng.sdrt.stac': 0.10863119181524443, 'eval_recall@eng.sdrt.stac': 0.1634449497854198, 'eval_loss@eng.sdrt.stac': 1.8371806144714355, 'eval_runtime': 14.0087, 'eval_samples_per_second': 81.735, 'eval_steps_per_second': 2.57, 'epoch': 2.0}
{'train@eng.sdrt.stac_loss': 1.7664668560028076, 'train@eng.sdrt.stac_accuracy@eng.sdrt.stac': 0.4514613778705637, 'train@eng.sdrt.stac_f1@eng.sdrt.stac': 0.15653863046634026, 'train@eng.sdrt.stac_precision@eng.sdrt.stac': 0.13903688357819838, 'train@eng.sdrt.stac_recall@eng.sdrt.stac': 0.19512614187752297, 'train@eng.sdrt.stac_loss@eng.sdrt.stac': 1.7664669752120972, 'train@eng.sdrt.stac_runtime': 114.4875, 'train@eng.sdrt.stac_samples_per_second': 83.677, 'train@eng.sdrt.stac_steps_per_second': 2.62, 'epoch': 3.0}
{'loss': 1.8641, 'learning_rate': 1.5000000000000002e-05, 'epoch': 3.0}
{'eval_loss': 1.7330223321914673, 'eval_accuracy@eng.sdrt.stac': 0.44366812227074237, 'eval_f1@eng.sdrt.stac': 0.14822514635431577, 'eval_precision@eng.sdrt.stac': 0.19055277634126971, 'eval_recall@eng.sdrt.stac': 0.18779299841394903, 'eval_loss@eng.sdrt.stac': 1.7330224514007568, 'eval_runtime': 13.9988, 'eval_samples_per_second': 81.793, 'eval_steps_per_second': 2.572, 'epoch': 3.0}
{'train@eng.sdrt.stac_loss': 1.6953424215316772, 'train@eng.sdrt.stac_accuracy@eng.sdrt.stac': 0.468580375782881, 'train@eng.sdrt.stac_f1@eng.sdrt.stac': 0.18194919468443943, 'train@eng.sdrt.stac_precision@eng.sdrt.stac': 0.2181247681147636, 'train@eng.sdrt.stac_recall@eng.sdrt.stac': 0.2116952187928718, 'train@eng.sdrt.stac_loss@eng.sdrt.stac': 1.6953424215316772, 'train@eng.sdrt.stac_runtime': 114.5449, 'train@eng.sdrt.stac_samples_per_second': 83.635, 'train@eng.sdrt.stac_steps_per_second': 2.619, 'epoch': 4.0}
{'loss': 1.7804, 'learning_rate': 1.3333333333333333e-05, 'epoch': 4.0}
{'eval_loss': 1.6542267799377441, 'eval_accuracy@eng.sdrt.stac': 0.46550218340611355, 'eval_f1@eng.sdrt.stac': 0.16851017584345407, 'eval_precision@eng.sdrt.stac': 0.2538854936342727, 'eval_recall@eng.sdrt.stac': 0.20119011438045895, 'eval_loss@eng.sdrt.stac': 1.6542267799377441, 'eval_runtime': 14.0313, 'eval_samples_per_second': 81.603, 'eval_steps_per_second': 2.566, 'epoch': 4.0}
{'train@eng.sdrt.stac_loss': 1.6443325281143188, 'train@eng.sdrt.stac_accuracy@eng.sdrt.stac': 0.4873695198329854, 'train@eng.sdrt.stac_f1@eng.sdrt.stac': 0.2098442774016629, 'train@eng.sdrt.stac_precision@eng.sdrt.stac': 0.2466386276007662, 'train@eng.sdrt.stac_recall@eng.sdrt.stac': 0.23346118266593865, 'train@eng.sdrt.stac_loss@eng.sdrt.stac': 1.6443324089050293, 'train@eng.sdrt.stac_runtime': 114.5805, 'train@eng.sdrt.stac_samples_per_second': 83.609, 'train@eng.sdrt.stac_steps_per_second': 2.618, 'epoch': 5.0}
{'loss': 1.7209, 'learning_rate': 1.1666666666666668e-05, 'epoch': 5.0}
{'eval_loss': 1.6049869060516357, 'eval_accuracy@eng.sdrt.stac': 0.4794759825327511, 'eval_f1@eng.sdrt.stac': 0.19017585160770106, 'eval_precision@eng.sdrt.stac': 0.2083379446321004, 'eval_recall@eng.sdrt.stac': 0.21539721786061583, 'eval_loss@eng.sdrt.stac': 1.6049869060516357, 'eval_runtime': 13.9755, 'eval_samples_per_second': 81.929, 'eval_steps_per_second': 2.576, 'epoch': 5.0}
{'train@eng.sdrt.stac_loss': 1.6036396026611328, 'train@eng.sdrt.stac_accuracy@eng.sdrt.stac': 0.48893528183716073, 'train@eng.sdrt.stac_f1@eng.sdrt.stac': 0.20917105767153543, 'train@eng.sdrt.stac_precision@eng.sdrt.stac': 0.25613448754516827, 'train@eng.sdrt.stac_recall@eng.sdrt.stac': 0.2312169310702622, 'train@eng.sdrt.stac_loss@eng.sdrt.stac': 1.6036392450332642, 'train@eng.sdrt.stac_runtime': 114.4904, 'train@eng.sdrt.stac_samples_per_second': 83.675, 'train@eng.sdrt.stac_steps_per_second': 2.62, 'epoch': 6.0}
{'loss': 1.6703, 'learning_rate': 1e-05, 'epoch': 6.0}
{'eval_loss': 1.5782841444015503, 'eval_accuracy@eng.sdrt.stac': 0.4847161572052402, 'eval_f1@eng.sdrt.stac': 0.1908223126892163, 'eval_precision@eng.sdrt.stac': 0.23121855684765766, 'eval_recall@eng.sdrt.stac': 0.21611758451178703, 'eval_loss@eng.sdrt.stac': 1.5782843828201294, 'eval_runtime': 14.0046, 'eval_samples_per_second': 81.759, 'eval_steps_per_second': 2.571, 'epoch': 6.0}
{'train@eng.sdrt.stac_loss': 1.5821093320846558, 'train@eng.sdrt.stac_accuracy@eng.sdrt.stac': 0.4997912317327766, 'train@eng.sdrt.stac_f1@eng.sdrt.stac': 0.2261533866408784, 'train@eng.sdrt.stac_precision@eng.sdrt.stac': 0.2486179925316673, 'train@eng.sdrt.stac_recall@eng.sdrt.stac': 0.2474889797446122, 'train@eng.sdrt.stac_loss@eng.sdrt.stac': 1.5821093320846558, 'train@eng.sdrt.stac_runtime': 114.4811, 'train@eng.sdrt.stac_samples_per_second': 83.682, 'train@eng.sdrt.stac_steps_per_second': 2.621, 'epoch': 7.0}
{'loss': 1.641, 'learning_rate': 8.333333333333334e-06, 'epoch': 7.0}
{'eval_loss': 1.5592362880706787, 'eval_accuracy@eng.sdrt.stac': 0.49519650655021835, 'eval_f1@eng.sdrt.stac': 0.21264237192420643, 'eval_precision@eng.sdrt.stac': 0.21775303180799677, 'eval_recall@eng.sdrt.stac': 0.23409412227946524, 'eval_loss@eng.sdrt.stac': 1.5592364072799683, 'eval_runtime': 14.0267, 'eval_samples_per_second': 81.63, 'eval_steps_per_second': 2.567, 'epoch': 7.0}
{'train@eng.sdrt.stac_loss': 1.5479061603546143, 'train@eng.sdrt.stac_accuracy@eng.sdrt.stac': 0.5126304801670146, 'train@eng.sdrt.stac_f1@eng.sdrt.stac': 0.25563571324787165, 'train@eng.sdrt.stac_precision@eng.sdrt.stac': 0.283181661926881, 'train@eng.sdrt.stac_recall@eng.sdrt.stac': 0.27205707703232473, 'train@eng.sdrt.stac_loss@eng.sdrt.stac': 1.5479060411453247, 'train@eng.sdrt.stac_runtime': 114.5546, 'train@eng.sdrt.stac_samples_per_second': 83.628, 'train@eng.sdrt.stac_steps_per_second': 2.619, 'epoch': 8.0}
{'loss': 1.6113, 'learning_rate': 6.666666666666667e-06, 'epoch': 8.0}
{'eval_loss': 1.5236157178878784, 'eval_accuracy@eng.sdrt.stac': 0.514410480349345, 'eval_f1@eng.sdrt.stac': 0.24085829322958496, 'eval_precision@eng.sdrt.stac': 0.2592859722813794, 'eval_recall@eng.sdrt.stac': 0.2584567744871127, 'eval_loss@eng.sdrt.stac': 1.5236157178878784, 'eval_runtime': 13.994, 'eval_samples_per_second': 81.821, 'eval_steps_per_second': 2.573, 'epoch': 8.0}
{'train@eng.sdrt.stac_loss': 1.5330747365951538, 'train@eng.sdrt.stac_accuracy@eng.sdrt.stac': 0.5177453027139874, 'train@eng.sdrt.stac_f1@eng.sdrt.stac': 0.27240089572349946, 'train@eng.sdrt.stac_precision@eng.sdrt.stac': 0.3757973097120001, 'train@eng.sdrt.stac_recall@eng.sdrt.stac': 0.28922266200368196, 'train@eng.sdrt.stac_loss@eng.sdrt.stac': 1.5330747365951538, 'train@eng.sdrt.stac_runtime': 114.5786, 'train@eng.sdrt.stac_samples_per_second': 83.611, 'train@eng.sdrt.stac_steps_per_second': 2.618, 'epoch': 9.0}
{'loss': 1.5889, 'learning_rate': 5e-06, 'epoch': 9.0}
{'eval_loss': 1.514548659324646, 'eval_accuracy@eng.sdrt.stac': 0.5161572052401747, 'eval_f1@eng.sdrt.stac': 0.25087753264503404, 'eval_precision@eng.sdrt.stac': 0.2554480514443298, 'eval_recall@eng.sdrt.stac': 0.2687937583494218, 'eval_loss@eng.sdrt.stac': 1.514548659324646, 'eval_runtime': 13.9711, 'eval_samples_per_second': 81.955, 'eval_steps_per_second': 2.577, 'epoch': 9.0}
{'train@eng.sdrt.stac_loss': 1.5180257558822632, 'train@eng.sdrt.stac_accuracy@eng.sdrt.stac': 0.5225469728601253, 'train@eng.sdrt.stac_f1@eng.sdrt.stac': 0.2873251781051049, 'train@eng.sdrt.stac_precision@eng.sdrt.stac': 0.36843088441462224, 'train@eng.sdrt.stac_recall@eng.sdrt.stac': 0.304289906073259, 'train@eng.sdrt.stac_loss@eng.sdrt.stac': 1.5180257558822632, 'train@eng.sdrt.stac_runtime': 114.5324, 'train@eng.sdrt.stac_samples_per_second': 83.644, 'train@eng.sdrt.stac_steps_per_second': 2.619, 'epoch': 10.0}
{'loss': 1.5763, 'learning_rate': 3.3333333333333333e-06, 'epoch': 10.0}
{'eval_loss': 1.4995880126953125, 'eval_accuracy@eng.sdrt.stac': 0.525764192139738, 'eval_f1@eng.sdrt.stac': 0.26200921132016003, 'eval_precision@eng.sdrt.stac': 0.2587598739847127, 'eval_recall@eng.sdrt.stac': 0.2819230433908041, 'eval_loss@eng.sdrt.stac': 1.499587893486023, 'eval_runtime': 13.9858, 'eval_samples_per_second': 81.869, 'eval_steps_per_second': 2.574, 'epoch': 10.0}
{'train@eng.sdrt.stac_loss': 1.508212685585022, 'train@eng.sdrt.stac_accuracy@eng.sdrt.stac': 0.5242171189979123, 'train@eng.sdrt.stac_f1@eng.sdrt.stac': 0.2939520366314214, 'train@eng.sdrt.stac_precision@eng.sdrt.stac': 0.37557345468586123, 'train@eng.sdrt.stac_recall@eng.sdrt.stac': 0.31004640205042555, 'train@eng.sdrt.stac_loss@eng.sdrt.stac': 1.5082125663757324, 'train@eng.sdrt.stac_runtime': 114.4165, 'train@eng.sdrt.stac_samples_per_second': 83.729, 'train@eng.sdrt.stac_steps_per_second': 2.622, 'epoch': 11.0}
{'loss': 1.5619, 'learning_rate': 1.6666666666666667e-06, 'epoch': 11.0}
{'eval_loss': 1.4945359230041504, 'eval_accuracy@eng.sdrt.stac': 0.5266375545851528, 'eval_f1@eng.sdrt.stac': 0.26306430389250884, 'eval_precision@eng.sdrt.stac': 0.2593252520731111, 'eval_recall@eng.sdrt.stac': 0.2824146636809092, 'eval_loss@eng.sdrt.stac': 1.4945358037948608, 'eval_runtime': 14.0006, 'eval_samples_per_second': 81.782, 'eval_steps_per_second': 2.571, 'epoch': 11.0}
{'train@eng.sdrt.stac_loss': 1.5069797039031982, 'train@eng.sdrt.stac_accuracy@eng.sdrt.stac': 0.5258872651356994, 'train@eng.sdrt.stac_f1@eng.sdrt.stac': 0.29427702581188897, 'train@eng.sdrt.stac_precision@eng.sdrt.stac': 0.37167968400921325, 'train@eng.sdrt.stac_recall@eng.sdrt.stac': 0.31066793635744105, 'train@eng.sdrt.stac_loss@eng.sdrt.stac': 1.5069795846939087, 'train@eng.sdrt.stac_runtime': 114.5363, 'train@eng.sdrt.stac_samples_per_second': 83.642, 'train@eng.sdrt.stac_steps_per_second': 2.619, 'epoch': 12.0}
{'loss': 1.5591, 'learning_rate': 0.0, 'epoch': 12.0}
{'eval_loss': 1.4924591779708862, 'eval_accuracy@eng.sdrt.stac': 0.5266375545851528, 'eval_f1@eng.sdrt.stac': 0.26278018881393445, 'eval_precision@eng.sdrt.stac': 0.25876178487873214, 'eval_recall@eng.sdrt.stac': 0.2825674007513359, 'eval_loss@eng.sdrt.stac': 1.4924592971801758, 'eval_runtime': 13.9967, 'eval_samples_per_second': 81.805, 'eval_steps_per_second': 2.572, 'epoch': 12.0}
{'train_runtime': 4423.4514, 'train_samples_per_second': 25.989, 'train_steps_per_second': 0.814, 'train_loss': 1.7562906816270616, 'epoch': 12.0}
***** train_log metrics *****
  epoch                    =       12.0
  train_loss               =     1.7563
  train_runtime            = 1:13:43.45
  train_samples_per_second =     25.989
  train_steps_per_second   =      0.814
{'train@fas.rst.prstc_loss': 2.369774580001831, 'train@fas.rst.prstc_accuracy@fas.rst.prstc': 0.24560975609756097, 'train@fas.rst.prstc_f1@fas.rst.prstc': 0.03230994630147646, 'train@fas.rst.prstc_precision@fas.rst.prstc': 0.031222152078813938, 'train@fas.rst.prstc_recall@fas.rst.prstc': 0.06165570210626531, 'train@fas.rst.prstc_loss@fas.rst.prstc': 2.369774580001831, 'train@fas.rst.prstc_runtime': 49.231, 'train@fas.rst.prstc_samples_per_second': 83.281, 'train@fas.rst.prstc_steps_per_second': 2.62, 'epoch': 1.0}
{'loss': 2.5834, 'learning_rate': 1.8333333333333333e-05, 'epoch': 1.0}
{'eval_loss': 2.292116403579712, 'eval_accuracy@fas.rst.prstc': 0.2545090180360721, 'eval_f1@fas.rst.prstc': 0.03708129811416196, 'eval_precision@fas.rst.prstc': 0.04587256748273697, 'eval_recall@fas.rst.prstc': 0.06979805179377525, 'eval_loss@fas.rst.prstc': 2.292116641998291, 'eval_runtime': 6.2661, 'eval_samples_per_second': 79.634, 'eval_steps_per_second': 2.553, 'epoch': 1.0}
{'train@fas.rst.prstc_loss': 2.3254764080047607, 'train@fas.rst.prstc_accuracy@fas.rst.prstc': 0.26097560975609757, 'train@fas.rst.prstc_f1@fas.rst.prstc': 0.04209013448311262, 'train@fas.rst.prstc_precision@fas.rst.prstc': 0.030567837536085288, 'train@fas.rst.prstc_recall@fas.rst.prstc': 0.06838205877004376, 'train@fas.rst.prstc_loss@fas.rst.prstc': 2.3254759311676025, 'train@fas.rst.prstc_runtime': 49.2116, 'train@fas.rst.prstc_samples_per_second': 83.314, 'train@fas.rst.prstc_steps_per_second': 2.621, 'epoch': 2.0}
{'loss': 2.3669, 'learning_rate': 1.6666666666666667e-05, 'epoch': 2.0}
{'eval_loss': 2.2393879890441895, 'eval_accuracy@fas.rst.prstc': 0.2845691382765531, 'eval_f1@fas.rst.prstc': 0.05121164114299126, 'eval_precision@fas.rst.prstc': 0.038490913273521964, 'eval_recall@fas.rst.prstc': 0.07945830363506771, 'eval_loss@fas.rst.prstc': 2.2393879890441895, 'eval_runtime': 6.2849, 'eval_samples_per_second': 79.397, 'eval_steps_per_second': 2.546, 'epoch': 2.0}
{'train@fas.rst.prstc_loss': 2.3037264347076416, 'train@fas.rst.prstc_accuracy@fas.rst.prstc': 0.275609756097561, 'train@fas.rst.prstc_f1@fas.rst.prstc': 0.04347615112413595, 'train@fas.rst.prstc_precision@fas.rst.prstc': 0.032823669360287855, 'train@fas.rst.prstc_recall@fas.rst.prstc': 0.07123840698308784, 'train@fas.rst.prstc_loss@fas.rst.prstc': 2.3037264347076416, 'train@fas.rst.prstc_runtime': 49.2732, 'train@fas.rst.prstc_samples_per_second': 83.209, 'train@fas.rst.prstc_steps_per_second': 2.618, 'epoch': 3.0}
{'loss': 2.3318, 'learning_rate': 1.5000000000000002e-05, 'epoch': 3.0}
{'eval_loss': 2.218853235244751, 'eval_accuracy@fas.rst.prstc': 0.2845691382765531, 'eval_f1@fas.rst.prstc': 0.05021505376344086, 'eval_precision@fas.rst.prstc': 0.040225311942959, 'eval_recall@fas.rst.prstc': 0.07899263483012593, 'eval_loss@fas.rst.prstc': 2.218853235244751, 'eval_runtime': 6.2769, 'eval_samples_per_second': 79.498, 'eval_steps_per_second': 2.549, 'epoch': 3.0}
{'train@fas.rst.prstc_loss': 2.269629716873169, 'train@fas.rst.prstc_accuracy@fas.rst.prstc': 0.2758536585365854, 'train@fas.rst.prstc_f1@fas.rst.prstc': 0.04199486662934371, 'train@fas.rst.prstc_precision@fas.rst.prstc': 0.035576106097143775, 'train@fas.rst.prstc_recall@fas.rst.prstc': 0.0703787854476215, 'train@fas.rst.prstc_loss@fas.rst.prstc': 2.269629955291748, 'train@fas.rst.prstc_runtime': 49.2348, 'train@fas.rst.prstc_samples_per_second': 83.274, 'train@fas.rst.prstc_steps_per_second': 2.62, 'epoch': 4.0}
{'loss': 2.3098, 'learning_rate': 1.3333333333333333e-05, 'epoch': 4.0}
{'eval_loss': 2.193843364715576, 'eval_accuracy@fas.rst.prstc': 0.2905811623246493, 'eval_f1@fas.rst.prstc': 0.04958547021215624, 'eval_precision@fas.rst.prstc': 0.04719985441972612, 'eval_recall@fas.rst.prstc': 0.0802328344024709, 'eval_loss@fas.rst.prstc': 2.193843364715576, 'eval_runtime': 6.2803, 'eval_samples_per_second': 79.454, 'eval_steps_per_second': 2.548, 'epoch': 4.0}
{'train@fas.rst.prstc_loss': 2.207455635070801, 'train@fas.rst.prstc_accuracy@fas.rst.prstc': 0.32195121951219513, 'train@fas.rst.prstc_f1@fas.rst.prstc': 0.05203905924114001, 'train@fas.rst.prstc_precision@fas.rst.prstc': 0.09718159346516181, 'train@fas.rst.prstc_recall@fas.rst.prstc': 0.08394436001984111, 'train@fas.rst.prstc_loss@fas.rst.prstc': 2.2074553966522217, 'train@fas.rst.prstc_runtime': 49.2215, 'train@fas.rst.prstc_samples_per_second': 83.297, 'train@fas.rst.prstc_steps_per_second': 2.621, 'epoch': 5.0}
{'loss': 2.2783, 'learning_rate': 1.1666666666666668e-05, 'epoch': 5.0}
{'eval_loss': 2.1314709186553955, 'eval_accuracy@fas.rst.prstc': 0.31462925851703405, 'eval_f1@fas.rst.prstc': 0.05672265770810004, 'eval_precision@fas.rst.prstc': 0.044072720813985944, 'eval_recall@fas.rst.prstc': 0.08765502494654312, 'eval_loss@fas.rst.prstc': 2.1314711570739746, 'eval_runtime': 6.2821, 'eval_samples_per_second': 79.432, 'eval_steps_per_second': 2.547, 'epoch': 5.0}
{'train@fas.rst.prstc_loss': 2.09291672706604, 'train@fas.rst.prstc_accuracy@fas.rst.prstc': 0.37902439024390244, 'train@fas.rst.prstc_f1@fas.rst.prstc': 0.08612857277461873, 'train@fas.rst.prstc_precision@fas.rst.prstc': 0.08415148579498617, 'train@fas.rst.prstc_recall@fas.rst.prstc': 0.11280412906935011, 'train@fas.rst.prstc_loss@fas.rst.prstc': 2.092916488647461, 'train@fas.rst.prstc_runtime': 49.1902, 'train@fas.rst.prstc_samples_per_second': 83.35, 'train@fas.rst.prstc_steps_per_second': 2.622, 'epoch': 6.0}
{'loss': 2.1942, 'learning_rate': 1e-05, 'epoch': 6.0}
{'eval_loss': 2.0189015865325928, 'eval_accuracy@fas.rst.prstc': 0.38276553106212424, 'eval_f1@fas.rst.prstc': 0.09184303350970018, 'eval_precision@fas.rst.prstc': 0.10963228719599429, 'eval_recall@fas.rst.prstc': 0.1181368245989171, 'eval_loss@fas.rst.prstc': 2.0189015865325928, 'eval_runtime': 6.27, 'eval_samples_per_second': 79.585, 'eval_steps_per_second': 2.552, 'epoch': 6.0}
{'train@fas.rst.prstc_loss': 2.048804759979248, 'train@fas.rst.prstc_accuracy@fas.rst.prstc': 0.38926829268292684, 'train@fas.rst.prstc_f1@fas.rst.prstc': 0.09165699878263668, 'train@fas.rst.prstc_precision@fas.rst.prstc': 0.11791130923025227, 'train@fas.rst.prstc_recall@fas.rst.prstc': 0.1190035728720134, 'train@fas.rst.prstc_loss@fas.rst.prstc': 2.048804521560669, 'train@fas.rst.prstc_runtime': 49.1969, 'train@fas.rst.prstc_samples_per_second': 83.339, 'train@fas.rst.prstc_steps_per_second': 2.622, 'epoch': 7.0}
{'loss': 2.1099, 'learning_rate': 8.333333333333334e-06, 'epoch': 7.0}
{'eval_loss': 1.9722570180892944, 'eval_accuracy@fas.rst.prstc': 0.3967935871743487, 'eval_f1@fas.rst.prstc': 0.10218694959814068, 'eval_precision@fas.rst.prstc': 0.10579990661484903, 'eval_recall@fas.rst.prstc': 0.12813469882832526, 'eval_loss@fas.rst.prstc': 1.972257137298584, 'eval_runtime': 6.2641, 'eval_samples_per_second': 79.66, 'eval_steps_per_second': 2.554, 'epoch': 7.0}
{'train@fas.rst.prstc_loss': 2.019404649734497, 'train@fas.rst.prstc_accuracy@fas.rst.prstc': 0.3904878048780488, 'train@fas.rst.prstc_f1@fas.rst.prstc': 0.09244594161036727, 'train@fas.rst.prstc_precision@fas.rst.prstc': 0.183981821048216, 'train@fas.rst.prstc_recall@fas.rst.prstc': 0.1212628891525795, 'train@fas.rst.prstc_loss@fas.rst.prstc': 2.019404649734497, 'train@fas.rst.prstc_runtime': 49.1984, 'train@fas.rst.prstc_samples_per_second': 83.336, 'train@fas.rst.prstc_steps_per_second': 2.622, 'epoch': 8.0}
{'loss': 2.0692, 'learning_rate': 6.666666666666667e-06, 'epoch': 8.0}
{'eval_loss': 1.9532570838928223, 'eval_accuracy@fas.rst.prstc': 0.3967935871743487, 'eval_f1@fas.rst.prstc': 0.1031170601447631, 'eval_precision@fas.rst.prstc': 0.09331580418978794, 'eval_recall@fas.rst.prstc': 0.13159220217328782, 'eval_loss@fas.rst.prstc': 1.9532570838928223, 'eval_runtime': 6.2714, 'eval_samples_per_second': 79.567, 'eval_steps_per_second': 2.551, 'epoch': 8.0}
{'train@fas.rst.prstc_loss': 2.0101940631866455, 'train@fas.rst.prstc_accuracy@fas.rst.prstc': 0.38634146341463416, 'train@fas.rst.prstc_f1@fas.rst.prstc': 0.0933420690783647, 'train@fas.rst.prstc_precision@fas.rst.prstc': 0.15005318107460938, 'train@fas.rst.prstc_recall@fas.rst.prstc': 0.12071336580494527, 'train@fas.rst.prstc_loss@fas.rst.prstc': 2.0101943016052246, 'train@fas.rst.prstc_runtime': 49.245, 'train@fas.rst.prstc_samples_per_second': 83.257, 'train@fas.rst.prstc_steps_per_second': 2.62, 'epoch': 9.0}
{'loss': 2.0331, 'learning_rate': 5e-06, 'epoch': 9.0}
{'eval_loss': 1.9505187273025513, 'eval_accuracy@fas.rst.prstc': 0.38276553106212424, 'eval_f1@fas.rst.prstc': 0.09998496719096618, 'eval_precision@fas.rst.prstc': 0.08970267516418469, 'eval_recall@fas.rst.prstc': 0.12743444498630754, 'eval_loss@fas.rst.prstc': 1.9505188465118408, 'eval_runtime': 6.2778, 'eval_samples_per_second': 79.487, 'eval_steps_per_second': 2.549, 'epoch': 9.0}
{'train@fas.rst.prstc_loss': 2.000494956970215, 'train@fas.rst.prstc_accuracy@fas.rst.prstc': 0.39, 'train@fas.rst.prstc_f1@fas.rst.prstc': 0.09692119067324893, 'train@fas.rst.prstc_precision@fas.rst.prstc': 0.1375329044407493, 'train@fas.rst.prstc_recall@fas.rst.prstc': 0.12269931962322493, 'train@fas.rst.prstc_loss@fas.rst.prstc': 2.0004947185516357, 'train@fas.rst.prstc_runtime': 49.1968, 'train@fas.rst.prstc_samples_per_second': 83.339, 'train@fas.rst.prstc_steps_per_second': 2.622, 'epoch': 10.0}
{'loss': 2.0346, 'learning_rate': 3.3333333333333333e-06, 'epoch': 10.0}
{'eval_loss': 1.944867730140686, 'eval_accuracy@fas.rst.prstc': 0.3867735470941884, 'eval_f1@fas.rst.prstc': 0.10379708740082264, 'eval_precision@fas.rst.prstc': 0.12339025522699644, 'eval_recall@fas.rst.prstc': 0.1297235568406368, 'eval_loss@fas.rst.prstc': 1.9448673725128174, 'eval_runtime': 6.276, 'eval_samples_per_second': 79.51, 'eval_steps_per_second': 2.549, 'epoch': 10.0}
{'train@fas.rst.prstc_loss': 1.9962645769119263, 'train@fas.rst.prstc_accuracy@fas.rst.prstc': 0.39146341463414636, 'train@fas.rst.prstc_f1@fas.rst.prstc': 0.09785234838953272, 'train@fas.rst.prstc_precision@fas.rst.prstc': 0.14128387267995218, 'train@fas.rst.prstc_recall@fas.rst.prstc': 0.12367842832275763, 'train@fas.rst.prstc_loss@fas.rst.prstc': 1.9962644577026367, 'train@fas.rst.prstc_runtime': 49.2413, 'train@fas.rst.prstc_samples_per_second': 83.263, 'train@fas.rst.prstc_steps_per_second': 2.62, 'epoch': 11.0}
{'loss': 2.0254, 'learning_rate': 1.6666666666666667e-06, 'epoch': 11.0}
{'eval_loss': 1.9389469623565674, 'eval_accuracy@fas.rst.prstc': 0.39478957915831664, 'eval_f1@fas.rst.prstc': 0.10572613261132366, 'eval_precision@fas.rst.prstc': 0.1254787043022337, 'eval_recall@fas.rst.prstc': 0.13207565947784278, 'eval_loss@fas.rst.prstc': 1.9389469623565674, 'eval_runtime': 7.1173, 'eval_samples_per_second': 70.111, 'eval_steps_per_second': 2.248, 'epoch': 11.0}
{'train@fas.rst.prstc_loss': 1.99700129032135, 'train@fas.rst.prstc_accuracy@fas.rst.prstc': 0.3907317073170732, 'train@fas.rst.prstc_f1@fas.rst.prstc': 0.09760744342437186, 'train@fas.rst.prstc_precision@fas.rst.prstc': 0.14326941467590887, 'train@fas.rst.prstc_recall@fas.rst.prstc': 0.12324785099606018, 'train@fas.rst.prstc_loss@fas.rst.prstc': 1.9970014095306396, 'train@fas.rst.prstc_runtime': 49.2782, 'train@fas.rst.prstc_samples_per_second': 83.201, 'train@fas.rst.prstc_steps_per_second': 2.618, 'epoch': 12.0}
{'loss': 2.018, 'learning_rate': 0.0, 'epoch': 12.0}
{'eval_loss': 1.9410477876663208, 'eval_accuracy@fas.rst.prstc': 0.38877755511022044, 'eval_f1@fas.rst.prstc': 0.10401324065330392, 'eval_precision@fas.rst.prstc': 0.12337640612793269, 'eval_recall@fas.rst.prstc': 0.13030326698556433, 'eval_loss@fas.rst.prstc': 1.9410479068756104, 'eval_runtime': 6.2765, 'eval_samples_per_second': 79.503, 'eval_steps_per_second': 2.549, 'epoch': 12.0}
{'train_runtime': 1902.0716, 'train_samples_per_second': 25.867, 'train_steps_per_second': 0.814, 'train_loss': 2.196233921888879, 'epoch': 12.0}
***** train_logger metrics *****
  epoch                    =       12.0
  train_loss               =     1.7563
  train_runtime            = 1:13:43.45
  train_samples_per_second =     25.989
  train_steps_per_second   =      0.814
-------------------------------------------------------------------
Lang1:  fra.sdrt.annodis    Lang2:  fas.rst.prstc
Saving run to:  runs/full_shot/FullShot=v4_fra.sdrt.annodis_fas.rst.prstc_lrfunc=Adafactor_lr=2e-05_plm=bert-base-multilingual-cased
Running with params: BERT_MODEL=bert-base-multilingual-cased lr=2e-05
read 2185 examples
read 528 examples
read 625 examples
read 4100 examples
read 499 examples
read 592 examples
Total prediction labels:  30
BertAdapterModel(
  (shared_parameters): ModuleDict()
  (bert): BertModel(
    (shared_parameters): ModuleDict()
    (invertible_adapters): ModuleDict(
      (fr): NICECouplingBlock(
        (F): Sequential(
          (0): Linear(in_features=384, out_features=192, bias=True)
          (1): Activation_Function_Class(
            (f): NewGELUActivation()
          )
          (2): Linear(in_features=192, out_features=384, bias=True)
        )
        (G): Sequential(
          (0): Linear(in_features=384, out_features=192, bias=True)
          (1): Activation_Function_Class(
            (f): NewGELUActivation()
          )
          (2): Linear(in_features=192, out_features=384, bias=True)
        )
      )
      (ar): NICECouplingBlock(
        (F): Sequential(
          (0): Linear(in_features=384, out_features=192, bias=True)
          (1): Activation_Function_Class(
            (f): ReLU()
          )
          (2): Linear(in_features=192, out_features=384, bias=True)
        )
        (G): Sequential(
          (0): Linear(in_features=384, out_features=192, bias=True)
          (1): Activation_Function_Class(
            (f): ReLU()
          )
          (2): Linear(in_features=192, out_features=384, bias=True)
        )
      )
    )
    (embeddings): BertEmbeddings(
      (word_embeddings): Embedding(119547, 768, padding_idx=0)
      (position_embeddings): Embedding(512, 768)
      (token_type_embeddings): Embedding(2, 768)
      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
      (dropout): Dropout(p=0.1, inplace=False)
    )
    (encoder): BertEncoder(
      (layer): ModuleList(
        (0): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (fr): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (ar): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (1): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (fr): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (ar): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (2): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (fr): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (ar): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (3): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (fr): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (ar): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (4): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (fr): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (ar): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (5): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (fr): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (ar): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (6): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (fr): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (ar): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (7): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (fr): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (ar): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (8): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (fr): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (ar): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (9): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (fr): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (ar): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (10): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (fr): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (ar): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (11): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (fr): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (ar): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
      )
    )
    (pooler): BertPooler(
      (dense): Linear(in_features=768, out_features=768, bias=True)
      (activation): Tanh()
    )
    (prefix_tuning): PrefixTuningPool(
      (prefix_tunings): ModuleDict()
    )
  )
  (heads): ModuleDict(
    (disrpt): ClassificationHead(
      (0): Dropout(p=0.1, inplace=False)
      (1): Linear(in_features=768, out_features=768, bias=True)
      (2): Activation_Function_Class(
        (f): Tanh()
      )
      (3): Dropout(p=0.1, inplace=False)
      (4): Linear(in_features=768, out_features=30, bias=True)
    )
  )
)
{'train@fra.sdrt.annodis_loss': 2.5595383644104004, 'train@fra.sdrt.annodis_accuracy@fra.sdrt.annodis': 0.21098398169336385, 'train@fra.sdrt.annodis_f1@fra.sdrt.annodis': 0.02368036416499691, 'train@fra.sdrt.annodis_precision@fra.sdrt.annodis': 0.056052768696446854, 'train@fra.sdrt.annodis_recall@fra.sdrt.annodis': 0.057622165642457174, 'train@fra.sdrt.annodis_loss@fra.sdrt.annodis': 2.5595383644104004, 'train@fra.sdrt.annodis_runtime': 26.7422, 'train@fra.sdrt.annodis_samples_per_second': 81.706, 'train@fra.sdrt.annodis_steps_per_second': 2.58, 'epoch': 1.0}
{'loss': 2.9492, 'learning_rate': 1.8333333333333333e-05, 'epoch': 1.0}
{'eval_loss': 2.5635759830474854, 'eval_accuracy@fra.sdrt.annodis': 0.20265151515151514, 'eval_f1@fra.sdrt.annodis': 0.021163545568039954, 'eval_precision@fra.sdrt.annodis': 0.025150150150150152, 'eval_recall@fra.sdrt.annodis': 0.05588887606886914, 'eval_loss@fra.sdrt.annodis': 2.5635757446289062, 'eval_runtime': 6.7231, 'eval_samples_per_second': 78.536, 'eval_steps_per_second': 2.529, 'epoch': 1.0}
{'train@fra.sdrt.annodis_loss': 2.3563120365142822, 'train@fra.sdrt.annodis_accuracy@fra.sdrt.annodis': 0.2778032036613272, 'train@fra.sdrt.annodis_f1@fra.sdrt.annodis': 0.05878934266131857, 'train@fra.sdrt.annodis_precision@fra.sdrt.annodis': 0.04712286554712099, 'train@fra.sdrt.annodis_recall@fra.sdrt.annodis': 0.08204883703967752, 'train@fra.sdrt.annodis_loss@fra.sdrt.annodis': 2.356311798095703, 'train@fra.sdrt.annodis_runtime': 26.8113, 'train@fra.sdrt.annodis_samples_per_second': 81.495, 'train@fra.sdrt.annodis_steps_per_second': 2.574, 'epoch': 2.0}
{'loss': 2.4622, 'learning_rate': 1.6666666666666667e-05, 'epoch': 2.0}
{'eval_loss': 2.3627779483795166, 'eval_accuracy@fra.sdrt.annodis': 0.2537878787878788, 'eval_f1@fra.sdrt.annodis': 0.05292975666307284, 'eval_precision@fra.sdrt.annodis': 0.042361111111111106, 'eval_recall@fra.sdrt.annodis': 0.07345490176919738, 'eval_loss@fra.sdrt.annodis': 2.3627779483795166, 'eval_runtime': 6.7395, 'eval_samples_per_second': 78.344, 'eval_steps_per_second': 2.522, 'epoch': 2.0}
{'train@fra.sdrt.annodis_loss': 2.2875595092773438, 'train@fra.sdrt.annodis_accuracy@fra.sdrt.annodis': 0.28466819221967965, 'train@fra.sdrt.annodis_f1@fra.sdrt.annodis': 0.060594691608367515, 'train@fra.sdrt.annodis_precision@fra.sdrt.annodis': 0.07142170960455804, 'train@fra.sdrt.annodis_recall@fra.sdrt.annodis': 0.08313922790006997, 'train@fra.sdrt.annodis_loss@fra.sdrt.annodis': 2.2875595092773438, 'train@fra.sdrt.annodis_runtime': 26.8444, 'train@fra.sdrt.annodis_samples_per_second': 81.395, 'train@fra.sdrt.annodis_steps_per_second': 2.57, 'epoch': 3.0}
{'loss': 2.3488, 'learning_rate': 1.5000000000000002e-05, 'epoch': 3.0}
{'eval_loss': 2.3033697605133057, 'eval_accuracy@fra.sdrt.annodis': 0.26515151515151514, 'eval_f1@fra.sdrt.annodis': 0.054741830487087945, 'eval_precision@fra.sdrt.annodis': 0.04702528060961796, 'eval_recall@fra.sdrt.annodis': 0.07657189900180555, 'eval_loss@fra.sdrt.annodis': 2.3033699989318848, 'eval_runtime': 6.7015, 'eval_samples_per_second': 78.789, 'eval_steps_per_second': 2.537, 'epoch': 3.0}
{'train@fra.sdrt.annodis_loss': 2.2348837852478027, 'train@fra.sdrt.annodis_accuracy@fra.sdrt.annodis': 0.30160183066361557, 'train@fra.sdrt.annodis_f1@fra.sdrt.annodis': 0.06738761475655888, 'train@fra.sdrt.annodis_precision@fra.sdrt.annodis': 0.0874467643598251, 'train@fra.sdrt.annodis_recall@fra.sdrt.annodis': 0.08922088586700767, 'train@fra.sdrt.annodis_loss@fra.sdrt.annodis': 2.2348837852478027, 'train@fra.sdrt.annodis_runtime': 26.8097, 'train@fra.sdrt.annodis_samples_per_second': 81.5, 'train@fra.sdrt.annodis_steps_per_second': 2.574, 'epoch': 4.0}
{'loss': 2.2947, 'learning_rate': 1.3333333333333333e-05, 'epoch': 4.0}
{'eval_loss': 2.2573280334472656, 'eval_accuracy@fra.sdrt.annodis': 0.2859848484848485, 'eval_f1@fra.sdrt.annodis': 0.0606561342625603, 'eval_precision@fra.sdrt.annodis': 0.05115124061900674, 'eval_recall@fra.sdrt.annodis': 0.08253996467911316, 'eval_loss@fra.sdrt.annodis': 2.2573280334472656, 'eval_runtime': 6.7256, 'eval_samples_per_second': 78.506, 'eval_steps_per_second': 2.528, 'epoch': 4.0}
{'train@fra.sdrt.annodis_loss': 2.186418294906616, 'train@fra.sdrt.annodis_accuracy@fra.sdrt.annodis': 0.3336384439359268, 'train@fra.sdrt.annodis_f1@fra.sdrt.annodis': 0.07756834412850841, 'train@fra.sdrt.annodis_precision@fra.sdrt.annodis': 0.12768202286398012, 'train@fra.sdrt.annodis_recall@fra.sdrt.annodis': 0.10293968313323477, 'train@fra.sdrt.annodis_loss@fra.sdrt.annodis': 2.186418294906616, 'train@fra.sdrt.annodis_runtime': 26.8313, 'train@fra.sdrt.annodis_samples_per_second': 81.435, 'train@fra.sdrt.annodis_steps_per_second': 2.572, 'epoch': 5.0}
{'loss': 2.2402, 'learning_rate': 1.1666666666666668e-05, 'epoch': 5.0}
{'eval_loss': 2.2187840938568115, 'eval_accuracy@fra.sdrt.annodis': 0.3106060606060606, 'eval_f1@fra.sdrt.annodis': 0.06871195861211223, 'eval_precision@fra.sdrt.annodis': 0.10732370775681144, 'eval_recall@fra.sdrt.annodis': 0.0941271377459352, 'eval_loss@fra.sdrt.annodis': 2.2187840938568115, 'eval_runtime': 6.7191, 'eval_samples_per_second': 78.582, 'eval_steps_per_second': 2.53, 'epoch': 5.0}
{'train@fra.sdrt.annodis_loss': 2.1421780586242676, 'train@fra.sdrt.annodis_accuracy@fra.sdrt.annodis': 0.36109839816933637, 'train@fra.sdrt.annodis_f1@fra.sdrt.annodis': 0.10627102405521766, 'train@fra.sdrt.annodis_precision@fra.sdrt.annodis': 0.12057306424417615, 'train@fra.sdrt.annodis_recall@fra.sdrt.annodis': 0.12388581483522781, 'train@fra.sdrt.annodis_loss@fra.sdrt.annodis': 2.1421782970428467, 'train@fra.sdrt.annodis_runtime': 26.8168, 'train@fra.sdrt.annodis_samples_per_second': 81.479, 'train@fra.sdrt.annodis_steps_per_second': 2.573, 'epoch': 6.0}
{'loss': 2.2012, 'learning_rate': 1e-05, 'epoch': 6.0}
{'eval_loss': 2.1812636852264404, 'eval_accuracy@fra.sdrt.annodis': 0.32196969696969696, 'eval_f1@fra.sdrt.annodis': 0.08033995734187589, 'eval_precision@fra.sdrt.annodis': 0.16406383341867212, 'eval_recall@fra.sdrt.annodis': 0.1021646817785536, 'eval_loss@fra.sdrt.annodis': 2.1812636852264404, 'eval_runtime': 6.6985, 'eval_samples_per_second': 78.824, 'eval_steps_per_second': 2.538, 'epoch': 6.0}
{'train@fra.sdrt.annodis_loss': 2.100646495819092, 'train@fra.sdrt.annodis_accuracy@fra.sdrt.annodis': 0.39679633867276887, 'train@fra.sdrt.annodis_f1@fra.sdrt.annodis': 0.12595736593297596, 'train@fra.sdrt.annodis_precision@fra.sdrt.annodis': 0.1287541636886881, 'train@fra.sdrt.annodis_recall@fra.sdrt.annodis': 0.1452588905795319, 'train@fra.sdrt.annodis_loss@fra.sdrt.annodis': 2.100646734237671, 'train@fra.sdrt.annodis_runtime': 26.8177, 'train@fra.sdrt.annodis_samples_per_second': 81.476, 'train@fra.sdrt.annodis_steps_per_second': 2.573, 'epoch': 7.0}
{'loss': 2.1646, 'learning_rate': 8.333333333333334e-06, 'epoch': 7.0}
{'eval_loss': 2.147970676422119, 'eval_accuracy@fra.sdrt.annodis': 0.3371212121212121, 'eval_f1@fra.sdrt.annodis': 0.09201597271044573, 'eval_precision@fra.sdrt.annodis': 0.10321835664645028, 'eval_recall@fra.sdrt.annodis': 0.1116683397780495, 'eval_loss@fra.sdrt.annodis': 2.147970676422119, 'eval_runtime': 6.7244, 'eval_samples_per_second': 78.52, 'eval_steps_per_second': 2.528, 'epoch': 7.0}
{'train@fra.sdrt.annodis_loss': 2.064617156982422, 'train@fra.sdrt.annodis_accuracy@fra.sdrt.annodis': 0.4068649885583524, 'train@fra.sdrt.annodis_f1@fra.sdrt.annodis': 0.1314842083866642, 'train@fra.sdrt.annodis_precision@fra.sdrt.annodis': 0.12651378963484, 'train@fra.sdrt.annodis_recall@fra.sdrt.annodis': 0.1556871510658126, 'train@fra.sdrt.annodis_loss@fra.sdrt.annodis': 2.064617395401001, 'train@fra.sdrt.annodis_runtime': 26.8076, 'train@fra.sdrt.annodis_samples_per_second': 81.507, 'train@fra.sdrt.annodis_steps_per_second': 2.574, 'epoch': 8.0}
{'loss': 2.1332, 'learning_rate': 6.666666666666667e-06, 'epoch': 8.0}
{'eval_loss': 2.1191141605377197, 'eval_accuracy@fra.sdrt.annodis': 0.3409090909090909, 'eval_f1@fra.sdrt.annodis': 0.09446181045822624, 'eval_precision@fra.sdrt.annodis': 0.09624548182613957, 'eval_recall@fra.sdrt.annodis': 0.11534447534390876, 'eval_loss@fra.sdrt.annodis': 2.1191141605377197, 'eval_runtime': 6.7598, 'eval_samples_per_second': 78.108, 'eval_steps_per_second': 2.515, 'epoch': 8.0}
{'train@fra.sdrt.annodis_loss': 2.037104368209839, 'train@fra.sdrt.annodis_accuracy@fra.sdrt.annodis': 0.41601830663615563, 'train@fra.sdrt.annodis_f1@fra.sdrt.annodis': 0.13381790830918952, 'train@fra.sdrt.annodis_precision@fra.sdrt.annodis': 0.12833865566716174, 'train@fra.sdrt.annodis_recall@fra.sdrt.annodis': 0.1618316127769983, 'train@fra.sdrt.annodis_loss@fra.sdrt.annodis': 2.037104606628418, 'train@fra.sdrt.annodis_runtime': 26.8057, 'train@fra.sdrt.annodis_samples_per_second': 81.512, 'train@fra.sdrt.annodis_steps_per_second': 2.574, 'epoch': 9.0}
{'loss': 2.1026, 'learning_rate': 5e-06, 'epoch': 9.0}
{'eval_loss': 2.0974583625793457, 'eval_accuracy@fra.sdrt.annodis': 0.3503787878787879, 'eval_f1@fra.sdrt.annodis': 0.09610837914097173, 'eval_precision@fra.sdrt.annodis': 0.09567480077656004, 'eval_recall@fra.sdrt.annodis': 0.11922196572700866, 'eval_loss@fra.sdrt.annodis': 2.0974583625793457, 'eval_runtime': 6.7195, 'eval_samples_per_second': 78.577, 'eval_steps_per_second': 2.53, 'epoch': 9.0}
{'train@fra.sdrt.annodis_loss': 2.016948938369751, 'train@fra.sdrt.annodis_accuracy@fra.sdrt.annodis': 0.41556064073226545, 'train@fra.sdrt.annodis_f1@fra.sdrt.annodis': 0.13400734801423525, 'train@fra.sdrt.annodis_precision@fra.sdrt.annodis': 0.12602570562710785, 'train@fra.sdrt.annodis_recall@fra.sdrt.annodis': 0.1623015036243405, 'train@fra.sdrt.annodis_loss@fra.sdrt.annodis': 2.016948938369751, 'train@fra.sdrt.annodis_runtime': 27.0455, 'train@fra.sdrt.annodis_samples_per_second': 80.79, 'train@fra.sdrt.annodis_steps_per_second': 2.551, 'epoch': 10.0}
{'loss': 2.0763, 'learning_rate': 3.3333333333333333e-06, 'epoch': 10.0}
{'eval_loss': 2.0820887088775635, 'eval_accuracy@fra.sdrt.annodis': 0.3484848484848485, 'eval_f1@fra.sdrt.annodis': 0.09719412205737908, 'eval_precision@fra.sdrt.annodis': 0.09468211999188456, 'eval_recall@fra.sdrt.annodis': 0.11942869419460671, 'eval_loss@fra.sdrt.annodis': 2.0820887088775635, 'eval_runtime': 6.7248, 'eval_samples_per_second': 78.515, 'eval_steps_per_second': 2.528, 'epoch': 10.0}
{'train@fra.sdrt.annodis_loss': 2.0046534538269043, 'train@fra.sdrt.annodis_accuracy@fra.sdrt.annodis': 0.4205949656750572, 'train@fra.sdrt.annodis_f1@fra.sdrt.annodis': 0.1356277848438635, 'train@fra.sdrt.annodis_precision@fra.sdrt.annodis': 0.12636703591292994, 'train@fra.sdrt.annodis_recall@fra.sdrt.annodis': 0.1652578234604941, 'train@fra.sdrt.annodis_loss@fra.sdrt.annodis': 2.004653215408325, 'train@fra.sdrt.annodis_runtime': 26.8498, 'train@fra.sdrt.annodis_samples_per_second': 81.379, 'train@fra.sdrt.annodis_steps_per_second': 2.57, 'epoch': 11.0}
{'loss': 2.0542, 'learning_rate': 1.6666666666666667e-06, 'epoch': 11.0}
{'eval_loss': 2.0714588165283203, 'eval_accuracy@fra.sdrt.annodis': 0.3503787878787879, 'eval_f1@fra.sdrt.annodis': 0.09879580239777626, 'eval_precision@fra.sdrt.annodis': 0.09277783470160772, 'eval_recall@fra.sdrt.annodis': 0.12219616755031534, 'eval_loss@fra.sdrt.annodis': 2.071458339691162, 'eval_runtime': 6.7284, 'eval_samples_per_second': 78.474, 'eval_steps_per_second': 2.527, 'epoch': 11.0}
{'train@fra.sdrt.annodis_loss': 2.0004043579101562, 'train@fra.sdrt.annodis_accuracy@fra.sdrt.annodis': 0.4215102974828375, 'train@fra.sdrt.annodis_f1@fra.sdrt.annodis': 0.13581354920598057, 'train@fra.sdrt.annodis_precision@fra.sdrt.annodis': 0.12538960448874886, 'train@fra.sdrt.annodis_recall@fra.sdrt.annodis': 0.16550997909931867, 'train@fra.sdrt.annodis_loss@fra.sdrt.annodis': 2.0004043579101562, 'train@fra.sdrt.annodis_runtime': 26.8117, 'train@fra.sdrt.annodis_samples_per_second': 81.494, 'train@fra.sdrt.annodis_steps_per_second': 2.574, 'epoch': 12.0}
{'loss': 2.0503, 'learning_rate': 0.0, 'epoch': 12.0}
{'eval_loss': 2.0682826042175293, 'eval_accuracy@fra.sdrt.annodis': 0.3503787878787879, 'eval_f1@fra.sdrt.annodis': 0.1001765797567443, 'eval_precision@fra.sdrt.annodis': 0.09450426777161441, 'eval_recall@fra.sdrt.annodis': 0.12288468621932974, 'eval_loss@fra.sdrt.annodis': 2.0682826042175293, 'eval_runtime': 6.7472, 'eval_samples_per_second': 78.255, 'eval_steps_per_second': 2.52, 'epoch': 12.0}
{'train_runtime': 1072.8516, 'train_samples_per_second': 24.44, 'train_steps_per_second': 0.772, 'train_loss': 2.2564600331771776, 'epoch': 12.0}
***** train_log metrics *****
  epoch                    =       12.0
  train_loss               =     2.2565
  train_runtime            = 0:17:52.85
  train_samples_per_second =      24.44
  train_steps_per_second   =      0.772
{'train@fas.rst.prstc_loss': 2.3740739822387695, 'train@fas.rst.prstc_accuracy@fas.rst.prstc': 0.23804878048780487, 'train@fas.rst.prstc_f1@fas.rst.prstc': 0.02287772116613352, 'train@fas.rst.prstc_precision@fas.rst.prstc': 0.037520649285355166, 'train@fas.rst.prstc_recall@fas.rst.prstc': 0.05890226030401249, 'train@fas.rst.prstc_loss@fas.rst.prstc': 2.3740742206573486, 'train@fas.rst.prstc_runtime': 49.1783, 'train@fas.rst.prstc_samples_per_second': 83.37, 'train@fas.rst.prstc_steps_per_second': 2.623, 'epoch': 1.0}
{'loss': 2.6331, 'learning_rate': 1.8333333333333333e-05, 'epoch': 1.0}
{'eval_loss': 2.293849468231201, 'eval_accuracy@fas.rst.prstc': 0.24649298597194388, 'eval_f1@fas.rst.prstc': 0.027385984427141265, 'eval_precision@fas.rst.prstc': 0.08299866131191432, 'eval_recall@fas.rst.prstc': 0.0672463768115942, 'eval_loss@fas.rst.prstc': 2.293849468231201, 'eval_runtime': 6.2359, 'eval_samples_per_second': 80.02, 'eval_steps_per_second': 2.566, 'epoch': 1.0}
{'train@fas.rst.prstc_loss': 2.344763994216919, 'train@fas.rst.prstc_accuracy@fas.rst.prstc': 0.2702439024390244, 'train@fas.rst.prstc_f1@fas.rst.prstc': 0.04439915884971751, 'train@fas.rst.prstc_precision@fas.rst.prstc': 0.03307738792149655, 'train@fas.rst.prstc_recall@fas.rst.prstc': 0.07243177904003938, 'train@fas.rst.prstc_loss@fas.rst.prstc': 2.344763994216919, 'train@fas.rst.prstc_runtime': 49.1424, 'train@fas.rst.prstc_samples_per_second': 83.431, 'train@fas.rst.prstc_steps_per_second': 2.625, 'epoch': 2.0}
{'loss': 2.3767, 'learning_rate': 1.6666666666666667e-05, 'epoch': 2.0}
{'eval_loss': 2.260779619216919, 'eval_accuracy@fas.rst.prstc': 0.3106212424849699, 'eval_f1@fas.rst.prstc': 0.056092538401663165, 'eval_precision@fas.rst.prstc': 0.041367669753086415, 'eval_recall@fas.rst.prstc': 0.0870943216916132, 'eval_loss@fas.rst.prstc': 2.260779619216919, 'eval_runtime': 6.2465, 'eval_samples_per_second': 79.884, 'eval_steps_per_second': 2.561, 'epoch': 2.0}
{'train@fas.rst.prstc_loss': 2.3305230140686035, 'train@fas.rst.prstc_accuracy@fas.rst.prstc': 0.26682926829268294, 'train@fas.rst.prstc_f1@fas.rst.prstc': 0.03851822547642109, 'train@fas.rst.prstc_precision@fas.rst.prstc': 0.033034238129936444, 'train@fas.rst.prstc_recall@fas.rst.prstc': 0.067622134505739, 'train@fas.rst.prstc_loss@fas.rst.prstc': 2.3305232524871826, 'train@fas.rst.prstc_runtime': 49.1665, 'train@fas.rst.prstc_samples_per_second': 83.39, 'train@fas.rst.prstc_steps_per_second': 2.624, 'epoch': 3.0}
{'loss': 2.352, 'learning_rate': 1.5000000000000002e-05, 'epoch': 3.0}
{'eval_loss': 2.247164726257324, 'eval_accuracy@fas.rst.prstc': 0.280561122244489, 'eval_f1@fas.rst.prstc': 0.04513939734344337, 'eval_precision@fas.rst.prstc': 0.041889367816091955, 'eval_recall@fas.rst.prstc': 0.07726775956284153, 'eval_loss@fas.rst.prstc': 2.247164726257324, 'eval_runtime': 6.2503, 'eval_samples_per_second': 79.836, 'eval_steps_per_second': 2.56, 'epoch': 3.0}
{'train@fas.rst.prstc_loss': 2.3085217475891113, 'train@fas.rst.prstc_accuracy@fas.rst.prstc': 0.25121951219512195, 'train@fas.rst.prstc_f1@fas.rst.prstc': 0.031364903281411824, 'train@fas.rst.prstc_precision@fas.rst.prstc': 0.03300898698388238, 'train@fas.rst.prstc_recall@fas.rst.prstc': 0.06283174480921665, 'train@fas.rst.prstc_loss@fas.rst.prstc': 2.3085215091705322, 'train@fas.rst.prstc_runtime': 49.2067, 'train@fas.rst.prstc_samples_per_second': 83.322, 'train@fas.rst.prstc_steps_per_second': 2.622, 'epoch': 4.0}
{'loss': 2.3364, 'learning_rate': 1.3333333333333333e-05, 'epoch': 4.0}
{'eval_loss': 2.2289676666259766, 'eval_accuracy@fas.rst.prstc': 0.26452905811623245, 'eval_f1@fas.rst.prstc': 0.03810724079584573, 'eval_precision@fas.rst.prstc': 0.04372423596304194, 'eval_recall@fas.rst.prstc': 0.07253029223093371, 'eval_loss@fas.rst.prstc': 2.2289671897888184, 'eval_runtime': 6.2727, 'eval_samples_per_second': 79.551, 'eval_steps_per_second': 2.551, 'epoch': 4.0}
{'train@fas.rst.prstc_loss': 2.2409775257110596, 'train@fas.rst.prstc_accuracy@fas.rst.prstc': 0.31097560975609756, 'train@fas.rst.prstc_f1@fas.rst.prstc': 0.05173728405875673, 'train@fas.rst.prstc_precision@fas.rst.prstc': 0.038602682349750894, 'train@fas.rst.prstc_recall@fas.rst.prstc': 0.08247039568691633, 'train@fas.rst.prstc_loss@fas.rst.prstc': 2.2409772872924805, 'train@fas.rst.prstc_runtime': 49.1692, 'train@fas.rst.prstc_samples_per_second': 83.386, 'train@fas.rst.prstc_steps_per_second': 2.624, 'epoch': 5.0}
{'loss': 2.3017, 'learning_rate': 1.1666666666666668e-05, 'epoch': 5.0}
{'eval_loss': 2.150771379470825, 'eval_accuracy@fas.rst.prstc': 0.3366733466933868, 'eval_f1@fas.rst.prstc': 0.06215888097123252, 'eval_precision@fas.rst.prstc': 0.04731619763622202, 'eval_recall@fas.rst.prstc': 0.09459729151817535, 'eval_loss@fas.rst.prstc': 2.1507716178894043, 'eval_runtime': 6.2414, 'eval_samples_per_second': 79.949, 'eval_steps_per_second': 2.564, 'epoch': 5.0}
{'train@fas.rst.prstc_loss': 2.1811978816986084, 'train@fas.rst.prstc_accuracy@fas.rst.prstc': 0.33219512195121953, 'train@fas.rst.prstc_f1@fas.rst.prstc': 0.05700877199954954, 'train@fas.rst.prstc_precision@fas.rst.prstc': 0.045858631491299325, 'train@fas.rst.prstc_recall@fas.rst.prstc': 0.0890163987035076, 'train@fas.rst.prstc_loss@fas.rst.prstc': 2.1811976432800293, 'train@fas.rst.prstc_runtime': 49.1683, 'train@fas.rst.prstc_samples_per_second': 83.387, 'train@fas.rst.prstc_steps_per_second': 2.624, 'epoch': 6.0}
{'loss': 2.2333, 'learning_rate': 1e-05, 'epoch': 6.0}
{'eval_loss': 2.0804312229156494, 'eval_accuracy@fas.rst.prstc': 0.3627254509018036, 'eval_f1@fas.rst.prstc': 0.06908424908424908, 'eval_precision@fas.rst.prstc': 0.05700081630468238, 'eval_recall@fas.rst.prstc': 0.10229983368971253, 'eval_loss@fas.rst.prstc': 2.0804309844970703, 'eval_runtime': 6.2559, 'eval_samples_per_second': 79.765, 'eval_steps_per_second': 2.558, 'epoch': 6.0}
{'train@fas.rst.prstc_loss': 2.1646878719329834, 'train@fas.rst.prstc_accuracy@fas.rst.prstc': 0.33926829268292685, 'train@fas.rst.prstc_f1@fas.rst.prstc': 0.05815256376803707, 'train@fas.rst.prstc_precision@fas.rst.prstc': 0.04651476144255293, 'train@fas.rst.prstc_recall@fas.rst.prstc': 0.09083961789844143, 'train@fas.rst.prstc_loss@fas.rst.prstc': 2.1646876335144043, 'train@fas.rst.prstc_runtime': 49.1782, 'train@fas.rst.prstc_samples_per_second': 83.37, 'train@fas.rst.prstc_steps_per_second': 2.623, 'epoch': 7.0}
{'loss': 2.196, 'learning_rate': 8.333333333333334e-06, 'epoch': 7.0}
{'eval_loss': 2.057757616043091, 'eval_accuracy@fas.rst.prstc': 0.3687374749498998, 'eval_f1@fas.rst.prstc': 0.07029159104658196, 'eval_precision@fas.rst.prstc': 0.057922994937383426, 'eval_recall@fas.rst.prstc': 0.10397244000950344, 'eval_loss@fas.rst.prstc': 2.057757616043091, 'eval_runtime': 6.2574, 'eval_samples_per_second': 79.746, 'eval_steps_per_second': 2.557, 'epoch': 7.0}
{'train@fas.rst.prstc_loss': 2.133326292037964, 'train@fas.rst.prstc_accuracy@fas.rst.prstc': 0.3495121951219512, 'train@fas.rst.prstc_f1@fas.rst.prstc': 0.06454568398852406, 'train@fas.rst.prstc_precision@fas.rst.prstc': 0.10267513939596941, 'train@fas.rst.prstc_recall@fas.rst.prstc': 0.09540569878188034, 'train@fas.rst.prstc_loss@fas.rst.prstc': 2.1333260536193848, 'train@fas.rst.prstc_runtime': 49.229, 'train@fas.rst.prstc_samples_per_second': 83.284, 'train@fas.rst.prstc_steps_per_second': 2.62, 'epoch': 8.0}
{'loss': 2.1753, 'learning_rate': 6.666666666666667e-06, 'epoch': 8.0}
{'eval_loss': 2.030888319015503, 'eval_accuracy@fas.rst.prstc': 0.3687374749498998, 'eval_f1@fas.rst.prstc': 0.07003892710499654, 'eval_precision@fas.rst.prstc': 0.057117892540427755, 'eval_recall@fas.rst.prstc': 0.10393917795200759, 'eval_loss@fas.rst.prstc': 2.030888080596924, 'eval_runtime': 6.2698, 'eval_samples_per_second': 79.588, 'eval_steps_per_second': 2.552, 'epoch': 8.0}
{'train@fas.rst.prstc_loss': 2.104637384414673, 'train@fas.rst.prstc_accuracy@fas.rst.prstc': 0.3726829268292683, 'train@fas.rst.prstc_f1@fas.rst.prstc': 0.08296746898309856, 'train@fas.rst.prstc_precision@fas.rst.prstc': 0.08841525600819714, 'train@fas.rst.prstc_recall@fas.rst.prstc': 0.10915492833719345, 'train@fas.rst.prstc_loss@fas.rst.prstc': 2.104637622833252, 'train@fas.rst.prstc_runtime': 49.1561, 'train@fas.rst.prstc_samples_per_second': 83.408, 'train@fas.rst.prstc_steps_per_second': 2.624, 'epoch': 9.0}
{'loss': 2.1499, 'learning_rate': 5e-06, 'epoch': 9.0}
{'eval_loss': 2.0067331790924072, 'eval_accuracy@fas.rst.prstc': 0.3787575150300601, 'eval_f1@fas.rst.prstc': 0.08906787023730815, 'eval_precision@fas.rst.prstc': 0.11218099077637285, 'eval_recall@fas.rst.prstc': 0.11489414912905929, 'eval_loss@fas.rst.prstc': 2.006732940673828, 'eval_runtime': 6.2657, 'eval_samples_per_second': 79.64, 'eval_steps_per_second': 2.554, 'epoch': 9.0}
{'train@fas.rst.prstc_loss': 2.077784299850464, 'train@fas.rst.prstc_accuracy@fas.rst.prstc': 0.3851219512195122, 'train@fas.rst.prstc_f1@fas.rst.prstc': 0.09078597226725492, 'train@fas.rst.prstc_precision@fas.rst.prstc': 0.08581679947728854, 'train@fas.rst.prstc_recall@fas.rst.prstc': 0.11783247853791835, 'train@fas.rst.prstc_loss@fas.rst.prstc': 2.077784299850464, 'train@fas.rst.prstc_runtime': 49.1313, 'train@fas.rst.prstc_samples_per_second': 83.45, 'train@fas.rst.prstc_steps_per_second': 2.626, 'epoch': 10.0}
{'loss': 2.126, 'learning_rate': 3.3333333333333333e-06, 'epoch': 10.0}
{'eval_loss': 1.9850181341171265, 'eval_accuracy@fas.rst.prstc': 0.3967935871743487, 'eval_f1@fas.rst.prstc': 0.10343922121173306, 'eval_precision@fas.rst.prstc': 0.10658121898153348, 'eval_recall@fas.rst.prstc': 0.12826774705830862, 'eval_loss@fas.rst.prstc': 1.985018253326416, 'eval_runtime': 6.248, 'eval_samples_per_second': 79.865, 'eval_steps_per_second': 2.561, 'epoch': 10.0}
{'train@fas.rst.prstc_loss': 2.064561367034912, 'train@fas.rst.prstc_accuracy@fas.rst.prstc': 0.38682926829268294, 'train@fas.rst.prstc_f1@fas.rst.prstc': 0.09114806002730608, 'train@fas.rst.prstc_precision@fas.rst.prstc': 0.08430579536058957, 'train@fas.rst.prstc_recall@fas.rst.prstc': 0.11881258878693517, 'train@fas.rst.prstc_loss@fas.rst.prstc': 2.064561128616333, 'train@fas.rst.prstc_runtime': 49.1777, 'train@fas.rst.prstc_samples_per_second': 83.371, 'train@fas.rst.prstc_steps_per_second': 2.623, 'epoch': 11.0}
{'loss': 2.1108, 'learning_rate': 1.6666666666666667e-06, 'epoch': 11.0}
{'eval_loss': 1.9742764234542847, 'eval_accuracy@fas.rst.prstc': 0.3967935871743487, 'eval_f1@fas.rst.prstc': 0.10337553417521746, 'eval_precision@fas.rst.prstc': 0.10645378151260504, 'eval_recall@fas.rst.prstc': 0.12826774705830862, 'eval_loss@fas.rst.prstc': 1.9742763042449951, 'eval_runtime': 6.2615, 'eval_samples_per_second': 79.694, 'eval_steps_per_second': 2.555, 'epoch': 11.0}
{'train@fas.rst.prstc_loss': 2.0613327026367188, 'train@fas.rst.prstc_accuracy@fas.rst.prstc': 0.3897560975609756, 'train@fas.rst.prstc_f1@fas.rst.prstc': 0.09128634037791727, 'train@fas.rst.prstc_precision@fas.rst.prstc': 0.0833408659484717, 'train@fas.rst.prstc_recall@fas.rst.prstc': 0.11960193666553136, 'train@fas.rst.prstc_loss@fas.rst.prstc': 2.0613327026367188, 'train@fas.rst.prstc_runtime': 49.2522, 'train@fas.rst.prstc_samples_per_second': 83.245, 'train@fas.rst.prstc_steps_per_second': 2.619, 'epoch': 12.0}
{'loss': 2.1012, 'learning_rate': 0.0, 'epoch': 12.0}
{'eval_loss': 1.9727177619934082, 'eval_accuracy@fas.rst.prstc': 0.4028056112224449, 'eval_f1@fas.rst.prstc': 0.10572753690319452, 'eval_precision@fas.rst.prstc': 0.1073307911772076, 'eval_recall@fas.rst.prstc': 0.13111502919808432, 'eval_loss@fas.rst.prstc': 1.9727176427841187, 'eval_runtime': 6.2565, 'eval_samples_per_second': 79.758, 'eval_steps_per_second': 2.557, 'epoch': 12.0}
{'train_runtime': 1901.5264, 'train_samples_per_second': 25.874, 'train_steps_per_second': 0.814, 'train_loss': 2.257688734266493, 'epoch': 12.0}
***** train_logger metrics *****
  epoch                    =       12.0
  train_loss               =     2.2565
  train_runtime            = 0:17:52.85
  train_samples_per_second =      24.44
  train_steps_per_second   =      0.772
-------------------------------------------------------------------
Lang1:  nld.rst.nldt    Lang2:  fas.rst.prstc
Saving run to:  runs/full_shot/FullShot=v4_nld.rst.nldt_fas.rst.prstc_lrfunc=Adafactor_lr=2e-05_plm=bert-base-multilingual-cased
Running with params: BERT_MODEL=bert-base-multilingual-cased lr=2e-05
read 1608 examples
read 331 examples
read 326 examples
read 4100 examples
read 499 examples
read 592 examples
Total prediction labels:  42
BertAdapterModel(
  (shared_parameters): ModuleDict()
  (bert): BertModel(
    (shared_parameters): ModuleDict()
    (invertible_adapters): ModuleDict(
      (de): NICECouplingBlock(
        (F): Sequential(
          (0): Linear(in_features=384, out_features=192, bias=True)
          (1): Activation_Function_Class(
            (f): NewGELUActivation()
          )
          (2): Linear(in_features=192, out_features=384, bias=True)
        )
        (G): Sequential(
          (0): Linear(in_features=384, out_features=192, bias=True)
          (1): Activation_Function_Class(
            (f): NewGELUActivation()
          )
          (2): Linear(in_features=192, out_features=384, bias=True)
        )
      )
      (ar): NICECouplingBlock(
        (F): Sequential(
          (0): Linear(in_features=384, out_features=192, bias=True)
          (1): Activation_Function_Class(
            (f): ReLU()
          )
          (2): Linear(in_features=192, out_features=384, bias=True)
        )
        (G): Sequential(
          (0): Linear(in_features=384, out_features=192, bias=True)
          (1): Activation_Function_Class(
            (f): ReLU()
          )
          (2): Linear(in_features=192, out_features=384, bias=True)
        )
      )
    )
    (embeddings): BertEmbeddings(
      (word_embeddings): Embedding(119547, 768, padding_idx=0)
      (position_embeddings): Embedding(512, 768)
      (token_type_embeddings): Embedding(2, 768)
      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
      (dropout): Dropout(p=0.1, inplace=False)
    )
    (encoder): BertEncoder(
      (layer): ModuleList(
        (0): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (de): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (ar): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (1): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (de): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (ar): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (2): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (de): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (ar): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (3): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (de): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (ar): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (4): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (de): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (ar): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (5): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (de): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (ar): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (6): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (de): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (ar): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (7): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (de): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (ar): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (8): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (de): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (ar): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (9): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (de): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (ar): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (10): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (de): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (ar): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (11): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (de): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (ar): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
      )
    )
    (pooler): BertPooler(
      (dense): Linear(in_features=768, out_features=768, bias=True)
      (activation): Tanh()
    )
    (prefix_tuning): PrefixTuningPool(
      (prefix_tunings): ModuleDict()
    )
  )
  (heads): ModuleDict(
    (disrpt): ClassificationHead(
      (0): Dropout(p=0.1, inplace=False)
      (1): Linear(in_features=768, out_features=768, bias=True)
      (2): Activation_Function_Class(
        (f): Tanh()
      )
      (3): Dropout(p=0.1, inplace=False)
      (4): Linear(in_features=768, out_features=42, bias=True)
    )
  )
)
{'train@nld.rst.nldt_loss': 3.2452526092529297, 'train@nld.rst.nldt_accuracy@nld.rst.nldt': 0.26430348258706465, 'train@nld.rst.nldt_f1@nld.rst.nldt': 0.01306566650270536, 'train@nld.rst.nldt_precision@nld.rst.nldt': 0.00825948383084577, 'train@nld.rst.nldt_recall@nld.rst.nldt': 0.03125, 'train@nld.rst.nldt_loss@nld.rst.nldt': 3.2452526092529297, 'train@nld.rst.nldt_runtime': 19.8079, 'train@nld.rst.nldt_samples_per_second': 81.18, 'train@nld.rst.nldt_steps_per_second': 2.575, 'epoch': 1.0}
{'loss': 3.5226, 'learning_rate': 1.8333333333333333e-05, 'epoch': 1.0}
{'eval_loss': 3.213930606842041, 'eval_accuracy@nld.rst.nldt': 0.27794561933534745, 'eval_f1@nld.rst.nldt': 0.016110673321075214, 'eval_precision@nld.rst.nldt': 0.010294282197605462, 'eval_recall@nld.rst.nldt': 0.037037037037037035, 'eval_loss@nld.rst.nldt': 3.213930606842041, 'eval_runtime': 4.3923, 'eval_samples_per_second': 75.36, 'eval_steps_per_second': 2.504, 'epoch': 1.0}
{'train@nld.rst.nldt_loss': 2.9005212783813477, 'train@nld.rst.nldt_accuracy@nld.rst.nldt': 0.26430348258706465, 'train@nld.rst.nldt_f1@nld.rst.nldt': 0.01306566650270536, 'train@nld.rst.nldt_precision@nld.rst.nldt': 0.00825948383084577, 'train@nld.rst.nldt_recall@nld.rst.nldt': 0.03125, 'train@nld.rst.nldt_loss@nld.rst.nldt': 2.9005212783813477, 'train@nld.rst.nldt_runtime': 19.9053, 'train@nld.rst.nldt_samples_per_second': 80.782, 'train@nld.rst.nldt_steps_per_second': 2.562, 'epoch': 2.0}
{'loss': 3.0645, 'learning_rate': 1.6666666666666667e-05, 'epoch': 2.0}
{'eval_loss': 2.832736015319824, 'eval_accuracy@nld.rst.nldt': 0.27794561933534745, 'eval_f1@nld.rst.nldt': 0.016110673321075214, 'eval_precision@nld.rst.nldt': 0.010294282197605462, 'eval_recall@nld.rst.nldt': 0.037037037037037035, 'eval_loss@nld.rst.nldt': 2.8327362537384033, 'eval_runtime': 4.4144, 'eval_samples_per_second': 74.982, 'eval_steps_per_second': 2.492, 'epoch': 2.0}
{'train@nld.rst.nldt_loss': 2.8064544200897217, 'train@nld.rst.nldt_accuracy@nld.rst.nldt': 0.26492537313432835, 'train@nld.rst.nldt_f1@nld.rst.nldt': 0.014019420319823546, 'train@nld.rst.nldt_precision@nld.rst.nldt': 0.011805555555555555, 'train@nld.rst.nldt_recall@nld.rst.nldt': 0.03167250233426704, 'train@nld.rst.nldt_loss@nld.rst.nldt': 2.8064544200897217, 'train@nld.rst.nldt_runtime': 19.8958, 'train@nld.rst.nldt_samples_per_second': 80.821, 'train@nld.rst.nldt_steps_per_second': 2.563, 'epoch': 3.0}
{'loss': 2.8877, 'learning_rate': 1.5000000000000002e-05, 'epoch': 3.0}
{'eval_loss': 2.742511034011841, 'eval_accuracy@nld.rst.nldt': 0.2809667673716012, 'eval_f1@nld.rst.nldt': 0.01816380973103328, 'eval_precision@nld.rst.nldt': 0.01967946539811983, 'eval_recall@nld.rst.nldt': 0.03809523809523809, 'eval_loss@nld.rst.nldt': 2.74251127243042, 'eval_runtime': 4.4077, 'eval_samples_per_second': 75.096, 'eval_steps_per_second': 2.496, 'epoch': 3.0}
{'train@nld.rst.nldt_loss': 2.758049964904785, 'train@nld.rst.nldt_accuracy@nld.rst.nldt': 0.277363184079602, 'train@nld.rst.nldt_f1@nld.rst.nldt': 0.022482320606731547, 'train@nld.rst.nldt_precision@nld.rst.nldt': 0.03084095984273008, 'train@nld.rst.nldt_recall@nld.rst.nldt': 0.03704131652661065, 'train@nld.rst.nldt_loss@nld.rst.nldt': 2.758049964904785, 'train@nld.rst.nldt_runtime': 19.904, 'train@nld.rst.nldt_samples_per_second': 80.788, 'train@nld.rst.nldt_steps_per_second': 2.562, 'epoch': 4.0}
{'loss': 2.7905, 'learning_rate': 1.3333333333333333e-05, 'epoch': 4.0}
{'eval_loss': 2.700316905975342, 'eval_accuracy@nld.rst.nldt': 0.3021148036253776, 'eval_f1@nld.rst.nldt': 0.036278593310590766, 'eval_precision@nld.rst.nldt': 0.05683945424569329, 'eval_recall@nld.rst.nldt': 0.04950029394473839, 'eval_loss@nld.rst.nldt': 2.700316905975342, 'eval_runtime': 4.3945, 'eval_samples_per_second': 75.321, 'eval_steps_per_second': 2.503, 'epoch': 4.0}
{'train@nld.rst.nldt_loss': 2.7234439849853516, 'train@nld.rst.nldt_accuracy@nld.rst.nldt': 0.2904228855721393, 'train@nld.rst.nldt_f1@nld.rst.nldt': 0.029971085715768696, 'train@nld.rst.nldt_precision@nld.rst.nldt': 0.03128786928151442, 'train@nld.rst.nldt_recall@nld.rst.nldt': 0.0437640056022409, 'train@nld.rst.nldt_loss@nld.rst.nldt': 2.7234437465667725, 'train@nld.rst.nldt_runtime': 19.9096, 'train@nld.rst.nldt_samples_per_second': 80.765, 'train@nld.rst.nldt_steps_per_second': 2.562, 'epoch': 5.0}
{'loss': 2.7586, 'learning_rate': 1.1666666666666668e-05, 'epoch': 5.0}
{'eval_loss': 2.6691734790802, 'eval_accuracy@nld.rst.nldt': 0.30513595166163143, 'eval_f1@nld.rst.nldt': 0.035684536942964945, 'eval_precision@nld.rst.nldt': 0.04173312803302016, 'eval_recall@nld.rst.nldt': 0.05055849500293945, 'eval_loss@nld.rst.nldt': 2.6691734790802, 'eval_runtime': 4.4101, 'eval_samples_per_second': 75.054, 'eval_steps_per_second': 2.494, 'epoch': 5.0}
{'train@nld.rst.nldt_loss': 2.691235065460205, 'train@nld.rst.nldt_accuracy@nld.rst.nldt': 0.2997512437810945, 'train@nld.rst.nldt_f1@nld.rst.nldt': 0.03330376731851539, 'train@nld.rst.nldt_precision@nld.rst.nldt': 0.028735025061631433, 'train@nld.rst.nldt_recall@nld.rst.nldt': 0.04911531279178338, 'train@nld.rst.nldt_loss@nld.rst.nldt': 2.691234588623047, 'train@nld.rst.nldt_runtime': 19.9047, 'train@nld.rst.nldt_samples_per_second': 80.785, 'train@nld.rst.nldt_steps_per_second': 2.562, 'epoch': 6.0}
{'loss': 2.7269, 'learning_rate': 1e-05, 'epoch': 6.0}
{'eval_loss': 2.64206600189209, 'eval_accuracy@nld.rst.nldt': 0.31722054380664655, 'eval_f1@nld.rst.nldt': 0.04139433551198257, 'eval_precision@nld.rst.nldt': 0.04148860398860399, 'eval_recall@nld.rst.nldt': 0.057789535567313345, 'eval_loss@nld.rst.nldt': 2.6420657634735107, 'eval_runtime': 4.4125, 'eval_samples_per_second': 75.015, 'eval_steps_per_second': 2.493, 'epoch': 6.0}
{'train@nld.rst.nldt_loss': 2.662989616394043, 'train@nld.rst.nldt_accuracy@nld.rst.nldt': 0.29912935323383083, 'train@nld.rst.nldt_f1@nld.rst.nldt': 0.03380395294472147, 'train@nld.rst.nldt_precision@nld.rst.nldt': 0.026405387861084064, 'train@nld.rst.nldt_recall@nld.rst.nldt': 0.0521842903828198, 'train@nld.rst.nldt_loss@nld.rst.nldt': 2.662989616394043, 'train@nld.rst.nldt_runtime': 19.9198, 'train@nld.rst.nldt_samples_per_second': 80.724, 'train@nld.rst.nldt_steps_per_second': 2.56, 'epoch': 7.0}
{'loss': 2.7041, 'learning_rate': 8.333333333333334e-06, 'epoch': 7.0}
{'eval_loss': 2.619107484817505, 'eval_accuracy@nld.rst.nldt': 0.32326283987915405, 'eval_f1@nld.rst.nldt': 0.04296668006552474, 'eval_precision@nld.rst.nldt': 0.03939993737699052, 'eval_recall@nld.rst.nldt': 0.06156097436290673, 'eval_loss@nld.rst.nldt': 2.619107484817505, 'eval_runtime': 4.4381, 'eval_samples_per_second': 74.582, 'eval_steps_per_second': 2.479, 'epoch': 7.0}
{'train@nld.rst.nldt_loss': 2.640432596206665, 'train@nld.rst.nldt_accuracy@nld.rst.nldt': 0.30845771144278605, 'train@nld.rst.nldt_f1@nld.rst.nldt': 0.03656177867616646, 'train@nld.rst.nldt_precision@nld.rst.nldt': 0.02750285168048326, 'train@nld.rst.nldt_recall@nld.rst.nldt': 0.05800128384687208, 'train@nld.rst.nldt_loss@nld.rst.nldt': 2.640432596206665, 'train@nld.rst.nldt_runtime': 19.8732, 'train@nld.rst.nldt_samples_per_second': 80.913, 'train@nld.rst.nldt_steps_per_second': 2.566, 'epoch': 8.0}
{'loss': 2.6856, 'learning_rate': 6.666666666666667e-06, 'epoch': 8.0}
{'eval_loss': 2.6020517349243164, 'eval_accuracy@nld.rst.nldt': 0.32628398791540786, 'eval_f1@nld.rst.nldt': 0.04194293508488086, 'eval_precision@nld.rst.nldt': 0.035906146607253615, 'eval_recall@nld.rst.nldt': 0.06527362421082228, 'eval_loss@nld.rst.nldt': 2.6020517349243164, 'eval_runtime': 4.4189, 'eval_samples_per_second': 74.905, 'eval_steps_per_second': 2.489, 'epoch': 8.0}
{'train@nld.rst.nldt_loss': 2.6274538040161133, 'train@nld.rst.nldt_accuracy@nld.rst.nldt': 0.30907960199004975, 'train@nld.rst.nldt_f1@nld.rst.nldt': 0.03682826007384169, 'train@nld.rst.nldt_precision@nld.rst.nldt': 0.027694072914061372, 'train@nld.rst.nldt_recall@nld.rst.nldt': 0.058190943043884225, 'train@nld.rst.nldt_loss@nld.rst.nldt': 2.6274538040161133, 'train@nld.rst.nldt_runtime': 19.9514, 'train@nld.rst.nldt_samples_per_second': 80.596, 'train@nld.rst.nldt_steps_per_second': 2.556, 'epoch': 9.0}
{'loss': 2.6625, 'learning_rate': 5e-06, 'epoch': 9.0}
{'eval_loss': 2.59074330329895, 'eval_accuracy@nld.rst.nldt': 0.32326283987915405, 'eval_f1@nld.rst.nldt': 0.04114262584207939, 'eval_precision@nld.rst.nldt': 0.035534056823594534, 'eval_recall@nld.rst.nldt': 0.063216011042098, 'eval_loss@nld.rst.nldt': 2.59074330329895, 'eval_runtime': 4.4305, 'eval_samples_per_second': 74.709, 'eval_steps_per_second': 2.483, 'epoch': 9.0}
{'train@nld.rst.nldt_loss': 2.6181647777557373, 'train@nld.rst.nldt_accuracy@nld.rst.nldt': 0.30907960199004975, 'train@nld.rst.nldt_f1@nld.rst.nldt': 0.03692218202836771, 'train@nld.rst.nldt_precision@nld.rst.nldt': 0.027809319549356545, 'train@nld.rst.nldt_recall@nld.rst.nldt': 0.05824929971988796, 'train@nld.rst.nldt_loss@nld.rst.nldt': 2.6181647777557373, 'train@nld.rst.nldt_runtime': 19.8933, 'train@nld.rst.nldt_samples_per_second': 80.831, 'train@nld.rst.nldt_steps_per_second': 2.564, 'epoch': 10.0}
{'loss': 2.6524, 'learning_rate': 3.3333333333333333e-06, 'epoch': 10.0}
{'eval_loss': 2.5824129581451416, 'eval_accuracy@nld.rst.nldt': 0.32326283987915405, 'eval_f1@nld.rst.nldt': 0.04114262584207939, 'eval_precision@nld.rst.nldt': 0.035534056823594534, 'eval_recall@nld.rst.nldt': 0.063216011042098, 'eval_loss@nld.rst.nldt': 2.5824131965637207, 'eval_runtime': 4.4007, 'eval_samples_per_second': 75.215, 'eval_steps_per_second': 2.5, 'epoch': 10.0}
{'train@nld.rst.nldt_loss': 2.6111061573028564, 'train@nld.rst.nldt_accuracy@nld.rst.nldt': 0.31281094527363185, 'train@nld.rst.nldt_f1@nld.rst.nldt': 0.03773779093470275, 'train@nld.rst.nldt_precision@nld.rst.nldt': 0.028157649308004164, 'train@nld.rst.nldt_recall@nld.rst.nldt': 0.060086951447245567, 'train@nld.rst.nldt_loss@nld.rst.nldt': 2.6111061573028564, 'train@nld.rst.nldt_runtime': 19.8608, 'train@nld.rst.nldt_samples_per_second': 80.963, 'train@nld.rst.nldt_steps_per_second': 2.568, 'epoch': 11.0}
{'loss': 2.639, 'learning_rate': 1.6666666666666667e-06, 'epoch': 11.0}
{'eval_loss': 2.5768542289733887, 'eval_accuracy@nld.rst.nldt': 0.32628398791540786, 'eval_f1@nld.rst.nldt': 0.04194293508488086, 'eval_precision@nld.rst.nldt': 0.035906146607253615, 'eval_recall@nld.rst.nldt': 0.06527362421082228, 'eval_loss@nld.rst.nldt': 2.5768542289733887, 'eval_runtime': 4.3957, 'eval_samples_per_second': 75.302, 'eval_steps_per_second': 2.502, 'epoch': 11.0}
{'train@nld.rst.nldt_loss': 2.6083850860595703, 'train@nld.rst.nldt_accuracy@nld.rst.nldt': 0.31405472636815923, 'train@nld.rst.nldt_f1@nld.rst.nldt': 0.038024569583931134, 'train@nld.rst.nldt_precision@nld.rst.nldt': 0.028328113652868553, 'train@nld.rst.nldt_recall@nld.rst.nldt': 0.060641339869281045, 'train@nld.rst.nldt_loss@nld.rst.nldt': 2.6083850860595703, 'train@nld.rst.nldt_runtime': 19.9179, 'train@nld.rst.nldt_samples_per_second': 80.732, 'train@nld.rst.nldt_steps_per_second': 2.561, 'epoch': 12.0}
{'loss': 2.6424, 'learning_rate': 0.0, 'epoch': 12.0}
{'eval_loss': 2.575185775756836, 'eval_accuracy@nld.rst.nldt': 0.32628398791540786, 'eval_f1@nld.rst.nldt': 0.04194293508488086, 'eval_precision@nld.rst.nldt': 0.035906146607253615, 'eval_recall@nld.rst.nldt': 0.06527362421082228, 'eval_loss@nld.rst.nldt': 2.575186014175415, 'eval_runtime': 4.4064, 'eval_samples_per_second': 75.118, 'eval_steps_per_second': 2.496, 'epoch': 12.0}
{'train_runtime': 785.2974, 'train_samples_per_second': 24.572, 'train_steps_per_second': 0.779, 'train_loss': 2.8114055558746935, 'epoch': 12.0}
***** train_log metrics *****
  epoch                    =       12.0
  train_loss               =     2.8114
  train_runtime            = 0:13:05.29
  train_samples_per_second =     24.572
  train_steps_per_second   =      0.779
{'train@fas.rst.prstc_loss': 2.4326889514923096, 'train@fas.rst.prstc_accuracy@fas.rst.prstc': 0.23780487804878048, 'train@fas.rst.prstc_f1@fas.rst.prstc': 0.02260214430599826, 'train@fas.rst.prstc_precision@fas.rst.prstc': 0.013988522238163558, 'train@fas.rst.prstc_recall@fas.rst.prstc': 0.058823529411764705, 'train@fas.rst.prstc_loss@fas.rst.prstc': 2.4326887130737305, 'train@fas.rst.prstc_runtime': 49.3323, 'train@fas.rst.prstc_samples_per_second': 83.11, 'train@fas.rst.prstc_steps_per_second': 2.615, 'epoch': 1.0}
{'loss': 2.8548, 'learning_rate': 1.8333333333333333e-05, 'epoch': 1.0}
{'eval_loss': 2.348007917404175, 'eval_accuracy@fas.rst.prstc': 0.24448897795591182, 'eval_f1@fas.rst.prstc': 0.026194310252281264, 'eval_precision@fas.rst.prstc': 0.016299265197060788, 'eval_recall@fas.rst.prstc': 0.06666666666666667, 'eval_loss@fas.rst.prstc': 2.348007917404175, 'eval_runtime': 6.3395, 'eval_samples_per_second': 78.713, 'eval_steps_per_second': 2.524, 'epoch': 1.0}
{'train@fas.rst.prstc_loss': 2.3641412258148193, 'train@fas.rst.prstc_accuracy@fas.rst.prstc': 0.26634146341463416, 'train@fas.rst.prstc_f1@fas.rst.prstc': 0.039260000503387155, 'train@fas.rst.prstc_precision@fas.rst.prstc': 0.03263683159011808, 'train@fas.rst.prstc_recall@fas.rst.prstc': 0.06771306012857953, 'train@fas.rst.prstc_loss@fas.rst.prstc': 2.3641409873962402, 'train@fas.rst.prstc_runtime': 49.2299, 'train@fas.rst.prstc_samples_per_second': 83.283, 'train@fas.rst.prstc_steps_per_second': 2.62, 'epoch': 2.0}
{'loss': 2.4127, 'learning_rate': 1.6666666666666667e-05, 'epoch': 2.0}
{'eval_loss': 2.274657726287842, 'eval_accuracy@fas.rst.prstc': 0.2785571142284569, 'eval_f1@fas.rst.prstc': 0.04575150109212346, 'eval_precision@fas.rst.prstc': 0.038621617636216174, 'eval_recall@fas.rst.prstc': 0.07685435970539321, 'eval_loss@fas.rst.prstc': 2.274657964706421, 'eval_runtime': 6.3499, 'eval_samples_per_second': 78.584, 'eval_steps_per_second': 2.52, 'epoch': 2.0}
{'train@fas.rst.prstc_loss': 2.3428075313568115, 'train@fas.rst.prstc_accuracy@fas.rst.prstc': 0.25170731707317073, 'train@fas.rst.prstc_f1@fas.rst.prstc': 0.031157419858339835, 'train@fas.rst.prstc_precision@fas.rst.prstc': 0.03469697032738573, 'train@fas.rst.prstc_recall@fas.rst.prstc': 0.0629156103248719, 'train@fas.rst.prstc_loss@fas.rst.prstc': 2.3428072929382324, 'train@fas.rst.prstc_runtime': 49.265, 'train@fas.rst.prstc_samples_per_second': 83.223, 'train@fas.rst.prstc_steps_per_second': 2.618, 'epoch': 3.0}
{'loss': 2.367, 'learning_rate': 1.5000000000000002e-05, 'epoch': 3.0}
{'eval_loss': 2.256011962890625, 'eval_accuracy@fas.rst.prstc': 0.2545090180360721, 'eval_f1@fas.rst.prstc': 0.03277650842007277, 'eval_precision@fas.rst.prstc': 0.043333333333333335, 'eval_recall@fas.rst.prstc': 0.0695984794488002, 'eval_loss@fas.rst.prstc': 2.256011962890625, 'eval_runtime': 6.3422, 'eval_samples_per_second': 78.68, 'eval_steps_per_second': 2.523, 'epoch': 3.0}
{'train@fas.rst.prstc_loss': 2.328842878341675, 'train@fas.rst.prstc_accuracy@fas.rst.prstc': 0.23780487804878048, 'train@fas.rst.prstc_f1@fas.rst.prstc': 0.022735181188370812, 'train@fas.rst.prstc_precision@fas.rst.prstc': 0.028693704044117647, 'train@fas.rst.prstc_recall@fas.rst.prstc': 0.05883272894536975, 'train@fas.rst.prstc_loss@fas.rst.prstc': 2.328842878341675, 'train@fas.rst.prstc_runtime': 49.2109, 'train@fas.rst.prstc_samples_per_second': 83.315, 'train@fas.rst.prstc_steps_per_second': 2.621, 'epoch': 4.0}
{'loss': 2.3504, 'learning_rate': 1.3333333333333333e-05, 'epoch': 4.0}
{'eval_loss': 2.2480719089508057, 'eval_accuracy@fas.rst.prstc': 0.24448897795591182, 'eval_f1@fas.rst.prstc': 0.026194310252281264, 'eval_precision@fas.rst.prstc': 0.016299265197060788, 'eval_recall@fas.rst.prstc': 0.06666666666666667, 'eval_loss@fas.rst.prstc': 2.2480721473693848, 'eval_runtime': 6.3456, 'eval_samples_per_second': 78.637, 'eval_steps_per_second': 2.521, 'epoch': 4.0}
{'train@fas.rst.prstc_loss': 2.3181490898132324, 'train@fas.rst.prstc_accuracy@fas.rst.prstc': 0.2651219512195122, 'train@fas.rst.prstc_f1@fas.rst.prstc': 0.036937209487239095, 'train@fas.rst.prstc_precision@fas.rst.prstc': 0.033471002474656716, 'train@fas.rst.prstc_recall@fas.rst.prstc': 0.06689622712150872, 'train@fas.rst.prstc_loss@fas.rst.prstc': 2.3181493282318115, 'train@fas.rst.prstc_runtime': 49.2292, 'train@fas.rst.prstc_samples_per_second': 83.284, 'train@fas.rst.prstc_steps_per_second': 2.62, 'epoch': 5.0}
{'loss': 2.3403, 'learning_rate': 1.1666666666666668e-05, 'epoch': 5.0}
{'eval_loss': 2.2339866161346436, 'eval_accuracy@fas.rst.prstc': 0.27655310621242485, 'eval_f1@fas.rst.prstc': 0.043232440033262313, 'eval_precision@fas.rst.prstc': 0.04397942843064794, 'eval_recall@fas.rst.prstc': 0.07604181515799477, 'eval_loss@fas.rst.prstc': 2.2339866161346436, 'eval_runtime': 6.3332, 'eval_samples_per_second': 78.791, 'eval_steps_per_second': 2.526, 'epoch': 5.0}
{'train@fas.rst.prstc_loss': 2.3081085681915283, 'train@fas.rst.prstc_accuracy@fas.rst.prstc': 0.24170731707317072, 'train@fas.rst.prstc_f1@fas.rst.prstc': 0.025501116093127666, 'train@fas.rst.prstc_precision@fas.rst.prstc': 0.031322409798080104, 'train@fas.rst.prstc_recall@fas.rst.prstc': 0.05998202881807388, 'train@fas.rst.prstc_loss@fas.rst.prstc': 2.3081085681915283, 'train@fas.rst.prstc_runtime': 49.2679, 'train@fas.rst.prstc_samples_per_second': 83.219, 'train@fas.rst.prstc_steps_per_second': 2.618, 'epoch': 6.0}
{'loss': 2.3246, 'learning_rate': 1e-05, 'epoch': 6.0}
{'eval_loss': 2.2316153049468994, 'eval_accuracy@fas.rst.prstc': 0.250501002004008, 'eval_f1@fas.rst.prstc': 0.029755649622611926, 'eval_precision@fas.rst.prstc': 0.04983096686950642, 'eval_recall@fas.rst.prstc': 0.06840579710144928, 'eval_loss@fas.rst.prstc': 2.2316150665283203, 'eval_runtime': 6.3365, 'eval_samples_per_second': 78.751, 'eval_steps_per_second': 2.525, 'epoch': 6.0}
{'train@fas.rst.prstc_loss': 2.2886722087860107, 'train@fas.rst.prstc_accuracy@fas.rst.prstc': 0.2809756097560976, 'train@fas.rst.prstc_f1@fas.rst.prstc': 0.04180086168521, 'train@fas.rst.prstc_precision@fas.rst.prstc': 0.03405253993489288, 'train@fas.rst.prstc_recall@fas.rst.prstc': 0.07156295797096798, 'train@fas.rst.prstc_loss@fas.rst.prstc': 2.2886719703674316, 'train@fas.rst.prstc_runtime': 49.2347, 'train@fas.rst.prstc_samples_per_second': 83.275, 'train@fas.rst.prstc_steps_per_second': 2.62, 'epoch': 7.0}
{'loss': 2.3185, 'learning_rate': 8.333333333333334e-06, 'epoch': 7.0}
{'eval_loss': 2.2071237564086914, 'eval_accuracy@fas.rst.prstc': 0.2905811623246493, 'eval_f1@fas.rst.prstc': 0.04822055137844612, 'eval_precision@fas.rst.prstc': 0.04328511530398323, 'eval_recall@fas.rst.prstc': 0.08016631028747921, 'eval_loss@fas.rst.prstc': 2.2071237564086914, 'eval_runtime': 6.3216, 'eval_samples_per_second': 78.935, 'eval_steps_per_second': 2.531, 'epoch': 7.0}
{'train@fas.rst.prstc_loss': 2.2572381496429443, 'train@fas.rst.prstc_accuracy@fas.rst.prstc': 0.30048780487804877, 'train@fas.rst.prstc_f1@fas.rst.prstc': 0.047490228350858255, 'train@fas.rst.prstc_precision@fas.rst.prstc': 0.034811751595140286, 'train@fas.rst.prstc_recall@fas.rst.prstc': 0.07797182375405154, 'train@fas.rst.prstc_loss@fas.rst.prstc': 2.2572381496429443, 'train@fas.rst.prstc_runtime': 49.1942, 'train@fas.rst.prstc_samples_per_second': 83.343, 'train@fas.rst.prstc_steps_per_second': 2.622, 'epoch': 8.0}
{'loss': 2.299, 'learning_rate': 6.666666666666667e-06, 'epoch': 8.0}
{'eval_loss': 2.177780866622925, 'eval_accuracy@fas.rst.prstc': 0.312625250501002, 'eval_f1@fas.rst.prstc': 0.055346429428502854, 'eval_precision@fas.rst.prstc': 0.04227328408626897, 'eval_recall@fas.rst.prstc': 0.08697552862912805, 'eval_loss@fas.rst.prstc': 2.177781105041504, 'eval_runtime': 6.3249, 'eval_samples_per_second': 78.895, 'eval_steps_per_second': 2.53, 'epoch': 8.0}
{'train@fas.rst.prstc_loss': 2.2194721698760986, 'train@fas.rst.prstc_accuracy@fas.rst.prstc': 0.30878048780487805, 'train@fas.rst.prstc_f1@fas.rst.prstc': 0.049628795796358094, 'train@fas.rst.prstc_precision@fas.rst.prstc': 0.03583106060120659, 'train@fas.rst.prstc_recall@fas.rst.prstc': 0.08085106382978724, 'train@fas.rst.prstc_loss@fas.rst.prstc': 2.2194719314575195, 'train@fas.rst.prstc_runtime': 49.2347, 'train@fas.rst.prstc_samples_per_second': 83.275, 'train@fas.rst.prstc_steps_per_second': 2.62, 'epoch': 9.0}
{'loss': 2.2616, 'learning_rate': 5e-06, 'epoch': 9.0}
{'eval_loss': 2.1374149322509766, 'eval_accuracy@fas.rst.prstc': 0.3226452905811623, 'eval_f1@fas.rst.prstc': 0.05800532906612932, 'eval_precision@fas.rst.prstc': 0.042761203667829005, 'eval_recall@fas.rst.prstc': 0.09023996198622, 'eval_loss@fas.rst.prstc': 2.1374151706695557, 'eval_runtime': 6.3369, 'eval_samples_per_second': 78.746, 'eval_steps_per_second': 2.525, 'epoch': 9.0}
{'train@fas.rst.prstc_loss': 2.184678077697754, 'train@fas.rst.prstc_accuracy@fas.rst.prstc': 0.32170731707317074, 'train@fas.rst.prstc_f1@fas.rst.prstc': 0.05270836107809971, 'train@fas.rst.prstc_precision@fas.rst.prstc': 0.03848999548751556, 'train@fas.rst.prstc_recall@fas.rst.prstc': 0.08502380111892002, 'train@fas.rst.prstc_loss@fas.rst.prstc': 2.184677839279175, 'train@fas.rst.prstc_runtime': 49.2421, 'train@fas.rst.prstc_samples_per_second': 83.262, 'train@fas.rst.prstc_steps_per_second': 2.62, 'epoch': 10.0}
{'loss': 2.2347, 'learning_rate': 3.3333333333333333e-06, 'epoch': 10.0}
{'eval_loss': 2.1037232875823975, 'eval_accuracy@fas.rst.prstc': 0.3346693386773547, 'eval_f1@fas.rst.prstc': 0.060968360617196876, 'eval_precision@fas.rst.prstc': 0.045465318547510326, 'eval_recall@fas.rst.prstc': 0.09395105725825612, 'eval_loss@fas.rst.prstc': 2.1037232875823975, 'eval_runtime': 7.0446, 'eval_samples_per_second': 70.834, 'eval_steps_per_second': 2.271, 'epoch': 10.0}
{'train@fas.rst.prstc_loss': 2.166079521179199, 'train@fas.rst.prstc_accuracy@fas.rst.prstc': 0.33439024390243904, 'train@fas.rst.prstc_f1@fas.rst.prstc': 0.05496565686267088, 'train@fas.rst.prstc_precision@fas.rst.prstc': 0.040350571589320844, 'train@fas.rst.prstc_recall@fas.rst.prstc': 0.08870382850357819, 'train@fas.rst.prstc_loss@fas.rst.prstc': 2.166079521179199, 'train@fas.rst.prstc_runtime': 49.3159, 'train@fas.rst.prstc_samples_per_second': 83.138, 'train@fas.rst.prstc_steps_per_second': 2.616, 'epoch': 11.0}
{'loss': 2.206, 'learning_rate': 1.6666666666666667e-06, 'epoch': 11.0}
{'eval_loss': 2.08496356010437, 'eval_accuracy@fas.rst.prstc': 0.3486973947895792, 'eval_f1@fas.rst.prstc': 0.06361895394153458, 'eval_precision@fas.rst.prstc': 0.04762704054765666, 'eval_recall@fas.rst.prstc': 0.09797576621525303, 'eval_loss@fas.rst.prstc': 2.084963798522949, 'eval_runtime': 6.3515, 'eval_samples_per_second': 78.564, 'eval_steps_per_second': 2.519, 'epoch': 11.0}
{'train@fas.rst.prstc_loss': 2.1626522541046143, 'train@fas.rst.prstc_accuracy@fas.rst.prstc': 0.3378048780487805, 'train@fas.rst.prstc_f1@fas.rst.prstc': 0.05543923649592981, 'train@fas.rst.prstc_precision@fas.rst.prstc': 0.04059839013210606, 'train@fas.rst.prstc_recall@fas.rst.prstc': 0.08956687312131617, 'train@fas.rst.prstc_loss@fas.rst.prstc': 2.1626522541046143, 'train@fas.rst.prstc_runtime': 49.2808, 'train@fas.rst.prstc_samples_per_second': 83.197, 'train@fas.rst.prstc_steps_per_second': 2.618, 'epoch': 12.0}
{'loss': 2.1922, 'learning_rate': 0.0, 'epoch': 12.0}
{'eval_loss': 2.0830419063568115, 'eval_accuracy@fas.rst.prstc': 0.35470941883767537, 'eval_f1@fas.rst.prstc': 0.06468306844828227, 'eval_precision@fas.rst.prstc': 0.048368845551944144, 'eval_recall@fas.rst.prstc': 0.09964837253504395, 'eval_loss@fas.rst.prstc': 2.0830419063568115, 'eval_runtime': 6.3556, 'eval_samples_per_second': 78.513, 'eval_steps_per_second': 2.517, 'epoch': 12.0}
{'train_runtime': 1904.3833, 'train_samples_per_second': 25.835, 'train_steps_per_second': 0.813, 'train_loss': 2.346813054047814, 'epoch': 12.0}
***** train_logger metrics *****
  epoch                    =       12.0
  train_loss               =     2.8114
  train_runtime            = 0:13:05.29
  train_samples_per_second =     24.572
  train_steps_per_second   =      0.779
-------------------------------------------------------------------
Lang1:  por.rst.cstn    Lang2:  fas.rst.prstc
Saving run to:  runs/full_shot/FullShot=v4_por.rst.cstn_fas.rst.prstc_lrfunc=Adafactor_lr=2e-05_plm=bert-base-multilingual-cased
Running with params: BERT_MODEL=bert-base-multilingual-cased lr=2e-05
read 4148 examples
read 573 examples
read 272 examples
read 4100 examples
read 499 examples
read 592 examples
Total prediction labels:  39
BertAdapterModel(
  (shared_parameters): ModuleDict()
  (bert): BertModel(
    (shared_parameters): ModuleDict()
    (invertible_adapters): ModuleDict(
      (pt): NICECouplingBlock(
        (F): Sequential(
          (0): Linear(in_features=384, out_features=192, bias=True)
          (1): Activation_Function_Class(
            (f): NewGELUActivation()
          )
          (2): Linear(in_features=192, out_features=384, bias=True)
        )
        (G): Sequential(
          (0): Linear(in_features=384, out_features=192, bias=True)
          (1): Activation_Function_Class(
            (f): NewGELUActivation()
          )
          (2): Linear(in_features=192, out_features=384, bias=True)
        )
      )
      (ar): NICECouplingBlock(
        (F): Sequential(
          (0): Linear(in_features=384, out_features=192, bias=True)
          (1): Activation_Function_Class(
            (f): ReLU()
          )
          (2): Linear(in_features=192, out_features=384, bias=True)
        )
        (G): Sequential(
          (0): Linear(in_features=384, out_features=192, bias=True)
          (1): Activation_Function_Class(
            (f): ReLU()
          )
          (2): Linear(in_features=192, out_features=384, bias=True)
        )
      )
    )
    (embeddings): BertEmbeddings(
      (word_embeddings): Embedding(119547, 768, padding_idx=0)
      (position_embeddings): Embedding(512, 768)
      (token_type_embeddings): Embedding(2, 768)
      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
      (dropout): Dropout(p=0.1, inplace=False)
    )
    (encoder): BertEncoder(
      (layer): ModuleList(
        (0): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (pt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (ar): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (1): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (pt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (ar): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (2): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (pt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (ar): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (3): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (pt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (ar): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (4): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (pt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (ar): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (5): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (pt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (ar): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (6): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (pt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (ar): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (7): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (pt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (ar): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (8): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (pt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (ar): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (9): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (pt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (ar): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (10): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (pt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (ar): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (11): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (pt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (ar): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
      )
    )
    (pooler): BertPooler(
      (dense): Linear(in_features=768, out_features=768, bias=True)
      (activation): Tanh()
    )
    (prefix_tuning): PrefixTuningPool(
      (prefix_tunings): ModuleDict()
    )
  )
  (heads): ModuleDict(
    (pt): BertStyleMaskedLMHead(
      (0): Linear(in_features=768, out_features=768, bias=True)
      (1): Activation_Function_Class(
        (f): GELUActivation()
      )
      (2): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
      (3): Linear(in_features=768, out_features=119547, bias=True)
    )
    (disrpt): ClassificationHead(
      (0): Dropout(p=0.1, inplace=False)
      (1): Linear(in_features=768, out_features=768, bias=True)
      (2): Activation_Function_Class(
        (f): Tanh()
      )
      (3): Dropout(p=0.1, inplace=False)
      (4): Linear(in_features=768, out_features=39, bias=True)
    )
  )
)
{'train@por.rst.cstn_loss': 2.4873809814453125, 'train@por.rst.cstn_accuracy@por.rst.cstn': 0.2774831243972999, 'train@por.rst.cstn_f1@por.rst.cstn': 0.013575674655595393, 'train@por.rst.cstn_precision@por.rst.cstn': 0.008671347637415621, 'train@por.rst.cstn_recall@por.rst.cstn': 0.03125, 'train@por.rst.cstn_loss@por.rst.cstn': 2.4873807430267334, 'train@por.rst.cstn_runtime': 50.6306, 'train@por.rst.cstn_samples_per_second': 81.927, 'train@por.rst.cstn_steps_per_second': 2.568, 'epoch': 1.0}
{'loss': 3.0153, 'learning_rate': 1.8333333333333333e-05, 'epoch': 1.0}
{'eval_loss': 2.5671184062957764, 'eval_accuracy@por.rst.cstn': 0.28097731239092494, 'eval_f1@por.rst.cstn': 0.019940549913301957, 'eval_precision@por.rst.cstn': 0.012771696017769315, 'eval_recall@por.rst.cstn': 0.045454545454545456, 'eval_loss@por.rst.cstn': 2.5671186447143555, 'eval_runtime': 7.3028, 'eval_samples_per_second': 78.463, 'eval_steps_per_second': 2.465, 'epoch': 1.0}
{'train@por.rst.cstn_loss': 2.1870059967041016, 'train@por.rst.cstn_accuracy@por.rst.cstn': 0.3878977820636451, 'train@por.rst.cstn_f1@por.rst.cstn': 0.05850108021193913, 'train@por.rst.cstn_precision@por.rst.cstn': 0.06615843502958278, 'train@por.rst.cstn_recall@por.rst.cstn': 0.06778859449954956, 'train@por.rst.cstn_loss@por.rst.cstn': 2.1870062351226807, 'train@por.rst.cstn_runtime': 50.6769, 'train@por.rst.cstn_samples_per_second': 81.852, 'train@por.rst.cstn_steps_per_second': 2.565, 'epoch': 2.0}
{'loss': 2.3661, 'learning_rate': 1.6666666666666667e-05, 'epoch': 2.0}
{'eval_loss': 2.314218044281006, 'eval_accuracy@por.rst.cstn': 0.343804537521815, 'eval_f1@por.rst.cstn': 0.08484192575101666, 'eval_precision@por.rst.cstn': 0.0947191210349105, 'eval_recall@por.rst.cstn': 0.09908800027377103, 'eval_loss@por.rst.cstn': 2.314218044281006, 'eval_runtime': 7.3218, 'eval_samples_per_second': 78.259, 'eval_steps_per_second': 2.458, 'epoch': 2.0}
{'train@por.rst.cstn_loss': 1.9508633613586426, 'train@por.rst.cstn_accuracy@por.rst.cstn': 0.4534715525554484, 'train@por.rst.cstn_f1@por.rst.cstn': 0.0698465050409542, 'train@por.rst.cstn_precision@por.rst.cstn': 0.07005399655476054, 'train@por.rst.cstn_recall@por.rst.cstn': 0.08770939482886621, 'train@por.rst.cstn_loss@por.rst.cstn': 1.9508633613586426, 'train@por.rst.cstn_runtime': 50.6234, 'train@por.rst.cstn_samples_per_second': 81.938, 'train@por.rst.cstn_steps_per_second': 2.568, 'epoch': 3.0}
{'loss': 2.1192, 'learning_rate': 1.5000000000000002e-05, 'epoch': 3.0}
{'eval_loss': 2.1025569438934326, 'eval_accuracy@por.rst.cstn': 0.3769633507853403, 'eval_f1@por.rst.cstn': 0.09579535249178105, 'eval_precision@por.rst.cstn': 0.09882648660637275, 'eval_recall@por.rst.cstn': 0.12613709534346107, 'eval_loss@por.rst.cstn': 2.1025569438934326, 'eval_runtime': 7.3405, 'eval_samples_per_second': 78.06, 'eval_steps_per_second': 2.452, 'epoch': 3.0}
{'train@por.rst.cstn_loss': 1.8101831674575806, 'train@por.rst.cstn_accuracy@por.rst.cstn': 0.5207328833172613, 'train@por.rst.cstn_f1@por.rst.cstn': 0.10027209226984249, 'train@por.rst.cstn_precision@por.rst.cstn': 0.10330585361038325, 'train@por.rst.cstn_recall@por.rst.cstn': 0.11052599019231435, 'train@por.rst.cstn_loss@por.rst.cstn': 1.8101831674575806, 'train@por.rst.cstn_runtime': 50.6051, 'train@por.rst.cstn_samples_per_second': 81.968, 'train@por.rst.cstn_steps_per_second': 2.569, 'epoch': 4.0}
{'loss': 1.9349, 'learning_rate': 1.3333333333333333e-05, 'epoch': 4.0}
{'eval_loss': 1.9720361232757568, 'eval_accuracy@por.rst.cstn': 0.4293193717277487, 'eval_f1@por.rst.cstn': 0.13191301428901997, 'eval_precision@por.rst.cstn': 0.17757006251324434, 'eval_recall@por.rst.cstn': 0.15481171337734684, 'eval_loss@por.rst.cstn': 1.972036361694336, 'eval_runtime': 7.343, 'eval_samples_per_second': 78.033, 'eval_steps_per_second': 2.451, 'epoch': 4.0}
{'train@por.rst.cstn_loss': 1.7157649993896484, 'train@por.rst.cstn_accuracy@por.rst.cstn': 0.5351976856316297, 'train@por.rst.cstn_f1@por.rst.cstn': 0.10977334279301396, 'train@por.rst.cstn_precision@por.rst.cstn': 0.11215242360306027, 'train@por.rst.cstn_recall@por.rst.cstn': 0.12492063625020955, 'train@por.rst.cstn_loss@por.rst.cstn': 1.7157649993896484, 'train@por.rst.cstn_runtime': 50.6337, 'train@por.rst.cstn_samples_per_second': 81.922, 'train@por.rst.cstn_steps_per_second': 2.567, 'epoch': 5.0}
{'loss': 1.8148, 'learning_rate': 1.1666666666666668e-05, 'epoch': 5.0}
{'eval_loss': 1.8731576204299927, 'eval_accuracy@por.rst.cstn': 0.43455497382198954, 'eval_f1@por.rst.cstn': 0.1423056862709629, 'eval_precision@por.rst.cstn': 0.17212293928167396, 'eval_recall@por.rst.cstn': 0.16802075256412682, 'eval_loss@por.rst.cstn': 1.8731576204299927, 'eval_runtime': 7.3227, 'eval_samples_per_second': 78.25, 'eval_steps_per_second': 2.458, 'epoch': 5.0}
{'train@por.rst.cstn_loss': 1.6574651002883911, 'train@por.rst.cstn_accuracy@por.rst.cstn': 0.5511089681774349, 'train@por.rst.cstn_f1@por.rst.cstn': 0.11739453203124037, 'train@por.rst.cstn_precision@por.rst.cstn': 0.11484752327842272, 'train@por.rst.cstn_recall@por.rst.cstn': 0.13267306114240185, 'train@por.rst.cstn_loss@por.rst.cstn': 1.6574651002883911, 'train@por.rst.cstn_runtime': 50.6635, 'train@por.rst.cstn_samples_per_second': 81.874, 'train@por.rst.cstn_steps_per_second': 2.566, 'epoch': 6.0}
{'loss': 1.7398, 'learning_rate': 1e-05, 'epoch': 6.0}
{'eval_loss': 1.8102275133132935, 'eval_accuracy@por.rst.cstn': 0.462478184991274, 'eval_f1@por.rst.cstn': 0.1634658093859943, 'eval_precision@por.rst.cstn': 0.15714400545828938, 'eval_recall@por.rst.cstn': 0.18509179434392664, 'eval_loss@por.rst.cstn': 1.8102272748947144, 'eval_runtime': 7.3245, 'eval_samples_per_second': 78.231, 'eval_steps_per_second': 2.458, 'epoch': 6.0}
{'train@por.rst.cstn_loss': 1.616545557975769, 'train@por.rst.cstn_accuracy@por.rst.cstn': 0.56099324975892, 'train@por.rst.cstn_f1@por.rst.cstn': 0.12303020105496776, 'train@por.rst.cstn_precision@por.rst.cstn': 0.12491964276601711, 'train@por.rst.cstn_recall@por.rst.cstn': 0.13806372583310578, 'train@por.rst.cstn_loss@por.rst.cstn': 1.616545557975769, 'train@por.rst.cstn_runtime': 50.7043, 'train@por.rst.cstn_samples_per_second': 81.808, 'train@por.rst.cstn_steps_per_second': 2.564, 'epoch': 7.0}
{'loss': 1.6874, 'learning_rate': 8.333333333333334e-06, 'epoch': 7.0}
{'eval_loss': 1.7658019065856934, 'eval_accuracy@por.rst.cstn': 0.4537521815008726, 'eval_f1@por.rst.cstn': 0.16441648711749943, 'eval_precision@por.rst.cstn': 0.15398995579743716, 'eval_recall@por.rst.cstn': 0.18740563229849683, 'eval_loss@por.rst.cstn': 1.7658021450042725, 'eval_runtime': 7.3101, 'eval_samples_per_second': 78.384, 'eval_steps_per_second': 2.462, 'epoch': 7.0}
{'train@por.rst.cstn_loss': 1.589591383934021, 'train@por.rst.cstn_accuracy@por.rst.cstn': 0.5653326904532304, 'train@por.rst.cstn_f1@por.rst.cstn': 0.1254425606002816, 'train@por.rst.cstn_precision@por.rst.cstn': 0.12358753541215903, 'train@por.rst.cstn_recall@por.rst.cstn': 0.14026661552830405, 'train@por.rst.cstn_loss@por.rst.cstn': 1.589591383934021, 'train@por.rst.cstn_runtime': 50.7116, 'train@por.rst.cstn_samples_per_second': 81.796, 'train@por.rst.cstn_steps_per_second': 2.564, 'epoch': 8.0}
{'loss': 1.6581, 'learning_rate': 6.666666666666667e-06, 'epoch': 8.0}
{'eval_loss': 1.7404909133911133, 'eval_accuracy@por.rst.cstn': 0.4677137870855148, 'eval_f1@por.rst.cstn': 0.17068554933757238, 'eval_precision@por.rst.cstn': 0.18175943323992058, 'eval_recall@por.rst.cstn': 0.19225091789841117, 'eval_loss@por.rst.cstn': 1.7404910326004028, 'eval_runtime': 7.3386, 'eval_samples_per_second': 78.081, 'eval_steps_per_second': 2.453, 'epoch': 8.0}
{'train@por.rst.cstn_loss': 1.5710006952285767, 'train@por.rst.cstn_accuracy@por.rst.cstn': 0.5699132111861138, 'train@por.rst.cstn_f1@por.rst.cstn': 0.12856956252060342, 'train@por.rst.cstn_precision@por.rst.cstn': 0.13092583664969626, 'train@por.rst.cstn_recall@por.rst.cstn': 0.14233992022607694, 'train@por.rst.cstn_loss@por.rst.cstn': 1.5710006952285767, 'train@por.rst.cstn_runtime': 50.6808, 'train@por.rst.cstn_samples_per_second': 81.846, 'train@por.rst.cstn_steps_per_second': 2.565, 'epoch': 9.0}
{'loss': 1.6346, 'learning_rate': 5e-06, 'epoch': 9.0}
{'eval_loss': 1.7210837602615356, 'eval_accuracy@por.rst.cstn': 0.47469458987783597, 'eval_f1@por.rst.cstn': 0.1750004427042404, 'eval_precision@por.rst.cstn': 0.1918930479029221, 'eval_recall@por.rst.cstn': 0.195771168248805, 'eval_loss@por.rst.cstn': 1.7210837602615356, 'eval_runtime': 7.346, 'eval_samples_per_second': 78.001, 'eval_steps_per_second': 2.45, 'epoch': 9.0}
{'train@por.rst.cstn_loss': 1.5554217100143433, 'train@por.rst.cstn_accuracy@por.rst.cstn': 0.5728061716489875, 'train@por.rst.cstn_f1@por.rst.cstn': 0.13063808803445973, 'train@por.rst.cstn_precision@por.rst.cstn': 0.13243960653861614, 'train@por.rst.cstn_recall@por.rst.cstn': 0.14501255472249083, 'train@por.rst.cstn_loss@por.rst.cstn': 1.5554217100143433, 'train@por.rst.cstn_runtime': 50.7478, 'train@por.rst.cstn_samples_per_second': 81.737, 'train@por.rst.cstn_steps_per_second': 2.562, 'epoch': 10.0}
{'loss': 1.6088, 'learning_rate': 3.3333333333333333e-06, 'epoch': 10.0}
{'eval_loss': 1.7095259428024292, 'eval_accuracy@por.rst.cstn': 0.4677137870855148, 'eval_f1@por.rst.cstn': 0.17209061161708206, 'eval_precision@por.rst.cstn': 0.16911546986351222, 'eval_recall@por.rst.cstn': 0.19774084495584396, 'eval_loss@por.rst.cstn': 1.7095259428024292, 'eval_runtime': 7.3588, 'eval_samples_per_second': 77.866, 'eval_steps_per_second': 2.446, 'epoch': 10.0}
{'train@por.rst.cstn_loss': 1.546680212020874, 'train@por.rst.cstn_accuracy@por.rst.cstn': 0.5754580520732884, 'train@por.rst.cstn_f1@por.rst.cstn': 0.1322037787742526, 'train@por.rst.cstn_precision@por.rst.cstn': 0.13431446066567693, 'train@por.rst.cstn_recall@por.rst.cstn': 0.14634390132237796, 'train@por.rst.cstn_loss@por.rst.cstn': 1.5466800928115845, 'train@por.rst.cstn_runtime': 50.6918, 'train@por.rst.cstn_samples_per_second': 81.828, 'train@por.rst.cstn_steps_per_second': 2.565, 'epoch': 11.0}
{'loss': 1.6034, 'learning_rate': 1.6666666666666667e-06, 'epoch': 11.0}
{'eval_loss': 1.7022589445114136, 'eval_accuracy@por.rst.cstn': 0.47294938917975565, 'eval_f1@por.rst.cstn': 0.1735322280348234, 'eval_precision@por.rst.cstn': 0.17028412457883382, 'eval_recall@por.rst.cstn': 0.19917583889655868, 'eval_loss@por.rst.cstn': 1.7022590637207031, 'eval_runtime': 7.3139, 'eval_samples_per_second': 78.344, 'eval_steps_per_second': 2.461, 'epoch': 11.0}
{'train@por.rst.cstn_loss': 1.5440784692764282, 'train@por.rst.cstn_accuracy@por.rst.cstn': 0.5759402121504339, 'train@por.rst.cstn_f1@por.rst.cstn': 0.13276392779741264, 'train@por.rst.cstn_precision@por.rst.cstn': 0.13442884298856178, 'train@por.rst.cstn_recall@por.rst.cstn': 0.14670498271867766, 'train@por.rst.cstn_loss@por.rst.cstn': 1.5440783500671387, 'train@por.rst.cstn_runtime': 50.6806, 'train@por.rst.cstn_samples_per_second': 81.846, 'train@por.rst.cstn_steps_per_second': 2.565, 'epoch': 12.0}
{'loss': 1.5995, 'learning_rate': 0.0, 'epoch': 12.0}
{'eval_loss': 1.69864821434021, 'eval_accuracy@por.rst.cstn': 0.4781849912739965, 'eval_f1@por.rst.cstn': 0.17598354197713043, 'eval_precision@por.rst.cstn': 0.17575200813073882, 'eval_recall@por.rst.cstn': 0.2004531630909829, 'eval_loss@por.rst.cstn': 1.69864821434021, 'eval_runtime': 7.3301, 'eval_samples_per_second': 78.171, 'eval_steps_per_second': 2.456, 'epoch': 12.0}
{'train_runtime': 1975.8268, 'train_samples_per_second': 25.192, 'train_steps_per_second': 0.79, 'train_loss': 1.8984847631209936, 'epoch': 12.0}
***** train_log metrics *****
  epoch                    =       12.0
  train_loss               =     1.8985
  train_runtime            = 0:32:55.82
  train_samples_per_second =     25.192
  train_steps_per_second   =       0.79
{'train@fas.rst.prstc_loss': 2.427842617034912, 'train@fas.rst.prstc_accuracy@fas.rst.prstc': 0.23780487804878048, 'train@fas.rst.prstc_f1@fas.rst.prstc': 0.022611055066615645, 'train@fas.rst.prstc_precision@fas.rst.prstc': 0.013995349237791749, 'train@fas.rst.prstc_recall@fas.rst.prstc': 0.058823529411764705, 'train@fas.rst.prstc_loss@fas.rst.prstc': 2.427842617034912, 'train@fas.rst.prstc_runtime': 49.2505, 'train@fas.rst.prstc_samples_per_second': 83.248, 'train@fas.rst.prstc_steps_per_second': 2.619, 'epoch': 1.0}
{'loss': 2.8374, 'learning_rate': 1.8333333333333333e-05, 'epoch': 1.0}
{'eval_loss': 2.3473823070526123, 'eval_accuracy@fas.rst.prstc': 0.24448897795591182, 'eval_f1@fas.rst.prstc': 0.026236559139784947, 'eval_precision@fas.rst.prstc': 0.016331994645247656, 'eval_recall@fas.rst.prstc': 0.06666666666666667, 'eval_loss@fas.rst.prstc': 2.347381830215454, 'eval_runtime': 6.3298, 'eval_samples_per_second': 78.833, 'eval_steps_per_second': 2.528, 'epoch': 1.0}
{'train@fas.rst.prstc_loss': 2.361137866973877, 'train@fas.rst.prstc_accuracy@fas.rst.prstc': 0.27731707317073173, 'train@fas.rst.prstc_f1@fas.rst.prstc': 0.04447351900571284, 'train@fas.rst.prstc_precision@fas.rst.prstc': 0.0321471390121588, 'train@fas.rst.prstc_recall@fas.rst.prstc': 0.07252548591722559, 'train@fas.rst.prstc_loss@fas.rst.prstc': 2.361137866973877, 'train@fas.rst.prstc_runtime': 49.2183, 'train@fas.rst.prstc_samples_per_second': 83.302, 'train@fas.rst.prstc_steps_per_second': 2.621, 'epoch': 2.0}
{'loss': 2.4167, 'learning_rate': 1.6666666666666667e-05, 'epoch': 2.0}
{'eval_loss': 2.2720062732696533, 'eval_accuracy@fas.rst.prstc': 0.312625250501002, 'eval_f1@fas.rst.prstc': 0.05598315378276924, 'eval_precision@fas.rst.prstc': 0.041903194844371314, 'eval_recall@fas.rst.prstc': 0.0872416250890948, 'eval_loss@fas.rst.prstc': 2.272006034851074, 'eval_runtime': 6.2959, 'eval_samples_per_second': 79.258, 'eval_steps_per_second': 2.541, 'epoch': 2.0}
{'train@fas.rst.prstc_loss': 2.338277816772461, 'train@fas.rst.prstc_accuracy@fas.rst.prstc': 0.26439024390243904, 'train@fas.rst.prstc_f1@fas.rst.prstc': 0.038201382198220175, 'train@fas.rst.prstc_precision@fas.rst.prstc': 0.0321569512076939, 'train@fas.rst.prstc_recall@fas.rst.prstc': 0.06703721532257202, 'train@fas.rst.prstc_loss@fas.rst.prstc': 2.338277816772461, 'train@fas.rst.prstc_runtime': 49.2077, 'train@fas.rst.prstc_samples_per_second': 83.32, 'train@fas.rst.prstc_steps_per_second': 2.622, 'epoch': 3.0}
{'loss': 2.3646, 'learning_rate': 1.5000000000000002e-05, 'epoch': 3.0}
{'eval_loss': 2.248157501220703, 'eval_accuracy@fas.rst.prstc': 0.28256513026052105, 'eval_f1@fas.rst.prstc': 0.045488429880416216, 'eval_precision@fas.rst.prstc': 0.04022766737410766, 'eval_recall@fas.rst.prstc': 0.07784746970776907, 'eval_loss@fas.rst.prstc': 2.248157501220703, 'eval_runtime': 6.3214, 'eval_samples_per_second': 78.938, 'eval_steps_per_second': 2.531, 'epoch': 3.0}
{'train@fas.rst.prstc_loss': 2.3122308254241943, 'train@fas.rst.prstc_accuracy@fas.rst.prstc': 0.24195121951219511, 'train@fas.rst.prstc_f1@fas.rst.prstc': 0.026679701239163064, 'train@fas.rst.prstc_precision@fas.rst.prstc': 0.033914263517352536, 'train@fas.rst.prstc_recall@fas.rst.prstc': 0.06014355551276703, 'train@fas.rst.prstc_loss@fas.rst.prstc': 2.3122305870056152, 'train@fas.rst.prstc_runtime': 49.1883, 'train@fas.rst.prstc_samples_per_second': 83.353, 'train@fas.rst.prstc_steps_per_second': 2.623, 'epoch': 4.0}
{'loss': 2.3463, 'learning_rate': 1.3333333333333333e-05, 'epoch': 4.0}
{'eval_loss': 2.227954864501953, 'eval_accuracy@fas.rst.prstc': 0.24849699398797595, 'eval_f1@fas.rst.prstc': 0.030517527811073957, 'eval_precision@fas.rst.prstc': 0.03322981366459628, 'eval_recall@fas.rst.prstc': 0.06789261107151343, 'eval_loss@fas.rst.prstc': 2.227954626083374, 'eval_runtime': 6.3216, 'eval_samples_per_second': 78.936, 'eval_steps_per_second': 2.531, 'epoch': 4.0}
{'train@fas.rst.prstc_loss': 2.263432264328003, 'train@fas.rst.prstc_accuracy@fas.rst.prstc': 0.3053658536585366, 'train@fas.rst.prstc_f1@fas.rst.prstc': 0.04982469914908413, 'train@fas.rst.prstc_precision@fas.rst.prstc': 0.03623881571557535, 'train@fas.rst.prstc_recall@fas.rst.prstc': 0.08042039729148615, 'train@fas.rst.prstc_loss@fas.rst.prstc': 2.263432025909424, 'train@fas.rst.prstc_runtime': 50.0242, 'train@fas.rst.prstc_samples_per_second': 81.96, 'train@fas.rst.prstc_steps_per_second': 2.579, 'epoch': 5.0}
{'loss': 2.3213, 'learning_rate': 1.1666666666666668e-05, 'epoch': 5.0}
{'eval_loss': 2.1711058616638184, 'eval_accuracy@fas.rst.prstc': 0.3346693386773547, 'eval_f1@fas.rst.prstc': 0.060694186868425655, 'eval_precision@fas.rst.prstc': 0.04496301306621144, 'eval_recall@fas.rst.prstc': 0.09375148491328106, 'eval_loss@fas.rst.prstc': 2.17110538482666, 'eval_runtime': 6.3429, 'eval_samples_per_second': 78.671, 'eval_steps_per_second': 2.523, 'epoch': 5.0}
{'train@fas.rst.prstc_loss': 2.189300298690796, 'train@fas.rst.prstc_accuracy@fas.rst.prstc': 0.31414634146341464, 'train@fas.rst.prstc_f1@fas.rst.prstc': 0.052411915170409426, 'train@fas.rst.prstc_precision@fas.rst.prstc': 0.03933029452916935, 'train@fas.rst.prstc_recall@fas.rst.prstc': 0.08343870008450735, 'train@fas.rst.prstc_loss@fas.rst.prstc': 2.189300298690796, 'train@fas.rst.prstc_runtime': 49.2802, 'train@fas.rst.prstc_samples_per_second': 83.198, 'train@fas.rst.prstc_steps_per_second': 2.618, 'epoch': 6.0}
{'loss': 2.2632, 'learning_rate': 1e-05, 'epoch': 6.0}
{'eval_loss': 2.088989496231079, 'eval_accuracy@fas.rst.prstc': 0.3466933867735471, 'eval_f1@fas.rst.prstc': 0.06475385788886932, 'eval_precision@fas.rst.prstc': 0.050234527611093566, 'eval_recall@fas.rst.prstc': 0.09746258018531719, 'eval_loss@fas.rst.prstc': 2.088989734649658, 'eval_runtime': 6.318, 'eval_samples_per_second': 78.981, 'eval_steps_per_second': 2.532, 'epoch': 6.0}
{'train@fas.rst.prstc_loss': 2.1514477729797363, 'train@fas.rst.prstc_accuracy@fas.rst.prstc': 0.32170731707317074, 'train@fas.rst.prstc_f1@fas.rst.prstc': 0.05477307538519447, 'train@fas.rst.prstc_precision@fas.rst.prstc': 0.04275748269369814, 'train@fas.rst.prstc_recall@fas.rst.prstc': 0.08582416054255854, 'train@fas.rst.prstc_loss@fas.rst.prstc': 2.1514477729797363, 'train@fas.rst.prstc_runtime': 49.2966, 'train@fas.rst.prstc_samples_per_second': 83.17, 'train@fas.rst.prstc_steps_per_second': 2.617, 'epoch': 7.0}
{'loss': 2.2095, 'learning_rate': 8.333333333333334e-06, 'epoch': 7.0}
{'eval_loss': 2.0379700660705566, 'eval_accuracy@fas.rst.prstc': 0.3587174348697395, 'eval_f1@fas.rst.prstc': 0.06794424722230949, 'eval_precision@fas.rst.prstc': 0.0543619645660462, 'eval_recall@fas.rst.prstc': 0.10097410311237824, 'eval_loss@fas.rst.prstc': 2.0379698276519775, 'eval_runtime': 6.3408, 'eval_samples_per_second': 78.697, 'eval_steps_per_second': 2.523, 'epoch': 7.0}
{'train@fas.rst.prstc_loss': 2.1202709674835205, 'train@fas.rst.prstc_accuracy@fas.rst.prstc': 0.3497560975609756, 'train@fas.rst.prstc_f1@fas.rst.prstc': 0.07752219420193496, 'train@fas.rst.prstc_precision@fas.rst.prstc': 0.09170426461638437, 'train@fas.rst.prstc_recall@fas.rst.prstc': 0.10136879674934841, 'train@fas.rst.prstc_loss@fas.rst.prstc': 2.1202709674835205, 'train@fas.rst.prstc_runtime': 49.2839, 'train@fas.rst.prstc_samples_per_second': 83.191, 'train@fas.rst.prstc_steps_per_second': 2.617, 'epoch': 8.0}
{'loss': 2.1768, 'learning_rate': 6.666666666666667e-06, 'epoch': 8.0}
{'eval_loss': 2.0073766708374023, 'eval_accuracy@fas.rst.prstc': 0.3787575150300601, 'eval_f1@fas.rst.prstc': 0.09013990384144883, 'eval_precision@fas.rst.prstc': 0.1146897479687419, 'eval_recall@fas.rst.prstc': 0.11496067324405096, 'eval_loss@fas.rst.prstc': 2.0073766708374023, 'eval_runtime': 6.3171, 'eval_samples_per_second': 78.992, 'eval_steps_per_second': 2.533, 'epoch': 8.0}
{'train@fas.rst.prstc_loss': 2.0905604362487793, 'train@fas.rst.prstc_accuracy@fas.rst.prstc': 0.37146341463414634, 'train@fas.rst.prstc_f1@fas.rst.prstc': 0.0883095197788226, 'train@fas.rst.prstc_precision@fas.rst.prstc': 0.08622714236643313, 'train@fas.rst.prstc_recall@fas.rst.prstc': 0.11335865894132789, 'train@fas.rst.prstc_loss@fas.rst.prstc': 2.0905604362487793, 'train@fas.rst.prstc_runtime': 49.2745, 'train@fas.rst.prstc_samples_per_second': 83.207, 'train@fas.rst.prstc_steps_per_second': 2.618, 'epoch': 9.0}
{'loss': 2.1472, 'learning_rate': 5e-06, 'epoch': 9.0}
{'eval_loss': 1.977687120437622, 'eval_accuracy@fas.rst.prstc': 0.3927855711422846, 'eval_f1@fas.rst.prstc': 0.1050155743668048, 'eval_precision@fas.rst.prstc': 0.11533883879890423, 'eval_recall@fas.rst.prstc': 0.12724137499843693, 'eval_loss@fas.rst.prstc': 1.977687120437622, 'eval_runtime': 6.3361, 'eval_samples_per_second': 78.756, 'eval_steps_per_second': 2.525, 'epoch': 9.0}
{'train@fas.rst.prstc_loss': 2.0705225467681885, 'train@fas.rst.prstc_accuracy@fas.rst.prstc': 0.37658536585365854, 'train@fas.rst.prstc_f1@fas.rst.prstc': 0.09024538556780029, 'train@fas.rst.prstc_precision@fas.rst.prstc': 0.17333207898764005, 'train@fas.rst.prstc_recall@fas.rst.prstc': 0.11580209405238709, 'train@fas.rst.prstc_loss@fas.rst.prstc': 2.0705225467681885, 'train@fas.rst.prstc_runtime': 49.2971, 'train@fas.rst.prstc_samples_per_second': 83.169, 'train@fas.rst.prstc_steps_per_second': 2.617, 'epoch': 10.0}
{'loss': 2.1363, 'learning_rate': 3.3333333333333333e-06, 'epoch': 10.0}
{'eval_loss': 1.959491491317749, 'eval_accuracy@fas.rst.prstc': 0.40080160320641284, 'eval_f1@fas.rst.prstc': 0.10683667890561778, 'eval_precision@fas.rst.prstc': 0.1175475254015077, 'eval_recall@fas.rst.prstc': 0.1295602155781471, 'eval_loss@fas.rst.prstc': 1.9594917297363281, 'eval_runtime': 6.3335, 'eval_samples_per_second': 78.787, 'eval_steps_per_second': 2.526, 'epoch': 10.0}
{'train@fas.rst.prstc_loss': 2.0558838844299316, 'train@fas.rst.prstc_accuracy@fas.rst.prstc': 0.3780487804878049, 'train@fas.rst.prstc_f1@fas.rst.prstc': 0.09133883843050887, 'train@fas.rst.prstc_precision@fas.rst.prstc': 0.15752367699163336, 'train@fas.rst.prstc_recall@fas.rst.prstc': 0.11690631355114674, 'train@fas.rst.prstc_loss@fas.rst.prstc': 2.0558841228485107, 'train@fas.rst.prstc_runtime': 49.2437, 'train@fas.rst.prstc_samples_per_second': 83.259, 'train@fas.rst.prstc_steps_per_second': 2.62, 'epoch': 11.0}
{'loss': 2.1179, 'learning_rate': 1.6666666666666667e-06, 'epoch': 11.0}
{'eval_loss': 1.9465084075927734, 'eval_accuracy@fas.rst.prstc': 0.4028056112224449, 'eval_f1@fas.rst.prstc': 0.10551026494543825, 'eval_precision@fas.rst.prstc': 0.10871275410749094, 'eval_recall@fas.rst.prstc': 0.13013992572307462, 'eval_loss@fas.rst.prstc': 1.946508526802063, 'eval_runtime': 6.328, 'eval_samples_per_second': 78.856, 'eval_steps_per_second': 2.528, 'epoch': 11.0}
{'train@fas.rst.prstc_loss': 2.0529978275299072, 'train@fas.rst.prstc_accuracy@fas.rst.prstc': 0.37829268292682927, 'train@fas.rst.prstc_f1@fas.rst.prstc': 0.09160194861175812, 'train@fas.rst.prstc_precision@fas.rst.prstc': 0.15742792309803047, 'train@fas.rst.prstc_recall@fas.rst.prstc': 0.11727336291521585, 'train@fas.rst.prstc_loss@fas.rst.prstc': 2.0529980659484863, 'train@fas.rst.prstc_runtime': 49.1871, 'train@fas.rst.prstc_samples_per_second': 83.355, 'train@fas.rst.prstc_steps_per_second': 2.623, 'epoch': 12.0}
{'loss': 2.1089, 'learning_rate': 0.0, 'epoch': 12.0}
{'eval_loss': 1.9450663328170776, 'eval_accuracy@fas.rst.prstc': 0.40080160320641284, 'eval_f1@fas.rst.prstc': 0.10419896898332219, 'eval_precision@fas.rst.prstc': 0.10477332474918181, 'eval_recall@fas.rst.prstc': 0.12952695352065122, 'eval_loss@fas.rst.prstc': 1.9450663328170776, 'eval_runtime': 6.3209, 'eval_samples_per_second': 78.945, 'eval_steps_per_second': 2.531, 'epoch': 12.0}
{'train_runtime': 1911.4092, 'train_samples_per_second': 25.74, 'train_steps_per_second': 0.81, 'train_loss': 2.287166388459908, 'epoch': 12.0}
***** train_logger metrics *****
  epoch                    =       12.0
  train_loss               =     1.8985
  train_runtime            = 0:32:55.82
  train_samples_per_second =     25.192
  train_steps_per_second   =       0.79
-------------------------------------------------------------------
Lang1:  rus.rst.rrt    Lang2:  fas.rst.prstc
Saving run to:  runs/full_shot/FullShot=v4_rus.rst.rrt_fas.rst.prstc_lrfunc=Adafactor_lr=2e-05_plm=bert-base-multilingual-cased
Running with params: BERT_MODEL=bert-base-multilingual-cased lr=2e-05
read 28822 examples
read 2855 examples
read 2843 examples
read 4100 examples
read 499 examples
read 592 examples
Total prediction labels:  30
BertAdapterModel(
  (shared_parameters): ModuleDict()
  (bert): BertModel(
    (shared_parameters): ModuleDict()
    (invertible_adapters): ModuleDict(
      (ru): NICECouplingBlock(
        (F): Sequential(
          (0): Linear(in_features=384, out_features=192, bias=True)
          (1): Activation_Function_Class(
            (f): NewGELUActivation()
          )
          (2): Linear(in_features=192, out_features=384, bias=True)
        )
        (G): Sequential(
          (0): Linear(in_features=384, out_features=192, bias=True)
          (1): Activation_Function_Class(
            (f): NewGELUActivation()
          )
          (2): Linear(in_features=192, out_features=384, bias=True)
        )
      )
      (ar): NICECouplingBlock(
        (F): Sequential(
          (0): Linear(in_features=384, out_features=192, bias=True)
          (1): Activation_Function_Class(
            (f): ReLU()
          )
          (2): Linear(in_features=192, out_features=384, bias=True)
        )
        (G): Sequential(
          (0): Linear(in_features=384, out_features=192, bias=True)
          (1): Activation_Function_Class(
            (f): ReLU()
          )
          (2): Linear(in_features=192, out_features=384, bias=True)
        )
      )
    )
    (embeddings): BertEmbeddings(
      (word_embeddings): Embedding(119547, 768, padding_idx=0)
      (position_embeddings): Embedding(512, 768)
      (token_type_embeddings): Embedding(2, 768)
      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
      (dropout): Dropout(p=0.1, inplace=False)
    )
    (encoder): BertEncoder(
      (layer): ModuleList(
        (0): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (ru): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (ar): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (1): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (ru): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (ar): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (2): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (ru): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (ar): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (3): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (ru): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (ar): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (4): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (ru): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (ar): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (5): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (ru): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (ar): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (6): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (ru): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (ar): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (7): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (ru): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (ar): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (8): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (ru): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (ar): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (9): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (ru): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (ar): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (10): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (ru): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (ar): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (11): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (ru): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (ar): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
      )
    )
    (pooler): BertPooler(
      (dense): Linear(in_features=768, out_features=768, bias=True)
      (activation): Tanh()
    )
    (prefix_tuning): PrefixTuningPool(
      (prefix_tunings): ModuleDict()
    )
  )
  (heads): ModuleDict(
    (disrpt): ClassificationHead(
      (0): Dropout(p=0.1, inplace=False)
      (1): Linear(in_features=768, out_features=768, bias=True)
      (2): Activation_Function_Class(
        (f): Tanh()
      )
      (3): Dropout(p=0.1, inplace=False)
      (4): Linear(in_features=768, out_features=30, bias=True)
    )
  )
)
{'train@rus.rst.rrt_loss': 1.719152808189392, 'train@rus.rst.rrt_accuracy@rus.rst.rrt': 0.4937547706613004, 'train@rus.rst.rrt_f1@rus.rst.rrt': 0.1788227821528923, 'train@rus.rst.rrt_precision@rus.rst.rrt': 0.22934523819164346, 'train@rus.rst.rrt_recall@rus.rst.rrt': 0.1952003240719219, 'train@rus.rst.rrt_loss@rus.rst.rrt': 1.7191526889801025, 'train@rus.rst.rrt_runtime': 350.6568, 'train@rus.rst.rrt_samples_per_second': 82.194, 'train@rus.rst.rrt_steps_per_second': 2.569, 'epoch': 1.0}
{'loss': 2.1359, 'learning_rate': 1.8333333333333333e-05, 'epoch': 1.0}
{'eval_loss': 1.7592859268188477, 'eval_accuracy@rus.rst.rrt': 0.47285464098073554, 'eval_f1@rus.rst.rrt': 0.1979967091276548, 'eval_precision@rus.rst.rrt': 0.21001406117279944, 'eval_recall@rus.rst.rrt': 0.21537399878526906, 'eval_loss@rus.rst.rrt': 1.7592859268188477, 'eval_runtime': 35.0736, 'eval_samples_per_second': 81.4, 'eval_steps_per_second': 2.566, 'epoch': 1.0}
{'train@rus.rst.rrt_loss': 1.4976814985275269, 'train@rus.rst.rrt_accuracy@rus.rst.rrt': 0.5433002567483173, 'train@rus.rst.rrt_f1@rus.rst.rrt': 0.22799491798357863, 'train@rus.rst.rrt_precision@rus.rst.rrt': 0.30686889301198156, 'train@rus.rst.rrt_recall@rus.rst.rrt': 0.23398676317969117, 'train@rus.rst.rrt_loss@rus.rst.rrt': 1.4976814985275269, 'train@rus.rst.rrt_runtime': 350.5363, 'train@rus.rst.rrt_samples_per_second': 82.223, 'train@rus.rst.rrt_steps_per_second': 2.57, 'epoch': 2.0}
{'loss': 1.6373, 'learning_rate': 1.6666666666666667e-05, 'epoch': 2.0}
{'eval_loss': 1.5647050142288208, 'eval_accuracy@rus.rst.rrt': 0.5197898423817864, 'eval_f1@rus.rst.rrt': 0.2536550207746853, 'eval_precision@rus.rst.rrt': 0.3503790385134577, 'eval_recall@rus.rst.rrt': 0.25930496123247776, 'eval_loss@rus.rst.rrt': 1.5647052526474, 'eval_runtime': 35.0129, 'eval_samples_per_second': 81.541, 'eval_steps_per_second': 2.57, 'epoch': 2.0}
{'train@rus.rst.rrt_loss': 1.4188978672027588, 'train@rus.rst.rrt_accuracy@rus.rst.rrt': 0.5662341267087642, 'train@rus.rst.rrt_f1@rus.rst.rrt': 0.27506465715238304, 'train@rus.rst.rrt_precision@rus.rst.rrt': 0.3678378572167496, 'train@rus.rst.rrt_recall@rus.rst.rrt': 0.2722712458987208, 'train@rus.rst.rrt_loss@rus.rst.rrt': 1.4188978672027588, 'train@rus.rst.rrt_runtime': 350.4287, 'train@rus.rst.rrt_samples_per_second': 82.248, 'train@rus.rst.rrt_steps_per_second': 2.571, 'epoch': 3.0}
{'loss': 1.5077, 'learning_rate': 1.5000000000000002e-05, 'epoch': 3.0}
{'eval_loss': 1.4892246723175049, 'eval_accuracy@rus.rst.rrt': 0.5380035026269703, 'eval_f1@rus.rst.rrt': 0.30344102785785837, 'eval_precision@rus.rst.rrt': 0.3919686174821264, 'eval_recall@rus.rst.rrt': 0.30168827636182344, 'eval_loss@rus.rst.rrt': 1.4892246723175049, 'eval_runtime': 35.0205, 'eval_samples_per_second': 81.524, 'eval_steps_per_second': 2.57, 'epoch': 3.0}
{'train@rus.rst.rrt_loss': 1.3658275604248047, 'train@rus.rst.rrt_accuracy@rus.rst.rrt': 0.5817431128998681, 'train@rus.rst.rrt_f1@rus.rst.rrt': 0.31219562893893393, 'train@rus.rst.rrt_precision@rus.rst.rrt': 0.4480029959155059, 'train@rus.rst.rrt_recall@rus.rst.rrt': 0.29555430390130055, 'train@rus.rst.rrt_loss@rus.rst.rrt': 1.3658275604248047, 'train@rus.rst.rrt_runtime': 350.7755, 'train@rus.rst.rrt_samples_per_second': 82.167, 'train@rus.rst.rrt_steps_per_second': 2.569, 'epoch': 4.0}
{'loss': 1.4436, 'learning_rate': 1.3333333333333333e-05, 'epoch': 4.0}
{'eval_loss': 1.4423162937164307, 'eval_accuracy@rus.rst.rrt': 0.5586690017513135, 'eval_f1@rus.rst.rrt': 0.3574100860516182, 'eval_precision@rus.rst.rrt': 0.49488976406618607, 'eval_recall@rus.rst.rrt': 0.34016031676581976, 'eval_loss@rus.rst.rrt': 1.4423162937164307, 'eval_runtime': 35.0459, 'eval_samples_per_second': 81.465, 'eval_steps_per_second': 2.568, 'epoch': 4.0}
{'train@rus.rst.rrt_loss': 1.3322585821151733, 'train@rus.rst.rrt_accuracy@rus.rst.rrt': 0.5902782596627576, 'train@rus.rst.rrt_f1@rus.rst.rrt': 0.3292963243677744, 'train@rus.rst.rrt_precision@rus.rst.rrt': 0.4398133282127111, 'train@rus.rst.rrt_recall@rus.rst.rrt': 0.31144664795786015, 'train@rus.rst.rrt_loss@rus.rst.rrt': 1.3322585821151733, 'train@rus.rst.rrt_runtime': 350.6501, 'train@rus.rst.rrt_samples_per_second': 82.196, 'train@rus.rst.rrt_steps_per_second': 2.57, 'epoch': 5.0}
{'loss': 1.4032, 'learning_rate': 1.1666666666666668e-05, 'epoch': 5.0}
{'eval_loss': 1.4179515838623047, 'eval_accuracy@rus.rst.rrt': 0.5621716287215411, 'eval_f1@rus.rst.rrt': 0.3739774716066572, 'eval_precision@rus.rst.rrt': 0.48486250923177326, 'eval_recall@rus.rst.rrt': 0.358635352826388, 'eval_loss@rus.rst.rrt': 1.4179515838623047, 'eval_runtime': 35.0713, 'eval_samples_per_second': 81.406, 'eval_steps_per_second': 2.566, 'epoch': 5.0}
{'train@rus.rst.rrt_loss': 1.3086639642715454, 'train@rus.rst.rrt_accuracy@rus.rst.rrt': 0.5969398376240372, 'train@rus.rst.rrt_f1@rus.rst.rrt': 0.34182825643075226, 'train@rus.rst.rrt_precision@rus.rst.rrt': 0.4522652655181774, 'train@rus.rst.rrt_recall@rus.rst.rrt': 0.3228265536349008, 'train@rus.rst.rrt_loss@rus.rst.rrt': 1.3086639642715454, 'train@rus.rst.rrt_runtime': 350.4853, 'train@rus.rst.rrt_samples_per_second': 82.235, 'train@rus.rst.rrt_steps_per_second': 2.571, 'epoch': 6.0}
{'loss': 1.3753, 'learning_rate': 1e-05, 'epoch': 6.0}
{'eval_loss': 1.3979980945587158, 'eval_accuracy@rus.rst.rrt': 0.5712784588441331, 'eval_f1@rus.rst.rrt': 0.3883949001031205, 'eval_precision@rus.rst.rrt': 0.48602043092658487, 'eval_recall@rus.rst.rrt': 0.37239122186026924, 'eval_loss@rus.rst.rrt': 1.3979980945587158, 'eval_runtime': 35.0439, 'eval_samples_per_second': 81.469, 'eval_steps_per_second': 2.568, 'epoch': 6.0}
{'train@rus.rst.rrt_loss': 1.2932500839233398, 'train@rus.rst.rrt_accuracy@rus.rst.rrt': 0.6020401082506419, 'train@rus.rst.rrt_f1@rus.rst.rrt': 0.3536196865019467, 'train@rus.rst.rrt_precision@rus.rst.rrt': 0.4469893330048689, 'train@rus.rst.rrt_recall@rus.rst.rrt': 0.334824072460037, 'train@rus.rst.rrt_loss@rus.rst.rrt': 1.2932498455047607, 'train@rus.rst.rrt_runtime': 350.6267, 'train@rus.rst.rrt_samples_per_second': 82.201, 'train@rus.rst.rrt_steps_per_second': 2.57, 'epoch': 7.0}
{'loss': 1.3548, 'learning_rate': 8.333333333333334e-06, 'epoch': 7.0}
{'eval_loss': 1.3894281387329102, 'eval_accuracy@rus.rst.rrt': 0.5709281961471103, 'eval_f1@rus.rst.rrt': 0.3975079807113678, 'eval_precision@rus.rst.rrt': 0.4783160497666669, 'eval_recall@rus.rst.rrt': 0.38289533711670165, 'eval_loss@rus.rst.rrt': 1.3894282579421997, 'eval_runtime': 35.0545, 'eval_samples_per_second': 81.445, 'eval_steps_per_second': 2.567, 'epoch': 7.0}
{'train@rus.rst.rrt_loss': 1.2802083492279053, 'train@rus.rst.rrt_accuracy@rus.rst.rrt': 0.604676982860315, 'train@rus.rst.rrt_f1@rus.rst.rrt': 0.36176824853356226, 'train@rus.rst.rrt_precision@rus.rst.rrt': 0.45200817749739053, 'train@rus.rst.rrt_recall@rus.rst.rrt': 0.3446318864323392, 'train@rus.rst.rrt_loss@rus.rst.rrt': 1.2802082300186157, 'train@rus.rst.rrt_runtime': 350.6884, 'train@rus.rst.rrt_samples_per_second': 82.187, 'train@rus.rst.rrt_steps_per_second': 2.569, 'epoch': 8.0}
{'loss': 1.3388, 'learning_rate': 6.666666666666667e-06, 'epoch': 8.0}
{'eval_loss': 1.3778952360153198, 'eval_accuracy@rus.rst.rrt': 0.5761821366024519, 'eval_f1@rus.rst.rrt': 0.4062138040310314, 'eval_precision@rus.rst.rrt': 0.4661151085750017, 'eval_recall@rus.rst.rrt': 0.39557228089902663, 'eval_loss@rus.rst.rrt': 1.3778953552246094, 'eval_runtime': 35.0638, 'eval_samples_per_second': 81.423, 'eval_steps_per_second': 2.567, 'epoch': 8.0}
{'train@rus.rst.rrt_loss': 1.2683708667755127, 'train@rus.rst.rrt_accuracy@rus.rst.rrt': 0.6074526403441816, 'train@rus.rst.rrt_f1@rus.rst.rrt': 0.365003522394932, 'train@rus.rst.rrt_precision@rus.rst.rrt': 0.46103867600300447, 'train@rus.rst.rrt_recall@rus.rst.rrt': 0.34643342628428253, 'train@rus.rst.rrt_loss@rus.rst.rrt': 1.2683708667755127, 'train@rus.rst.rrt_runtime': 350.5433, 'train@rus.rst.rrt_samples_per_second': 82.221, 'train@rus.rst.rrt_steps_per_second': 2.57, 'epoch': 9.0}
{'loss': 1.3297, 'learning_rate': 5e-06, 'epoch': 9.0}
{'eval_loss': 1.3705729246139526, 'eval_accuracy@rus.rst.rrt': 0.5758318739054291, 'eval_f1@rus.rst.rrt': 0.4067736747448929, 'eval_precision@rus.rst.rrt': 0.4849213136211118, 'eval_recall@rus.rst.rrt': 0.3937003295699884, 'eval_loss@rus.rst.rrt': 1.3705730438232422, 'eval_runtime': 35.038, 'eval_samples_per_second': 81.483, 'eval_steps_per_second': 2.569, 'epoch': 9.0}
{'train@rus.rst.rrt_loss': 1.2622367143630981, 'train@rus.rst.rrt_accuracy@rus.rst.rrt': 0.6089445562417598, 'train@rus.rst.rrt_f1@rus.rst.rrt': 0.3668832969517442, 'train@rus.rst.rrt_precision@rus.rst.rrt': 0.4702734946390571, 'train@rus.rst.rrt_recall@rus.rst.rrt': 0.34500208937180676, 'train@rus.rst.rrt_loss@rus.rst.rrt': 1.2622367143630981, 'train@rus.rst.rrt_runtime': 350.6424, 'train@rus.rst.rrt_samples_per_second': 82.198, 'train@rus.rst.rrt_steps_per_second': 2.57, 'epoch': 10.0}
{'loss': 1.3183, 'learning_rate': 3.3333333333333333e-06, 'epoch': 10.0}
{'eval_loss': 1.3683720827102661, 'eval_accuracy@rus.rst.rrt': 0.5796847635726795, 'eval_f1@rus.rst.rrt': 0.4092471067882571, 'eval_precision@rus.rst.rrt': 0.480894900867084, 'eval_recall@rus.rst.rrt': 0.39361076970073555, 'eval_loss@rus.rst.rrt': 1.3683720827102661, 'eval_runtime': 35.0132, 'eval_samples_per_second': 81.541, 'eval_steps_per_second': 2.57, 'epoch': 10.0}
{'train@rus.rst.rrt_loss': 1.2580629587173462, 'train@rus.rst.rrt_accuracy@rus.rst.rrt': 0.6092221219901465, 'train@rus.rst.rrt_f1@rus.rst.rrt': 0.36788086802041114, 'train@rus.rst.rrt_precision@rus.rst.rrt': 0.45893548076901813, 'train@rus.rst.rrt_recall@rus.rst.rrt': 0.34902033813436545, 'train@rus.rst.rrt_loss@rus.rst.rrt': 1.2580629587173462, 'train@rus.rst.rrt_runtime': 350.5423, 'train@rus.rst.rrt_samples_per_second': 82.221, 'train@rus.rst.rrt_steps_per_second': 2.57, 'epoch': 11.0}
{'loss': 1.3118, 'learning_rate': 1.6666666666666667e-06, 'epoch': 11.0}
{'eval_loss': 1.3675055503845215, 'eval_accuracy@rus.rst.rrt': 0.5817863397548161, 'eval_f1@rus.rst.rrt': 0.4164286457723006, 'eval_precision@rus.rst.rrt': 0.492903661095512, 'eval_recall@rus.rst.rrt': 0.4020116175024299, 'eval_loss@rus.rst.rrt': 1.3675055503845215, 'eval_runtime': 35.0343, 'eval_samples_per_second': 81.491, 'eval_steps_per_second': 2.569, 'epoch': 11.0}
{'train@rus.rst.rrt_loss': 1.256618857383728, 'train@rus.rst.rrt_accuracy@rus.rst.rrt': 0.6097425577683714, 'train@rus.rst.rrt_f1@rus.rst.rrt': 0.3692449253420847, 'train@rus.rst.rrt_precision@rus.rst.rrt': 0.46294125779629186, 'train@rus.rst.rrt_recall@rus.rst.rrt': 0.3499461394049424, 'train@rus.rst.rrt_loss@rus.rst.rrt': 1.256618857383728, 'train@rus.rst.rrt_runtime': 350.5842, 'train@rus.rst.rrt_samples_per_second': 82.211, 'train@rus.rst.rrt_steps_per_second': 2.57, 'epoch': 12.0}
{'loss': 1.3078, 'learning_rate': 0.0, 'epoch': 12.0}
{'eval_loss': 1.366000771522522, 'eval_accuracy@rus.rst.rrt': 0.5796847635726795, 'eval_f1@rus.rst.rrt': 0.41163252779667087, 'eval_precision@rus.rst.rrt': 0.4897049126503273, 'eval_recall@rus.rst.rrt': 0.3987712088945578, 'eval_loss@rus.rst.rrt': 1.3660008907318115, 'eval_runtime': 35.009, 'eval_samples_per_second': 81.55, 'eval_steps_per_second': 2.571, 'epoch': 12.0}
{'train_runtime': 13464.1247, 'train_samples_per_second': 25.688, 'train_steps_per_second': 0.803, 'train_loss': 1.4553418542295309, 'epoch': 12.0}
***** train_log metrics *****
  epoch                    =       12.0
  train_loss               =     1.4553
  train_runtime            = 3:44:24.12
  train_samples_per_second =     25.688
  train_steps_per_second   =      0.803
{'train@fas.rst.prstc_loss': 2.3831653594970703, 'train@fas.rst.prstc_accuracy@fas.rst.prstc': 0.23829268292682926, 'train@fas.rst.prstc_f1@fas.rst.prstc': 0.023713699142138236, 'train@fas.rst.prstc_precision@fas.rst.prstc': 0.02499370293428089, 'train@fas.rst.prstc_recall@fas.rst.prstc': 0.05901778933068044, 'train@fas.rst.prstc_loss@fas.rst.prstc': 2.3831653594970703, 'train@fas.rst.prstc_runtime': 49.1474, 'train@fas.rst.prstc_samples_per_second': 83.423, 'train@fas.rst.prstc_steps_per_second': 2.625, 'epoch': 1.0}
{'loss': 2.6807, 'learning_rate': 1.8333333333333333e-05, 'epoch': 1.0}
{'eval_loss': 2.307976245880127, 'eval_accuracy@fas.rst.prstc': 0.24448897795591182, 'eval_f1@fas.rst.prstc': 0.026321467098166126, 'eval_precision@fas.rst.prstc': 0.016397849462365593, 'eval_recall@fas.rst.prstc': 0.06666666666666667, 'eval_loss@fas.rst.prstc': 2.307976007461548, 'eval_runtime': 6.2588, 'eval_samples_per_second': 79.728, 'eval_steps_per_second': 2.556, 'epoch': 1.0}
{'train@fas.rst.prstc_loss': 2.3229808807373047, 'train@fas.rst.prstc_accuracy@fas.rst.prstc': 0.2707317073170732, 'train@fas.rst.prstc_f1@fas.rst.prstc': 0.043484044188202925, 'train@fas.rst.prstc_precision@fas.rst.prstc': 0.03138236785586446, 'train@fas.rst.prstc_recall@fas.rst.prstc': 0.0708045313051571, 'train@fas.rst.prstc_loss@fas.rst.prstc': 2.322981119155884, 'train@fas.rst.prstc_runtime': 49.1492, 'train@fas.rst.prstc_samples_per_second': 83.419, 'train@fas.rst.prstc_steps_per_second': 2.625, 'epoch': 2.0}
{'loss': 2.3857, 'learning_rate': 1.6666666666666667e-05, 'epoch': 2.0}
{'eval_loss': 2.236849069595337, 'eval_accuracy@fas.rst.prstc': 0.2965931863727455, 'eval_f1@fas.rst.prstc': 0.05229996722995028, 'eval_precision@fas.rst.prstc': 0.039259652225169465, 'eval_recall@fas.rst.prstc': 0.08253741981468282, 'eval_loss@fas.rst.prstc': 2.236849546432495, 'eval_runtime': 6.2446, 'eval_samples_per_second': 79.909, 'eval_steps_per_second': 2.562, 'epoch': 2.0}
{'train@fas.rst.prstc_loss': 2.267688751220703, 'train@fas.rst.prstc_accuracy@fas.rst.prstc': 0.30365853658536585, 'train@fas.rst.prstc_f1@fas.rst.prstc': 0.050392157161857444, 'train@fas.rst.prstc_precision@fas.rst.prstc': 0.09476927162621168, 'train@fas.rst.prstc_recall@fas.rst.prstc': 0.08021257151786335, 'train@fas.rst.prstc_loss@fas.rst.prstc': 2.2676889896392822, 'train@fas.rst.prstc_runtime': 49.2107, 'train@fas.rst.prstc_samples_per_second': 83.315, 'train@fas.rst.prstc_steps_per_second': 2.621, 'epoch': 3.0}
{'loss': 2.33, 'learning_rate': 1.5000000000000002e-05, 'epoch': 3.0}
{'eval_loss': 2.183698892593384, 'eval_accuracy@fas.rst.prstc': 0.31462925851703405, 'eval_f1@fas.rst.prstc': 0.05569072559141084, 'eval_precision@fas.rst.prstc': 0.04120491345616974, 'eval_recall@fas.rst.prstc': 0.08765502494654312, 'eval_loss@fas.rst.prstc': 2.183699131011963, 'eval_runtime': 6.2644, 'eval_samples_per_second': 79.657, 'eval_steps_per_second': 2.554, 'epoch': 3.0}
{'train@fas.rst.prstc_loss': 2.151834487915039, 'train@fas.rst.prstc_accuracy@fas.rst.prstc': 0.3570731707317073, 'train@fas.rst.prstc_f1@fas.rst.prstc': 0.08661622933870744, 'train@fas.rst.prstc_precision@fas.rst.prstc': 0.11349751098203036, 'train@fas.rst.prstc_recall@fas.rst.prstc': 0.1143626123605241, 'train@fas.rst.prstc_loss@fas.rst.prstc': 2.1518349647521973, 'train@fas.rst.prstc_runtime': 49.1724, 'train@fas.rst.prstc_samples_per_second': 83.38, 'train@fas.rst.prstc_steps_per_second': 2.623, 'epoch': 4.0}
{'loss': 2.2594, 'learning_rate': 1.3333333333333333e-05, 'epoch': 4.0}
{'eval_loss': 2.0761828422546387, 'eval_accuracy@fas.rst.prstc': 0.3406813627254509, 'eval_f1@fas.rst.prstc': 0.08528490377889446, 'eval_precision@fas.rst.prstc': 0.07320367573011079, 'eval_recall@fas.rst.prstc': 0.11489464931037502, 'eval_loss@fas.rst.prstc': 2.0761828422546387, 'eval_runtime': 6.2666, 'eval_samples_per_second': 79.629, 'eval_steps_per_second': 2.553, 'epoch': 4.0}
{'train@fas.rst.prstc_loss': 2.0266497135162354, 'train@fas.rst.prstc_accuracy@fas.rst.prstc': 0.41097560975609754, 'train@fas.rst.prstc_f1@fas.rst.prstc': 0.11947085644244625, 'train@fas.rst.prstc_precision@fas.rst.prstc': 0.12199497667455729, 'train@fas.rst.prstc_recall@fas.rst.prstc': 0.14438413339842193, 'train@fas.rst.prstc_loss@fas.rst.prstc': 2.0266497135162354, 'train@fas.rst.prstc_runtime': 49.215, 'train@fas.rst.prstc_samples_per_second': 83.308, 'train@fas.rst.prstc_steps_per_second': 2.621, 'epoch': 5.0}
{'loss': 2.1358, 'learning_rate': 1.1666666666666668e-05, 'epoch': 5.0}
{'eval_loss': 1.9433077573776245, 'eval_accuracy@fas.rst.prstc': 0.4308617234468938, 'eval_f1@fas.rst.prstc': 0.1345463352505606, 'eval_precision@fas.rst.prstc': 0.14030142796834547, 'eval_recall@fas.rst.prstc': 0.15580064440026176, 'eval_loss@fas.rst.prstc': 1.9433077573776245, 'eval_runtime': 6.2707, 'eval_samples_per_second': 79.577, 'eval_steps_per_second': 2.552, 'epoch': 5.0}
{'train@fas.rst.prstc_loss': 1.9513628482818604, 'train@fas.rst.prstc_accuracy@fas.rst.prstc': 0.4265853658536585, 'train@fas.rst.prstc_f1@fas.rst.prstc': 0.13716265036737033, 'train@fas.rst.prstc_precision@fas.rst.prstc': 0.13129270798116122, 'train@fas.rst.prstc_recall@fas.rst.prstc': 0.16022042497920194, 'train@fas.rst.prstc_loss@fas.rst.prstc': 1.95136296749115, 'train@fas.rst.prstc_runtime': 49.1261, 'train@fas.rst.prstc_samples_per_second': 83.459, 'train@fas.rst.prstc_steps_per_second': 2.626, 'epoch': 6.0}
{'loss': 2.0319, 'learning_rate': 1e-05, 'epoch': 6.0}
{'eval_loss': 1.8754708766937256, 'eval_accuracy@fas.rst.prstc': 0.4468937875751503, 'eval_f1@fas.rst.prstc': 0.15828357134056062, 'eval_precision@fas.rst.prstc': 0.16777067004999963, 'eval_recall@fas.rst.prstc': 0.173831655600129, 'eval_loss@fas.rst.prstc': 1.875470757484436, 'eval_runtime': 6.2503, 'eval_samples_per_second': 79.837, 'eval_steps_per_second': 2.56, 'epoch': 6.0}
{'train@fas.rst.prstc_loss': 1.915138602256775, 'train@fas.rst.prstc_accuracy@fas.rst.prstc': 0.43585365853658536, 'train@fas.rst.prstc_f1@fas.rst.prstc': 0.14118016199553193, 'train@fas.rst.prstc_precision@fas.rst.prstc': 0.13676309622664107, 'train@fas.rst.prstc_recall@fas.rst.prstc': 0.1644334124106792, 'train@fas.rst.prstc_loss@fas.rst.prstc': 1.9151387214660645, 'train@fas.rst.prstc_runtime': 49.1352, 'train@fas.rst.prstc_samples_per_second': 83.443, 'train@fas.rst.prstc_steps_per_second': 2.625, 'epoch': 7.0}
{'loss': 1.9881, 'learning_rate': 8.333333333333334e-06, 'epoch': 7.0}
{'eval_loss': 1.843161702156067, 'eval_accuracy@fas.rst.prstc': 0.44889779559118237, 'eval_f1@fas.rst.prstc': 0.15899735693031783, 'eval_precision@fas.rst.prstc': 0.16393764069456482, 'eval_recall@fas.rst.prstc': 0.1741405458978567, 'eval_loss@fas.rst.prstc': 1.8431615829467773, 'eval_runtime': 6.2496, 'eval_samples_per_second': 79.845, 'eval_steps_per_second': 2.56, 'epoch': 7.0}
{'train@fas.rst.prstc_loss': 1.8910667896270752, 'train@fas.rst.prstc_accuracy@fas.rst.prstc': 0.4431707317073171, 'train@fas.rst.prstc_f1@fas.rst.prstc': 0.1485091245319227, 'train@fas.rst.prstc_precision@fas.rst.prstc': 0.1400199498928383, 'train@fas.rst.prstc_recall@fas.rst.prstc': 0.17012948259927355, 'train@fas.rst.prstc_loss@fas.rst.prstc': 1.8910670280456543, 'train@fas.rst.prstc_runtime': 49.1324, 'train@fas.rst.prstc_samples_per_second': 83.448, 'train@fas.rst.prstc_steps_per_second': 2.626, 'epoch': 8.0}
{'loss': 1.9567, 'learning_rate': 6.666666666666667e-06, 'epoch': 8.0}
{'eval_loss': 1.8242062330245972, 'eval_accuracy@fas.rst.prstc': 0.46092184368737477, 'eval_f1@fas.rst.prstc': 0.17663790888870573, 'eval_precision@fas.rst.prstc': 0.1855541936033963, 'eval_recall@fas.rst.prstc': 0.18693758231157, 'eval_loss@fas.rst.prstc': 1.8242062330245972, 'eval_runtime': 6.2432, 'eval_samples_per_second': 79.927, 'eval_steps_per_second': 2.563, 'epoch': 8.0}
{'train@fas.rst.prstc_loss': 1.872948169708252, 'train@fas.rst.prstc_accuracy@fas.rst.prstc': 0.4480487804878049, 'train@fas.rst.prstc_f1@fas.rst.prstc': 0.15456414499174662, 'train@fas.rst.prstc_precision@fas.rst.prstc': 0.1394500790351773, 'train@fas.rst.prstc_recall@fas.rst.prstc': 0.17920997253980442, 'train@fas.rst.prstc_loss@fas.rst.prstc': 1.8729482889175415, 'train@fas.rst.prstc_runtime': 49.1327, 'train@fas.rst.prstc_samples_per_second': 83.447, 'train@fas.rst.prstc_steps_per_second': 2.626, 'epoch': 9.0}
{'loss': 1.9345, 'learning_rate': 5e-06, 'epoch': 9.0}
{'eval_loss': 1.803868293762207, 'eval_accuracy@fas.rst.prstc': 0.46292585170340683, 'eval_f1@fas.rst.prstc': 0.18002342456602166, 'eval_precision@fas.rst.prstc': 0.17528203580526303, 'eval_recall@fas.rst.prstc': 0.1949087447889666, 'eval_loss@fas.rst.prstc': 1.803868055343628, 'eval_runtime': 6.2516, 'eval_samples_per_second': 79.82, 'eval_steps_per_second': 2.559, 'epoch': 9.0}
{'train@fas.rst.prstc_loss': 1.8615927696228027, 'train@fas.rst.prstc_accuracy@fas.rst.prstc': 0.4529268292682927, 'train@fas.rst.prstc_f1@fas.rst.prstc': 0.15796409685494428, 'train@fas.rst.prstc_precision@fas.rst.prstc': 0.14187092625984324, 'train@fas.rst.prstc_recall@fas.rst.prstc': 0.1833586118478025, 'train@fas.rst.prstc_loss@fas.rst.prstc': 1.8615926504135132, 'train@fas.rst.prstc_runtime': 49.121, 'train@fas.rst.prstc_samples_per_second': 83.467, 'train@fas.rst.prstc_steps_per_second': 2.626, 'epoch': 10.0}
{'loss': 1.9207, 'learning_rate': 3.3333333333333333e-06, 'epoch': 10.0}
{'eval_loss': 1.796225666999817, 'eval_accuracy@fas.rst.prstc': 0.4709418837675351, 'eval_f1@fas.rst.prstc': 0.18655002590157663, 'eval_precision@fas.rst.prstc': 0.18036896581301137, 'eval_recall@fas.rst.prstc': 0.20154631867059938, 'eval_loss@fas.rst.prstc': 1.7962260246276855, 'eval_runtime': 6.2462, 'eval_samples_per_second': 79.888, 'eval_steps_per_second': 2.562, 'epoch': 10.0}
{'train@fas.rst.prstc_loss': 1.8544602394104004, 'train@fas.rst.prstc_accuracy@fas.rst.prstc': 0.4531707317073171, 'train@fas.rst.prstc_f1@fas.rst.prstc': 0.1572027623190553, 'train@fas.rst.prstc_precision@fas.rst.prstc': 0.1505524322667668, 'train@fas.rst.prstc_recall@fas.rst.prstc': 0.1818349692566653, 'train@fas.rst.prstc_loss@fas.rst.prstc': 1.8544602394104004, 'train@fas.rst.prstc_runtime': 49.1311, 'train@fas.rst.prstc_samples_per_second': 83.45, 'train@fas.rst.prstc_steps_per_second': 2.626, 'epoch': 11.0}
{'loss': 1.9096, 'learning_rate': 1.6666666666666667e-06, 'epoch': 11.0}
{'eval_loss': 1.7889509201049805, 'eval_accuracy@fas.rst.prstc': 0.4749498997995992, 'eval_f1@fas.rst.prstc': 0.18824472178577026, 'eval_precision@fas.rst.prstc': 0.1838779863032985, 'eval_recall@fas.rst.prstc': 0.20270573896045446, 'eval_loss@fas.rst.prstc': 1.788950800895691, 'eval_runtime': 6.2415, 'eval_samples_per_second': 79.948, 'eval_steps_per_second': 2.563, 'epoch': 11.0}
{'train@fas.rst.prstc_loss': 1.8540323972702026, 'train@fas.rst.prstc_accuracy@fas.rst.prstc': 0.45365853658536587, 'train@fas.rst.prstc_f1@fas.rst.prstc': 0.15603196397414906, 'train@fas.rst.prstc_precision@fas.rst.prstc': 0.15306881110639528, 'train@fas.rst.prstc_recall@fas.rst.prstc': 0.1807871074306569, 'train@fas.rst.prstc_loss@fas.rst.prstc': 1.8540325164794922, 'train@fas.rst.prstc_runtime': 49.1286, 'train@fas.rst.prstc_samples_per_second': 83.454, 'train@fas.rst.prstc_steps_per_second': 2.626, 'epoch': 12.0}
{'loss': 1.9094, 'learning_rate': 0.0, 'epoch': 12.0}
{'eval_loss': 1.7912518978118896, 'eval_accuracy@fas.rst.prstc': 0.4729458917835671, 'eval_f1@fas.rst.prstc': 0.1879771479243504, 'eval_precision@fas.rst.prstc': 0.18436720142602497, 'eval_recall@fas.rst.prstc': 0.20212602881552694, 'eval_loss@fas.rst.prstc': 1.7912517786026, 'eval_runtime': 6.2578, 'eval_samples_per_second': 79.74, 'eval_steps_per_second': 2.557, 'epoch': 12.0}
{'train_runtime': 1900.1604, 'train_samples_per_second': 25.893, 'train_steps_per_second': 0.815, 'train_loss': 2.120196162576207, 'epoch': 12.0}
***** train_logger metrics *****
  epoch                    =       12.0
  train_loss               =     1.4553
  train_runtime            = 3:44:24.12
  train_samples_per_second =     25.688
  train_steps_per_second   =      0.803
-------------------------------------------------------------------
Lang1:  spa.rst.rststb    Lang2:  fas.rst.prstc
Saving run to:  runs/full_shot/FullShot=v4_spa.rst.rststb_fas.rst.prstc_lrfunc=Adafactor_lr=2e-05_plm=bert-base-multilingual-cased
Running with params: BERT_MODEL=bert-base-multilingual-cased lr=2e-05
read 2240 examples
read 383 examples
read 426 examples
read 4100 examples
read 499 examples
read 592 examples
Total prediction labels:  37
BertAdapterModel(
  (shared_parameters): ModuleDict()
  (bert): BertModel(
    (shared_parameters): ModuleDict()
    (invertible_adapters): ModuleDict(
      (es): NICECouplingBlock(
        (F): Sequential(
          (0): Linear(in_features=384, out_features=192, bias=True)
          (1): Activation_Function_Class(
            (f): NewGELUActivation()
          )
          (2): Linear(in_features=192, out_features=384, bias=True)
        )
        (G): Sequential(
          (0): Linear(in_features=384, out_features=192, bias=True)
          (1): Activation_Function_Class(
            (f): NewGELUActivation()
          )
          (2): Linear(in_features=192, out_features=384, bias=True)
        )
      )
      (ar): NICECouplingBlock(
        (F): Sequential(
          (0): Linear(in_features=384, out_features=192, bias=True)
          (1): Activation_Function_Class(
            (f): ReLU()
          )
          (2): Linear(in_features=192, out_features=384, bias=True)
        )
        (G): Sequential(
          (0): Linear(in_features=384, out_features=192, bias=True)
          (1): Activation_Function_Class(
            (f): ReLU()
          )
          (2): Linear(in_features=192, out_features=384, bias=True)
        )
      )
    )
    (embeddings): BertEmbeddings(
      (word_embeddings): Embedding(119547, 768, padding_idx=0)
      (position_embeddings): Embedding(512, 768)
      (token_type_embeddings): Embedding(2, 768)
      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
      (dropout): Dropout(p=0.1, inplace=False)
    )
    (encoder): BertEncoder(
      (layer): ModuleList(
        (0): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (es): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (ar): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (1): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (es): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (ar): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (2): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (es): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (ar): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (3): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (es): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (ar): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (4): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (es): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (ar): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (5): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (es): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (ar): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (6): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (es): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (ar): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (7): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (es): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (ar): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (8): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (es): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (ar): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (9): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (es): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (ar): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (10): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (es): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (ar): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (11): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (es): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (ar): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
      )
    )
    (pooler): BertPooler(
      (dense): Linear(in_features=768, out_features=768, bias=True)
      (activation): Tanh()
    )
    (prefix_tuning): PrefixTuningPool(
      (prefix_tunings): ModuleDict()
    )
  )
  (heads): ModuleDict(
    (disrpt): ClassificationHead(
      (0): Dropout(p=0.1, inplace=False)
      (1): Linear(in_features=768, out_features=768, bias=True)
      (2): Activation_Function_Class(
        (f): Tanh()
      )
      (3): Dropout(p=0.1, inplace=False)
      (4): Linear(in_features=768, out_features=37, bias=True)
    )
  )
)
{'train@spa.rst.rststb_loss': 2.8957619667053223, 'train@spa.rst.rststb_accuracy@spa.rst.rststb': 0.22723214285714285, 'train@spa.rst.rststb_f1@spa.rst.rststb': 0.023946340760860833, 'train@spa.rst.rststb_precision@spa.rst.rststb': 0.028439653773222644, 'train@spa.rst.rststb_recall@spa.rst.rststb': 0.04340163195041911, 'train@spa.rst.rststb_loss@spa.rst.rststb': 2.8957619667053223, 'train@spa.rst.rststb_runtime': 27.3998, 'train@spa.rst.rststb_samples_per_second': 81.752, 'train@spa.rst.rststb_steps_per_second': 2.555, 'epoch': 1.0}
{'loss': 3.2708, 'learning_rate': 1.8333333333333333e-05, 'epoch': 1.0}
{'eval_loss': 2.9371488094329834, 'eval_accuracy@spa.rst.rststb': 0.20887728459530025, 'eval_f1@spa.rst.rststb': 0.016836230757386272, 'eval_precision@spa.rst.rststb': 0.015370600414078674, 'eval_recall@spa.rst.rststb': 0.044466403162055336, 'eval_loss@spa.rst.rststb': 2.937148332595825, 'eval_runtime': 7.5851, 'eval_samples_per_second': 50.494, 'eval_steps_per_second': 1.582, 'epoch': 1.0}
{'train@spa.rst.rststb_loss': 2.5953478813171387, 'train@spa.rst.rststb_accuracy@spa.rst.rststb': 0.2741071428571429, 'train@spa.rst.rststb_f1@spa.rst.rststb': 0.03861760631067083, 'train@spa.rst.rststb_precision@spa.rst.rststb': 0.04308978299768513, 'train@spa.rst.rststb_recall@spa.rst.rststb': 0.05576803399499234, 'train@spa.rst.rststb_loss@spa.rst.rststb': 2.5953478813171387, 'train@spa.rst.rststb_runtime': 27.5061, 'train@spa.rst.rststb_samples_per_second': 81.437, 'train@spa.rst.rststb_steps_per_second': 2.545, 'epoch': 2.0}
{'loss': 2.7454, 'learning_rate': 1.6666666666666667e-05, 'epoch': 2.0}
{'eval_loss': 2.706897735595703, 'eval_accuracy@spa.rst.rststb': 0.22715404699738903, 'eval_f1@spa.rst.rststb': 0.0302936582424129, 'eval_precision@spa.rst.rststb': 0.03959859551779444, 'eval_recall@spa.rst.rststb': 0.052018326837817715, 'eval_loss@spa.rst.rststb': 2.706897735595703, 'eval_runtime': 4.9833, 'eval_samples_per_second': 76.856, 'eval_steps_per_second': 2.408, 'epoch': 2.0}
{'train@spa.rst.rststb_loss': 2.465639352798462, 'train@spa.rst.rststb_accuracy@spa.rst.rststb': 0.31205357142857143, 'train@spa.rst.rststb_f1@spa.rst.rststb': 0.046449005799434166, 'train@spa.rst.rststb_precision@spa.rst.rststb': 0.054632449444160194, 'train@spa.rst.rststb_recall@spa.rst.rststb': 0.06802028073488682, 'train@spa.rst.rststb_loss@spa.rst.rststb': 2.465639352798462, 'train@spa.rst.rststb_runtime': 27.534, 'train@spa.rst.rststb_samples_per_second': 81.354, 'train@spa.rst.rststb_steps_per_second': 2.542, 'epoch': 3.0}
{'loss': 2.5623, 'learning_rate': 1.5000000000000002e-05, 'epoch': 3.0}
{'eval_loss': 2.6092607975006104, 'eval_accuracy@spa.rst.rststb': 0.2689295039164491, 'eval_f1@spa.rst.rststb': 0.045488935580687166, 'eval_precision@spa.rst.rststb': 0.03724698417462045, 'eval_recall@spa.rst.rststb': 0.06905496035109389, 'eval_loss@spa.rst.rststb': 2.6092607975006104, 'eval_runtime': 4.9953, 'eval_samples_per_second': 76.672, 'eval_steps_per_second': 2.402, 'epoch': 3.0}
{'train@spa.rst.rststb_loss': 2.3665759563446045, 'train@spa.rst.rststb_accuracy@spa.rst.rststb': 0.33348214285714284, 'train@spa.rst.rststb_f1@spa.rst.rststb': 0.05960901946349957, 'train@spa.rst.rststb_precision@spa.rst.rststb': 0.07915501416665484, 'train@spa.rst.rststb_recall@spa.rst.rststb': 0.07581754626229563, 'train@spa.rst.rststb_loss@spa.rst.rststb': 2.3665759563446045, 'train@spa.rst.rststb_runtime': 27.531, 'train@spa.rst.rststb_samples_per_second': 81.363, 'train@spa.rst.rststb_steps_per_second': 2.543, 'epoch': 4.0}
{'loss': 2.4658, 'learning_rate': 1.3333333333333333e-05, 'epoch': 4.0}
{'eval_loss': 2.538492202758789, 'eval_accuracy@spa.rst.rststb': 0.2950391644908616, 'eval_f1@spa.rst.rststb': 0.05906888979013747, 'eval_precision@spa.rst.rststb': 0.0677767499229109, 'eval_recall@spa.rst.rststb': 0.07800363413190857, 'eval_loss@spa.rst.rststb': 2.538492202758789, 'eval_runtime': 5.0096, 'eval_samples_per_second': 76.453, 'eval_steps_per_second': 2.395, 'epoch': 4.0}
{'train@spa.rst.rststb_loss': 2.2746427059173584, 'train@spa.rst.rststb_accuracy@spa.rst.rststb': 0.36875, 'train@spa.rst.rststb_f1@spa.rst.rststb': 0.07998828466773349, 'train@spa.rst.rststb_precision@spa.rst.rststb': 0.08328423302566523, 'train@spa.rst.rststb_recall@spa.rst.rststb': 0.095780935942585, 'train@spa.rst.rststb_loss@spa.rst.rststb': 2.2746429443359375, 'train@spa.rst.rststb_runtime': 27.5684, 'train@spa.rst.rststb_samples_per_second': 81.253, 'train@spa.rst.rststb_steps_per_second': 2.539, 'epoch': 5.0}
{'loss': 2.3699, 'learning_rate': 1.1666666666666668e-05, 'epoch': 5.0}
{'eval_loss': 2.4668750762939453, 'eval_accuracy@spa.rst.rststb': 0.3342036553524804, 'eval_f1@spa.rst.rststb': 0.09154157895780977, 'eval_precision@spa.rst.rststb': 0.11264315807223223, 'eval_recall@spa.rst.rststb': 0.10530410865143366, 'eval_loss@spa.rst.rststb': 2.466874599456787, 'eval_runtime': 5.0358, 'eval_samples_per_second': 76.055, 'eval_steps_per_second': 2.383, 'epoch': 5.0}
{'train@spa.rst.rststb_loss': 2.205296516418457, 'train@spa.rst.rststb_accuracy@spa.rst.rststb': 0.3875, 'train@spa.rst.rststb_f1@spa.rst.rststb': 0.08800460240279476, 'train@spa.rst.rststb_precision@spa.rst.rststb': 0.08561009546789289, 'train@spa.rst.rststb_recall@spa.rst.rststb': 0.1068962771268884, 'train@spa.rst.rststb_loss@spa.rst.rststb': 2.205296516418457, 'train@spa.rst.rststb_runtime': 27.5025, 'train@spa.rst.rststb_samples_per_second': 81.447, 'train@spa.rst.rststb_steps_per_second': 2.545, 'epoch': 6.0}
{'loss': 2.2929, 'learning_rate': 1e-05, 'epoch': 6.0}
{'eval_loss': 2.413149356842041, 'eval_accuracy@spa.rst.rststb': 0.35509138381201044, 'eval_f1@spa.rst.rststb': 0.09755644958963355, 'eval_precision@spa.rst.rststb': 0.11514322750648137, 'eval_recall@spa.rst.rststb': 0.11459228882568981, 'eval_loss@spa.rst.rststb': 2.413149356842041, 'eval_runtime': 4.9933, 'eval_samples_per_second': 76.702, 'eval_steps_per_second': 2.403, 'epoch': 6.0}
{'train@spa.rst.rststb_loss': 2.145974636077881, 'train@spa.rst.rststb_accuracy@spa.rst.rststb': 0.40714285714285714, 'train@spa.rst.rststb_f1@spa.rst.rststb': 0.09703477100343201, 'train@spa.rst.rststb_precision@spa.rst.rststb': 0.08797097877857732, 'train@spa.rst.rststb_recall@spa.rst.rststb': 0.12028168793595016, 'train@spa.rst.rststb_loss@spa.rst.rststb': 2.14597487449646, 'train@spa.rst.rststb_runtime': 27.5098, 'train@spa.rst.rststb_samples_per_second': 81.425, 'train@spa.rst.rststb_steps_per_second': 2.545, 'epoch': 7.0}
{'loss': 2.231, 'learning_rate': 8.333333333333334e-06, 'epoch': 7.0}
{'eval_loss': 2.369424343109131, 'eval_accuracy@spa.rst.rststb': 0.3733681462140992, 'eval_f1@spa.rst.rststb': 0.10814998364647652, 'eval_precision@spa.rst.rststb': 0.11044368120380953, 'eval_recall@spa.rst.rststb': 0.12887776041633883, 'eval_loss@spa.rst.rststb': 2.369424343109131, 'eval_runtime': 5.0051, 'eval_samples_per_second': 76.523, 'eval_steps_per_second': 2.398, 'epoch': 7.0}
{'train@spa.rst.rststb_loss': 2.1025748252868652, 'train@spa.rst.rststb_accuracy@spa.rst.rststb': 0.4147321428571429, 'train@spa.rst.rststb_f1@spa.rst.rststb': 0.10050749237519051, 'train@spa.rst.rststb_precision@spa.rst.rststb': 0.10115468607481984, 'train@spa.rst.rststb_recall@spa.rst.rststb': 0.12493229316965829, 'train@spa.rst.rststb_loss@spa.rst.rststb': 2.1025748252868652, 'train@spa.rst.rststb_runtime': 27.5026, 'train@spa.rst.rststb_samples_per_second': 81.447, 'train@spa.rst.rststb_steps_per_second': 2.545, 'epoch': 8.0}
{'loss': 2.1807, 'learning_rate': 6.666666666666667e-06, 'epoch': 8.0}
{'eval_loss': 2.33526873588562, 'eval_accuracy@spa.rst.rststb': 0.3785900783289817, 'eval_f1@spa.rst.rststb': 0.11159912361666223, 'eval_precision@spa.rst.rststb': 0.1080173004372089, 'eval_recall@spa.rst.rststb': 0.13486290075756063, 'eval_loss@spa.rst.rststb': 2.335268974304199, 'eval_runtime': 5.0044, 'eval_samples_per_second': 76.533, 'eval_steps_per_second': 2.398, 'epoch': 8.0}
{'train@spa.rst.rststb_loss': 2.069504976272583, 'train@spa.rst.rststb_accuracy@spa.rst.rststb': 0.41696428571428573, 'train@spa.rst.rststb_f1@spa.rst.rststb': 0.1007518064141416, 'train@spa.rst.rststb_precision@spa.rst.rststb': 0.11403065865113542, 'train@spa.rst.rststb_recall@spa.rst.rststb': 0.12754902110210845, 'train@spa.rst.rststb_loss@spa.rst.rststb': 2.069504976272583, 'train@spa.rst.rststb_runtime': 27.4779, 'train@spa.rst.rststb_samples_per_second': 81.52, 'train@spa.rst.rststb_steps_per_second': 2.547, 'epoch': 9.0}
{'loss': 2.1465, 'learning_rate': 5e-06, 'epoch': 9.0}
{'eval_loss': 2.3066117763519287, 'eval_accuracy@spa.rst.rststb': 0.39425587467362927, 'eval_f1@spa.rst.rststb': 0.11179656378497367, 'eval_precision@spa.rst.rststb': 0.10248084109856054, 'eval_recall@spa.rst.rststb': 0.14100199244647615, 'eval_loss@spa.rst.rststb': 2.3066115379333496, 'eval_runtime': 4.9934, 'eval_samples_per_second': 76.701, 'eval_steps_per_second': 2.403, 'epoch': 9.0}
{'train@spa.rst.rststb_loss': 2.0474231243133545, 'train@spa.rst.rststb_accuracy@spa.rst.rststb': 0.4232142857142857, 'train@spa.rst.rststb_f1@spa.rst.rststb': 0.10340140198970567, 'train@spa.rst.rststb_precision@spa.rst.rststb': 0.1101647845787399, 'train@spa.rst.rststb_recall@spa.rst.rststb': 0.12980904454353662, 'train@spa.rst.rststb_loss@spa.rst.rststb': 2.0474228858947754, 'train@spa.rst.rststb_runtime': 27.4886, 'train@spa.rst.rststb_samples_per_second': 81.488, 'train@spa.rst.rststb_steps_per_second': 2.547, 'epoch': 10.0}
{'loss': 2.1138, 'learning_rate': 3.3333333333333333e-06, 'epoch': 10.0}
{'eval_loss': 2.2914717197418213, 'eval_accuracy@spa.rst.rststb': 0.39425587467362927, 'eval_f1@spa.rst.rststb': 0.11541472490642772, 'eval_precision@spa.rst.rststb': 0.1042604496080488, 'eval_recall@spa.rst.rststb': 0.14588641732264565, 'eval_loss@spa.rst.rststb': 2.2914717197418213, 'eval_runtime': 5.0007, 'eval_samples_per_second': 76.589, 'eval_steps_per_second': 2.4, 'epoch': 10.0}
{'train@spa.rst.rststb_loss': 2.0348353385925293, 'train@spa.rst.rststb_accuracy@spa.rst.rststb': 0.4263392857142857, 'train@spa.rst.rststb_f1@spa.rst.rststb': 0.1050136158248131, 'train@spa.rst.rststb_precision@spa.rst.rststb': 0.11258143154030809, 'train@spa.rst.rststb_recall@spa.rst.rststb': 0.13068974315906545, 'train@spa.rst.rststb_loss@spa.rst.rststb': 2.0348355770111084, 'train@spa.rst.rststb_runtime': 27.5508, 'train@spa.rst.rststb_samples_per_second': 81.304, 'train@spa.rst.rststb_steps_per_second': 2.541, 'epoch': 11.0}
{'loss': 2.0973, 'learning_rate': 1.6666666666666667e-06, 'epoch': 11.0}
{'eval_loss': 2.28102445602417, 'eval_accuracy@spa.rst.rststb': 0.39947780678851175, 'eval_f1@spa.rst.rststb': 0.11711350154914602, 'eval_precision@spa.rst.rststb': 0.10655445277215131, 'eval_recall@spa.rst.rststb': 0.14708570432188173, 'eval_loss@spa.rst.rststb': 2.28102445602417, 'eval_runtime': 5.003, 'eval_samples_per_second': 76.553, 'eval_steps_per_second': 2.399, 'epoch': 11.0}
{'train@spa.rst.rststb_loss': 2.0308241844177246, 'train@spa.rst.rststb_accuracy@spa.rst.rststb': 0.4276785714285714, 'train@spa.rst.rststb_f1@spa.rst.rststb': 0.10582026113541979, 'train@spa.rst.rststb_precision@spa.rst.rststb': 0.11315918445837794, 'train@spa.rst.rststb_recall@spa.rst.rststb': 0.13154279647223507, 'train@spa.rst.rststb_loss@spa.rst.rststb': 2.0308239459991455, 'train@spa.rst.rststb_runtime': 27.5293, 'train@spa.rst.rststb_samples_per_second': 81.368, 'train@spa.rst.rststb_steps_per_second': 2.543, 'epoch': 12.0}
{'loss': 2.0865, 'learning_rate': 0.0, 'epoch': 12.0}
{'eval_loss': 2.278146743774414, 'eval_accuracy@spa.rst.rststb': 0.3968668407310705, 'eval_f1@spa.rst.rststb': 0.11691727396375073, 'eval_precision@spa.rst.rststb': 0.10669971201873629, 'eval_recall@spa.rst.rststb': 0.1460975620293916, 'eval_loss@spa.rst.rststb': 2.278146505355835, 'eval_runtime': 4.9961, 'eval_samples_per_second': 76.66, 'eval_steps_per_second': 2.402, 'epoch': 12.0}
{'train_runtime': 1078.8467, 'train_samples_per_second': 24.915, 'train_steps_per_second': 0.779, 'train_loss': 2.3802457355317617, 'epoch': 12.0}
***** train_log metrics *****
  epoch                    =       12.0
  train_loss               =     2.3802
  train_runtime            = 0:17:58.84
  train_samples_per_second =     24.915
  train_steps_per_second   =      0.779
{'train@fas.rst.prstc_loss': 2.411264657974243, 'train@fas.rst.prstc_accuracy@fas.rst.prstc': 0.23804878048780487, 'train@fas.rst.prstc_f1@fas.rst.prstc': 0.022745497342846945, 'train@fas.rst.prstc_precision@fas.rst.prstc': 0.07281546431697832, 'train@fas.rst.prstc_recall@fas.rst.prstc': 0.05889306077040746, 'train@fas.rst.prstc_loss@fas.rst.prstc': 2.411264419555664, 'train@fas.rst.prstc_runtime': 49.2283, 'train@fas.rst.prstc_samples_per_second': 83.285, 'train@fas.rst.prstc_steps_per_second': 2.62, 'epoch': 1.0}
{'loss': 2.8091, 'learning_rate': 1.8333333333333333e-05, 'epoch': 1.0}
{'eval_loss': 2.3274149894714355, 'eval_accuracy@fas.rst.prstc': 0.24448897795591182, 'eval_f1@fas.rst.prstc': 0.026194310252281264, 'eval_precision@fas.rst.prstc': 0.016299265197060788, 'eval_recall@fas.rst.prstc': 0.06666666666666667, 'eval_loss@fas.rst.prstc': 2.3274147510528564, 'eval_runtime': 6.3184, 'eval_samples_per_second': 78.976, 'eval_steps_per_second': 2.532, 'epoch': 1.0}
{'train@fas.rst.prstc_loss': 2.3586337566375732, 'train@fas.rst.prstc_accuracy@fas.rst.prstc': 0.25902439024390245, 'train@fas.rst.prstc_f1@fas.rst.prstc': 0.04085366753553306, 'train@fas.rst.prstc_precision@fas.rst.prstc': 0.030111790724998276, 'train@fas.rst.prstc_recall@fas.rst.prstc': 0.06712664334691869, 'train@fas.rst.prstc_loss@fas.rst.prstc': 2.3586337566375732, 'train@fas.rst.prstc_runtime': 49.2243, 'train@fas.rst.prstc_samples_per_second': 83.292, 'train@fas.rst.prstc_steps_per_second': 2.621, 'epoch': 2.0}
{'loss': 2.4041, 'learning_rate': 1.6666666666666667e-05, 'epoch': 2.0}
{'eval_loss': 2.2724854946136475, 'eval_accuracy@fas.rst.prstc': 0.27655310621242485, 'eval_f1@fas.rst.prstc': 0.04893426201718419, 'eval_precision@fas.rst.prstc': 0.03629203988507689, 'eval_recall@fas.rst.prstc': 0.07703967688287004, 'eval_loss@fas.rst.prstc': 2.2724862098693848, 'eval_runtime': 6.3062, 'eval_samples_per_second': 79.128, 'eval_steps_per_second': 2.537, 'epoch': 2.0}
{'train@fas.rst.prstc_loss': 2.3410089015960693, 'train@fas.rst.prstc_accuracy@fas.rst.prstc': 0.24170731707317072, 'train@fas.rst.prstc_f1@fas.rst.prstc': 0.02559989190574051, 'train@fas.rst.prstc_precision@fas.rst.prstc': 0.03316804663041688, 'train@fas.rst.prstc_recall@fas.rst.prstc': 0.05999122835167891, 'train@fas.rst.prstc_loss@fas.rst.prstc': 2.3410091400146484, 'train@fas.rst.prstc_runtime': 49.2868, 'train@fas.rst.prstc_samples_per_second': 83.187, 'train@fas.rst.prstc_steps_per_second': 2.617, 'epoch': 3.0}
{'loss': 2.3641, 'learning_rate': 1.5000000000000002e-05, 'epoch': 3.0}
{'eval_loss': 2.2570931911468506, 'eval_accuracy@fas.rst.prstc': 0.24048096192384769, 'eval_f1@fas.rst.prstc': 0.026868686868686865, 'eval_precision@fas.rst.prstc': 0.029392712550607287, 'eval_recall@fas.rst.prstc': 0.06560703254929912, 'eval_loss@fas.rst.prstc': 2.2570931911468506, 'eval_runtime': 6.2986, 'eval_samples_per_second': 79.224, 'eval_steps_per_second': 2.54, 'epoch': 3.0}
{'train@fas.rst.prstc_loss': 2.3267931938171387, 'train@fas.rst.prstc_accuracy@fas.rst.prstc': 0.23780487804878048, 'train@fas.rst.prstc_f1@fas.rst.prstc': 0.02260214430599826, 'train@fas.rst.prstc_precision@fas.rst.prstc': 0.013988522238163558, 'train@fas.rst.prstc_recall@fas.rst.prstc': 0.058823529411764705, 'train@fas.rst.prstc_loss@fas.rst.prstc': 2.3267934322357178, 'train@fas.rst.prstc_runtime': 49.1888, 'train@fas.rst.prstc_samples_per_second': 83.352, 'train@fas.rst.prstc_steps_per_second': 2.623, 'epoch': 4.0}
{'loss': 2.3506, 'learning_rate': 1.3333333333333333e-05, 'epoch': 4.0}
{'eval_loss': 2.247664213180542, 'eval_accuracy@fas.rst.prstc': 0.24448897795591182, 'eval_f1@fas.rst.prstc': 0.026194310252281264, 'eval_precision@fas.rst.prstc': 0.016299265197060788, 'eval_recall@fas.rst.prstc': 0.06666666666666667, 'eval_loss@fas.rst.prstc': 2.247664213180542, 'eval_runtime': 6.3244, 'eval_samples_per_second': 78.901, 'eval_steps_per_second': 2.53, 'epoch': 4.0}
{'train@fas.rst.prstc_loss': 2.3156368732452393, 'train@fas.rst.prstc_accuracy@fas.rst.prstc': 0.25926829268292684, 'train@fas.rst.prstc_f1@fas.rst.prstc': 0.035708287106651374, 'train@fas.rst.prstc_precision@fas.rst.prstc': 0.035706307644571465, 'train@fas.rst.prstc_recall@fas.rst.prstc': 0.06529187124931805, 'train@fas.rst.prstc_loss@fas.rst.prstc': 2.3156368732452393, 'train@fas.rst.prstc_runtime': 49.2276, 'train@fas.rst.prstc_samples_per_second': 83.287, 'train@fas.rst.prstc_steps_per_second': 2.62, 'epoch': 5.0}
{'loss': 2.3402, 'learning_rate': 1.1666666666666668e-05, 'epoch': 5.0}
{'eval_loss': 2.235229730606079, 'eval_accuracy@fas.rst.prstc': 0.2605210420841683, 'eval_f1@fas.rst.prstc': 0.03756818042532328, 'eval_precision@fas.rst.prstc': 0.04112368318376902, 'eval_recall@fas.rst.prstc': 0.07143739605607033, 'eval_loss@fas.rst.prstc': 2.235229969024658, 'eval_runtime': 6.309, 'eval_samples_per_second': 79.094, 'eval_steps_per_second': 2.536, 'epoch': 5.0}
{'train@fas.rst.prstc_loss': 2.3039767742156982, 'train@fas.rst.prstc_accuracy@fas.rst.prstc': 0.23829268292682926, 'train@fas.rst.prstc_f1@fas.rst.prstc': 0.022896789465029113, 'train@fas.rst.prstc_precision@fas.rst.prstc': 0.043413947610294115, 'train@fas.rst.prstc_recall@fas.rst.prstc': 0.058962592129050195, 'train@fas.rst.prstc_loss@fas.rst.prstc': 2.3039770126342773, 'train@fas.rst.prstc_runtime': 49.234, 'train@fas.rst.prstc_samples_per_second': 83.276, 'train@fas.rst.prstc_steps_per_second': 2.62, 'epoch': 6.0}
{'loss': 2.3193, 'learning_rate': 1e-05, 'epoch': 6.0}
{'eval_loss': 2.230809450149536, 'eval_accuracy@fas.rst.prstc': 0.24448897795591182, 'eval_f1@fas.rst.prstc': 0.026194310252281264, 'eval_precision@fas.rst.prstc': 0.016299265197060788, 'eval_recall@fas.rst.prstc': 0.06666666666666667, 'eval_loss@fas.rst.prstc': 2.230809450149536, 'eval_runtime': 6.3263, 'eval_samples_per_second': 78.878, 'eval_steps_per_second': 2.529, 'epoch': 6.0}
{'train@fas.rst.prstc_loss': 2.2859208583831787, 'train@fas.rst.prstc_accuracy@fas.rst.prstc': 0.2804878048780488, 'train@fas.rst.prstc_f1@fas.rst.prstc': 0.04234333417444742, 'train@fas.rst.prstc_precision@fas.rst.prstc': 0.036527068297002495, 'train@fas.rst.prstc_recall@fas.rst.prstc': 0.07144229432089257, 'train@fas.rst.prstc_loss@fas.rst.prstc': 2.2859208583831787, 'train@fas.rst.prstc_runtime': 49.2006, 'train@fas.rst.prstc_samples_per_second': 83.332, 'train@fas.rst.prstc_steps_per_second': 2.622, 'epoch': 7.0}
{'loss': 2.313, 'learning_rate': 8.333333333333334e-06, 'epoch': 7.0}
{'eval_loss': 2.2087368965148926, 'eval_accuracy@fas.rst.prstc': 0.28256513026052105, 'eval_f1@fas.rst.prstc': 0.046618357487922694, 'eval_precision@fas.rst.prstc': 0.04295023031120098, 'eval_recall@fas.rst.prstc': 0.07791399382276075, 'eval_loss@fas.rst.prstc': 2.2087371349334717, 'eval_runtime': 6.3706, 'eval_samples_per_second': 78.329, 'eval_steps_per_second': 2.512, 'epoch': 7.0}
{'train@fas.rst.prstc_loss': 2.2609291076660156, 'train@fas.rst.prstc_accuracy@fas.rst.prstc': 0.3182926829268293, 'train@fas.rst.prstc_f1@fas.rst.prstc': 0.051140739423747666, 'train@fas.rst.prstc_precision@fas.rst.prstc': 0.037063145625006205, 'train@fas.rst.prstc_recall@fas.rst.prstc': 0.0832868008087032, 'train@fas.rst.prstc_loss@fas.rst.prstc': 2.2609291076660156, 'train@fas.rst.prstc_runtime': 49.1924, 'train@fas.rst.prstc_samples_per_second': 83.346, 'train@fas.rst.prstc_steps_per_second': 2.622, 'epoch': 8.0}
{'loss': 2.299, 'learning_rate': 6.666666666666667e-06, 'epoch': 8.0}
{'eval_loss': 2.1882786750793457, 'eval_accuracy@fas.rst.prstc': 0.32665330661322645, 'eval_f1@fas.rst.prstc': 0.05843279083785413, 'eval_precision@fas.rst.prstc': 0.04337484835331174, 'eval_recall@fas.rst.prstc': 0.09119980993110002, 'eval_loss@fas.rst.prstc': 2.1882784366607666, 'eval_runtime': 6.2856, 'eval_samples_per_second': 79.387, 'eval_steps_per_second': 2.545, 'epoch': 8.0}
{'train@fas.rst.prstc_loss': 2.230248212814331, 'train@fas.rst.prstc_accuracy@fas.rst.prstc': 0.30585365853658536, 'train@fas.rst.prstc_f1@fas.rst.prstc': 0.0486888323834534, 'train@fas.rst.prstc_precision@fas.rst.prstc': 0.03571356632739736, 'train@fas.rst.prstc_recall@fas.rst.prstc': 0.07954751131221718, 'train@fas.rst.prstc_loss@fas.rst.prstc': 2.230248212814331, 'train@fas.rst.prstc_runtime': 49.1878, 'train@fas.rst.prstc_samples_per_second': 83.354, 'train@fas.rst.prstc_steps_per_second': 2.623, 'epoch': 9.0}
{'loss': 2.2716, 'learning_rate': 5e-06, 'epoch': 9.0}
{'eval_loss': 2.15512752532959, 'eval_accuracy@fas.rst.prstc': 0.30861723446893785, 'eval_f1@fas.rst.prstc': 0.054822866597551645, 'eval_precision@fas.rst.prstc': 0.04171657415522446, 'eval_recall@fas.rst.prstc': 0.0859158945117605, 'eval_loss@fas.rst.prstc': 2.1551272869110107, 'eval_runtime': 6.2974, 'eval_samples_per_second': 79.239, 'eval_steps_per_second': 2.541, 'epoch': 9.0}
{'train@fas.rst.prstc_loss': 2.1987431049346924, 'train@fas.rst.prstc_accuracy@fas.rst.prstc': 0.31585365853658537, 'train@fas.rst.prstc_f1@fas.rst.prstc': 0.0521384815050814, 'train@fas.rst.prstc_precision@fas.rst.prstc': 0.05392601890071555, 'train@fas.rst.prstc_recall@fas.rst.prstc': 0.083449432736041, 'train@fas.rst.prstc_loss@fas.rst.prstc': 2.1987433433532715, 'train@fas.rst.prstc_runtime': 49.1337, 'train@fas.rst.prstc_samples_per_second': 83.446, 'train@fas.rst.prstc_steps_per_second': 2.625, 'epoch': 10.0}
{'loss': 2.2548, 'learning_rate': 3.3333333333333333e-06, 'epoch': 10.0}
{'eval_loss': 2.1186633110046387, 'eval_accuracy@fas.rst.prstc': 0.32665330661322645, 'eval_f1@fas.rst.prstc': 0.06201953286686672, 'eval_precision@fas.rst.prstc': 0.07676329284750336, 'eval_recall@fas.rst.prstc': 0.09259559795554093, 'eval_loss@fas.rst.prstc': 2.1186635494232178, 'eval_runtime': 6.2929, 'eval_samples_per_second': 79.296, 'eval_steps_per_second': 2.543, 'epoch': 10.0}
{'train@fas.rst.prstc_loss': 2.182495594024658, 'train@fas.rst.prstc_accuracy@fas.rst.prstc': 0.31609756097560976, 'train@fas.rst.prstc_f1@fas.rst.prstc': 0.05453277247900442, 'train@fas.rst.prstc_precision@fas.rst.prstc': 0.05822731609350817, 'train@fas.rst.prstc_recall@fas.rst.prstc': 0.08458288268050472, 'train@fas.rst.prstc_loss@fas.rst.prstc': 2.182495594024658, 'train@fas.rst.prstc_runtime': 49.1911, 'train@fas.rst.prstc_samples_per_second': 83.348, 'train@fas.rst.prstc_steps_per_second': 2.622, 'epoch': 11.0}
{'loss': 2.2275, 'learning_rate': 1.6666666666666667e-06, 'epoch': 11.0}
{'eval_loss': 2.1009113788604736, 'eval_accuracy@fas.rst.prstc': 0.3306613226452906, 'eval_f1@fas.rst.prstc': 0.06298069166074906, 'eval_precision@fas.rst.prstc': 0.0776137482019835, 'eval_recall@fas.rst.prstc': 0.09375501824539602, 'eval_loss@fas.rst.prstc': 2.1009113788604736, 'eval_runtime': 6.3071, 'eval_samples_per_second': 79.117, 'eval_steps_per_second': 2.537, 'epoch': 11.0}
{'train@fas.rst.prstc_loss': 2.1782419681549072, 'train@fas.rst.prstc_accuracy@fas.rst.prstc': 0.31634146341463415, 'train@fas.rst.prstc_f1@fas.rst.prstc': 0.05532724575231625, 'train@fas.rst.prstc_precision@fas.rst.prstc': 0.05971998976956107, 'train@fas.rst.prstc_recall@fas.rst.prstc': 0.08499518567103047, 'train@fas.rst.prstc_loss@fas.rst.prstc': 2.1782422065734863, 'train@fas.rst.prstc_runtime': 49.2157, 'train@fas.rst.prstc_samples_per_second': 83.307, 'train@fas.rst.prstc_steps_per_second': 2.621, 'epoch': 12.0}
{'loss': 2.2175, 'learning_rate': 0.0, 'epoch': 12.0}
{'eval_loss': 2.0957398414611816, 'eval_accuracy@fas.rst.prstc': 0.3306613226452906, 'eval_f1@fas.rst.prstc': 0.06298069166074906, 'eval_precision@fas.rst.prstc': 0.0776137482019835, 'eval_recall@fas.rst.prstc': 0.09375501824539602, 'eval_loss@fas.rst.prstc': 2.0957398414611816, 'eval_runtime': 6.2936, 'eval_samples_per_second': 79.287, 'eval_steps_per_second': 2.542, 'epoch': 12.0}
{'train_runtime': 1900.0692, 'train_samples_per_second': 25.894, 'train_steps_per_second': 0.815, 'train_loss': 2.3475728589434954, 'epoch': 12.0}
***** train_logger metrics *****
  epoch                    =       12.0
  train_loss               =     2.3802
  train_runtime            = 0:17:58.84
  train_samples_per_second =     24.915
  train_steps_per_second   =      0.779
-------------------------------------------------------------------
Lang1:  spa.rst.sctb    Lang2:  fas.rst.prstc
Saving run to:  runs/full_shot/FullShot=v4_spa.rst.sctb_fas.rst.prstc_lrfunc=Adafactor_lr=2e-05_plm=bert-base-multilingual-cased
Running with params: BERT_MODEL=bert-base-multilingual-cased lr=2e-05
read 439 examples
read 94 examples
read 159 examples
read 4100 examples
read 499 examples
read 592 examples
Total prediction labels:  33
BertAdapterModel(
  (shared_parameters): ModuleDict()
  (bert): BertModel(
    (shared_parameters): ModuleDict()
    (invertible_adapters): ModuleDict(
      (es): NICECouplingBlock(
        (F): Sequential(
          (0): Linear(in_features=384, out_features=192, bias=True)
          (1): Activation_Function_Class(
            (f): NewGELUActivation()
          )
          (2): Linear(in_features=192, out_features=384, bias=True)
        )
        (G): Sequential(
          (0): Linear(in_features=384, out_features=192, bias=True)
          (1): Activation_Function_Class(
            (f): NewGELUActivation()
          )
          (2): Linear(in_features=192, out_features=384, bias=True)
        )
      )
      (ar): NICECouplingBlock(
        (F): Sequential(
          (0): Linear(in_features=384, out_features=192, bias=True)
          (1): Activation_Function_Class(
            (f): ReLU()
          )
          (2): Linear(in_features=192, out_features=384, bias=True)
        )
        (G): Sequential(
          (0): Linear(in_features=384, out_features=192, bias=True)
          (1): Activation_Function_Class(
            (f): ReLU()
          )
          (2): Linear(in_features=192, out_features=384, bias=True)
        )
      )
    )
    (embeddings): BertEmbeddings(
      (word_embeddings): Embedding(119547, 768, padding_idx=0)
      (position_embeddings): Embedding(512, 768)
      (token_type_embeddings): Embedding(2, 768)
      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
      (dropout): Dropout(p=0.1, inplace=False)
    )
    (encoder): BertEncoder(
      (layer): ModuleList(
        (0): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (es): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (ar): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (1): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (es): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (ar): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (2): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (es): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (ar): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (3): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (es): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (ar): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (4): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (es): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (ar): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (5): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (es): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (ar): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (6): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (es): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (ar): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (7): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (es): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (ar): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (8): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (es): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (ar): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (9): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (es): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (ar): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (10): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (es): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (ar): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (11): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (es): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (ar): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
      )
    )
    (pooler): BertPooler(
      (dense): Linear(in_features=768, out_features=768, bias=True)
      (activation): Tanh()
    )
    (prefix_tuning): PrefixTuningPool(
      (prefix_tunings): ModuleDict()
    )
  )
  (heads): ModuleDict(
    (disrpt): ClassificationHead(
      (0): Dropout(p=0.1, inplace=False)
      (1): Linear(in_features=768, out_features=768, bias=True)
      (2): Activation_Function_Class(
        (f): Tanh()
      )
      (3): Dropout(p=0.1, inplace=False)
      (4): Linear(in_features=768, out_features=33, bias=True)
    )
  )
)
{'train@spa.rst.sctb_loss': 3.1824076175689697, 'train@spa.rst.sctb_accuracy@spa.rst.sctb': 0.34851936218678814, 'train@spa.rst.sctb_f1@spa.rst.sctb': 0.024681266104375918, 'train@spa.rst.sctb_precision@spa.rst.sctb': 0.03113830613830614, 'train@spa.rst.sctb_recall@spa.rst.sctb': 0.04318100358422939, 'train@spa.rst.sctb_loss@spa.rst.sctb': 3.1824066638946533, 'train@spa.rst.sctb_runtime': 5.5805, 'train@spa.rst.sctb_samples_per_second': 78.667, 'train@spa.rst.sctb_steps_per_second': 2.509, 'epoch': 1.0}
{'loss': 3.3373, 'learning_rate': 1.8333333333333333e-05, 'epoch': 1.0}
{'eval_loss': 3.1890053749084473, 'eval_accuracy@spa.rst.sctb': 0.3617021276595745, 'eval_f1@spa.rst.sctb': 0.03693765460567692, 'eval_precision@spa.rst.sctb': 0.03651685393258427, 'eval_recall@spa.rst.sctb': 0.06191950464396284, 'eval_loss@spa.rst.sctb': 3.1890053749084473, 'eval_runtime': 1.443, 'eval_samples_per_second': 65.142, 'eval_steps_per_second': 2.079, 'epoch': 1.0}
{'train@spa.rst.sctb_loss': 2.9351441860198975, 'train@spa.rst.sctb_accuracy@spa.rst.sctb': 0.357630979498861, 'train@spa.rst.sctb_f1@spa.rst.sctb': 0.027252376880124756, 'train@spa.rst.sctb_precision@spa.rst.sctb': 0.04373543123543123, 'train@spa.rst.sctb_recall@spa.rst.sctb': 0.044802867383512544, 'train@spa.rst.sctb_loss@spa.rst.sctb': 2.9351439476013184, 'train@spa.rst.sctb_runtime': 5.6109, 'train@spa.rst.sctb_samples_per_second': 78.241, 'train@spa.rst.sctb_steps_per_second': 2.495, 'epoch': 2.0}
{'loss': 3.0798, 'learning_rate': 1.6666666666666667e-05, 'epoch': 2.0}
{'eval_loss': 2.945632219314575, 'eval_accuracy@spa.rst.sctb': 0.3617021276595745, 'eval_f1@spa.rst.sctb': 0.036656891495601175, 'eval_precision@spa.rst.sctb': 0.04093945270415859, 'eval_recall@spa.rst.sctb': 0.06191950464396284, 'eval_loss@spa.rst.sctb': 2.94563364982605, 'eval_runtime': 1.4535, 'eval_samples_per_second': 64.67, 'eval_steps_per_second': 2.064, 'epoch': 2.0}
{'train@spa.rst.sctb_loss': 2.698014497756958, 'train@spa.rst.sctb_accuracy@spa.rst.sctb': 0.3530751708428246, 'train@spa.rst.sctb_f1@spa.rst.sctb': 0.026289814568361283, 'train@spa.rst.sctb_precision@spa.rst.sctb': 0.037232724440668365, 'train@spa.rst.sctb_recall@spa.rst.sctb': 0.04407706093189964, 'train@spa.rst.sctb_loss@spa.rst.sctb': 2.698014497756958, 'train@spa.rst.sctb_runtime': 5.6428, 'train@spa.rst.sctb_samples_per_second': 77.799, 'train@spa.rst.sctb_steps_per_second': 2.481, 'epoch': 3.0}
{'loss': 2.8552, 'learning_rate': 1.5000000000000002e-05, 'epoch': 3.0}
{'eval_loss': 2.718498945236206, 'eval_accuracy@spa.rst.sctb': 0.3723404255319149, 'eval_f1@spa.rst.sctb': 0.041794024078348195, 'eval_precision@spa.rst.sctb': 0.050980392156862744, 'eval_recall@spa.rst.sctb': 0.06501547987616098, 'eval_loss@spa.rst.sctb': 2.718498468399048, 'eval_runtime': 1.4598, 'eval_samples_per_second': 64.392, 'eval_steps_per_second': 2.055, 'epoch': 3.0}
{'train@spa.rst.sctb_loss': 2.5082082748413086, 'train@spa.rst.sctb_accuracy@spa.rst.sctb': 0.35079726651480636, 'train@spa.rst.sctb_f1@spa.rst.sctb': 0.02549032759304906, 'train@spa.rst.sctb_precision@spa.rst.sctb': 0.03530497280497281, 'train@spa.rst.sctb_recall@spa.rst.sctb': 0.04362903225806452, 'train@spa.rst.sctb_loss@spa.rst.sctb': 2.5082077980041504, 'train@spa.rst.sctb_runtime': 5.6567, 'train@spa.rst.sctb_samples_per_second': 77.607, 'train@spa.rst.sctb_steps_per_second': 2.475, 'epoch': 4.0}
{'loss': 2.6345, 'learning_rate': 1.3333333333333333e-05, 'epoch': 4.0}
{'eval_loss': 2.541870594024658, 'eval_accuracy@spa.rst.sctb': 0.3723404255319149, 'eval_f1@spa.rst.sctb': 0.042004485078488876, 'eval_precision@spa.rst.sctb': 0.06054729584141348, 'eval_recall@spa.rst.sctb': 0.06501547987616098, 'eval_loss@spa.rst.sctb': 2.5418708324432373, 'eval_runtime': 1.4575, 'eval_samples_per_second': 64.494, 'eval_steps_per_second': 2.058, 'epoch': 4.0}
{'train@spa.rst.sctb_loss': 2.3812408447265625, 'train@spa.rst.sctb_accuracy@spa.rst.sctb': 0.357630979498861, 'train@spa.rst.sctb_f1@spa.rst.sctb': 0.027868559324365216, 'train@spa.rst.sctb_precision@spa.rst.sctb': 0.04231720010408535, 'train@spa.rst.sctb_recall@spa.rst.sctb': 0.044973118279569894, 'train@spa.rst.sctb_loss@spa.rst.sctb': 2.3812406063079834, 'train@spa.rst.sctb_runtime': 5.6203, 'train@spa.rst.sctb_samples_per_second': 78.109, 'train@spa.rst.sctb_steps_per_second': 2.491, 'epoch': 5.0}
{'loss': 2.4786, 'learning_rate': 1.1666666666666668e-05, 'epoch': 5.0}
{'eval_loss': 2.4228882789611816, 'eval_accuracy@spa.rst.sctb': 0.3723404255319149, 'eval_f1@spa.rst.sctb': 0.042004485078488876, 'eval_precision@spa.rst.sctb': 0.06054729584141348, 'eval_recall@spa.rst.sctb': 0.06501547987616098, 'eval_loss@spa.rst.sctb': 2.4228882789611816, 'eval_runtime': 1.4742, 'eval_samples_per_second': 63.765, 'eval_steps_per_second': 2.035, 'epoch': 5.0}
{'train@spa.rst.sctb_loss': 2.3054990768432617, 'train@spa.rst.sctb_accuracy@spa.rst.sctb': 0.357630979498861, 'train@spa.rst.sctb_f1@spa.rst.sctb': 0.027804662967264592, 'train@spa.rst.sctb_precision@spa.rst.sctb': 0.036864517819706503, 'train@spa.rst.sctb_recall@spa.rst.sctb': 0.044973118279569894, 'train@spa.rst.sctb_loss@spa.rst.sctb': 2.3054988384246826, 'train@spa.rst.sctb_runtime': 5.6282, 'train@spa.rst.sctb_samples_per_second': 78.001, 'train@spa.rst.sctb_steps_per_second': 2.487, 'epoch': 6.0}
{'loss': 2.3743, 'learning_rate': 1e-05, 'epoch': 6.0}
{'eval_loss': 2.3535468578338623, 'eval_accuracy@spa.rst.sctb': 0.3723404255319149, 'eval_f1@spa.rst.sctb': 0.041794024078348195, 'eval_precision@spa.rst.sctb': 0.050980392156862744, 'eval_recall@spa.rst.sctb': 0.06501547987616098, 'eval_loss@spa.rst.sctb': 2.3535468578338623, 'eval_runtime': 1.4668, 'eval_samples_per_second': 64.086, 'eval_steps_per_second': 2.045, 'epoch': 6.0}
{'train@spa.rst.sctb_loss': 2.257941722869873, 'train@spa.rst.sctb_accuracy@spa.rst.sctb': 0.36674259681093396, 'train@spa.rst.sctb_f1@spa.rst.sctb': 0.03101009497937719, 'train@spa.rst.sctb_precision@spa.rst.sctb': 0.03576472962066182, 'train@spa.rst.sctb_recall@spa.rst.sctb': 0.04693548387096774, 'train@spa.rst.sctb_loss@spa.rst.sctb': 2.257941961288452, 'train@spa.rst.sctb_runtime': 5.6541, 'train@spa.rst.sctb_samples_per_second': 77.642, 'train@spa.rst.sctb_steps_per_second': 2.476, 'epoch': 7.0}
{'loss': 2.3196, 'learning_rate': 8.333333333333334e-06, 'epoch': 7.0}
{'eval_loss': 2.3132636547088623, 'eval_accuracy@spa.rst.sctb': 0.39361702127659576, 'eval_f1@spa.rst.sctb': 0.04970801880074064, 'eval_precision@spa.rst.sctb': 0.04898116109188773, 'eval_recall@spa.rst.sctb': 0.07120743034055728, 'eval_loss@spa.rst.sctb': 2.3132636547088623, 'eval_runtime': 1.4717, 'eval_samples_per_second': 63.871, 'eval_steps_per_second': 2.038, 'epoch': 7.0}
{'train@spa.rst.sctb_loss': 2.2299320697784424, 'train@spa.rst.sctb_accuracy@spa.rst.sctb': 0.36446469248291574, 'train@spa.rst.sctb_f1@spa.rst.sctb': 0.030659485338120882, 'train@spa.rst.sctb_precision@spa.rst.sctb': 0.031976223382473384, 'train@spa.rst.sctb_recall@spa.rst.sctb': 0.046657706093189956, 'train@spa.rst.sctb_loss@spa.rst.sctb': 2.2299318313598633, 'train@spa.rst.sctb_runtime': 5.6368, 'train@spa.rst.sctb_samples_per_second': 77.882, 'train@spa.rst.sctb_steps_per_second': 2.484, 'epoch': 8.0}
{'loss': 2.2674, 'learning_rate': 6.666666666666667e-06, 'epoch': 8.0}
{'eval_loss': 2.290009021759033, 'eval_accuracy@spa.rst.sctb': 0.39361702127659576, 'eval_f1@spa.rst.sctb': 0.04940968429812243, 'eval_precision@spa.rst.sctb': 0.046638655462184875, 'eval_recall@spa.rst.sctb': 0.07120743034055728, 'eval_loss@spa.rst.sctb': 2.2900092601776123, 'eval_runtime': 1.4487, 'eval_samples_per_second': 64.887, 'eval_steps_per_second': 2.071, 'epoch': 8.0}
{'train@spa.rst.sctb_loss': 2.210083484649658, 'train@spa.rst.sctb_accuracy@spa.rst.sctb': 0.3690205011389522, 'train@spa.rst.sctb_f1@spa.rst.sctb': 0.03244476242210652, 'train@spa.rst.sctb_precision@spa.rst.sctb': 0.030771748349743704, 'train@spa.rst.sctb_recall@spa.rst.sctb': 0.04789426523297491, 'train@spa.rst.sctb_loss@spa.rst.sctb': 2.210083484649658, 'train@spa.rst.sctb_runtime': 5.6405, 'train@spa.rst.sctb_samples_per_second': 77.83, 'train@spa.rst.sctb_steps_per_second': 2.482, 'epoch': 9.0}
{'loss': 2.2709, 'learning_rate': 5e-06, 'epoch': 9.0}
{'eval_loss': 2.2738311290740967, 'eval_accuracy@spa.rst.sctb': 0.4148936170212766, 'eval_f1@spa.rst.sctb': 0.05652998927481231, 'eval_precision@spa.rst.sctb': 0.05308464849354376, 'eval_recall@spa.rst.sctb': 0.07739938080495357, 'eval_loss@spa.rst.sctb': 2.273831367492676, 'eval_runtime': 1.4651, 'eval_samples_per_second': 64.161, 'eval_steps_per_second': 2.048, 'epoch': 9.0}
{'train@spa.rst.sctb_loss': 2.196774959564209, 'train@spa.rst.sctb_accuracy@spa.rst.sctb': 0.38724373576309795, 'train@spa.rst.sctb_f1@spa.rst.sctb': 0.03645839768379522, 'train@spa.rst.sctb_precision@spa.rst.sctb': 0.03330218411023161, 'train@spa.rst.sctb_recall@spa.rst.sctb': 0.05147849462365591, 'train@spa.rst.sctb_loss@spa.rst.sctb': 2.196774959564209, 'train@spa.rst.sctb_runtime': 5.6698, 'train@spa.rst.sctb_samples_per_second': 77.428, 'train@spa.rst.sctb_steps_per_second': 2.469, 'epoch': 10.0}
{'loss': 2.2477, 'learning_rate': 3.3333333333333333e-06, 'epoch': 10.0}
{'eval_loss': 2.2642500400543213, 'eval_accuracy@spa.rst.sctb': 0.43617021276595747, 'eval_f1@spa.rst.sctb': 0.06287760478286246, 'eval_precision@spa.rst.sctb': 0.0578781512605042, 'eval_recall@spa.rst.sctb': 0.08359133126934984, 'eval_loss@spa.rst.sctb': 2.2642500400543213, 'eval_runtime': 1.4518, 'eval_samples_per_second': 64.746, 'eval_steps_per_second': 2.066, 'epoch': 10.0}
{'train@spa.rst.sctb_loss': 2.1895458698272705, 'train@spa.rst.sctb_accuracy@spa.rst.sctb': 0.3917995444191344, 'train@spa.rst.sctb_f1@spa.rst.sctb': 0.03739519352637224, 'train@spa.rst.sctb_precision@spa.rst.sctb': 0.033925405268490375, 'train@spa.rst.sctb_recall@spa.rst.sctb': 0.05237455197132617, 'train@spa.rst.sctb_loss@spa.rst.sctb': 2.1895461082458496, 'train@spa.rst.sctb_runtime': 5.6595, 'train@spa.rst.sctb_samples_per_second': 77.569, 'train@spa.rst.sctb_steps_per_second': 2.474, 'epoch': 11.0}
{'loss': 2.227, 'learning_rate': 1.6666666666666667e-06, 'epoch': 11.0}
{'eval_loss': 2.2583718299865723, 'eval_accuracy@spa.rst.sctb': 0.43617021276595747, 'eval_f1@spa.rst.sctb': 0.06287760478286246, 'eval_precision@spa.rst.sctb': 0.0578781512605042, 'eval_recall@spa.rst.sctb': 0.08359133126934984, 'eval_loss@spa.rst.sctb': 2.2583718299865723, 'eval_runtime': 1.4525, 'eval_samples_per_second': 64.717, 'eval_steps_per_second': 2.065, 'epoch': 11.0}
{'train@spa.rst.sctb_loss': 2.187117099761963, 'train@spa.rst.sctb_accuracy@spa.rst.sctb': 0.3917995444191344, 'train@spa.rst.sctb_f1@spa.rst.sctb': 0.03757183908045977, 'train@spa.rst.sctb_precision@spa.rst.sctb': 0.0335419675814476, 'train@spa.rst.sctb_recall@spa.rst.sctb': 0.05254480286738351, 'train@spa.rst.sctb_loss@spa.rst.sctb': 2.187117338180542, 'train@spa.rst.sctb_runtime': 5.6456, 'train@spa.rst.sctb_samples_per_second': 77.76, 'train@spa.rst.sctb_steps_per_second': 2.48, 'epoch': 12.0}
{'loss': 2.2137, 'learning_rate': 0.0, 'epoch': 12.0}
{'eval_loss': 2.256720542907715, 'eval_accuracy@spa.rst.sctb': 0.43617021276595747, 'eval_f1@spa.rst.sctb': 0.06287760478286246, 'eval_precision@spa.rst.sctb': 0.0578781512605042, 'eval_recall@spa.rst.sctb': 0.08359133126934984, 'eval_loss@spa.rst.sctb': 2.256720542907715, 'eval_runtime': 1.4538, 'eval_samples_per_second': 64.659, 'eval_steps_per_second': 2.064, 'epoch': 12.0}
{'train_runtime': 219.7902, 'train_samples_per_second': 23.968, 'train_steps_per_second': 0.764, 'train_loss': 2.525507438750494, 'epoch': 12.0}
***** train_log metrics *****
  epoch                    =       12.0
  train_loss               =     2.5255
  train_runtime            = 0:03:39.79
  train_samples_per_second =     23.968
  train_steps_per_second   =      0.764
{'train@fas.rst.prstc_loss': 2.413245439529419, 'train@fas.rst.prstc_accuracy@fas.rst.prstc': 0.23780487804878048, 'train@fas.rst.prstc_f1@fas.rst.prstc': 0.02260214430599826, 'train@fas.rst.prstc_precision@fas.rst.prstc': 0.013988522238163558, 'train@fas.rst.prstc_recall@fas.rst.prstc': 0.058823529411764705, 'train@fas.rst.prstc_loss@fas.rst.prstc': 2.413245439529419, 'train@fas.rst.prstc_runtime': 49.24, 'train@fas.rst.prstc_samples_per_second': 83.266, 'train@fas.rst.prstc_steps_per_second': 2.62, 'epoch': 1.0}
{'loss': 2.7386, 'learning_rate': 1.8333333333333333e-05, 'epoch': 1.0}
{'eval_loss': 2.332294464111328, 'eval_accuracy@fas.rst.prstc': 0.24448897795591182, 'eval_f1@fas.rst.prstc': 0.026194310252281264, 'eval_precision@fas.rst.prstc': 0.016299265197060788, 'eval_recall@fas.rst.prstc': 0.06666666666666667, 'eval_loss@fas.rst.prstc': 2.332294464111328, 'eval_runtime': 6.2622, 'eval_samples_per_second': 79.685, 'eval_steps_per_second': 2.555, 'epoch': 1.0}
{'train@fas.rst.prstc_loss': 2.358684778213501, 'train@fas.rst.prstc_accuracy@fas.rst.prstc': 0.2709756097560976, 'train@fas.rst.prstc_f1@fas.rst.prstc': 0.0421486699863121, 'train@fas.rst.prstc_precision@fas.rst.prstc': 0.03160717295999838, 'train@fas.rst.prstc_recall@fas.rst.prstc': 0.06985291443364033, 'train@fas.rst.prstc_loss@fas.rst.prstc': 2.358684778213501, 'train@fas.rst.prstc_runtime': 49.1699, 'train@fas.rst.prstc_samples_per_second': 83.384, 'train@fas.rst.prstc_steps_per_second': 2.624, 'epoch': 2.0}
{'loss': 2.3976, 'learning_rate': 1.6666666666666667e-05, 'epoch': 2.0}
{'eval_loss': 2.2714972496032715, 'eval_accuracy@fas.rst.prstc': 0.280561122244489, 'eval_f1@fas.rst.prstc': 0.04852801519468186, 'eval_precision@fas.rst.prstc': 0.04069024992951556, 'eval_recall@fas.rst.prstc': 0.07766690425279163, 'eval_loss@fas.rst.prstc': 2.2714972496032715, 'eval_runtime': 6.2646, 'eval_samples_per_second': 79.654, 'eval_steps_per_second': 2.554, 'epoch': 2.0}
{'train@fas.rst.prstc_loss': 2.3414413928985596, 'train@fas.rst.prstc_accuracy@fas.rst.prstc': 0.24024390243902438, 'train@fas.rst.prstc_f1@fas.rst.prstc': 0.024187718369155037, 'train@fas.rst.prstc_precision@fas.rst.prstc': 0.03638609409153848, 'train@fas.rst.prstc_recall@fas.rst.prstc': 0.05952804253179722, 'train@fas.rst.prstc_loss@fas.rst.prstc': 2.3414416313171387, 'train@fas.rst.prstc_runtime': 49.1773, 'train@fas.rst.prstc_samples_per_second': 83.372, 'train@fas.rst.prstc_steps_per_second': 2.623, 'epoch': 3.0}
{'loss': 2.3601, 'learning_rate': 1.5000000000000002e-05, 'epoch': 3.0}
{'eval_loss': 2.2547099590301514, 'eval_accuracy@fas.rst.prstc': 0.24448897795591182, 'eval_f1@fas.rst.prstc': 0.027203144489089563, 'eval_precision@fas.rst.prstc': 0.04956405097250168, 'eval_recall@fas.rst.prstc': 0.06669992872416251, 'eval_loss@fas.rst.prstc': 2.2547097206115723, 'eval_runtime': 6.2697, 'eval_samples_per_second': 79.59, 'eval_steps_per_second': 2.552, 'epoch': 3.0}
{'train@fas.rst.prstc_loss': 2.3265910148620605, 'train@fas.rst.prstc_accuracy@fas.rst.prstc': 0.23780487804878048, 'train@fas.rst.prstc_f1@fas.rst.prstc': 0.02260214430599826, 'train@fas.rst.prstc_precision@fas.rst.prstc': 0.013988522238163558, 'train@fas.rst.prstc_recall@fas.rst.prstc': 0.058823529411764705, 'train@fas.rst.prstc_loss@fas.rst.prstc': 2.3265912532806396, 'train@fas.rst.prstc_runtime': 49.1748, 'train@fas.rst.prstc_samples_per_second': 83.376, 'train@fas.rst.prstc_steps_per_second': 2.623, 'epoch': 4.0}
{'loss': 2.348, 'learning_rate': 1.3333333333333333e-05, 'epoch': 4.0}
{'eval_loss': 2.2446141242980957, 'eval_accuracy@fas.rst.prstc': 0.24448897795591182, 'eval_f1@fas.rst.prstc': 0.026194310252281264, 'eval_precision@fas.rst.prstc': 0.016299265197060788, 'eval_recall@fas.rst.prstc': 0.06666666666666667, 'eval_loss@fas.rst.prstc': 2.2446141242980957, 'eval_runtime': 6.2396, 'eval_samples_per_second': 79.973, 'eval_steps_per_second': 2.564, 'epoch': 4.0}
{'train@fas.rst.prstc_loss': 2.3155810832977295, 'train@fas.rst.prstc_accuracy@fas.rst.prstc': 0.2570731707317073, 'train@fas.rst.prstc_f1@fas.rst.prstc': 0.03314964410881885, 'train@fas.rst.prstc_precision@fas.rst.prstc': 0.03420827581532886, 'train@fas.rst.prstc_recall@fas.rst.prstc': 0.06442690114780228, 'train@fas.rst.prstc_loss@fas.rst.prstc': 2.3155808448791504, 'train@fas.rst.prstc_runtime': 49.1201, 'train@fas.rst.prstc_samples_per_second': 83.469, 'train@fas.rst.prstc_steps_per_second': 2.626, 'epoch': 5.0}
{'loss': 2.3349, 'learning_rate': 1.1666666666666668e-05, 'epoch': 5.0}
{'eval_loss': 2.2305808067321777, 'eval_accuracy@fas.rst.prstc': 0.24849699398797595, 'eval_f1@fas.rst.prstc': 0.03283372052343385, 'eval_precision@fas.rst.prstc': 0.033227176220806794, 'eval_recall@fas.rst.prstc': 0.06799239724400094, 'eval_loss@fas.rst.prstc': 2.2305808067321777, 'eval_runtime': 6.2572, 'eval_samples_per_second': 79.748, 'eval_steps_per_second': 2.557, 'epoch': 5.0}
{'train@fas.rst.prstc_loss': 2.3051681518554688, 'train@fas.rst.prstc_accuracy@fas.rst.prstc': 0.23926829268292682, 'train@fas.rst.prstc_f1@fas.rst.prstc': 0.023488986841241847, 'train@fas.rst.prstc_precision@fas.rst.prstc': 0.037569299445604436, 'train@fas.rst.prstc_recall@fas.rst.prstc': 0.05924071756362119, 'train@fas.rst.prstc_loss@fas.rst.prstc': 2.305168390274048, 'train@fas.rst.prstc_runtime': 49.1382, 'train@fas.rst.prstc_samples_per_second': 83.438, 'train@fas.rst.prstc_steps_per_second': 2.625, 'epoch': 6.0}
{'loss': 2.3199, 'learning_rate': 1e-05, 'epoch': 6.0}
{'eval_loss': 2.228088617324829, 'eval_accuracy@fas.rst.prstc': 0.24649298597194388, 'eval_f1@fas.rst.prstc': 0.027385984427141265, 'eval_precision@fas.rst.prstc': 0.08299866131191432, 'eval_recall@fas.rst.prstc': 0.0672463768115942, 'eval_loss@fas.rst.prstc': 2.228088855743408, 'eval_runtime': 7.1235, 'eval_samples_per_second': 70.05, 'eval_steps_per_second': 2.246, 'epoch': 6.0}
{'train@fas.rst.prstc_loss': 2.290872097015381, 'train@fas.rst.prstc_accuracy@fas.rst.prstc': 0.2651219512195122, 'train@fas.rst.prstc_f1@fas.rst.prstc': 0.03672796402978519, 'train@fas.rst.prstc_precision@fas.rst.prstc': 0.03341183504967298, 'train@fas.rst.prstc_recall@fas.rst.prstc': 0.06685942898708856, 'train@fas.rst.prstc_loss@fas.rst.prstc': 2.29087233543396, 'train@fas.rst.prstc_runtime': 49.1705, 'train@fas.rst.prstc_samples_per_second': 83.383, 'train@fas.rst.prstc_steps_per_second': 2.624, 'epoch': 7.0}
{'loss': 2.3165, 'learning_rate': 8.333333333333334e-06, 'epoch': 7.0}
{'eval_loss': 2.2062206268310547, 'eval_accuracy@fas.rst.prstc': 0.25851703406813625, 'eval_f1@fas.rst.prstc': 0.03828703703703704, 'eval_precision@fas.rst.prstc': 0.03762767172458803, 'eval_recall@fas.rst.prstc': 0.07095747208363032, 'eval_loss@fas.rst.prstc': 2.206220865249634, 'eval_runtime': 6.2577, 'eval_samples_per_second': 79.742, 'eval_steps_per_second': 2.557, 'epoch': 7.0}
{'train@fas.rst.prstc_loss': 2.2732455730438232, 'train@fas.rst.prstc_accuracy@fas.rst.prstc': 0.2948780487804878, 'train@fas.rst.prstc_f1@fas.rst.prstc': 0.045817431318172806, 'train@fas.rst.prstc_precision@fas.rst.prstc': 0.03441219343202627, 'train@fas.rst.prstc_recall@fas.rst.prstc': 0.07598622209385664, 'train@fas.rst.prstc_loss@fas.rst.prstc': 2.2732458114624023, 'train@fas.rst.prstc_runtime': 49.1629, 'train@fas.rst.prstc_samples_per_second': 83.396, 'train@fas.rst.prstc_steps_per_second': 2.624, 'epoch': 8.0}
{'loss': 2.3002, 'learning_rate': 6.666666666666667e-06, 'epoch': 8.0}
{'eval_loss': 2.1946890354156494, 'eval_accuracy@fas.rst.prstc': 0.2965931863727455, 'eval_f1@fas.rst.prstc': 0.0515423937951701, 'eval_precision@fas.rst.prstc': 0.04036046400153389, 'eval_recall@fas.rst.prstc': 0.08227132335471608, 'eval_loss@fas.rst.prstc': 2.1946890354156494, 'eval_runtime': 6.2755, 'eval_samples_per_second': 79.515, 'eval_steps_per_second': 2.55, 'epoch': 8.0}
{'train@fas.rst.prstc_loss': 2.258833408355713, 'train@fas.rst.prstc_accuracy@fas.rst.prstc': 0.281219512195122, 'train@fas.rst.prstc_f1@fas.rst.prstc': 0.04219087455542095, 'train@fas.rst.prstc_precision@fas.rst.prstc': 0.03439007914612501, 'train@fas.rst.prstc_recall@fas.rst.prstc': 0.07170608559845106, 'train@fas.rst.prstc_loss@fas.rst.prstc': 2.258833408355713, 'train@fas.rst.prstc_runtime': 49.1956, 'train@fas.rst.prstc_samples_per_second': 83.341, 'train@fas.rst.prstc_steps_per_second': 2.622, 'epoch': 9.0}
{'loss': 2.2839, 'learning_rate': 5e-06, 'epoch': 9.0}
{'eval_loss': 2.1786282062530518, 'eval_accuracy@fas.rst.prstc': 0.280561122244489, 'eval_f1@fas.rst.prstc': 0.04699100717741407, 'eval_precision@fas.rst.prstc': 0.04084719831707783, 'eval_recall@fas.rst.prstc': 0.07746733190781659, 'eval_loss@fas.rst.prstc': 2.1786282062530518, 'eval_runtime': 6.2685, 'eval_samples_per_second': 79.604, 'eval_steps_per_second': 2.552, 'epoch': 9.0}
{'train@fas.rst.prstc_loss': 2.244105339050293, 'train@fas.rst.prstc_accuracy@fas.rst.prstc': 0.29609756097560974, 'train@fas.rst.prstc_f1@fas.rst.prstc': 0.046562655187264775, 'train@fas.rst.prstc_precision@fas.rst.prstc': 0.04890812285560746, 'train@fas.rst.prstc_recall@fas.rst.prstc': 0.07659266027225978, 'train@fas.rst.prstc_loss@fas.rst.prstc': 2.244105815887451, 'train@fas.rst.prstc_runtime': 49.1554, 'train@fas.rst.prstc_samples_per_second': 83.409, 'train@fas.rst.prstc_steps_per_second': 2.624, 'epoch': 10.0}
{'loss': 2.2822, 'learning_rate': 3.3333333333333333e-06, 'epoch': 10.0}
{'eval_loss': 2.1639959812164307, 'eval_accuracy@fas.rst.prstc': 0.2965931863727455, 'eval_f1@fas.rst.prstc': 0.052261285570894114, 'eval_precision@fas.rst.prstc': 0.03949853467925757, 'eval_recall@fas.rst.prstc': 0.08250415775718697, 'eval_loss@fas.rst.prstc': 2.163996458053589, 'eval_runtime': 6.2816, 'eval_samples_per_second': 79.438, 'eval_steps_per_second': 2.547, 'epoch': 10.0}
{'train@fas.rst.prstc_loss': 2.2348623275756836, 'train@fas.rst.prstc_accuracy@fas.rst.prstc': 0.3026829268292683, 'train@fas.rst.prstc_f1@fas.rst.prstc': 0.04969007174284895, 'train@fas.rst.prstc_precision@fas.rst.prstc': 0.05937133324851209, 'train@fas.rst.prstc_recall@fas.rst.prstc': 0.07924754555543041, 'train@fas.rst.prstc_loss@fas.rst.prstc': 2.2348623275756836, 'train@fas.rst.prstc_runtime': 49.1804, 'train@fas.rst.prstc_samples_per_second': 83.367, 'train@fas.rst.prstc_steps_per_second': 2.623, 'epoch': 11.0}
{'loss': 2.2694, 'learning_rate': 1.6666666666666667e-06, 'epoch': 11.0}
{'eval_loss': 2.1530942916870117, 'eval_accuracy@fas.rst.prstc': 0.30861723446893785, 'eval_f1@fas.rst.prstc': 0.055049476654878006, 'eval_precision@fas.rst.prstc': 0.041156603253377444, 'eval_recall@fas.rst.prstc': 0.08604894274174389, 'eval_loss@fas.rst.prstc': 2.153094530105591, 'eval_runtime': 6.2625, 'eval_samples_per_second': 79.681, 'eval_steps_per_second': 2.555, 'epoch': 11.0}
{'train@fas.rst.prstc_loss': 2.2327396869659424, 'train@fas.rst.prstc_accuracy@fas.rst.prstc': 0.3026829268292683, 'train@fas.rst.prstc_f1@fas.rst.prstc': 0.04967706257711943, 'train@fas.rst.prstc_precision@fas.rst.prstc': 0.05933275210271372, 'train@fas.rst.prstc_recall@fas.rst.prstc': 0.07924754555543041, 'train@fas.rst.prstc_loss@fas.rst.prstc': 2.2327401638031006, 'train@fas.rst.prstc_runtime': 49.1822, 'train@fas.rst.prstc_samples_per_second': 83.364, 'train@fas.rst.prstc_steps_per_second': 2.623, 'epoch': 12.0}
{'loss': 2.261, 'learning_rate': 0.0, 'epoch': 12.0}
{'eval_loss': 2.1507699489593506, 'eval_accuracy@fas.rst.prstc': 0.30861723446893785, 'eval_f1@fas.rst.prstc': 0.05503469301398427, 'eval_precision@fas.rst.prstc': 0.04111399602226158, 'eval_recall@fas.rst.prstc': 0.08604894274174389, 'eval_loss@fas.rst.prstc': 2.1507697105407715, 'eval_runtime': 6.2605, 'eval_samples_per_second': 79.706, 'eval_steps_per_second': 2.556, 'epoch': 12.0}
{'train_runtime': 1900.0374, 'train_samples_per_second': 25.894, 'train_steps_per_second': 0.815, 'train_loss': 2.3510310840853115, 'epoch': 12.0}
***** train_logger metrics *****
  epoch                    =       12.0
  train_loss               =     2.5255
  train_runtime            = 0:03:39.79
  train_samples_per_second =     23.968
  train_steps_per_second   =      0.764
-------------------------------------------------------------------
Lang1:  tur.pdtb.tdb    Lang2:  fas.rst.prstc
Saving run to:  runs/full_shot/FullShot=v4_tur.pdtb.tdb_fas.rst.prstc_lrfunc=Adafactor_lr=2e-05_plm=bert-base-multilingual-cased
Running with params: BERT_MODEL=bert-base-multilingual-cased lr=2e-05
read 2451 examples
read 312 examples
read 422 examples
read 4100 examples
read 499 examples
read 592 examples
Total prediction labels:  40
BertAdapterModel(
  (shared_parameters): ModuleDict()
  (bert): BertModel(
    (shared_parameters): ModuleDict()
    (invertible_adapters): ModuleDict(
      (tr): NICECouplingBlock(
        (F): Sequential(
          (0): Linear(in_features=384, out_features=192, bias=True)
          (1): Activation_Function_Class(
            (f): NewGELUActivation()
          )
          (2): Linear(in_features=192, out_features=384, bias=True)
        )
        (G): Sequential(
          (0): Linear(in_features=384, out_features=192, bias=True)
          (1): Activation_Function_Class(
            (f): NewGELUActivation()
          )
          (2): Linear(in_features=192, out_features=384, bias=True)
        )
      )
      (ar): NICECouplingBlock(
        (F): Sequential(
          (0): Linear(in_features=384, out_features=192, bias=True)
          (1): Activation_Function_Class(
            (f): ReLU()
          )
          (2): Linear(in_features=192, out_features=384, bias=True)
        )
        (G): Sequential(
          (0): Linear(in_features=384, out_features=192, bias=True)
          (1): Activation_Function_Class(
            (f): ReLU()
          )
          (2): Linear(in_features=192, out_features=384, bias=True)
        )
      )
    )
    (embeddings): BertEmbeddings(
      (word_embeddings): Embedding(119547, 768, padding_idx=0)
      (position_embeddings): Embedding(512, 768)
      (token_type_embeddings): Embedding(2, 768)
      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
      (dropout): Dropout(p=0.1, inplace=False)
    )
    (encoder): BertEncoder(
      (layer): ModuleList(
        (0): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (tr): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (ar): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (1): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (tr): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (ar): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (2): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (tr): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (ar): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (3): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (tr): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (ar): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (4): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (tr): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (ar): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (5): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (tr): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (ar): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (6): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (tr): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (ar): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (7): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (tr): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (ar): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (8): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (tr): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (ar): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (9): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (tr): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (ar): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (10): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (tr): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (ar): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (11): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (tr): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (ar): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
      )
    )
    (pooler): BertPooler(
      (dense): Linear(in_features=768, out_features=768, bias=True)
      (activation): Tanh()
    )
    (prefix_tuning): PrefixTuningPool(
      (prefix_tunings): ModuleDict()
    )
  )
  (heads): ModuleDict(
    (tr): BertStyleMaskedLMHead(
      (0): Linear(in_features=768, out_features=768, bias=True)
      (1): Activation_Function_Class(
        (f): GELUActivation()
      )
      (2): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
      (3): Linear(in_features=768, out_features=119547, bias=True)
    )
    (disrpt): ClassificationHead(
      (0): Dropout(p=0.1, inplace=False)
      (1): Linear(in_features=768, out_features=768, bias=True)
      (2): Activation_Function_Class(
        (f): Tanh()
      )
      (3): Dropout(p=0.1, inplace=False)
      (4): Linear(in_features=768, out_features=40, bias=True)
    )
  )
)
{'train@tur.pdtb.tdb_loss': 2.8681623935699463, 'train@tur.pdtb.tdb_accuracy@tur.pdtb.tdb': 0.25050999592003265, 'train@tur.pdtb.tdb_f1@tur.pdtb.tdb': 0.01741967515426626, 'train@tur.pdtb.tdb_precision@tur.pdtb.tdb': 0.010891738953044898, 'train@tur.pdtb.tdb_recall@tur.pdtb.tdb': 0.043478260869565216, 'train@tur.pdtb.tdb_loss@tur.pdtb.tdb': 2.868162155151367, 'train@tur.pdtb.tdb_runtime': 30.0843, 'train@tur.pdtb.tdb_samples_per_second': 81.471, 'train@tur.pdtb.tdb_steps_per_second': 2.559, 'epoch': 1.0}
{'loss': 3.2881, 'learning_rate': 1.8333333333333333e-05, 'epoch': 1.0}
{'eval_loss': 2.799588203430176, 'eval_accuracy@tur.pdtb.tdb': 0.266025641025641, 'eval_f1@tur.pdtb.tdb': 0.019102416570771, 'eval_precision@tur.pdtb.tdb': 0.012092074592074592, 'eval_recall@tur.pdtb.tdb': 0.045454545454545456, 'eval_loss@tur.pdtb.tdb': 2.799588441848755, 'eval_runtime': 4.1708, 'eval_samples_per_second': 74.807, 'eval_steps_per_second': 2.398, 'epoch': 1.0}
{'train@tur.pdtb.tdb_loss': 2.468580722808838, 'train@tur.pdtb.tdb_accuracy@tur.pdtb.tdb': 0.25050999592003265, 'train@tur.pdtb.tdb_f1@tur.pdtb.tdb': 0.01741967515426626, 'train@tur.pdtb.tdb_precision@tur.pdtb.tdb': 0.010891738953044898, 'train@tur.pdtb.tdb_recall@tur.pdtb.tdb': 0.043478260869565216, 'train@tur.pdtb.tdb_loss@tur.pdtb.tdb': 2.468580722808838, 'train@tur.pdtb.tdb_runtime': 30.0567, 'train@tur.pdtb.tdb_samples_per_second': 81.546, 'train@tur.pdtb.tdb_steps_per_second': 2.562, 'epoch': 2.0}
{'loss': 2.638, 'learning_rate': 1.6666666666666667e-05, 'epoch': 2.0}
{'eval_loss': 2.35579514503479, 'eval_accuracy@tur.pdtb.tdb': 0.266025641025641, 'eval_f1@tur.pdtb.tdb': 0.019102416570771, 'eval_precision@tur.pdtb.tdb': 0.012092074592074592, 'eval_recall@tur.pdtb.tdb': 0.045454545454545456, 'eval_loss@tur.pdtb.tdb': 2.355795383453369, 'eval_runtime': 4.1955, 'eval_samples_per_second': 74.366, 'eval_steps_per_second': 2.384, 'epoch': 2.0}
{'train@tur.pdtb.tdb_loss': 2.3805785179138184, 'train@tur.pdtb.tdb_accuracy@tur.pdtb.tdb': 0.25050999592003265, 'train@tur.pdtb.tdb_f1@tur.pdtb.tdb': 0.01741967515426626, 'train@tur.pdtb.tdb_precision@tur.pdtb.tdb': 0.010891738953044898, 'train@tur.pdtb.tdb_recall@tur.pdtb.tdb': 0.043478260869565216, 'train@tur.pdtb.tdb_loss@tur.pdtb.tdb': 2.3805782794952393, 'train@tur.pdtb.tdb_runtime': 30.1418, 'train@tur.pdtb.tdb_samples_per_second': 81.316, 'train@tur.pdtb.tdb_steps_per_second': 2.555, 'epoch': 3.0}
{'loss': 2.4428, 'learning_rate': 1.5000000000000002e-05, 'epoch': 3.0}
{'eval_loss': 2.3035502433776855, 'eval_accuracy@tur.pdtb.tdb': 0.266025641025641, 'eval_f1@tur.pdtb.tdb': 0.019102416570771, 'eval_precision@tur.pdtb.tdb': 0.012092074592074592, 'eval_recall@tur.pdtb.tdb': 0.045454545454545456, 'eval_loss@tur.pdtb.tdb': 2.3035502433776855, 'eval_runtime': 4.1462, 'eval_samples_per_second': 75.25, 'eval_steps_per_second': 2.412, 'epoch': 3.0}
{'train@tur.pdtb.tdb_loss': 2.3220906257629395, 'train@tur.pdtb.tdb_accuracy@tur.pdtb.tdb': 0.2713178294573643, 'train@tur.pdtb.tdb_f1@tur.pdtb.tdb': 0.03420498488441155, 'train@tur.pdtb.tdb_precision@tur.pdtb.tdb': 0.045328132107999616, 'train@tur.pdtb.tdb_recall@tur.pdtb.tdb': 0.05365957877273554, 'train@tur.pdtb.tdb_loss@tur.pdtb.tdb': 2.3220906257629395, 'train@tur.pdtb.tdb_runtime': 30.0444, 'train@tur.pdtb.tdb_samples_per_second': 81.579, 'train@tur.pdtb.tdb_steps_per_second': 2.563, 'epoch': 4.0}
{'loss': 2.3744, 'learning_rate': 1.3333333333333333e-05, 'epoch': 4.0}
{'eval_loss': 2.267167329788208, 'eval_accuracy@tur.pdtb.tdb': 0.2724358974358974, 'eval_f1@tur.pdtb.tdb': 0.026180637544273907, 'eval_precision@tur.pdtb.tdb': 0.03885711418824001, 'eval_recall@tur.pdtb.tdb': 0.048843323382657185, 'eval_loss@tur.pdtb.tdb': 2.267167091369629, 'eval_runtime': 4.1491, 'eval_samples_per_second': 75.197, 'eval_steps_per_second': 2.41, 'epoch': 4.0}
{'train@tur.pdtb.tdb_loss': 2.2752082347869873, 'train@tur.pdtb.tdb_accuracy@tur.pdtb.tdb': 0.29498164014687883, 'train@tur.pdtb.tdb_f1@tur.pdtb.tdb': 0.048254859918091544, 'train@tur.pdtb.tdb_precision@tur.pdtb.tdb': 0.057259293822603884, 'train@tur.pdtb.tdb_recall@tur.pdtb.tdb': 0.06680920749970957, 'train@tur.pdtb.tdb_loss@tur.pdtb.tdb': 2.275207996368408, 'train@tur.pdtb.tdb_runtime': 30.0653, 'train@tur.pdtb.tdb_samples_per_second': 81.523, 'train@tur.pdtb.tdb_steps_per_second': 2.561, 'epoch': 5.0}
{'loss': 2.3293, 'learning_rate': 1.1666666666666668e-05, 'epoch': 5.0}
{'eval_loss': 2.2315900325775146, 'eval_accuracy@tur.pdtb.tdb': 0.27884615384615385, 'eval_f1@tur.pdtb.tdb': 0.03548058483123418, 'eval_precision@tur.pdtb.tdb': 0.04708174006003043, 'eval_recall@tur.pdtb.tdb': 0.05466899762111299, 'eval_loss@tur.pdtb.tdb': 2.2315902709960938, 'eval_runtime': 4.1635, 'eval_samples_per_second': 74.937, 'eval_steps_per_second': 2.402, 'epoch': 5.0}
{'train@tur.pdtb.tdb_loss': 2.2330002784729004, 'train@tur.pdtb.tdb_accuracy@tur.pdtb.tdb': 0.30558955528355775, 'train@tur.pdtb.tdb_f1@tur.pdtb.tdb': 0.05517125680722831, 'train@tur.pdtb.tdb_precision@tur.pdtb.tdb': 0.10066634840946068, 'train@tur.pdtb.tdb_recall@tur.pdtb.tdb': 0.07328838236955386, 'train@tur.pdtb.tdb_loss@tur.pdtb.tdb': 2.2330005168914795, 'train@tur.pdtb.tdb_runtime': 30.0859, 'train@tur.pdtb.tdb_samples_per_second': 81.467, 'train@tur.pdtb.tdb_steps_per_second': 2.559, 'epoch': 6.0}
{'loss': 2.294, 'learning_rate': 1e-05, 'epoch': 6.0}
{'eval_loss': 2.2006657123565674, 'eval_accuracy@tur.pdtb.tdb': 0.28205128205128205, 'eval_f1@tur.pdtb.tdb': 0.03720882292310864, 'eval_precision@tur.pdtb.tdb': 0.04634667134667134, 'eval_recall@tur.pdtb.tdb': 0.05664660502040713, 'eval_loss@tur.pdtb.tdb': 2.200665235519409, 'eval_runtime': 4.1726, 'eval_samples_per_second': 74.773, 'eval_steps_per_second': 2.397, 'epoch': 6.0}
{'train@tur.pdtb.tdb_loss': 2.2011144161224365, 'train@tur.pdtb.tdb_accuracy@tur.pdtb.tdb': 0.32843737250102, 'train@tur.pdtb.tdb_f1@tur.pdtb.tdb': 0.08344514430142405, 'train@tur.pdtb.tdb_precision@tur.pdtb.tdb': 0.09725798125852968, 'train@tur.pdtb.tdb_recall@tur.pdtb.tdb': 0.09539727219221027, 'train@tur.pdtb.tdb_loss@tur.pdtb.tdb': 2.2011146545410156, 'train@tur.pdtb.tdb_runtime': 30.1197, 'train@tur.pdtb.tdb_samples_per_second': 81.375, 'train@tur.pdtb.tdb_steps_per_second': 2.556, 'epoch': 7.0}
{'loss': 2.2563, 'learning_rate': 8.333333333333334e-06, 'epoch': 7.0}
{'eval_loss': 2.177475690841675, 'eval_accuracy@tur.pdtb.tdb': 0.2948717948717949, 'eval_f1@tur.pdtb.tdb': 0.07722256251534737, 'eval_precision@tur.pdtb.tdb': 0.09566030677141789, 'eval_recall@tur.pdtb.tdb': 0.0881793857597327, 'eval_loss@tur.pdtb.tdb': 2.177476167678833, 'eval_runtime': 4.1545, 'eval_samples_per_second': 75.1, 'eval_steps_per_second': 2.407, 'epoch': 7.0}
{'train@tur.pdtb.tdb_loss': 2.186723470687866, 'train@tur.pdtb.tdb_accuracy@tur.pdtb.tdb': 0.33578131374949, 'train@tur.pdtb.tdb_f1@tur.pdtb.tdb': 0.09085161623513086, 'train@tur.pdtb.tdb_precision@tur.pdtb.tdb': 0.13970362619729434, 'train@tur.pdtb.tdb_recall@tur.pdtb.tdb': 0.10221448151005373, 'train@tur.pdtb.tdb_loss@tur.pdtb.tdb': 2.186723470687866, 'train@tur.pdtb.tdb_runtime': 30.1075, 'train@tur.pdtb.tdb_samples_per_second': 81.408, 'train@tur.pdtb.tdb_steps_per_second': 2.558, 'epoch': 8.0}
{'loss': 2.2362, 'learning_rate': 6.666666666666667e-06, 'epoch': 8.0}
{'eval_loss': 2.170071840286255, 'eval_accuracy@tur.pdtb.tdb': 0.30128205128205127, 'eval_f1@tur.pdtb.tdb': 0.08054995033624363, 'eval_precision@tur.pdtb.tdb': 0.08456403042177102, 'eval_recall@tur.pdtb.tdb': 0.09734979599090776, 'eval_loss@tur.pdtb.tdb': 2.170071601867676, 'eval_runtime': 4.2, 'eval_samples_per_second': 74.286, 'eval_steps_per_second': 2.381, 'epoch': 8.0}
{'train@tur.pdtb.tdb_loss': 2.162299871444702, 'train@tur.pdtb.tdb_accuracy@tur.pdtb.tdb': 0.339453284373725, 'train@tur.pdtb.tdb_f1@tur.pdtb.tdb': 0.09327928770834433, 'train@tur.pdtb.tdb_precision@tur.pdtb.tdb': 0.12311571343623962, 'train@tur.pdtb.tdb_recall@tur.pdtb.tdb': 0.10852360604866766, 'train@tur.pdtb.tdb_loss@tur.pdtb.tdb': 2.162299633026123, 'train@tur.pdtb.tdb_runtime': 30.112, 'train@tur.pdtb.tdb_samples_per_second': 81.396, 'train@tur.pdtb.tdb_steps_per_second': 2.557, 'epoch': 9.0}
{'loss': 2.2124, 'learning_rate': 5e-06, 'epoch': 9.0}
{'eval_loss': 2.1494014263153076, 'eval_accuracy@tur.pdtb.tdb': 0.3108974358974359, 'eval_f1@tur.pdtb.tdb': 0.08367000384153221, 'eval_precision@tur.pdtb.tdb': 0.08033431511692381, 'eval_recall@tur.pdtb.tdb': 0.10892010263794076, 'eval_loss@tur.pdtb.tdb': 2.1494011878967285, 'eval_runtime': 4.1642, 'eval_samples_per_second': 74.924, 'eval_steps_per_second': 2.401, 'epoch': 9.0}
{'train@tur.pdtb.tdb_loss': 2.1501543521881104, 'train@tur.pdtb.tdb_accuracy@tur.pdtb.tdb': 0.3390452876376989, 'train@tur.pdtb.tdb_f1@tur.pdtb.tdb': 0.09315891771291318, 'train@tur.pdtb.tdb_precision@tur.pdtb.tdb': 0.10833253928647733, 'train@tur.pdtb.tdb_recall@tur.pdtb.tdb': 0.10969663879113033, 'train@tur.pdtb.tdb_loss@tur.pdtb.tdb': 2.1501543521881104, 'train@tur.pdtb.tdb_runtime': 30.1001, 'train@tur.pdtb.tdb_samples_per_second': 81.428, 'train@tur.pdtb.tdb_steps_per_second': 2.558, 'epoch': 10.0}
{'loss': 2.1986, 'learning_rate': 3.3333333333333333e-06, 'epoch': 10.0}
{'eval_loss': 2.1390457153320312, 'eval_accuracy@tur.pdtb.tdb': 0.3108974358974359, 'eval_f1@tur.pdtb.tdb': 0.08380684135966833, 'eval_precision@tur.pdtb.tdb': 0.08064642389538278, 'eval_recall@tur.pdtb.tdb': 0.10849707440161326, 'eval_loss@tur.pdtb.tdb': 2.139045476913452, 'eval_runtime': 4.1745, 'eval_samples_per_second': 74.739, 'eval_steps_per_second': 2.395, 'epoch': 10.0}
{'train@tur.pdtb.tdb_loss': 2.145193338394165, 'train@tur.pdtb.tdb_accuracy@tur.pdtb.tdb': 0.339453284373725, 'train@tur.pdtb.tdb_f1@tur.pdtb.tdb': 0.09301740224699592, 'train@tur.pdtb.tdb_precision@tur.pdtb.tdb': 0.13439525262175153, 'train@tur.pdtb.tdb_recall@tur.pdtb.tdb': 0.1099924204243307, 'train@tur.pdtb.tdb_loss@tur.pdtb.tdb': 2.145193338394165, 'train@tur.pdtb.tdb_runtime': 30.0796, 'train@tur.pdtb.tdb_samples_per_second': 81.484, 'train@tur.pdtb.tdb_steps_per_second': 2.56, 'epoch': 11.0}
{'loss': 2.1968, 'learning_rate': 1.6666666666666667e-06, 'epoch': 11.0}
{'eval_loss': 2.13679575920105, 'eval_accuracy@tur.pdtb.tdb': 0.3076923076923077, 'eval_f1@tur.pdtb.tdb': 0.08059473820909836, 'eval_precision@tur.pdtb.tdb': 0.07751075903568573, 'eval_recall@tur.pdtb.tdb': 0.107234448138987, 'eval_loss@tur.pdtb.tdb': 2.136796236038208, 'eval_runtime': 4.1662, 'eval_samples_per_second': 74.888, 'eval_steps_per_second': 2.4, 'epoch': 11.0}
{'train@tur.pdtb.tdb_loss': 2.1418967247009277, 'train@tur.pdtb.tdb_accuracy@tur.pdtb.tdb': 0.34108527131782945, 'train@tur.pdtb.tdb_f1@tur.pdtb.tdb': 0.09392911116763762, 'train@tur.pdtb.tdb_precision@tur.pdtb.tdb': 0.10550162105301256, 'train@tur.pdtb.tdb_recall@tur.pdtb.tdb': 0.11090232513597355, 'train@tur.pdtb.tdb_loss@tur.pdtb.tdb': 2.1418967247009277, 'train@tur.pdtb.tdb_runtime': 30.0956, 'train@tur.pdtb.tdb_samples_per_second': 81.44, 'train@tur.pdtb.tdb_steps_per_second': 2.559, 'epoch': 12.0}
{'loss': 2.1891, 'learning_rate': 0.0, 'epoch': 12.0}
{'eval_loss': 2.133662462234497, 'eval_accuracy@tur.pdtb.tdb': 0.3108974358974359, 'eval_f1@tur.pdtb.tdb': 0.08164224742064853, 'eval_precision@tur.pdtb.tdb': 0.07789815031214388, 'eval_recall@tur.pdtb.tdb': 0.10849707440161326, 'eval_loss@tur.pdtb.tdb': 2.133662223815918, 'eval_runtime': 4.163, 'eval_samples_per_second': 74.946, 'eval_steps_per_second': 2.402, 'epoch': 12.0}
{'train_runtime': 1163.8466, 'train_samples_per_second': 25.271, 'train_steps_per_second': 0.794, 'train_loss': 2.388007391066778, 'epoch': 12.0}
***** train_log metrics *****
  epoch                    =       12.0
  train_loss               =      2.388
  train_runtime            = 0:19:23.84
  train_samples_per_second =     25.271
  train_steps_per_second   =      0.794
{'train@fas.rst.prstc_loss': 2.395354986190796, 'train@fas.rst.prstc_accuracy@fas.rst.prstc': 0.23658536585365852, 'train@fas.rst.prstc_f1@fas.rst.prstc': 0.02278380565853568, 'train@fas.rst.prstc_precision@fas.rst.prstc': 0.02129544276084577, 'train@fas.rst.prstc_recall@fas.rst.prstc': 0.058540269353786256, 'train@fas.rst.prstc_loss@fas.rst.prstc': 2.395354747772217, 'train@fas.rst.prstc_runtime': 49.2766, 'train@fas.rst.prstc_samples_per_second': 83.204, 'train@fas.rst.prstc_steps_per_second': 2.618, 'epoch': 1.0}
{'loss': 2.6381, 'learning_rate': 1.8333333333333333e-05, 'epoch': 1.0}
{'eval_loss': 2.311378240585327, 'eval_accuracy@fas.rst.prstc': 0.24649298597194388, 'eval_f1@fas.rst.prstc': 0.027385984427141265, 'eval_precision@fas.rst.prstc': 0.08299866131191432, 'eval_recall@fas.rst.prstc': 0.0672463768115942, 'eval_loss@fas.rst.prstc': 2.311378240585327, 'eval_runtime': 6.3228, 'eval_samples_per_second': 78.921, 'eval_steps_per_second': 2.531, 'epoch': 1.0}
{'train@fas.rst.prstc_loss': 2.350879430770874, 'train@fas.rst.prstc_accuracy@fas.rst.prstc': 0.2709756097560976, 'train@fas.rst.prstc_f1@fas.rst.prstc': 0.043887653076010155, 'train@fas.rst.prstc_precision@fas.rst.prstc': 0.03169208782067646, 'train@fas.rst.prstc_recall@fas.rst.prstc': 0.07135243841126193, 'train@fas.rst.prstc_loss@fas.rst.prstc': 2.350879669189453, 'train@fas.rst.prstc_runtime': 49.2079, 'train@fas.rst.prstc_samples_per_second': 83.32, 'train@fas.rst.prstc_steps_per_second': 2.622, 'epoch': 2.0}
{'loss': 2.3852, 'learning_rate': 1.6666666666666667e-05, 'epoch': 2.0}
{'eval_loss': 2.2651116847991943, 'eval_accuracy@fas.rst.prstc': 0.2865731462925852, 'eval_f1@fas.rst.prstc': 0.051782267911300174, 'eval_precision@fas.rst.prstc': 0.038205087014725574, 'eval_recall@fas.rst.prstc': 0.08037063435495367, 'eval_loss@fas.rst.prstc': 2.2651114463806152, 'eval_runtime': 6.3276, 'eval_samples_per_second': 78.861, 'eval_steps_per_second': 2.529, 'epoch': 2.0}
{'train@fas.rst.prstc_loss': 2.3347294330596924, 'train@fas.rst.prstc_accuracy@fas.rst.prstc': 0.25390243902439025, 'train@fas.rst.prstc_f1@fas.rst.prstc': 0.032348911073645396, 'train@fas.rst.prstc_precision@fas.rst.prstc': 0.032821824381926684, 'train@fas.rst.prstc_recall@fas.rst.prstc': 0.06358739022068184, 'train@fas.rst.prstc_loss@fas.rst.prstc': 2.3347291946411133, 'train@fas.rst.prstc_runtime': 49.2344, 'train@fas.rst.prstc_samples_per_second': 83.275, 'train@fas.rst.prstc_steps_per_second': 2.62, 'epoch': 3.0}
{'loss': 2.3568, 'learning_rate': 1.5000000000000002e-05, 'epoch': 3.0}
{'eval_loss': 2.2511730194091797, 'eval_accuracy@fas.rst.prstc': 0.2605210420841683, 'eval_f1@fas.rst.prstc': 0.03749061338465312, 'eval_precision@fas.rst.prstc': 0.03921286297096232, 'eval_recall@fas.rst.prstc': 0.07143739605607033, 'eval_loss@fas.rst.prstc': 2.2511727809906006, 'eval_runtime': 6.3165, 'eval_samples_per_second': 78.999, 'eval_steps_per_second': 2.533, 'epoch': 3.0}
{'train@fas.rst.prstc_loss': 2.3205292224884033, 'train@fas.rst.prstc_accuracy@fas.rst.prstc': 0.2375609756097561, 'train@fas.rst.prstc_f1@fas.rst.prstc': 0.022974121299922087, 'train@fas.rst.prstc_precision@fas.rst.prstc': 0.028677909519972372, 'train@fas.rst.prstc_recall@fas.rst.prstc': 0.05879079618754211, 'train@fas.rst.prstc_loss@fas.rst.prstc': 2.3205292224884033, 'train@fas.rst.prstc_runtime': 49.1307, 'train@fas.rst.prstc_samples_per_second': 83.451, 'train@fas.rst.prstc_steps_per_second': 2.626, 'epoch': 4.0}
{'loss': 2.3469, 'learning_rate': 1.3333333333333333e-05, 'epoch': 4.0}
{'eval_loss': 2.2426939010620117, 'eval_accuracy@fas.rst.prstc': 0.24649298597194388, 'eval_f1@fas.rst.prstc': 0.027385984427141265, 'eval_precision@fas.rst.prstc': 0.08299866131191432, 'eval_recall@fas.rst.prstc': 0.0672463768115942, 'eval_loss@fas.rst.prstc': 2.2426939010620117, 'eval_runtime': 6.3377, 'eval_samples_per_second': 78.735, 'eval_steps_per_second': 2.525, 'epoch': 4.0}
{'train@fas.rst.prstc_loss': 2.3031556606292725, 'train@fas.rst.prstc_accuracy@fas.rst.prstc': 0.27707317073170734, 'train@fas.rst.prstc_f1@fas.rst.prstc': 0.04153396953812115, 'train@fas.rst.prstc_precision@fas.rst.prstc': 0.032486191135527864, 'train@fas.rst.prstc_recall@fas.rst.prstc': 0.07075404084165036, 'train@fas.rst.prstc_loss@fas.rst.prstc': 2.3031556606292725, 'train@fas.rst.prstc_runtime': 49.2033, 'train@fas.rst.prstc_samples_per_second': 83.328, 'train@fas.rst.prstc_steps_per_second': 2.622, 'epoch': 5.0}
{'loss': 2.3341, 'learning_rate': 1.1666666666666668e-05, 'epoch': 5.0}
{'eval_loss': 2.2221479415893555, 'eval_accuracy@fas.rst.prstc': 0.3026052104208417, 'eval_f1@fas.rst.prstc': 0.05288248714543536, 'eval_precision@fas.rst.prstc': 0.04265221878224974, 'eval_recall@fas.rst.prstc': 0.08391066761701117, 'eval_loss@fas.rst.prstc': 2.2221479415893555, 'eval_runtime': 6.3007, 'eval_samples_per_second': 79.197, 'eval_steps_per_second': 2.539, 'epoch': 5.0}
{'train@fas.rst.prstc_loss': 2.2676215171813965, 'train@fas.rst.prstc_accuracy@fas.rst.prstc': 0.2685365853658537, 'train@fas.rst.prstc_f1@fas.rst.prstc': 0.03831340753660656, 'train@fas.rst.prstc_precision@fas.rst.prstc': 0.032120012063109105, 'train@fas.rst.prstc_recall@fas.rst.prstc': 0.06799845961297776, 'train@fas.rst.prstc_loss@fas.rst.prstc': 2.2676212787628174, 'train@fas.rst.prstc_runtime': 49.2086, 'train@fas.rst.prstc_samples_per_second': 83.319, 'train@fas.rst.prstc_steps_per_second': 2.621, 'epoch': 6.0}
{'loss': 2.3057, 'learning_rate': 1e-05, 'epoch': 6.0}
{'eval_loss': 2.190382242202759, 'eval_accuracy@fas.rst.prstc': 0.2965931863727455, 'eval_f1@fas.rst.prstc': 0.05032581453634086, 'eval_precision@fas.rst.prstc': 0.04595178197064989, 'eval_recall@fas.rst.prstc': 0.08190544072226182, 'eval_loss@fas.rst.prstc': 2.190382242202759, 'eval_runtime': 6.3207, 'eval_samples_per_second': 78.946, 'eval_steps_per_second': 2.531, 'epoch': 6.0}
{'train@fas.rst.prstc_loss': 2.2173590660095215, 'train@fas.rst.prstc_accuracy@fas.rst.prstc': 0.30146341463414633, 'train@fas.rst.prstc_f1@fas.rst.prstc': 0.04960754259452501, 'train@fas.rst.prstc_precision@fas.rst.prstc': 0.036412366932623674, 'train@fas.rst.prstc_recall@fas.rst.prstc': 0.07959308109495844, 'train@fas.rst.prstc_loss@fas.rst.prstc': 2.2173588275909424, 'train@fas.rst.prstc_runtime': 49.2098, 'train@fas.rst.prstc_samples_per_second': 83.317, 'train@fas.rst.prstc_steps_per_second': 2.621, 'epoch': 7.0}
{'loss': 2.272, 'learning_rate': 8.333333333333334e-06, 'epoch': 7.0}
{'eval_loss': 2.125913143157959, 'eval_accuracy@fas.rst.prstc': 0.3226452905811623, 'eval_f1@fas.rst.prstc': 0.05948930296756383, 'eval_precision@fas.rst.prstc': 0.04505797101449275, 'eval_recall@fas.rst.prstc': 0.09057258256117844, 'eval_loss@fas.rst.prstc': 2.125913143157959, 'eval_runtime': 6.3182, 'eval_samples_per_second': 78.978, 'eval_steps_per_second': 2.532, 'epoch': 7.0}
{'train@fas.rst.prstc_loss': 2.176532030105591, 'train@fas.rst.prstc_accuracy@fas.rst.prstc': 0.31341463414634146, 'train@fas.rst.prstc_f1@fas.rst.prstc': 0.052713703551914334, 'train@fas.rst.prstc_precision@fas.rst.prstc': 0.04570841045644019, 'train@fas.rst.prstc_recall@fas.rst.prstc': 0.08336009392329793, 'train@fas.rst.prstc_loss@fas.rst.prstc': 2.17653226852417, 'train@fas.rst.prstc_runtime': 49.2501, 'train@fas.rst.prstc_samples_per_second': 83.248, 'train@fas.rst.prstc_steps_per_second': 2.619, 'epoch': 8.0}
{'loss': 2.2294, 'learning_rate': 6.666666666666667e-06, 'epoch': 8.0}
{'eval_loss': 2.08418607711792, 'eval_accuracy@fas.rst.prstc': 0.3346693386773547, 'eval_f1@fas.rst.prstc': 0.06236015325670497, 'eval_precision@fas.rst.prstc': 0.048253745318352054, 'eval_recall@fas.rst.prstc': 0.09411736754573533, 'eval_loss@fas.rst.prstc': 2.08418607711792, 'eval_runtime': 6.3292, 'eval_samples_per_second': 78.841, 'eval_steps_per_second': 2.528, 'epoch': 8.0}
{'train@fas.rst.prstc_loss': 2.1512794494628906, 'train@fas.rst.prstc_accuracy@fas.rst.prstc': 0.3246341463414634, 'train@fas.rst.prstc_f1@fas.rst.prstc': 0.06042211190425656, 'train@fas.rst.prstc_precision@fas.rst.prstc': 0.061889565232446694, 'train@fas.rst.prstc_recall@fas.rst.prstc': 0.08901552393417224, 'train@fas.rst.prstc_loss@fas.rst.prstc': 2.1512794494628906, 'train@fas.rst.prstc_runtime': 49.2029, 'train@fas.rst.prstc_samples_per_second': 83.328, 'train@fas.rst.prstc_steps_per_second': 2.622, 'epoch': 9.0}
{'loss': 2.1976, 'learning_rate': 5e-06, 'epoch': 9.0}
{'eval_loss': 2.0517537593841553, 'eval_accuracy@fas.rst.prstc': 0.3406813627254509, 'eval_f1@fas.rst.prstc': 0.06660775721084276, 'eval_precision@fas.rst.prstc': 0.07145682423297853, 'eval_recall@fas.rst.prstc': 0.09691966543000043, 'eval_loss@fas.rst.prstc': 2.0517539978027344, 'eval_runtime': 6.3364, 'eval_samples_per_second': 78.751, 'eval_steps_per_second': 2.525, 'epoch': 9.0}
{'train@fas.rst.prstc_loss': 2.1306862831115723, 'train@fas.rst.prstc_accuracy@fas.rst.prstc': 0.341219512195122, 'train@fas.rst.prstc_f1@fas.rst.prstc': 0.06880687747274979, 'train@fas.rst.prstc_precision@fas.rst.prstc': 0.06887211993188949, 'train@fas.rst.prstc_recall@fas.rst.prstc': 0.0967745982514443, 'train@fas.rst.prstc_loss@fas.rst.prstc': 2.1306865215301514, 'train@fas.rst.prstc_runtime': 49.2076, 'train@fas.rst.prstc_samples_per_second': 83.32, 'train@fas.rst.prstc_steps_per_second': 2.622, 'epoch': 10.0}
{'loss': 2.1828, 'learning_rate': 3.3333333333333333e-06, 'epoch': 10.0}
{'eval_loss': 2.0298731327056885, 'eval_accuracy@fas.rst.prstc': 0.35470941883767537, 'eval_f1@fas.rst.prstc': 0.07474495474495474, 'eval_precision@fas.rst.prstc': 0.08530901722391084, 'eval_recall@fas.rst.prstc': 0.10327028163093738, 'eval_loss@fas.rst.prstc': 2.0298731327056885, 'eval_runtime': 6.3334, 'eval_samples_per_second': 78.789, 'eval_steps_per_second': 2.526, 'epoch': 10.0}
{'train@fas.rst.prstc_loss': 2.1176681518554688, 'train@fas.rst.prstc_accuracy@fas.rst.prstc': 0.34560975609756095, 'train@fas.rst.prstc_f1@fas.rst.prstc': 0.07105792169936336, 'train@fas.rst.prstc_precision@fas.rst.prstc': 0.09708762197457979, 'train@fas.rst.prstc_recall@fas.rst.prstc': 0.09914744218162165, 'train@fas.rst.prstc_loss@fas.rst.prstc': 2.117668390274048, 'train@fas.rst.prstc_runtime': 49.2433, 'train@fas.rst.prstc_samples_per_second': 83.26, 'train@fas.rst.prstc_steps_per_second': 2.62, 'epoch': 11.0}
{'loss': 2.1615, 'learning_rate': 1.6666666666666667e-06, 'epoch': 11.0}
{'eval_loss': 2.0171918869018555, 'eval_accuracy@fas.rst.prstc': 0.3587174348697395, 'eval_f1@fas.rst.prstc': 0.0781814338763511, 'eval_precision@fas.rst.prstc': 0.09109540807654014, 'eval_recall@fas.rst.prstc': 0.1055593934852666, 'eval_loss@fas.rst.prstc': 2.0171918869018555, 'eval_runtime': 6.3059, 'eval_samples_per_second': 79.133, 'eval_steps_per_second': 2.537, 'epoch': 11.0}
{'train@fas.rst.prstc_loss': 2.1154677867889404, 'train@fas.rst.prstc_accuracy@fas.rst.prstc': 0.34853658536585364, 'train@fas.rst.prstc_f1@fas.rst.prstc': 0.073370164284472, 'train@fas.rst.prstc_precision@fas.rst.prstc': 0.11156013351812966, 'train@fas.rst.prstc_recall@fas.rst.prstc': 0.10117885791593949, 'train@fas.rst.prstc_loss@fas.rst.prstc': 2.1154675483703613, 'train@fas.rst.prstc_runtime': 49.163, 'train@fas.rst.prstc_samples_per_second': 83.396, 'train@fas.rst.prstc_steps_per_second': 2.624, 'epoch': 12.0}
{'loss': 2.1535, 'learning_rate': 0.0, 'epoch': 12.0}
{'eval_loss': 2.015411376953125, 'eval_accuracy@fas.rst.prstc': 0.35671342685370744, 'eval_f1@fas.rst.prstc': 0.07751263360253446, 'eval_precision@fas.rst.prstc': 0.09012775842044135, 'eval_recall@fas.rst.prstc': 0.10497968334033908, 'eval_loss@fas.rst.prstc': 2.015411615371704, 'eval_runtime': 6.317, 'eval_samples_per_second': 78.993, 'eval_steps_per_second': 2.533, 'epoch': 12.0}
{'train_runtime': 1906.496, 'train_samples_per_second': 25.807, 'train_steps_per_second': 0.812, 'train_loss': 2.2969532850792858, 'epoch': 12.0}
***** train_logger metrics *****
  epoch                    =       12.0
  train_loss               =      2.388
  train_runtime            = 0:19:23.84
  train_samples_per_second =     25.271
  train_steps_per_second   =      0.794
-------------------------------------------------------------------
Lang1:  zho.rst.sctb    Lang2:  fas.rst.prstc
Saving run to:  runs/full_shot/FullShot=v4_zho.rst.sctb_fas.rst.prstc_lrfunc=Adafactor_lr=2e-05_plm=bert-base-multilingual-cased
Running with params: BERT_MODEL=bert-base-multilingual-cased lr=2e-05
read 439 examples
read 94 examples
read 159 examples
read 4100 examples
read 499 examples
read 592 examples
Total prediction labels:  34
BertAdapterModel(
  (shared_parameters): ModuleDict()
  (bert): BertModel(
    (shared_parameters): ModuleDict()
    (invertible_adapters): ModuleDict(
      (zh): NICECouplingBlock(
        (F): Sequential(
          (0): Linear(in_features=384, out_features=192, bias=True)
          (1): Activation_Function_Class(
            (f): ReLU()
          )
          (2): Linear(in_features=192, out_features=384, bias=True)
        )
        (G): Sequential(
          (0): Linear(in_features=384, out_features=192, bias=True)
          (1): Activation_Function_Class(
            (f): ReLU()
          )
          (2): Linear(in_features=192, out_features=384, bias=True)
        )
      )
      (ar): NICECouplingBlock(
        (F): Sequential(
          (0): Linear(in_features=384, out_features=192, bias=True)
          (1): Activation_Function_Class(
            (f): ReLU()
          )
          (2): Linear(in_features=192, out_features=384, bias=True)
        )
        (G): Sequential(
          (0): Linear(in_features=384, out_features=192, bias=True)
          (1): Activation_Function_Class(
            (f): ReLU()
          )
          (2): Linear(in_features=192, out_features=384, bias=True)
        )
      )
    )
    (embeddings): BertEmbeddings(
      (word_embeddings): Embedding(119547, 768, padding_idx=0)
      (position_embeddings): Embedding(512, 768)
      (token_type_embeddings): Embedding(2, 768)
      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
      (dropout): Dropout(p=0.1, inplace=False)
    )
    (encoder): BertEncoder(
      (layer): ModuleList(
        (0): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (zh): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (ar): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (1): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (zh): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (ar): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (2): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (zh): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (ar): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (3): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (zh): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (ar): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (4): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (zh): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (ar): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (5): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (zh): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (ar): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (6): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (zh): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (ar): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (7): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (zh): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (ar): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (8): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (zh): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (ar): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (9): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (zh): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (ar): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (10): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (zh): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (ar): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (11): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (zh): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (ar): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
      )
    )
    (pooler): BertPooler(
      (dense): Linear(in_features=768, out_features=768, bias=True)
      (activation): Tanh()
    )
    (prefix_tuning): PrefixTuningPool(
      (prefix_tunings): ModuleDict()
    )
  )
  (heads): ModuleDict(
    (disrpt): ClassificationHead(
      (0): Dropout(p=0.1, inplace=False)
      (1): Linear(in_features=768, out_features=768, bias=True)
      (2): Activation_Function_Class(
        (f): Tanh()
      )
      (3): Dropout(p=0.1, inplace=False)
      (4): Linear(in_features=768, out_features=34, bias=True)
    )
  )
)
{'train@zho.rst.sctb_loss': 3.2409684658050537, 'train@zho.rst.sctb_accuracy@zho.rst.sctb': 0.3325740318906606, 'train@zho.rst.sctb_f1@zho.rst.sctb': 0.019197896120973047, 'train@zho.rst.sctb_precision@zho.rst.sctb': 0.012791308918871562, 'train@zho.rst.sctb_recall@zho.rst.sctb': 0.038461538461538464, 'train@zho.rst.sctb_loss@zho.rst.sctb': 3.240968704223633, 'train@zho.rst.sctb_runtime': 5.4943, 'train@zho.rst.sctb_samples_per_second': 79.902, 'train@zho.rst.sctb_steps_per_second': 2.548, 'epoch': 1.0}
{'loss': 3.3832, 'learning_rate': 1.8333333333333333e-05, 'epoch': 1.0}
{'eval_loss': 3.251443386077881, 'eval_accuracy@zho.rst.sctb': 0.3404255319148936, 'eval_f1@zho.rst.sctb': 0.026733500417710943, 'eval_precision@zho.rst.sctb': 0.01791713325867861, 'eval_recall@zho.rst.sctb': 0.05263157894736842, 'eval_loss@zho.rst.sctb': 3.2514424324035645, 'eval_runtime': 1.414, 'eval_samples_per_second': 66.477, 'eval_steps_per_second': 2.122, 'epoch': 1.0}
{'train@zho.rst.sctb_loss': 3.0400278568267822, 'train@zho.rst.sctb_accuracy@zho.rst.sctb': 0.3325740318906606, 'train@zho.rst.sctb_f1@zho.rst.sctb': 0.019197896120973047, 'train@zho.rst.sctb_precision@zho.rst.sctb': 0.012791308918871562, 'train@zho.rst.sctb_recall@zho.rst.sctb': 0.038461538461538464, 'train@zho.rst.sctb_loss@zho.rst.sctb': 3.040027618408203, 'train@zho.rst.sctb_runtime': 5.5388, 'train@zho.rst.sctb_samples_per_second': 79.259, 'train@zho.rst.sctb_steps_per_second': 2.528, 'epoch': 2.0}
{'loss': 3.1526, 'learning_rate': 1.6666666666666667e-05, 'epoch': 2.0}
{'eval_loss': 3.0613484382629395, 'eval_accuracy@zho.rst.sctb': 0.3404255319148936, 'eval_f1@zho.rst.sctb': 0.026733500417710943, 'eval_precision@zho.rst.sctb': 0.01791713325867861, 'eval_recall@zho.rst.sctb': 0.05263157894736842, 'eval_loss@zho.rst.sctb': 3.061349391937256, 'eval_runtime': 1.4839, 'eval_samples_per_second': 63.346, 'eval_steps_per_second': 2.022, 'epoch': 2.0}
{'train@zho.rst.sctb_loss': 2.8678479194641113, 'train@zho.rst.sctb_accuracy@zho.rst.sctb': 0.3325740318906606, 'train@zho.rst.sctb_f1@zho.rst.sctb': 0.019197896120973047, 'train@zho.rst.sctb_precision@zho.rst.sctb': 0.012791308918871562, 'train@zho.rst.sctb_recall@zho.rst.sctb': 0.038461538461538464, 'train@zho.rst.sctb_loss@zho.rst.sctb': 2.8678476810455322, 'train@zho.rst.sctb_runtime': 5.5542, 'train@zho.rst.sctb_samples_per_second': 79.04, 'train@zho.rst.sctb_steps_per_second': 2.521, 'epoch': 3.0}
{'loss': 2.989, 'learning_rate': 1.5000000000000002e-05, 'epoch': 3.0}
{'eval_loss': 2.8990488052368164, 'eval_accuracy@zho.rst.sctb': 0.3404255319148936, 'eval_f1@zho.rst.sctb': 0.026733500417710943, 'eval_precision@zho.rst.sctb': 0.01791713325867861, 'eval_recall@zho.rst.sctb': 0.05263157894736842, 'eval_loss@zho.rst.sctb': 2.8990490436553955, 'eval_runtime': 1.4479, 'eval_samples_per_second': 64.923, 'eval_steps_per_second': 2.072, 'epoch': 3.0}
{'train@zho.rst.sctb_loss': 2.7271130084991455, 'train@zho.rst.sctb_accuracy@zho.rst.sctb': 0.3325740318906606, 'train@zho.rst.sctb_f1@zho.rst.sctb': 0.019197896120973047, 'train@zho.rst.sctb_precision@zho.rst.sctb': 0.012791308918871562, 'train@zho.rst.sctb_recall@zho.rst.sctb': 0.038461538461538464, 'train@zho.rst.sctb_loss@zho.rst.sctb': 2.7271127700805664, 'train@zho.rst.sctb_runtime': 5.5521, 'train@zho.rst.sctb_samples_per_second': 79.069, 'train@zho.rst.sctb_steps_per_second': 2.522, 'epoch': 4.0}
{'loss': 2.8303, 'learning_rate': 1.3333333333333333e-05, 'epoch': 4.0}
{'eval_loss': 2.768373966217041, 'eval_accuracy@zho.rst.sctb': 0.3404255319148936, 'eval_f1@zho.rst.sctb': 0.026733500417710943, 'eval_precision@zho.rst.sctb': 0.01791713325867861, 'eval_recall@zho.rst.sctb': 0.05263157894736842, 'eval_loss@zho.rst.sctb': 2.768373966217041, 'eval_runtime': 1.455, 'eval_samples_per_second': 64.604, 'eval_steps_per_second': 2.062, 'epoch': 4.0}
{'train@zho.rst.sctb_loss': 2.615832805633545, 'train@zho.rst.sctb_accuracy@zho.rst.sctb': 0.3325740318906606, 'train@zho.rst.sctb_f1@zho.rst.sctb': 0.019197896120973047, 'train@zho.rst.sctb_precision@zho.rst.sctb': 0.012791308918871562, 'train@zho.rst.sctb_recall@zho.rst.sctb': 0.038461538461538464, 'train@zho.rst.sctb_loss@zho.rst.sctb': 2.615832567214966, 'train@zho.rst.sctb_runtime': 5.5622, 'train@zho.rst.sctb_samples_per_second': 78.926, 'train@zho.rst.sctb_steps_per_second': 2.517, 'epoch': 5.0}
{'loss': 2.7132, 'learning_rate': 1.1666666666666668e-05, 'epoch': 5.0}
{'eval_loss': 2.667205572128296, 'eval_accuracy@zho.rst.sctb': 0.3404255319148936, 'eval_f1@zho.rst.sctb': 0.026733500417710943, 'eval_precision@zho.rst.sctb': 0.01791713325867861, 'eval_recall@zho.rst.sctb': 0.05263157894736842, 'eval_loss@zho.rst.sctb': 2.667205333709717, 'eval_runtime': 1.4304, 'eval_samples_per_second': 65.714, 'eval_steps_per_second': 2.097, 'epoch': 5.0}
{'train@zho.rst.sctb_loss': 2.526787519454956, 'train@zho.rst.sctb_accuracy@zho.rst.sctb': 0.3325740318906606, 'train@zho.rst.sctb_f1@zho.rst.sctb': 0.019197896120973047, 'train@zho.rst.sctb_precision@zho.rst.sctb': 0.012791308918871562, 'train@zho.rst.sctb_recall@zho.rst.sctb': 0.038461538461538464, 'train@zho.rst.sctb_loss@zho.rst.sctb': 2.526787519454956, 'train@zho.rst.sctb_runtime': 5.5433, 'train@zho.rst.sctb_samples_per_second': 79.194, 'train@zho.rst.sctb_steps_per_second': 2.526, 'epoch': 6.0}
{'loss': 2.6106, 'learning_rate': 1e-05, 'epoch': 6.0}
{'eval_loss': 2.589127779006958, 'eval_accuracy@zho.rst.sctb': 0.3404255319148936, 'eval_f1@zho.rst.sctb': 0.026733500417710943, 'eval_precision@zho.rst.sctb': 0.01791713325867861, 'eval_recall@zho.rst.sctb': 0.05263157894736842, 'eval_loss@zho.rst.sctb': 2.5891273021698, 'eval_runtime': 1.4599, 'eval_samples_per_second': 64.387, 'eval_steps_per_second': 2.055, 'epoch': 6.0}
{'train@zho.rst.sctb_loss': 2.4595065116882324, 'train@zho.rst.sctb_accuracy@zho.rst.sctb': 0.33712984054669703, 'train@zho.rst.sctb_f1@zho.rst.sctb': 0.020866713061116596, 'train@zho.rst.sctb_precision@zho.rst.sctb': 0.038520348153375676, 'train@zho.rst.sctb_recall@zho.rst.sctb': 0.039271255060728746, 'train@zho.rst.sctb_loss@zho.rst.sctb': 2.4595065116882324, 'train@zho.rst.sctb_runtime': 5.5526, 'train@zho.rst.sctb_samples_per_second': 79.063, 'train@zho.rst.sctb_steps_per_second': 2.521, 'epoch': 7.0}
{'loss': 2.5274, 'learning_rate': 8.333333333333334e-06, 'epoch': 7.0}
{'eval_loss': 2.5314559936523438, 'eval_accuracy@zho.rst.sctb': 0.32978723404255317, 'eval_f1@zho.rst.sctb': 0.026105263157894736, 'eval_precision@zho.rst.sctb': 0.017543859649122806, 'eval_recall@zho.rst.sctb': 0.05098684210526316, 'eval_loss@zho.rst.sctb': 2.531456232070923, 'eval_runtime': 1.4415, 'eval_samples_per_second': 65.212, 'eval_steps_per_second': 2.081, 'epoch': 7.0}
{'train@zho.rst.sctb_loss': 2.413672685623169, 'train@zho.rst.sctb_accuracy@zho.rst.sctb': 0.33940774487471526, 'train@zho.rst.sctb_f1@zho.rst.sctb': 0.0216710875331565, 'train@zho.rst.sctb_precision@zho.rst.sctb': 0.03601559730591988, 'train@zho.rst.sctb_recall@zho.rst.sctb': 0.03967611336032389, 'train@zho.rst.sctb_loss@zho.rst.sctb': 2.41367244720459, 'train@zho.rst.sctb_runtime': 5.5607, 'train@zho.rst.sctb_samples_per_second': 78.947, 'train@zho.rst.sctb_steps_per_second': 2.518, 'epoch': 8.0}
{'loss': 2.4718, 'learning_rate': 6.666666666666667e-06, 'epoch': 8.0}
{'eval_loss': 2.493821144104004, 'eval_accuracy@zho.rst.sctb': 0.32978723404255317, 'eval_f1@zho.rst.sctb': 0.026105263157894736, 'eval_precision@zho.rst.sctb': 0.017543859649122806, 'eval_recall@zho.rst.sctb': 0.05098684210526316, 'eval_loss@zho.rst.sctb': 2.493820905685425, 'eval_runtime': 1.4392, 'eval_samples_per_second': 65.315, 'eval_steps_per_second': 2.085, 'epoch': 8.0}
{'train@zho.rst.sctb_loss': 2.3813304901123047, 'train@zho.rst.sctb_accuracy@zho.rst.sctb': 0.33940774487471526, 'train@zho.rst.sctb_f1@zho.rst.sctb': 0.02168168191869008, 'train@zho.rst.sctb_precision@zho.rst.sctb': 0.03219932492449813, 'train@zho.rst.sctb_recall@zho.rst.sctb': 0.03967611336032389, 'train@zho.rst.sctb_loss@zho.rst.sctb': 2.3813304901123047, 'train@zho.rst.sctb_runtime': 5.5566, 'train@zho.rst.sctb_samples_per_second': 79.005, 'train@zho.rst.sctb_steps_per_second': 2.52, 'epoch': 9.0}
{'loss': 2.4265, 'learning_rate': 5e-06, 'epoch': 9.0}
{'eval_loss': 2.4683146476745605, 'eval_accuracy@zho.rst.sctb': 0.32978723404255317, 'eval_f1@zho.rst.sctb': 0.026105263157894736, 'eval_precision@zho.rst.sctb': 0.017543859649122806, 'eval_recall@zho.rst.sctb': 0.05098684210526316, 'eval_loss@zho.rst.sctb': 2.4683148860931396, 'eval_runtime': 1.4481, 'eval_samples_per_second': 64.911, 'eval_steps_per_second': 2.072, 'epoch': 9.0}
{'train@zho.rst.sctb_loss': 2.359839677810669, 'train@zho.rst.sctb_accuracy@zho.rst.sctb': 0.3439635535307517, 'train@zho.rst.sctb_f1@zho.rst.sctb': 0.0231947762382545, 'train@zho.rst.sctb_precision@zho.rst.sctb': 0.0323202438587054, 'train@zho.rst.sctb_recall@zho.rst.sctb': 0.04048582995951417, 'train@zho.rst.sctb_loss@zho.rst.sctb': 2.35983943939209, 'train@zho.rst.sctb_runtime': 5.5744, 'train@zho.rst.sctb_samples_per_second': 78.753, 'train@zho.rst.sctb_steps_per_second': 2.511, 'epoch': 10.0}
{'loss': 2.4049, 'learning_rate': 3.3333333333333333e-06, 'epoch': 10.0}
{'eval_loss': 2.4525272846221924, 'eval_accuracy@zho.rst.sctb': 0.3404255319148936, 'eval_f1@zho.rst.sctb': 0.035909445745511324, 'eval_precision@zho.rst.sctb': 0.043859649122807015, 'eval_recall@zho.rst.sctb': 0.05553405572755418, 'eval_loss@zho.rst.sctb': 2.4525270462036133, 'eval_runtime': 1.4336, 'eval_samples_per_second': 65.57, 'eval_steps_per_second': 2.093, 'epoch': 10.0}
{'train@zho.rst.sctb_loss': 2.3483071327209473, 'train@zho.rst.sctb_accuracy@zho.rst.sctb': 0.34851936218678814, 'train@zho.rst.sctb_f1@zho.rst.sctb': 0.024632297128260334, 'train@zho.rst.sctb_precision@zho.rst.sctb': 0.03558668107848436, 'train@zho.rst.sctb_recall@zho.rst.sctb': 0.04129554655870445, 'train@zho.rst.sctb_loss@zho.rst.sctb': 2.3483071327209473, 'train@zho.rst.sctb_runtime': 5.53, 'train@zho.rst.sctb_samples_per_second': 79.385, 'train@zho.rst.sctb_steps_per_second': 2.532, 'epoch': 11.0}
{'loss': 2.3852, 'learning_rate': 1.6666666666666667e-06, 'epoch': 11.0}
{'eval_loss': 2.4438869953155518, 'eval_accuracy@zho.rst.sctb': 0.3404255319148936, 'eval_f1@zho.rst.sctb': 0.035909445745511324, 'eval_precision@zho.rst.sctb': 0.043859649122807015, 'eval_recall@zho.rst.sctb': 0.05553405572755418, 'eval_loss@zho.rst.sctb': 2.443887233734131, 'eval_runtime': 1.4437, 'eval_samples_per_second': 65.113, 'eval_steps_per_second': 2.078, 'epoch': 11.0}
{'train@zho.rst.sctb_loss': 2.344465732574463, 'train@zho.rst.sctb_accuracy@zho.rst.sctb': 0.35079726651480636, 'train@zho.rst.sctb_f1@zho.rst.sctb': 0.02533221763990995, 'train@zho.rst.sctb_precision@zho.rst.sctb': 0.03685029307997889, 'train@zho.rst.sctb_recall@zho.rst.sctb': 0.041700404858299595, 'train@zho.rst.sctb_loss@zho.rst.sctb': 2.344465494155884, 'train@zho.rst.sctb_runtime': 6.971, 'train@zho.rst.sctb_samples_per_second': 62.975, 'train@zho.rst.sctb_steps_per_second': 2.008, 'epoch': 12.0}
{'loss': 2.3706, 'learning_rate': 0.0, 'epoch': 12.0}
{'eval_loss': 2.4411749839782715, 'eval_accuracy@zho.rst.sctb': 0.3404255319148936, 'eval_f1@zho.rst.sctb': 0.03566768160069595, 'eval_precision@zho.rst.sctb': 0.03879361324659965, 'eval_recall@zho.rst.sctb': 0.05553405572755418, 'eval_loss@zho.rst.sctb': 2.4411745071411133, 'eval_runtime': 1.4502, 'eval_samples_per_second': 64.82, 'eval_steps_per_second': 2.069, 'epoch': 12.0}
{'train_runtime': 217.4464, 'train_samples_per_second': 24.227, 'train_steps_per_second': 0.773, 'train_loss': 2.6887734731038413, 'epoch': 12.0}
***** train_log metrics *****
  epoch                    =       12.0
  train_loss               =     2.6888
  train_runtime            = 0:03:37.44
  train_samples_per_second =     24.227
  train_steps_per_second   =      0.773
{'train@fas.rst.prstc_loss': 2.415306806564331, 'train@fas.rst.prstc_accuracy@fas.rst.prstc': 0.23780487804878048, 'train@fas.rst.prstc_f1@fas.rst.prstc': 0.02260214430599826, 'train@fas.rst.prstc_precision@fas.rst.prstc': 0.013988522238163558, 'train@fas.rst.prstc_recall@fas.rst.prstc': 0.058823529411764705, 'train@fas.rst.prstc_loss@fas.rst.prstc': 2.415306329727173, 'train@fas.rst.prstc_runtime': 49.1551, 'train@fas.rst.prstc_samples_per_second': 83.41, 'train@fas.rst.prstc_steps_per_second': 2.624, 'epoch': 1.0}
{'loss': 2.7913, 'learning_rate': 1.8333333333333333e-05, 'epoch': 1.0}
{'eval_loss': 2.333454132080078, 'eval_accuracy@fas.rst.prstc': 0.24448897795591182, 'eval_f1@fas.rst.prstc': 0.026194310252281264, 'eval_precision@fas.rst.prstc': 0.016299265197060788, 'eval_recall@fas.rst.prstc': 0.06666666666666667, 'eval_loss@fas.rst.prstc': 2.333454132080078, 'eval_runtime': 6.2818, 'eval_samples_per_second': 79.436, 'eval_steps_per_second': 2.547, 'epoch': 1.0}
{'train@fas.rst.prstc_loss': 2.3583321571350098, 'train@fas.rst.prstc_accuracy@fas.rst.prstc': 0.25682926829268293, 'train@fas.rst.prstc_f1@fas.rst.prstc': 0.04199852159352749, 'train@fas.rst.prstc_precision@fas.rst.prstc': 0.030598213429481963, 'train@fas.rst.prstc_recall@fas.rst.prstc': 0.06812917856722611, 'train@fas.rst.prstc_loss@fas.rst.prstc': 2.3583321571350098, 'train@fas.rst.prstc_runtime': 49.1755, 'train@fas.rst.prstc_samples_per_second': 83.375, 'train@fas.rst.prstc_steps_per_second': 2.623, 'epoch': 2.0}
{'loss': 2.3997, 'learning_rate': 1.6666666666666667e-05, 'epoch': 2.0}
{'eval_loss': 2.270866632461548, 'eval_accuracy@fas.rst.prstc': 0.2665330661322645, 'eval_f1@fas.rst.prstc': 0.04758900726823742, 'eval_precision@fas.rst.prstc': 0.035913640595903164, 'eval_recall@fas.rst.prstc': 0.0754383464005702, 'eval_loss@fas.rst.prstc': 2.2708663940429688, 'eval_runtime': 6.2766, 'eval_samples_per_second': 79.501, 'eval_steps_per_second': 2.549, 'epoch': 2.0}
{'train@fas.rst.prstc_loss': 2.3400650024414062, 'train@fas.rst.prstc_accuracy@fas.rst.prstc': 0.2502439024390244, 'train@fas.rst.prstc_f1@fas.rst.prstc': 0.030836331691697454, 'train@fas.rst.prstc_precision@fas.rst.prstc': 0.031459444535585815, 'train@fas.rst.prstc_recall@fas.rst.prstc': 0.06254441984104062, 'train@fas.rst.prstc_loss@fas.rst.prstc': 2.340064764022827, 'train@fas.rst.prstc_runtime': 49.2275, 'train@fas.rst.prstc_samples_per_second': 83.287, 'train@fas.rst.prstc_steps_per_second': 2.62, 'epoch': 3.0}
{'loss': 2.362, 'learning_rate': 1.5000000000000002e-05, 'epoch': 3.0}
{'eval_loss': 2.2537806034088135, 'eval_accuracy@fas.rst.prstc': 0.2725450901803607, 'eval_f1@fas.rst.prstc': 0.041730516213915325, 'eval_precision@fas.rst.prstc': 0.05150261924455472, 'eval_recall@fas.rst.prstc': 0.07484913281064386, 'eval_loss@fas.rst.prstc': 2.2537806034088135, 'eval_runtime': 6.2978, 'eval_samples_per_second': 79.234, 'eval_steps_per_second': 2.541, 'epoch': 3.0}
{'train@fas.rst.prstc_loss': 2.3252663612365723, 'train@fas.rst.prstc_accuracy@fas.rst.prstc': 0.23829268292682926, 'train@fas.rst.prstc_f1@fas.rst.prstc': 0.022888524544973025, 'train@fas.rst.prstc_precision@fas.rst.prstc': 0.07281887864955645, 'train@fas.rst.prstc_recall@fas.rst.prstc': 0.058962592129050195, 'train@fas.rst.prstc_loss@fas.rst.prstc': 2.325266122817993, 'train@fas.rst.prstc_runtime': 49.2025, 'train@fas.rst.prstc_samples_per_second': 83.329, 'train@fas.rst.prstc_steps_per_second': 2.622, 'epoch': 4.0}
{'loss': 2.3473, 'learning_rate': 1.3333333333333333e-05, 'epoch': 4.0}
{'eval_loss': 2.2435412406921387, 'eval_accuracy@fas.rst.prstc': 0.24448897795591182, 'eval_f1@fas.rst.prstc': 0.026194310252281264, 'eval_precision@fas.rst.prstc': 0.016299265197060788, 'eval_recall@fas.rst.prstc': 0.06666666666666667, 'eval_loss@fas.rst.prstc': 2.2435412406921387, 'eval_runtime': 6.2594, 'eval_samples_per_second': 79.72, 'eval_steps_per_second': 2.556, 'epoch': 4.0}
{'train@fas.rst.prstc_loss': 2.3138906955718994, 'train@fas.rst.prstc_accuracy@fas.rst.prstc': 0.26195121951219513, 'train@fas.rst.prstc_f1@fas.rst.prstc': 0.03635446493671657, 'train@fas.rst.prstc_precision@fas.rst.prstc': 0.03176197069258395, 'train@fas.rst.prstc_recall@fas.rst.prstc': 0.06610271386241348, 'train@fas.rst.prstc_loss@fas.rst.prstc': 2.3138906955718994, 'train@fas.rst.prstc_runtime': 49.1922, 'train@fas.rst.prstc_samples_per_second': 83.347, 'train@fas.rst.prstc_steps_per_second': 2.622, 'epoch': 5.0}
{'loss': 2.3339, 'learning_rate': 1.1666666666666668e-05, 'epoch': 5.0}
{'eval_loss': 2.229696750640869, 'eval_accuracy@fas.rst.prstc': 0.27655310621242485, 'eval_f1@fas.rst.prstc': 0.04389367447304387, 'eval_precision@fas.rst.prstc': 0.04182500586441473, 'eval_recall@fas.rst.prstc': 0.07610833927298645, 'eval_loss@fas.rst.prstc': 2.2296969890594482, 'eval_runtime': 6.2696, 'eval_samples_per_second': 79.59, 'eval_steps_per_second': 2.552, 'epoch': 5.0}
{'train@fas.rst.prstc_loss': 2.2984085083007812, 'train@fas.rst.prstc_accuracy@fas.rst.prstc': 0.24341463414634146, 'train@fas.rst.prstc_f1@fas.rst.prstc': 0.02641239964622305, 'train@fas.rst.prstc_precision@fas.rst.prstc': 0.03233222985838425, 'train@fas.rst.prstc_recall@fas.rst.prstc': 0.060468748328573116, 'train@fas.rst.prstc_loss@fas.rst.prstc': 2.2984085083007812, 'train@fas.rst.prstc_runtime': 49.1623, 'train@fas.rst.prstc_samples_per_second': 83.397, 'train@fas.rst.prstc_steps_per_second': 2.624, 'epoch': 6.0}
{'loss': 2.3195, 'learning_rate': 1e-05, 'epoch': 6.0}
{'eval_loss': 2.2208688259124756, 'eval_accuracy@fas.rst.prstc': 0.2565130260521042, 'eval_f1@fas.rst.prstc': 0.033813944974436905, 'eval_precision@fas.rst.prstc': 0.049965635738831615, 'eval_recall@fas.rst.prstc': 0.07017818959372772, 'eval_loss@fas.rst.prstc': 2.2208690643310547, 'eval_runtime': 6.2856, 'eval_samples_per_second': 79.388, 'eval_steps_per_second': 2.545, 'epoch': 6.0}
{'train@fas.rst.prstc_loss': 2.277937173843384, 'train@fas.rst.prstc_accuracy@fas.rst.prstc': 0.28512195121951217, 'train@fas.rst.prstc_f1@fas.rst.prstc': 0.043611904688538594, 'train@fas.rst.prstc_precision@fas.rst.prstc': 0.03405130002628845, 'train@fas.rst.prstc_recall@fas.rst.prstc': 0.07305777521046608, 'train@fas.rst.prstc_loss@fas.rst.prstc': 2.277937173843384, 'train@fas.rst.prstc_runtime': 49.2033, 'train@fas.rst.prstc_samples_per_second': 83.328, 'train@fas.rst.prstc_steps_per_second': 2.622, 'epoch': 7.0}
{'loss': 2.3047, 'learning_rate': 8.333333333333334e-06, 'epoch': 7.0}
{'eval_loss': 2.1942458152770996, 'eval_accuracy@fas.rst.prstc': 0.2965931863727455, 'eval_f1@fas.rst.prstc': 0.05143504258521913, 'eval_precision@fas.rst.prstc': 0.040581480233602876, 'eval_recall@fas.rst.prstc': 0.08223806129722024, 'eval_loss@fas.rst.prstc': 2.1942458152770996, 'eval_runtime': 6.2651, 'eval_samples_per_second': 79.647, 'eval_steps_per_second': 2.554, 'epoch': 7.0}
{'train@fas.rst.prstc_loss': 2.2439188957214355, 'train@fas.rst.prstc_accuracy@fas.rst.prstc': 0.30146341463414633, 'train@fas.rst.prstc_f1@fas.rst.prstc': 0.04828239874454658, 'train@fas.rst.prstc_precision@fas.rst.prstc': 0.034842144216632454, 'train@fas.rst.prstc_recall@fas.rst.prstc': 0.07875592353689977, 'train@fas.rst.prstc_loss@fas.rst.prstc': 2.2439188957214355, 'train@fas.rst.prstc_runtime': 49.2283, 'train@fas.rst.prstc_samples_per_second': 83.285, 'train@fas.rst.prstc_steps_per_second': 2.62, 'epoch': 8.0}
{'loss': 2.2858, 'learning_rate': 6.666666666666667e-06, 'epoch': 8.0}
{'eval_loss': 2.1626222133636475, 'eval_accuracy@fas.rst.prstc': 0.312625250501002, 'eval_f1@fas.rst.prstc': 0.05641400601186124, 'eval_precision@fas.rst.prstc': 0.04164631795399049, 'eval_recall@fas.rst.prstc': 0.08744119743406985, 'eval_loss@fas.rst.prstc': 2.1626222133636475, 'eval_runtime': 6.2855, 'eval_samples_per_second': 79.39, 'eval_steps_per_second': 2.546, 'epoch': 8.0}
{'train@fas.rst.prstc_loss': 2.2153520584106445, 'train@fas.rst.prstc_accuracy@fas.rst.prstc': 0.3026829268292683, 'train@fas.rst.prstc_f1@fas.rst.prstc': 0.04940499951176, 'train@fas.rst.prstc_precision@fas.rst.prstc': 0.03594859183893535, 'train@fas.rst.prstc_recall@fas.rst.prstc': 0.07968315094723104, 'train@fas.rst.prstc_loss@fas.rst.prstc': 2.2153518199920654, 'train@fas.rst.prstc_runtime': 49.1329, 'train@fas.rst.prstc_samples_per_second': 83.447, 'train@fas.rst.prstc_steps_per_second': 2.626, 'epoch': 9.0}
{'loss': 2.2532, 'learning_rate': 5e-06, 'epoch': 9.0}
{'eval_loss': 2.1262969970703125, 'eval_accuracy@fas.rst.prstc': 0.3226452905811623, 'eval_f1@fas.rst.prstc': 0.05920695921589657, 'eval_precision@fas.rst.prstc': 0.044465627944689154, 'eval_recall@fas.rst.prstc': 0.0904727963886909, 'eval_loss@fas.rst.prstc': 2.1262972354888916, 'eval_runtime': 6.2711, 'eval_samples_per_second': 79.571, 'eval_steps_per_second': 2.551, 'epoch': 9.0}
{'train@fas.rst.prstc_loss': 2.1967499256134033, 'train@fas.rst.prstc_accuracy@fas.rst.prstc': 0.3048780487804878, 'train@fas.rst.prstc_f1@fas.rst.prstc': 0.05026330165273904, 'train@fas.rst.prstc_precision@fas.rst.prstc': 0.036979726404494484, 'train@fas.rst.prstc_recall@fas.rst.prstc': 0.08057571964956195, 'train@fas.rst.prstc_loss@fas.rst.prstc': 2.1967501640319824, 'train@fas.rst.prstc_runtime': 49.1683, 'train@fas.rst.prstc_samples_per_second': 83.387, 'train@fas.rst.prstc_steps_per_second': 2.624, 'epoch': 10.0}
{'loss': 2.239, 'learning_rate': 3.3333333333333333e-06, 'epoch': 10.0}
{'eval_loss': 2.1049647331237793, 'eval_accuracy@fas.rst.prstc': 0.32665330661322645, 'eval_f1@fas.rst.prstc': 0.060419487827630826, 'eval_precision@fas.rst.prstc': 0.04595316674524595, 'eval_recall@fas.rst.prstc': 0.09169874079353765, 'eval_loss@fas.rst.prstc': 2.1049647331237793, 'eval_runtime': 6.2748, 'eval_samples_per_second': 79.525, 'eval_steps_per_second': 2.55, 'epoch': 10.0}
{'train@fas.rst.prstc_loss': 2.186680555343628, 'train@fas.rst.prstc_accuracy@fas.rst.prstc': 0.3070731707317073, 'train@fas.rst.prstc_f1@fas.rst.prstc': 0.05082028358749525, 'train@fas.rst.prstc_precision@fas.rst.prstc': 0.03760380121572021, 'train@fas.rst.prstc_recall@fas.rst.prstc': 0.08131189628060716, 'train@fas.rst.prstc_loss@fas.rst.prstc': 2.186680555343628, 'train@fas.rst.prstc_runtime': 49.1725, 'train@fas.rst.prstc_samples_per_second': 83.38, 'train@fas.rst.prstc_steps_per_second': 2.623, 'epoch': 11.0}
{'loss': 2.2191, 'learning_rate': 1.6666666666666667e-06, 'epoch': 11.0}
{'eval_loss': 2.0921339988708496, 'eval_accuracy@fas.rst.prstc': 0.33266533066132264, 'eval_f1@fas.rst.prstc': 0.06173835193241284, 'eval_precision@fas.rst.prstc': 0.047264038231780166, 'eval_recall@fas.rst.prstc': 0.09343787122832027, 'eval_loss@fas.rst.prstc': 2.0921337604522705, 'eval_runtime': 6.2674, 'eval_samples_per_second': 79.619, 'eval_steps_per_second': 2.553, 'epoch': 11.0}
{'train@fas.rst.prstc_loss': 2.187079668045044, 'train@fas.rst.prstc_accuracy@fas.rst.prstc': 0.30902439024390244, 'train@fas.rst.prstc_f1@fas.rst.prstc': 0.05109835511759709, 'train@fas.rst.prstc_precision@fas.rst.prstc': 0.037757534418706794, 'train@fas.rst.prstc_recall@fas.rst.prstc': 0.08179455088090883, 'train@fas.rst.prstc_loss@fas.rst.prstc': 2.187079429626465, 'train@fas.rst.prstc_runtime': 49.1554, 'train@fas.rst.prstc_samples_per_second': 83.409, 'train@fas.rst.prstc_steps_per_second': 2.624, 'epoch': 12.0}
{'loss': 2.2097, 'learning_rate': 0.0, 'epoch': 12.0}
{'eval_loss': 2.0922791957855225, 'eval_accuracy@fas.rst.prstc': 0.33266533066132264, 'eval_f1@fas.rst.prstc': 0.06162987132329801, 'eval_precision@fas.rst.prstc': 0.047068289476666435, 'eval_recall@fas.rst.prstc': 0.09343787122832027, 'eval_loss@fas.rst.prstc': 2.0922794342041016, 'eval_runtime': 6.2832, 'eval_samples_per_second': 79.419, 'eval_steps_per_second': 2.546, 'epoch': 12.0}
{'train_runtime': 1899.4725, 'train_samples_per_second': 25.902, 'train_steps_per_second': 0.815, 'train_loss': 2.3387429880541424, 'epoch': 12.0}
***** train_logger metrics *****
  epoch                    =       12.0
  train_loss               =     2.6888
  train_runtime            = 0:03:37.44
  train_samples_per_second =     24.227
  train_steps_per_second   =      0.773

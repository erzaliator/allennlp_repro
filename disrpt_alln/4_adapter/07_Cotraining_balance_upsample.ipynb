{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#seeding for comparing experiment in part 2\n",
    "import torch\n",
    "import json\n",
    "SEED = 2025\n",
    "torch.manual_seed(SEED)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "\n",
    "device = torch.device('cuda:2')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SNLI Bert\n",
    "## Second Tutorial\n",
    "https://towardsdatascience.com/fine-tuning-pre-trained-transformer-models-for-sentence-entailment-d87caf9ec9db\n",
    "Check his Github code for complete notebook. I never referred to it. Medium was enough.\n",
    "BERT in keras-tf: https://towardsdatascience.com/bert-in-keras-with-tensorflow-hub-76bcbc9417b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define macros\n",
    "BERT_MODEL = 'bert-base-german-cased'\n",
    "batch_size = 4\n",
    "batches_per_epoch = None\n",
    "\n",
    "save_path_suffix = 'cotraining_baseline_de_en_allshuffle_de_labelset_balance_upsample_en4000_'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## load the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# custom reader needed to handle quotechars\n",
    "def read_df_custom(file):\n",
    "    header = 'doc     unit1_toks      unit2_toks      unit1_txt       unit2_txt       s1_toks s2_toks unit1_sent      unit2_sent      dir     nuc_children    sat_children    genre   u1_discontinuous        u2_discontinuous       u1_issent        u2_issent       u1_length       u2_length       length_ratio    u1_speaker      u2_speaker      same_speaker    u1_func u1_pos  u1_depdir       u2_func u2_pos  u2_depdir       doclen  u1_position      u2_position     percent_distance        distance        lex_overlap_words       lex_overlap_length      unit1_case      unit2_case      label'\n",
    "    extracted_columns = ['unit1_txt', 'unit1_sent', 'unit2_txt', 'unit2_sent', 'dir', 'label', 'distance', 'u1_depdir', 'u2_depdir', 'u2_func', 'u1_position', 'u2_position', 'sat_children', 'nuc_children', 'genre', 'unit1_case', 'unit2_case',\n",
    "                            'u1_discontinuous', 'u2_discontinuous', 'same_speaker', 'lex_overlap_length', 'u1_func']\n",
    "    header = header.split()\n",
    "    df = pd.DataFrame(columns=extracted_columns)\n",
    "    file = open(file, 'r')\n",
    "\n",
    "    rows = []\n",
    "    count = 0 \n",
    "    for line in file:\n",
    "        line = line[:-1].split('\\t')\n",
    "        count+=1\n",
    "        if count ==1: continue\n",
    "        row = {}\n",
    "        for column in extracted_columns:\n",
    "            index = header.index(column)\n",
    "            try:\n",
    "                row[column] = line[index]\n",
    "            except:\n",
    "                print(count, line)\n",
    "            row[column] = line[index]\n",
    "        rows.append(row)\n",
    "\n",
    "    df = pd.concat([df, pd.DataFrame.from_records(rows)])\n",
    "    return df\n",
    "\n",
    "train_df_en = read_df_custom('../../processed/translated/eng.rst.rstdt_train_enriched_translated.rels')\n",
    "test_df_en = read_df_custom('../../processed/translated/eng.rst.rstdt_test_enriched_translated.rels')\n",
    "val_df_en = read_df_custom('../../processed/translated/eng.rst.rstdt_dev_enriched_translated.rels')\n",
    "train_df_de = read_df_custom('../../processed/deu.rst.pcc_train_enriched.rels')\n",
    "test_df_de = read_df_custom('../../processed/deu.rst.pcc_test_enriched.rels')\n",
    "val_df_de = read_df_custom('../../processed/deu.rst.pcc_dev_enriched.rels')\n",
    "\n",
    "lang='deu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>target</th>\n",
       "      <th>aux_new_count</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>label</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>joint</th>\n",
       "      <td>201</td>\n",
       "      <td>1076</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>elaboration</th>\n",
       "      <td>166</td>\n",
       "      <td>889</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>background</th>\n",
       "      <td>131</td>\n",
       "      <td>701</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>condition</th>\n",
       "      <td>99</td>\n",
       "      <td>265</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>cause</th>\n",
       "      <td>92</td>\n",
       "      <td>493</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>contrast</th>\n",
       "      <td>38</td>\n",
       "      <td>203</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>summary</th>\n",
       "      <td>6</td>\n",
       "      <td>32</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             target  aux_new_count\n",
       "label                             \n",
       "joint           201           1076\n",
       "elaboration     166            889\n",
       "background      131            701\n",
       "condition        99            265\n",
       "cause            92            493\n",
       "contrast         38            203\n",
       "summary           6             32"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def dist_scaling(df_target, df_aux, plt=False):\n",
    "    dist1 = df_target.groupby('label').count()['dir'].reset_index().rename(columns={\"dir\": \"target\"})\n",
    "    dist1[\"target\"] = dist1[\"target\"]\n",
    "    dist2 = df_aux.groupby('label').count()['dir'].reset_index().rename(columns={\"dir\": \"aux\"})\n",
    "    j = pd.merge(dist1,dist2,on='label')\n",
    "    j.sort_values(by=[\"target\"], inplace=True, ascending=False)\n",
    "    j.set_index('label', inplace=True)\n",
    "    j['ratios'] = j[\"aux\"]/j[\"target\"]\n",
    "    minimum_ratio = j['ratios'].min()\n",
    "    j['aux_new'] = j['target']*minimum_ratio*2\n",
    "    j.at['condition', 'aux_new'] = j.at['condition', 'aux']\n",
    "    j['aux_new_count'] = j['aux_new'].round().astype(int)\n",
    "    j = j[['target', 'aux_new_count']]\n",
    "\n",
    "    if plt:\n",
    "        ax = j.plot.bar(rot=0, title=\"New Label distribution for Aux and Target datasets\")\n",
    "        ax.set_xlabel(\"Labels\")\n",
    "        ax.set_ylabel(\"Percentages\")\n",
    "    return j\n",
    "\n",
    "dist_scaling(train_df_de, train_df_en, plt=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from copy import deepcopy\n",
    "\n",
    "data = pd.DataFrame({'cols1':[4, 5, 5, 4, 321, 32, 5, 40, 50, 60],\n",
    "                     'cols2':[45, 66, 6, 6, 1, 432, 3, 40, 50, 60],\n",
    "                     'label':['A', 'B', 'C', 'C', 'A', 'B', 'B', 'A', 'D', 'F']})\n",
    "\n",
    "freq = pd.DataFrame({'label':['A', 'B', 'C', 'E'],\n",
    "                     'nostoextract':[2, 2, 2, 30], })\n",
    "\n",
    "def bootstrap(data, freq):\n",
    "    data_labels = list(data['label'].unique())\n",
    "    #drop the labels in freq that dont overlap\n",
    "    freq_bak = freq.copy(deep=True)\n",
    "    freq = freq[freq['label'].isin(data_labels)]\n",
    "\n",
    "    #save the labels in data that dont overlap\n",
    "    freq_labels = list(freq['label'].unique())\n",
    "    extra_data = data[~data.label.isin(freq_labels)]\n",
    "\n",
    "    #drop the labels in data that dont overlap\n",
    "    data = data[data['label'].isin(freq_labels)]\n",
    "\n",
    "    #bootstrap!\n",
    "    freq = freq.set_index('label')\n",
    "\n",
    "    def sampleClass(classgroup):\n",
    "        cls = classgroup['label'].iloc[0]\n",
    "        nDesired = freq.nostoextract[cls]\n",
    "        nRows = len(classgroup)\n",
    "\n",
    "        nSamples = min(nRows, nDesired)\n",
    "        return classgroup.sample(nSamples)\n",
    "\n",
    "    samples = data.groupby('label').apply(sampleClass)\n",
    "    samples.index = samples.index.get_level_values(1)\n",
    "\n",
    "    #add back the extra samples\n",
    "    samples = pd.concat([samples, extra_data])\n",
    "\n",
    "    return samples\n",
    "\n",
    "def get_distr(target_df, aux_df):\n",
    "    '''this function takes the target distribution and multiplies it by constant. aux_new_count denotes the target's scaled distribution that needs to be applies on aux_df'''\n",
    "    distr_combined = dist_scaling(target_df, aux_df)\n",
    "    distr_aux_new = distr_combined[['aux_new_count']]\n",
    "    distr_aux_new = distr_aux_new.rename(columns={\"aux_new_count\": \"nostoextract\"}).reset_index()\n",
    "    return distr_aux_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11430\n",
      "before\n",
      "label\n",
      "background      711\n",
      "cause           563\n",
      "condition       265\n",
      "contrast        875\n",
      "elaboration    6454\n",
      "evaluation      459\n",
      "joint          1749\n",
      "means           183\n",
      "summary         171\n",
      "Name: dir, dtype: int64\n",
      "after\n",
      "label\n",
      "background      701\n",
      "cause           493\n",
      "condition       265\n",
      "contrast        203\n",
      "elaboration     889\n",
      "evaluation      455\n",
      "joint          1076\n",
      "means            43\n",
      "summary          32\n",
      "Name: dir, dtype: int64\n",
      "4157\n"
     ]
    }
   ],
   "source": [
    "def process_labels_and_merge_evaluation_for_german(df):\n",
    "    for delete_label in ['attribution', 'comparison', 'explanation', 'enablement', 'temporal', 'textual-organization', 'topic-change', 'topic-comment']:\n",
    "        df = df[df.label != delete_label]\n",
    "    df['label'] = df['label'].str.replace(\"manner-means\", \"means\")\n",
    "    df['label'] = df['label'].str.replace(\"evaluation-n\", \"evaluation\")\n",
    "    df['label'] = df['label'].str.replace(\"evaluation-s\", \"evaluation\")\n",
    "    return df\n",
    "\n",
    "def balance_dataset(df, distribution_df):\n",
    "    freq = get_distr(distribution_df, df)\n",
    "    print('before')\n",
    "    print(df.groupby('label').count()['dir'])\n",
    "    df = bootstrap(df, freq)\n",
    "    print('after')\n",
    "    print(df.groupby('label').count()['dir'])\n",
    "    return df\n",
    "\n",
    "train_df_de = process_labels_and_merge_evaluation_for_german(train_df_de)\n",
    "train_df_en = process_labels_and_merge_evaluation_for_german(train_df_en)\n",
    "test_df = process_labels_and_merge_evaluation_for_german(test_df_de)\n",
    "val_df = process_labels_and_merge_evaluation_for_german(val_df_de)\n",
    "\n",
    "print(len(train_df_en))\n",
    "train_df_en = balance_dataset(train_df_en, train_df_de)\n",
    "print(len(train_df_en))\n",
    "\n",
    "train_df = pd.concat([train_df_de, train_df_en])\n",
    "train_df = train_df.sample(frac=1).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6324 6321 1581\n"
     ]
    }
   ],
   "source": [
    "def get_batches_per_epoch(train_df, batch_size=4):\n",
    "    size = len(train_df)\n",
    "    if size%batch_size!=0:\n",
    "        return int(size/batch_size)+1\n",
    "    else:\n",
    "        return int(size/batch_size)\n",
    "\n",
    "batches_per_epoch = get_batches_per_epoch(train_df, batch_size)\n",
    "print(batches_per_epoch*batch_size, len(train_df), batches_per_epoch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clean the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dropping any empty values\n",
    "train_df.dropna(inplace=True)\n",
    "val_df.dropna(inplace=True)\n",
    "test_df.dropna(inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare a dataset handler class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>unit1_txt</th>\n",
       "      <th>unit1_sent</th>\n",
       "      <th>unit2_txt</th>\n",
       "      <th>unit2_sent</th>\n",
       "      <th>dir</th>\n",
       "      <th>label</th>\n",
       "      <th>distance</th>\n",
       "      <th>u1_depdir</th>\n",
       "      <th>u2_depdir</th>\n",
       "      <th>u2_func</th>\n",
       "      <th>...</th>\n",
       "      <th>sat_children</th>\n",
       "      <th>nuc_children</th>\n",
       "      <th>genre</th>\n",
       "      <th>unit1_case</th>\n",
       "      <th>unit2_case</th>\n",
       "      <th>u1_discontinuous</th>\n",
       "      <th>u2_discontinuous</th>\n",
       "      <th>same_speaker</th>\n",
       "      <th>lex_overlap_length</th>\n",
       "      <th>u1_func</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>##oyoyd verkauft derzeit nur Auto-Versicherung...</td>\n",
       "      <td>In den USA gibt es eine Vielzahl von Versicher...</td>\n",
       "      <td>der Vorschlag &gt; * &gt; bleibt im bürokratischen T...</td>\n",
       "      <td>der Vorschlag, direkt an die Öffentlichkeit zu...</td>\n",
       "      <td>1&lt;2</td>\n",
       "      <td>cause</td>\n",
       "      <td>4</td>\n",
       "      <td>ROOT</td>\n",
       "      <td>ROOT</td>\n",
       "      <td>root</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>news</td>\n",
       "      <td>cap_initial</td>\n",
       "      <td>cap_initial</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>0</td>\n",
       "      <td>root</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Im Gegensatz zu anderen Aktien mit hohen Preis...</td>\n",
       "      <td>In den USA wird der Aktienhandel seit dem 1. J...</td>\n",
       "      <td>weil diese Themen kaum von Investment-Trust-Fo...</td>\n",
       "      <td>In den USA wird der Aktienhandel seit dem 1. J...</td>\n",
       "      <td>1&lt;2</td>\n",
       "      <td>cause</td>\n",
       "      <td>1</td>\n",
       "      <td>RIGHT</td>\n",
       "      <td>LEFT</td>\n",
       "      <td>advcl</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>news</td>\n",
       "      <td>cap_initial</td>\n",
       "      <td>other</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>0</td>\n",
       "      <td>ccomp</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Doch nicht nur wegen solcher Vergesslichkeit w...</td>\n",
       "      <td>Doch nicht nur wegen solcher Vergesslichkeit w...</td>\n",
       "      <td>Häufig werden sie über all dem Feiertagstrubel...</td>\n",
       "      <td>Häufig werden sie über all dem Feiertagstrubel...</td>\n",
       "      <td>1&lt;2</td>\n",
       "      <td>joint</td>\n",
       "      <td>1</td>\n",
       "      <td>ROOT</td>\n",
       "      <td>ROOT</td>\n",
       "      <td>root</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>news</td>\n",
       "      <td>cap_initial</td>\n",
       "      <td>cap_initial</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>0</td>\n",
       "      <td>root</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>auch die Broker-Händler-Tochter, integrated re...</td>\n",
       "      <td>, auch die Broker-Händler-Tochter, integrated ...</td>\n",
       "      <td>Die Vertreter der Einheit verlangten eine Name...</td>\n",
       "      <td>, auch die Broker-Händler-Tochter, integrated ...</td>\n",
       "      <td>1&lt;2</td>\n",
       "      <td>background</td>\n",
       "      <td>2</td>\n",
       "      <td>ROOT</td>\n",
       "      <td>LEFT</td>\n",
       "      <td>parataxis</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>news</td>\n",
       "      <td>cap_initial</td>\n",
       "      <td>other</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>0</td>\n",
       "      <td>root</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>tokyo Aktien geschlossen firmer monday,</td>\n",
       "      <td>Die japanischen Aktien schlossen am Montag ihr...</td>\n",
       "      <td>, mit dem Nikkei-Index seinen fünften Tagesgew...</td>\n",
       "      <td>Die japanischen Aktien schlossen am Montag ihr...</td>\n",
       "      <td>1&lt;2</td>\n",
       "      <td>background</td>\n",
       "      <td>1</td>\n",
       "      <td>ROOT</td>\n",
       "      <td>LEFT</td>\n",
       "      <td>advcl</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>news</td>\n",
       "      <td>cap_initial</td>\n",
       "      <td>other</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>0</td>\n",
       "      <td>root</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 22 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                           unit1_txt  \\\n",
       "0  ##oyoyd verkauft derzeit nur Auto-Versicherung...   \n",
       "1  Im Gegensatz zu anderen Aktien mit hohen Preis...   \n",
       "2  Doch nicht nur wegen solcher Vergesslichkeit w...   \n",
       "3  auch die Broker-Händler-Tochter, integrated re...   \n",
       "4            tokyo Aktien geschlossen firmer monday,   \n",
       "\n",
       "                                          unit1_sent  \\\n",
       "0  In den USA gibt es eine Vielzahl von Versicher...   \n",
       "1  In den USA wird der Aktienhandel seit dem 1. J...   \n",
       "2  Doch nicht nur wegen solcher Vergesslichkeit w...   \n",
       "3  , auch die Broker-Händler-Tochter, integrated ...   \n",
       "4  Die japanischen Aktien schlossen am Montag ihr...   \n",
       "\n",
       "                                           unit2_txt  \\\n",
       "0  der Vorschlag > * > bleibt im bürokratischen T...   \n",
       "1  weil diese Themen kaum von Investment-Trust-Fo...   \n",
       "2  Häufig werden sie über all dem Feiertagstrubel...   \n",
       "3  Die Vertreter der Einheit verlangten eine Name...   \n",
       "4  , mit dem Nikkei-Index seinen fünften Tagesgew...   \n",
       "\n",
       "                                          unit2_sent  dir       label  \\\n",
       "0  der Vorschlag, direkt an die Öffentlichkeit zu...  1<2       cause   \n",
       "1  In den USA wird der Aktienhandel seit dem 1. J...  1<2       cause   \n",
       "2  Häufig werden sie über all dem Feiertagstrubel...  1<2       joint   \n",
       "3  , auch die Broker-Händler-Tochter, integrated ...  1<2  background   \n",
       "4  Die japanischen Aktien schlossen am Montag ihr...  1<2  background   \n",
       "\n",
       "  distance u1_depdir u2_depdir    u2_func  ... sat_children nuc_children  \\\n",
       "0        4      ROOT      ROOT       root  ...            1            3   \n",
       "1        1     RIGHT      LEFT      advcl  ...            0            3   \n",
       "2        1      ROOT      ROOT       root  ...            0            2   \n",
       "3        2      ROOT      LEFT  parataxis  ...            1            2   \n",
       "4        1      ROOT      LEFT      advcl  ...            0            4   \n",
       "\n",
       "  genre   unit1_case   unit2_case u1_discontinuous u2_discontinuous  \\\n",
       "0  news  cap_initial  cap_initial            False             True   \n",
       "1  news  cap_initial        other            False            False   \n",
       "2  news  cap_initial  cap_initial            False            False   \n",
       "3  news  cap_initial        other            False            False   \n",
       "4  news  cap_initial        other            False            False   \n",
       "\n",
       "  same_speaker lex_overlap_length u1_func  \n",
       "0         True                  0    root  \n",
       "1         True                  0   ccomp  \n",
       "2         True                  0    root  \n",
       "3         True                  0    root  \n",
       "4         True                  0    root  \n",
       "\n",
       "[5 rows x 22 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['unit1_txt', 'unit1_sent', 'unit2_txt', 'unit2_sent', 'dir', 'label',\n",
       "       'distance', 'u1_depdir', 'u2_depdir', 'u2_func', 'u1_position',\n",
       "       'u2_position', 'sat_children', 'nuc_children', 'genre', 'unit1_case',\n",
       "       'unit2_case', 'u1_discontinuous', 'u2_discontinuous', 'same_speaker',\n",
       "       'lex_overlap_length', 'u1_func'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# feats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-02-08 13:10:25.626917: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-02-08 13:10:25.827721: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/cuda/lib64:/usr/local/cuda-10.1/lib64:/usr/local/cuda/extras/CUPTI/lib64:/usr/local/cuda-10.2/targets/x86_64-linux/lib/:/usr/local/cuda/extras/CUPTI/lib64:/usr/local/cuda/lib64:/usr/local/nvidia/lib:/usr/local/nvidia/lib64\n",
      "2023-02-08 13:10:25.827747: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n",
      "2023-02-08 13:10:25.864050: E tensorflow/stream_executor/cuda/cuda_blas.cc:2981] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2023-02-08 13:10:26.773969: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/cuda/lib64:/usr/local/cuda-10.1/lib64:/usr/local/cuda/extras/CUPTI/lib64:/usr/local/cuda-10.2/targets/x86_64-linux/lib/:/usr/local/cuda/extras/CUPTI/lib64:/usr/local/cuda/lib64:/usr/local/nvidia/lib:/usr/local/nvidia/lib64\n",
      "2023-02-08 13:10:26.774053: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/cuda/lib64:/usr/local/cuda-10.1/lib64:/usr/local/cuda/extras/CUPTI/lib64:/usr/local/cuda-10.2/targets/x86_64-linux/lib/:/usr/local/cuda/extras/CUPTI/lib64:/usr/local/cuda/lib64:/usr/local/nvidia/lib:/usr/local/nvidia/lib64\n",
      "2023-02-08 13:10:26.774061: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n"
     ]
    }
   ],
   "source": [
    "from multiprocessing.sharedctypes import Value\n",
    "import torch\n",
    "from torch.utils.data import Dataset, TensorDataset, DataLoader, ConcatDataset\n",
    "from sys import path\n",
    "path.append('/home/VD/kaveri/anaconda3/envs/py310/lib/python3.10/site-packages/allennlp/data/data_loaders/')\n",
    "from allennlp.data import allennlp_collate, Vocabulary\n",
    "from features_custom_original import get_vocab_feature_name\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from transformers import BertTokenizer, BertTokenizer\n",
    "\n",
    "class MNLIDataBert(Dataset):\n",
    "\n",
    "  def __init__(self, train_df, val_df, test_df):\n",
    "    self.lang = lang\n",
    "    self.num_labels = set()\n",
    "    self.train_df = train_df\n",
    "    self.val_df = val_df\n",
    "    self.test_df = test_df\n",
    "\n",
    "    self.tokenizer = BertTokenizer.from_pretrained(BERT_MODEL, do_lower_case=True) # Using a pre-trained BERT tokenizer to encode sentences\n",
    "    self.train_data = None\n",
    "    self.val_data = None\n",
    "    self.test_data = None\n",
    "    self.train_idx = None\n",
    "    self.val_idx = None\n",
    "    self.test_idx = None\n",
    "    self.vocab = Vocabulary(counter=None, max_vocab_size=100000)\n",
    "    self.init_data()\n",
    "\n",
    "  def init_data(self):\n",
    "    self.get_label_mapping()\n",
    "    self.init_feature_list()\n",
    "    self.init_feature_mappings_and_bins()\n",
    "    self.apply_bins()\n",
    "    self.calculate_unique_values()\n",
    "    self.train_data, self.train_idx = self.load_data(self.train_df)\n",
    "    self.val_data, self.val_idx = self.load_data(self.val_df)\n",
    "    self.test_data, self.test_idx = self.load_data(self.test_df)\n",
    "    \n",
    "\n",
    "  def combine_unique_column_values_to_dict(self, column_name):\n",
    "    ini_set = set([*self.train_df[column_name].unique(), *self.val_df[column_name].unique()])\n",
    "    res = dict.fromkeys(ini_set, 0)\n",
    "    return res\n",
    "\n",
    "  def get_label_mapping(self):\n",
    "    labels = {}\n",
    "    labels_list = list(set(list(self.train_df['label'].unique()) + list(self.test_df['label'].unique()) + list(self.val_df['label'].unique())))\n",
    "    for i in range(len(labels_list)):\n",
    "        labels[labels_list[i]] = i\n",
    "    self.label_dict = labels\n",
    "    # needed later for classification report object to generate precision and recall on test dataset\n",
    "    self.rev_label_dict = {self.label_dict[k]:k for k in self.label_dict.keys()} \n",
    "\n",
    "  def init_feature_mappings_and_bins(self):\n",
    "    self.feature_maps = { 'genre': self.combine_unique_column_values_to_dict('genre'),\n",
    "                          'unit1_case': self.combine_unique_column_values_to_dict('unit1_case'),\n",
    "                          'unit2_case': self.combine_unique_column_values_to_dict('unit2_case'),\n",
    "                          'u1_func': self.combine_unique_column_values_to_dict('u1_func'),\n",
    "                          'u2_func': self.combine_unique_column_values_to_dict('u2_func') }\n",
    "\n",
    "    self.bins = {\n",
    "      'distance': [[-1e9, -8], [-8, -2], [-2, 0], [0, 2], [2, 8], [8, 1e9]],\n",
    "      'u1_position': [[0.0, 0.1], [0.1, 0.2], [0.2, 0.3], [0.3, 0.4], [0.4, 0.5], [0.5, 0.6], [0.6, 0.7], [0.7, 0.8], [0.8, 0.9], [0.9, 1.0], [1.0, 1e9]],\n",
    "      'u2_position': [[0.0, 0.1], [0.1, 0.2], [0.2, 0.3], [0.3, 0.4], [0.4, 0.5], [0.5, 0.6], [0.6, 0.7], [0.7, 0.8], [0.8, 0.9], [0.9, 1.0], [1.0, 1e9]],\n",
    "      'lex_overlap_length': [[0, 2], [2, 7], [7, 1e9]]\n",
    "    }   \n",
    "\n",
    "  def add_directionality(self, premise, hypothesis, dir):\n",
    "    if dir == \"1<2\":\n",
    "        hypothesis = '< ' + hypothesis + ' {'\n",
    "    else:\n",
    "        premise = '} ' + premise + ' >'\n",
    "    return premise, hypothesis\n",
    "\n",
    "  def init_feature_list(self):\n",
    "    if self.lang=='nld':\n",
    "      self.feature_list = ['distance', 'u1_depdir', 'sat_children', 'genre', 'u1_position']\n",
    "    elif self.lang=='deu':\n",
    "      self.feature_list = ['distance', 'u1_depdir', 'u2_depdir', 'u2_func', 'u1_position', 'u2_position', 'sat_children', 'nuc_children']\n",
    "    elif self.lang=='eng.rst.gum':\n",
    "      self.feature_list = ['distance', 'same_speaker', 'u2_func', 'u2_depdir', 'unit1_case', 'unit2_case', 'nuc_children',\n",
    "                      'sat_children', 'genre', 'lex_overlap_length', 'u2_discontinuous', 'u1_discontinuous', 'u1_position', 'u2_position']\n",
    "    elif self.lang=='fas':\n",
    "      self.feature_list = ['distance', 'nuc_children', 'sat_children', 'u2_discontinuous', 'genre']\n",
    "    elif self.lang=='spa.rst.sctb':\n",
    "      self.feature_list = ['distance', 'u1_position', 'sat_children']\n",
    "    elif self.lang=='zho.rst.sctb':\n",
    "      self.feature_list = ['sat_children', 'nuc_children', 'genre', 'u2_discontinuous', 'u1_discontinuous', 'u1_depdir', 'u1_func']\n",
    "    else: \n",
    "      raise ValueError()\n",
    "\n",
    "  def get_mapping_from_dictionary(self, column_name, dict_val):\n",
    "    return self.feature_maps[column_name][dict_val]\n",
    "\n",
    "  def get_allen_features_list(self, features, feature_name):\n",
    "    if feature_name in ['distance', 'u1_depdir', 'u2_depdir', 'u1_func', 'u2_func', \n",
    "    'u1_position', 'u2_position', 'genre', 'same_speaker', 'unit1_case', 'unit2_case',\n",
    "    'lex_overlap_length', 'u2_discontinuous', 'u1_discontinuous', 'dir']: feature_value = self.apply_vocab(features[feature_name], feature_name) #for categorical values\n",
    "    elif feature_name in ['sat_children', 'nuc_children']: feature_value = float(features[feature_name]) #for identiy values\n",
    "    else: \n",
    "      print(feature_name)\n",
    "      raise ValueError()\n",
    "    return feature_value\n",
    "\n",
    "  def transform_feature(self, features):\n",
    "    assert len(features)==17\n",
    "    #after applying the vocab. we need to pass them as int\n",
    "    return {feature_name: torch.tensor(int(self.get_allen_features_list(features, feature_name))).to(device) for feature_name in self.feature_list+['dir']}\n",
    "\n",
    "  def calculate_unique_values(self):\n",
    "    for feature_name in self.feature_list+['dir']:\n",
    "      vocab_feature_name = get_vocab_feature_name(feature_name)\n",
    "      self.vocab.add_tokens_to_namespace(train_df[feature_name].apply(lambda x: str(x)), namespace=vocab_feature_name)\n",
    "      self.vocab.add_tokens_to_namespace(val_df[feature_name].apply(lambda x: str(x)), namespace=vocab_feature_name)\n",
    "\n",
    "  def apply_bins(self):\n",
    "    for df in [self.train_df, self.test_df, self.val_df]:\n",
    "      for feature_name in self.bins.keys():\n",
    "        if feature_name=='u2_func':\n",
    "          print(df[feature_name].unique())\n",
    "          raise ValueError()\n",
    "        df[feature_name] = df[feature_name].apply(lambda x: self.get_mapping_from_bin(feature_name, float(x)))\n",
    "\n",
    "  def get_mapping_from_bin(self, column_name, dict_val):\n",
    "    bins = self.bins[column_name]\n",
    "    for b,i in zip(bins, range(len(bins))):\n",
    "      left = b[0]\n",
    "      right = b[1]\n",
    "      if left<=dict_val and right>=dict_val: return i\n",
    "\n",
    "  def apply_vocab(self, feature_value, feature_name):\n",
    "    return self.vocab.get_token_index(str(feature_value), namespace=get_vocab_feature_name(feature_name))\n",
    "\n",
    "  def set_labels(self):\n",
    "    self.num_labels = len(self.num_labels)\n",
    "    \n",
    "  def load_data(self, df):\n",
    "    MAX_LEN = 512 \n",
    "    token_ids = []\n",
    "    mask_ids = []\n",
    "    # seg_ids = []\n",
    "    y = []\n",
    "    feats = []\n",
    "    idx = []\n",
    "    idx_map = {}\n",
    "\n",
    "    self.num_labels.update(df['label'].unique())\n",
    "\n",
    "    count=0\n",
    "    for row in df.iterrows():\n",
    "      row = row[1]\n",
    "      premise = row['unit1_txt']\n",
    "      hypothesis = row['unit2_txt']\n",
    "      label = row['label']\n",
    "      dir = row['dir']\n",
    "\n",
    "      features = {'distance': row['distance'],\n",
    "                'u1_depdir': row['u1_depdir'],\n",
    "                'u2_depdir': row['u2_depdir'],\n",
    "                'u1_func': row['u1_func'],\n",
    "                'u2_func': row['u2_func'],\n",
    "                'u1_position': row['u1_position'],\n",
    "                'u2_position': row['u2_position'],\n",
    "                'sat_children': row['sat_children'],\n",
    "                'nuc_children': row['nuc_children'],\n",
    "                'genre': row['genre'],\n",
    "                'unit1_case': row['unit1_case'],\n",
    "                'unit2_case': row['unit2_case'],\n",
    "                'u1_discontinuous': row['u1_discontinuous'],\n",
    "                'u2_discontinuous': row['u2_discontinuous'],\n",
    "                'same_speaker': row['same_speaker'],\n",
    "                'lex_overlap_length': row['lex_overlap_length'],\n",
    "                'dir': row['dir']}\n",
    "\n",
    "      premise, hypothesis = self.add_directionality(premise, hypothesis, dir)\n",
    "      encoded = self.tokenizer.encode_plus(premise, hypothesis, add_special_tokens = True, max_length=MAX_LEN, truncation=True, padding=False) #padding='max_length'\n",
    "      pair_token_ids = torch.tensor(encoded['input_ids'])\n",
    "\n",
    "      # segment_ids = torch.tensor(encoded['token_type_ids'])\n",
    "      attention_mask_ids = torch.tensor(encoded['attention_mask'])\n",
    "      assert len(pair_token_ids)==len(attention_mask_ids)\n",
    "\n",
    "      features = self.transform_feature(features)\n",
    "\n",
    "      token_ids.append(pair_token_ids)\n",
    "      # seg_ids.append(segment_ids)\n",
    "      mask_ids.append(attention_mask_ids)\n",
    "      y.append(self.label_dict[label])\n",
    "      feats.append(features)\n",
    "      \n",
    "      idx_map[count] = [premise, hypothesis]\n",
    "      idx.append(count)\n",
    "      count+=1\n",
    "      \n",
    "    token_ids = pad_sequence(token_ids, batch_first=True)\n",
    "    mask_ids = pad_sequence(mask_ids, batch_first=True)\n",
    "    # seg_ids = pad_sequence(seg_ids, batch_first=True)\n",
    "    y = torch.tensor(y)\n",
    "    idx = torch.tensor(idx)\n",
    "\n",
    "    class featureDataset(Dataset):\n",
    "      def __init__(self, token_ids, mask_ids, feats, y, idx):\n",
    "          self.token_ids = token_ids\n",
    "          self.mask_ids = mask_ids\n",
    "          # self.seg_ids = seg_ids\n",
    "          self.feats = feats\n",
    "          self.y = y\n",
    "          self.idx = idx\n",
    "\n",
    "      def __len__(self):\n",
    "          return len(self.feats)\n",
    "\n",
    "      def __getitem__(self, idx):\n",
    "          return self.token_ids[idx], self.mask_ids[idx], self.feats[idx], self.y[idx], self.idx[idx]\n",
    "          # return self.token_ids[idx], self.mask_ids[idx], self.seg_ids[idx], self.feats[idx], self.y[idx], self.idx[idx]\n",
    "\n",
    "    # dataset = featureDataset(token_ids, mask_ids, seg_ids, feats, y, idx)\n",
    "    dataset = featureDataset(token_ids, mask_ids, feats, y, idx)\n",
    "    return dataset, idx_map\n",
    "\n",
    "  def get_data_loaders(self, batch_size=4, batches_per_epoch=402, shuffle=True): #1609 samples / 64:25=1600 / 402:4=1608\n",
    "    self.set_labels()\n",
    "    train_loader_torch = DataLoader(\n",
    "      self.train_data,\n",
    "      shuffle=shuffle,\n",
    "      batch_size=batch_size,\n",
    "    )\n",
    "\n",
    "    val_loader_torch = DataLoader(\n",
    "      self.val_data,\n",
    "      shuffle=shuffle,\n",
    "      batch_size=batch_size,\n",
    "    )\n",
    "\n",
    "    test_loader_torch = DataLoader(\n",
    "      self.test_data,\n",
    "      shuffle=False,\n",
    "      batch_size=batch_size,\n",
    "    )\n",
    "    \n",
    "    train_loader = LoaderWrapper(train_loader_torch, n_step=batches_per_epoch)\n",
    "    val_loader = LoaderWrapper(val_loader_torch, n_step=batches_per_epoch)\n",
    "    test_loader = LoaderWrapper(test_loader_torch, n_step=batches_per_epoch)\n",
    "\n",
    "    return train_loader, val_loader_torch, test_loader_torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LoaderWrapper:\n",
    "    def __init__(self, loader, n_step):\n",
    "        self.step = n_step\n",
    "        self.idx = 0\n",
    "        self.iter_loader = iter(loader)\n",
    "        self.loader = loader\n",
    "    \n",
    "    def __iter__(self):\n",
    "        return self\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.step\n",
    "\n",
    "    def __next__(self):\n",
    "        # if reached number of steps desired, stop\n",
    "        if self.idx == self.step:\n",
    "            self.idx = 0\n",
    "            raise StopIteration\n",
    "        else:\n",
    "            self.idx += 1\n",
    "        # while True\n",
    "        try:\n",
    "            return next(self.iter_loader)\n",
    "        except StopIteration:\n",
    "            # reinstate iter_loader, then continue\n",
    "            self.iter_loader = iter(self.loader)\n",
    "            return next(self.iter_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mnliloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "mnli_dataset = MNLIDataBert(train_df, val_df, test_df)\n",
    "mnli_dataset_en = MNLIDataBert(train_df_en, val_df_en, test_df_en) #to apply bins because df passed by reference\n",
    "\n",
    "train_loader, val_loader, test_loader = mnli_dataset.get_data_loaders(batch_size=batch_size, batches_per_epoch=batches_per_epoch) #64X250\n",
    "label_dict = mnli_dataset.label_dict # required by custom func to calculate accuracy, bert model\n",
    "rev_label_dict = mnli_dataset.rev_label_dict # required by custom func to calculate accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "distance :  {'@@PADDING@@': 0, '@@UNKNOWN@@': 1, '4': 2, '3': 3, '5': 4}\n",
      "distance :  {'@@PADDING@@': 0, '@@UNKNOWN@@': 1, '4': 2, '3': 3, '5': 4}\n",
      "u1_depdir :  {'@@PADDING@@': 0, '@@UNKNOWN@@': 1, 'ROOT': 2, 'RIGHT': 3, 'LEFT': 4}\n",
      "u1_depdir :  {'@@PADDING@@': 0, '@@UNKNOWN@@': 1, 'ROOT': 2, 'RIGHT': 3, 'LEFT': 4}\n",
      "u2_depdir :  {'@@PADDING@@': 0, '@@UNKNOWN@@': 1, 'ROOT': 2, 'RIGHT': 3, 'LEFT': 4}\n",
      "u2_depdir :  {'@@PADDING@@': 0, '@@UNKNOWN@@': 1, 'ROOT': 2, 'RIGHT': 3, 'LEFT': 4}\n",
      "u2_func :  {'@@PADDING@@': 0, '@@UNKNOWN@@': 1, 'root': 2, 'advcl': 3, 'parataxis': 4, 'punct': 5, 'ccomp': 6, 'acl:relcl': 7, 'conj': 8, 'advmod': 9, 'acl': 10, 'dep': 11, 'appos': 12, 'obl': 13, 'flat': 14, 'obj': 15, 'nsubj': 16, 'compound': 17, 'aux': 18, 'xcomp': 19, 'nmod': 20, 'nsubj:pass': 21, 'nummod': 22, 'det': 23, 'cc': 24, 'amod': 25, 'obl:tmod': 26, 'csubj': 27, 'list': 28, 'orphan': 29, 'obl:npmod': 30, 'csubj:pass': 31, 'case': 32, 'iobj': 33, 'nmod:npmod': 34, 'discourse': 35}\n",
      "u2_func :  {'@@PADDING@@': 0, '@@UNKNOWN@@': 1, 'root': 2, 'advcl': 3, 'parataxis': 4, 'punct': 5, 'ccomp': 6, 'acl:relcl': 7, 'conj': 8, 'advmod': 9, 'acl': 10, 'dep': 11, 'appos': 12, 'obl': 13, 'flat': 14, 'obj': 15, 'nsubj': 16, 'compound': 17, 'aux': 18, 'xcomp': 19, 'nmod': 20, 'nsubj:pass': 21, 'nummod': 22, 'det': 23, 'cc': 24, 'amod': 25, 'obl:tmod': 26, 'csubj': 27, 'list': 28, 'orphan': 29, 'obl:npmod': 30, 'csubj:pass': 31, 'case': 32, 'iobj': 33, 'nmod:npmod': 34, 'discourse': 35, 'nmod:poss': 36}\n",
      "u1_position :  {'@@PADDING@@': 0, '@@UNKNOWN@@': 1, '9': 2, '5': 3, '2': 4, '0': 5, '4': 6, '3': 7, '7': 8, '8': 9, '6': 10, '1': 11}\n",
      "u1_position :  {'@@PADDING@@': 0, '@@UNKNOWN@@': 1, '9': 2, '5': 3, '2': 4, '0': 5, '4': 6, '3': 7, '7': 8, '8': 9, '6': 10, '1': 11}\n",
      "u2_position :  {'@@PADDING@@': 0, '@@UNKNOWN@@': 1, '9': 2, '6': 3, '3': 4, '2': 5, '0': 6, '4': 7, '5': 8, '1': 9, '8': 10, '7': 11}\n",
      "u2_position :  {'@@PADDING@@': 0, '@@UNKNOWN@@': 1, '9': 2, '6': 3, '3': 4, '2': 5, '0': 6, '4': 7, '5': 8, '1': 9, '8': 10, '7': 11}\n",
      "sat_children :  {'@@PADDING@@': 0, '@@UNKNOWN@@': 1, '1': 2, '0': 3, '2': 4, '3': 5, '5': 6, '7': 7, '4': 8, '10': 9, '6': 10, '8': 11, '11': 12}\n",
      "sat_children :  {'@@PADDING@@': 0, '@@UNKNOWN@@': 1, '1': 2, '0': 3, '2': 4, '3': 5, '5': 6, '7': 7, '4': 8, '10': 9, '6': 10, '8': 11, '11': 12, '14': 13}\n",
      "nuc_children :  {'@@PADDING@@': 0, '@@UNKNOWN@@': 1, '3': 2, '2': 3, '4': 4, '1': 5, '5': 6, '6': 7, '8': 8, '7': 9, '9': 10, '10': 11, '16': 12, '12': 13, '11': 14, '15': 15, '21': 16, '13': 17}\n",
      "nuc_children :  {'@@PADDING@@': 0, '@@UNKNOWN@@': 1, '3': 2, '2': 3, '4': 4, '1': 5, '5': 6, '6': 7, '8': 8, '7': 9, '9': 10, '10': 11, '16': 12, '12': 13, '11': 14, '15': 15, '21': 16, '13': 17, '14': 18}\n",
      "dir :  {'@@PADDING@@': 0, '@@UNKNOWN@@': 1, '1<2': 2, '1>2': 3}\n"
     ]
    }
   ],
   "source": [
    "for feature in mnli_dataset.feature_list:\n",
    "    vocab_feature_name = get_vocab_feature_name(feature)\n",
    "    print(feature, ': ', mnli_dataset.vocab.get_token_to_index_vocabulary(vocab_feature_name))\n",
    "    mnli_dataset.vocab.add_tokens_to_namespace(val_df_en[feature].apply(lambda x: str(x)), namespace=vocab_feature_name)\n",
    "    print(feature, ': ', mnli_dataset.vocab.get_token_to_index_vocabulary(vocab_feature_name))\n",
    "print('dir', ': ', mnli_dataset.vocab.get_token_to_index_vocabulary('dir'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "for batch_idx, (pair_token_ids, mask_ids, feat, y, idx) in enumerate(train_loader):\n",
    "    assert len(feat)==len(mnli_dataset.feature_list)+1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## load pretrained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.3333333333333333\n",
      "1.0\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertForSequenceClassification, AdamW\n",
    "from torch import optim\n",
    "import os\n",
    "path.append(os.path.join(os.getcwd(), '../utils/'))\n",
    "from CategoricalAccuracy import CategoricalAccuracy as CA\n",
    "import numpy as np\n",
    "\n",
    "ca = CA()\n",
    "\n",
    "x = torch.tensor(np.array([[[1,0,0], [1,0,0], [1,0,0]]]))\n",
    "y1 = torch.tensor(np.array([[0], [1], [1]]))\n",
    "y2 = torch.tensor(np.array([[0], [0], [0]]))\n",
    "\n",
    "ca(x,y1)\n",
    "print(ca.get_metric(reset=True))\n",
    "ca(x,y2)\n",
    "print(ca.get_metric(reset=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## define evaulation metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to evaluate model for train and test. And also use classification report for testing\n",
    "import torch.nn as nn\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# helper function to calculate the batch accuracy\n",
    "def multi_acc(y_pred, y_test, allennlp=False):\n",
    "  if allennlp==False:\n",
    "    acc = (torch.log_softmax(y_pred, dim=1).argmax(dim=1) == y_test).sum().float() / float(y_test.size(0))\n",
    "    return acc\n",
    "\n",
    "# freeze model weights and measure validation / test \n",
    "def evaluate_accuracy(model, optimizer, data_loader, rev_label_dict, label_dict, is_training=True):\n",
    "  model.eval()\n",
    "  total_val_acc  = 0\n",
    "  total_val_loss = 0\n",
    "  \n",
    "  #for classification report\n",
    "  y_true = []\n",
    "  y_pred = []\n",
    "  idx_list = []\n",
    "  premise_list = []\n",
    "  hypo_list = []\n",
    "  idx_map = mnli_dataset.val_idx if is_training else mnli_dataset.test_idx\n",
    "\n",
    "  with torch.no_grad():\n",
    "    for batch_idx, (pair_token_ids, mask_ids, feat, y, idx) in enumerate(data_loader):      \n",
    "      optimizer.zero_grad()\n",
    "      pair_token_ids = pair_token_ids.to(device)\n",
    "      mask_ids = mask_ids.to(device)\n",
    "      # seg_ids = seg_ids.to(device)\n",
    "      labels = y.to(device)\n",
    "      # feat = feat.to(device)\n",
    "      \n",
    "      outputs = model(pair_token_ids, \n",
    "                            # token_type_ids=seg_ids, \n",
    "                            attention_mask=mask_ids, \n",
    "                            feat=feat)\n",
    "      criterion = nn.CrossEntropyLoss()\n",
    "      loss = criterion(outputs, labels)\n",
    "      acc = multi_acc(outputs, labels)\n",
    "\n",
    "      total_val_loss += loss.item()\n",
    "      total_val_acc  += acc.item()\n",
    "\n",
    "      # log predictions for classification report\n",
    "      argmax_predictions = torch.argmax(outputs,dim=1).tolist()\n",
    "      labels_list = labels.tolist()\n",
    "      assert(len(labels_list)==len(argmax_predictions))\n",
    "      for p in argmax_predictions: y_pred.append(rev_label_dict[int(p)])\n",
    "      for l in labels_list: y_true.append(rev_label_dict[l])\n",
    "      for i in idx.tolist():\n",
    "        idx_list.append(i)\n",
    "        premise_list.append(idx_map[i][0])\n",
    "        hypo_list.append(idx_map[i][1])\n",
    "\n",
    "  val_acc  = total_val_acc/len(data_loader)\n",
    "  val_loss = total_val_loss/len(data_loader)\n",
    "  cr = classification_report(y_true, y_pred)\n",
    "\n",
    "  idx_json = {'idx': idx_list, 'gold_label': y_true, 'pred_label': y_pred, 'premise': premise_list, 'hypothesis': hypo_list}\n",
    "  \n",
    "  return val_acc, val_loss, cr, model, optimizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## define custom bert model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ASSIGN: 25\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-german-cased were not used when initializing FeaturefulBert: ['cls.predictions.bias', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight']\n",
      "- This IS expected if you are initializing FeaturefulBert from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing FeaturefulBert from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of the model checkpoint at bert-base-german-cased were not used when initializing BertModel: ['cls.predictions.bias', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ModuleDict(\n",
      "  (distance): Embedding(5, 3, padding_idx=0)\n",
      "  (u1_depdir): Embedding(5, 3, padding_idx=0)\n",
      "  (u2_depdir): Embedding(5, 3, padding_idx=0)\n",
      "  (u2_func): Embedding(37, 7, padding_idx=0)\n",
      "  (u1_position): Embedding(12, 4, padding_idx=0)\n",
      "  (u2_position): Embedding(12, 4, padding_idx=0)\n",
      "  (sat_children): Identity()\n",
      "  (nuc_children): Identity()\n",
      ")\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/VD/kaveri/anaconda3/envs/py310/lib/python3.10/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import copy\n",
    "from typing import Any, Dict, Optional\n",
    "from transformers import BertModel, BertConfig\n",
    "import torch.nn as nn\n",
    "from tensorflow.keras.layers import TimeDistributed\n",
    "from featurefulbertembedder_custom_allennlp_dims import FeaturefulBertEmbedder\n",
    "from featureful_bert_custom_allennlp_dims import get_combined_feature_tensor_2 as get_combined_feature_tensor_forward\n",
    "from features_custom_allennlp_dims import get_feature_modules\n",
    "\n",
    "class CustomPooler2(nn.Module):\n",
    "    def __init__(self, *,\n",
    "                        requires_grad: bool = True,\n",
    "                        dropout: float = 0.0,\n",
    "                        transformer_kwargs: Optional[Dict[str, Any]] = None, ) -> None:\n",
    "        super().__init__()\n",
    "        bert = BertModel.from_pretrained(BERT_MODEL) #only used to pass config. BertAttentionClass used in FeatureFulBert\n",
    "        self._dropout = torch.nn.Dropout(p=dropout)\n",
    "        self.pooler = copy.deepcopy(bert.pooler)\n",
    "        for param in self.pooler.parameters():\n",
    "            param.requires_grad = requires_grad\n",
    "        self._embedding_dim = bert.config.hidden_size\n",
    "\n",
    "    def get_input_dim(self) -> int:\n",
    "        return self._embedding_dim\n",
    "\n",
    "    def get_output_dim(self) -> int:\n",
    "        return self._embedding_dim\n",
    "\n",
    "    def forward(self, tokens: torch.Tensor, mask: torch.BoolTensor = None, num_wrapping_dims: int = 0):\n",
    "        pooler = self.pooler\n",
    "        \n",
    "        for _ in range(num_wrapping_dims):\n",
    "            pooler = TimeDistributed(pooler)\n",
    "        pooled = pooler(tokens)\n",
    "        pooled = self._dropout(pooled)\n",
    "        return pooled\n",
    "\n",
    "class MyModule(nn.Module):    \n",
    "    def __init__(self, feature_list):\n",
    "        super(MyModule, self).__init__()\n",
    "        self.feature_list = feature_list\n",
    "        self.feature_modules = nn.ModuleDict()\n",
    "        self.dims = 0\n",
    "        self.feature_modules, dims = get_feature_modules(feature_list, mnli_dataset.vocab, use_allennlp_dims=True)\n",
    "        self.dims += dims\n",
    "\n",
    "        print(self.feature_modules)\n",
    "        for feature in feature_list:\n",
    "            if feature not in ['distance', 'u1_depdir', 'u2_depdir', 'u2_func', 'u1_position', 'u2_position', \n",
    "                                'nuc_children', 'sat_children']: raise ValueError()\n",
    "            # elif 'genre' in feature_list:               self.modules['genre'] = nn.Embedding(mnli_dataset_en.distance_unique, 3)\n",
    "            # elif 'unit1_case' in feature_list:          self.modules['unit1_case'] = nn.Embedding(mnli_dataset_en.distance_unique, 3)\n",
    "            # elif 'unit2_case' in feature_list:          self.modules['unit2_case'] = nn.Embedding(mnli_dataset_en.distance_unique, 3)\n",
    "            # elif 'u1_discontinuous' in feature_list:    self.modules['u1_discontinuous'] = nn.Embedding(mnli_dataset_en.distance_unique, 3)\n",
    "            # elif 'u2_discontinuous' in feature_list:    self.modules['u2_discontinuous'] = nn.Embedding(mnli_dataset_en.distance_unique, 3)\n",
    "            # elif 'same_speaker' in feature_list:        self.modules['same_speaker'] = nn.Embedding(mnli_dataset_en.distance_unique, 3)\n",
    "            # elif 'lex_overlap_length' in feature_list:  self.modules['lex_overlap_length'] = nn.Embedding(mnli_dataset_en.distance_unique, 3)\n",
    "            # elif 'u1_func' in feature_list:             self.modules['u1_func'] = nn.Embedding(mnli_dataset_en.distance_unique, 3)\n",
    "\n",
    "    def forward(self, features):\n",
    "        feature_list = ['distance', 'u1_depdir', 'u2_depdir', 'u2_func', 'u1_position', 'u2_position', 'sat_children', 'nuc_children']\n",
    "        for i in range(len(feature_list)):\n",
    "            feature_name = feature_list[i]\n",
    "            feature_modules = self.feature_modules[feature_name]\n",
    "            feature = features[...,i]\n",
    "            if feature_name not in ['sat_children', 'nuc_children']:\n",
    "                if torch.max(feature)>feature_modules.num_embeddings:\n",
    "                    print(feature_name, feature)\n",
    "                    raise ValueError()\n",
    "\n",
    "        return get_combined_feature_tensor_forward(features, self.feature_list, self.feature_modules)\n",
    "\n",
    "class CustomBERTModel(nn.Module):\n",
    "    def __init__(self, num_labels):\n",
    "          super(CustomBERTModel, self).__init__()\n",
    "          self.num_classes = num_labels\n",
    "          self.feature_list = mnli_dataset.feature_list\n",
    "          print('ASSIGN:', self.num_classes)\n",
    "\n",
    "          self.embedder = self.create_featureful_bert() #BERT MODEL\n",
    "          self.encoder = CustomPooler2()\n",
    "          self.module1 = MyModule(self.feature_list)\n",
    "          self.dropout1 = nn.Dropout(p=0.0)\n",
    "        #   self.dropout_decoder = nn.Dropout(p=0.5)\n",
    "          self.out_features = self.encoder.pooler.dense.out_features+self.module1.dims\n",
    "          self.relation_decoder = nn.Linear(self.out_features, self.num_classes)\n",
    "\n",
    "    def forward(self, pair_token_ids, attention_mask, feat):\n",
    "        direction_tensor = feat['dir'].to(device)\n",
    "        embedded_sentence = self.embedder(token_ids=pair_token_ids, #featurefulmebedder\n",
    "                        mask=attention_mask, \n",
    "                        # type_ids=token_type_ids,\n",
    "                        segment_concat_mask = None,\n",
    "                        direction_tensor = direction_tensor,\n",
    "                        feature_list = self.feature_list,\n",
    "                        features = feat)\n",
    "        # mask = token_type_ids\n",
    "        bertpooler_output = self.encoder(tokens=embedded_sentence, mask=None)\n",
    "        feat = self.convert_to_feature_list(feat)\n",
    "        feat = self.dropout1(feat)\n",
    "        feat = self.module1(feat)\n",
    "        try:\n",
    "            feat_concat = torch.concat((bertpooler_output, feat),-1)\n",
    "        except:\n",
    "            print(bertpooler_output.shape, feat.shape)\n",
    "            raise ValueError()\n",
    "        if feat_concat.shape[-1]!=self.module1.dims+bertpooler_output.shape[-1]: print(feat_concat.shape, self.module1.dims)\n",
    "        assert feat_concat.shape[-1] == self.module1.dims+bertpooler_output.shape[-1]\n",
    "        feat_concat = self.dropout1(feat_concat)\n",
    "        # feat_concat = self.dropout_decoder(feat_concat)\n",
    "        linear1_output = self.relation_decoder(feat_concat)\n",
    "        return linear1_output\n",
    "\n",
    "\n",
    "    def create_featureful_bert(self):\n",
    "        featureful_bert = FeaturefulBertEmbedder(model_name = BERT_MODEL,\n",
    "                                hidden_activation_allen = 'gelu',\n",
    "                                feature_list = self.feature_list, \n",
    "                                vocab=mnli_dataset.vocab,\n",
    "                                use_allen_dims=True)\n",
    "        return featureful_bert\n",
    "\n",
    "    def convert_to_feature_list(self, feat):\n",
    "        feature_linear = [feat[feature_name] for feature_name in self.feature_list]\n",
    "        feature_linear = torch.stack(feature_linear, dim=-1)\n",
    "        return feature_linear\n",
    "\n",
    "model = CustomBERTModel(mnli_dataset.num_labels)\n",
    "model.to(device)\n",
    "optimizer = AdamW(model.parameters(), lr=2e-6, correct_bias=False) # original 2e-5\n",
    "scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, factor=0.8, mode='max', patience=35, min_lr=5e-7, verbose=True) #original factor=0.6, min_lr=5e-7"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## define training regime"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### prinintg model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CustomBERTModel(\n",
      "  (embedder): FeaturefulBertEmbedder(\n",
      "    (transformer_model): FeaturefulBert(\n",
      "      (embeddings): BertEmbeddings(\n",
      "        (word_embeddings): Embedding(30000, 768, padding_idx=0)\n",
      "        (position_embeddings): Embedding(512, 768)\n",
      "        (token_type_embeddings): Embedding(2, 768)\n",
      "        (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "      (encoder): BertEncoder(\n",
      "        (layer): ModuleList(\n",
      "          (0): BertLayer(\n",
      "            (attention): BertAttention(\n",
      "              (self): BertSelfAttention(\n",
      "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "              (output): BertSelfOutput(\n",
      "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "            )\n",
      "            (intermediate): BertIntermediate(\n",
      "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "              (intermediate_act_fn): GELUActivation()\n",
      "            )\n",
      "            (output): BertOutput(\n",
      "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (1): BertLayer(\n",
      "            (attention): BertAttention(\n",
      "              (self): BertSelfAttention(\n",
      "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "              (output): BertSelfOutput(\n",
      "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "            )\n",
      "            (intermediate): BertIntermediate(\n",
      "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "              (intermediate_act_fn): GELUActivation()\n",
      "            )\n",
      "            (output): BertOutput(\n",
      "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (2): BertLayer(\n",
      "            (attention): BertAttention(\n",
      "              (self): BertSelfAttention(\n",
      "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "              (output): BertSelfOutput(\n",
      "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "            )\n",
      "            (intermediate): BertIntermediate(\n",
      "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "              (intermediate_act_fn): GELUActivation()\n",
      "            )\n",
      "            (output): BertOutput(\n",
      "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (3): BertLayer(\n",
      "            (attention): BertAttention(\n",
      "              (self): BertSelfAttention(\n",
      "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "              (output): BertSelfOutput(\n",
      "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "            )\n",
      "            (intermediate): BertIntermediate(\n",
      "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "              (intermediate_act_fn): GELUActivation()\n",
      "            )\n",
      "            (output): BertOutput(\n",
      "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (4): BertLayer(\n",
      "            (attention): BertAttention(\n",
      "              (self): BertSelfAttention(\n",
      "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "              (output): BertSelfOutput(\n",
      "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "            )\n",
      "            (intermediate): BertIntermediate(\n",
      "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "              (intermediate_act_fn): GELUActivation()\n",
      "            )\n",
      "            (output): BertOutput(\n",
      "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (5): BertLayer(\n",
      "            (attention): BertAttention(\n",
      "              (self): BertSelfAttention(\n",
      "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "              (output): BertSelfOutput(\n",
      "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "            )\n",
      "            (intermediate): BertIntermediate(\n",
      "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "              (intermediate_act_fn): GELUActivation()\n",
      "            )\n",
      "            (output): BertOutput(\n",
      "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (6): BertLayer(\n",
      "            (attention): BertAttention(\n",
      "              (self): BertSelfAttention(\n",
      "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "              (output): BertSelfOutput(\n",
      "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "            )\n",
      "            (intermediate): BertIntermediate(\n",
      "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "              (intermediate_act_fn): GELUActivation()\n",
      "            )\n",
      "            (output): BertOutput(\n",
      "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (7): BertLayer(\n",
      "            (attention): BertAttention(\n",
      "              (self): BertSelfAttention(\n",
      "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "              (output): BertSelfOutput(\n",
      "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "            )\n",
      "            (intermediate): BertIntermediate(\n",
      "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "              (intermediate_act_fn): GELUActivation()\n",
      "            )\n",
      "            (output): BertOutput(\n",
      "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (8): BertLayer(\n",
      "            (attention): BertAttention(\n",
      "              (self): BertSelfAttention(\n",
      "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "              (output): BertSelfOutput(\n",
      "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "            )\n",
      "            (intermediate): BertIntermediate(\n",
      "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "              (intermediate_act_fn): GELUActivation()\n",
      "            )\n",
      "            (output): BertOutput(\n",
      "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (9): BertLayer(\n",
      "            (attention): BertAttention(\n",
      "              (self): BertSelfAttention(\n",
      "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "              (output): BertSelfOutput(\n",
      "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "            )\n",
      "            (intermediate): BertIntermediate(\n",
      "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "              (intermediate_act_fn): GELUActivation()\n",
      "            )\n",
      "            (output): BertOutput(\n",
      "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (10): BertLayer(\n",
      "            (attention): BertAttention(\n",
      "              (self): BertSelfAttention(\n",
      "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "              (output): BertSelfOutput(\n",
      "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "            )\n",
      "            (intermediate): BertIntermediate(\n",
      "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "              (intermediate_act_fn): GELUActivation()\n",
      "            )\n",
      "            (output): BertOutput(\n",
      "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (11): BertLayer(\n",
      "            (attention): BertAttention(\n",
      "              (self): BertSelfAttention(\n",
      "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "              (output): BertSelfOutput(\n",
      "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "            )\n",
      "            (intermediate): BertIntermediate(\n",
      "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "              (intermediate_act_fn): GELUActivation()\n",
      "            )\n",
      "            (output): BertOutput(\n",
      "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (pooler): BertPooler(\n",
      "        (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (activation): Tanh()\n",
      "      )\n",
      "      (feature_modules): ModuleDict(\n",
      "        (distance): Embedding(5, 3, padding_idx=0)\n",
      "        (u1_depdir): Embedding(5, 3, padding_idx=0)\n",
      "        (u2_depdir): Embedding(5, 3, padding_idx=0)\n",
      "        (u2_func): Embedding(37, 7, padding_idx=0)\n",
      "        (u1_position): Embedding(12, 4, padding_idx=0)\n",
      "        (u2_position): Embedding(12, 4, padding_idx=0)\n",
      "        (sat_children): Identity()\n",
      "        (nuc_children): Identity()\n",
      "      )\n",
      "      (feature_projector): Linear(in_features=27, out_features=768, bias=True)\n",
      "    )\n",
      "  )\n",
      "  (encoder): CustomPooler2(\n",
      "    (_dropout): Dropout(p=0.0, inplace=False)\n",
      "    (pooler): BertPooler(\n",
      "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "      (activation): Tanh()\n",
      "    )\n",
      "  )\n",
      "  (module1): MyModule(\n",
      "    (feature_modules): ModuleDict(\n",
      "      (distance): Embedding(5, 3, padding_idx=0)\n",
      "      (u1_depdir): Embedding(5, 3, padding_idx=0)\n",
      "      (u2_depdir): Embedding(5, 3, padding_idx=0)\n",
      "      (u2_func): Embedding(37, 7, padding_idx=0)\n",
      "      (u1_position): Embedding(12, 4, padding_idx=0)\n",
      "      (u2_position): Embedding(12, 4, padding_idx=0)\n",
      "      (sat_children): Identity()\n",
      "      (nuc_children): Identity()\n",
      "    )\n",
      "  )\n",
      "  (dropout1): Dropout(p=0.0, inplace=False)\n",
      "  (relation_decoder): Linear(in_features=794, out_features=25, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "def writer_init(save_path_suffix):\n",
    "    writer_path = 'run1/'+save_path_suffix[:-1]+'/'\n",
    "    if os.path.isdir(writer_path):\n",
    "        filelist = [ f for f in os.listdir(writer_path) if 'events.out' in f ]\n",
    "        print(filelist)\n",
    "        for f in filelist:\n",
    "            os.remove(os.path.join(writer_path, f))\n",
    "    else:\n",
    "        os.mkdir(writer_path)\n",
    "    writer = SummaryWriter(log_dir=writer_path)\n",
    "    return writer\n",
    "\n",
    "writer = writer_init(save_path_suffix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import traceback\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from typing import Optional, Iterable, Dict, Any\n",
    "from EarlyStopperUtil import MetricTracker\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "EPOCHS = 100\n",
    "\n",
    "def train(model, train_loader, val_loader, optimizer, scheduler, rev_label_dict):  \n",
    "  EarlyStopper = MetricTracker(patience=20, metric_name='+accuracy')\n",
    "  best_val_acc = 0\n",
    "\n",
    "\n",
    "  for epoch in range(EPOCHS):\n",
    "    start = time.time()\n",
    "    model.train()\n",
    "    total_train_loss = 0\n",
    "    total_train_acc  = 0\n",
    "    \n",
    "    # logging for scheduler\n",
    "    losses = []\n",
    "    accuracies= []\n",
    "\n",
    "    train_size = 0\n",
    "\n",
    "    for batch_idx, (pair_token_ids, mask_ids, feat, y, idx) in enumerate(train_loader):\n",
    "      train_size+=1\n",
    "      optimizer.zero_grad()\n",
    "      pair_token_ids = pair_token_ids.to(device)\n",
    "      mask_ids = mask_ids.to(device)\n",
    "      # seg_ids = seg_ids.to(device)\n",
    "      labels = y.to(device)\n",
    "      # feat = feat.to(device)\n",
    "      outputs = model(pair_token_ids = pair_token_ids, \n",
    "                            # token_type_ids=seg_ids, \n",
    "                            attention_mask=mask_ids,\n",
    "                            feat=feat)\n",
    "      criterion = nn.CrossEntropyLoss()\n",
    "      loss = criterion(outputs, labels)\n",
    "      loss.backward()\n",
    "      acc = multi_acc(outputs, labels)\n",
    "      optimizer.step()\n",
    "      total_train_loss += loss.item()\n",
    "      total_train_acc  += acc.item()\n",
    "\n",
    "      losses.append(loss)\n",
    "      accuracies.append(acc)\n",
    "      \n",
    "    mean_loss = sum(losses)/len(losses)\n",
    "    scheduler.step(mean_loss)\n",
    "\n",
    "    train_acc  = total_train_acc/len(train_loader)\n",
    "    train_loss = total_train_loss/len(train_loader)\n",
    "\n",
    "    val_acc, val_loss, cr, model, optimizer = evaluate_accuracy(model, optimizer, val_loader, rev_label_dict, label_dict, None)\n",
    "    if val_acc>best_val_acc:\n",
    "      torch.save(model.state_dict(), 'run1/'+save_path_suffix+'_best.pt')\n",
    "      best_val_acc = val_acc\n",
    "      print(f'Epoch {epoch+1}: Best val_acc: {best_val_acc:.4f}')\n",
    "    if val_acc>=best_val_acc:\n",
    "      torch.save(model.state_dict(), 'run1/'+save_path_suffix+'_best_latest.pt')\n",
    "      best_val_acc = val_acc\n",
    "      print(f'Epoch {epoch+1}: Best val_acc: {best_val_acc:.4f}')\n",
    "    EarlyStopper.add_metric(val_acc)\n",
    "    if EarlyStopper.should_stop_early(): break\n",
    "\n",
    "    end = time.time()\n",
    "    hours, rem = divmod(end-start, 3600)\n",
    "    minutes, seconds = divmod(rem, 60)\n",
    "\n",
    "    print(f'Epoch {epoch+1}: train_loss: {train_loss:.4f} train_acc: {train_acc:.4f} | val_loss: {val_loss:.4f} val_acc: {val_acc:.4f}')\n",
    "    print(\"{:0>2}:{:0>2}:{:05.2f}\".format(int(hours),int(minutes),seconds))\n",
    "    print(f'train_size: {train_size}')\n",
    "\n",
    "    writer.add_scalar('train_loss', train_loss, epoch)\n",
    "    writer.add_scalar('train_acc', train_acc, epoch)\n",
    "    writer.add_scalar('val_loss', val_loss, epoch)\n",
    "    writer.add_scalar('val_acc', val_acc, epoch)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: Best val_acc: 0.1598\n",
      "Epoch 1: Best val_acc: 0.1598\n",
      "Epoch 1: train_loss: 2.3781 train_acc: 0.2683 | val_loss: 2.8883 val_acc: 0.1598\n",
      "00:02:26.63\n",
      "train_size: 1581\n",
      "Epoch 2: Best val_acc: 0.1762\n",
      "Epoch 2: Best val_acc: 0.1762\n",
      "Epoch 2: train_loss: 2.0371 train_acc: 0.3725 | val_loss: 2.6831 val_acc: 0.1762\n",
      "00:02:26.57\n",
      "train_size: 1581\n",
      "Epoch 3: Best val_acc: 0.2090\n",
      "Epoch 3: Best val_acc: 0.2090\n",
      "Epoch 3: train_loss: 1.8411 train_acc: 0.4315 | val_loss: 2.5633 val_acc: 0.2090\n",
      "00:02:27.64\n",
      "train_size: 1581\n",
      "Epoch 4: Best val_acc: 0.2172\n",
      "Epoch 4: Best val_acc: 0.2172\n",
      "Epoch 4: train_loss: 1.6951 train_acc: 0.4742 | val_loss: 2.4224 val_acc: 0.2172\n",
      "00:02:27.92\n",
      "train_size: 1581\n",
      "Epoch 5: Best val_acc: 0.2418\n",
      "Epoch 5: Best val_acc: 0.2418\n",
      "Epoch 5: train_loss: 1.5618 train_acc: 0.5180 | val_loss: 2.4374 val_acc: 0.2418\n",
      "00:02:28.34\n",
      "train_size: 1581\n",
      "Epoch 6: Best val_acc: 0.2541\n",
      "Epoch 6: Best val_acc: 0.2541\n",
      "Epoch 6: train_loss: 1.4376 train_acc: 0.5579 | val_loss: 2.3416 val_acc: 0.2541\n",
      "00:02:28.98\n",
      "train_size: 1581\n",
      "Epoch 7: Best val_acc: 0.2910\n",
      "Epoch 7: Best val_acc: 0.2910\n",
      "Epoch 7: train_loss: 1.3107 train_acc: 0.6026 | val_loss: 2.3310 val_acc: 0.2910\n",
      "00:02:29.56\n",
      "train_size: 1581\n",
      "Epoch 8: Best val_acc: 0.3074\n",
      "Epoch 8: Best val_acc: 0.3074\n",
      "Epoch 8: train_loss: 1.1881 train_acc: 0.6409 | val_loss: 2.3392 val_acc: 0.3074\n",
      "00:02:29.01\n",
      "train_size: 1581\n",
      "Epoch 9: Best val_acc: 0.3074\n",
      "Epoch 9: train_loss: 1.0661 train_acc: 0.6913 | val_loss: 2.4164 val_acc: 0.3074\n",
      "00:02:28.39\n",
      "train_size: 1581\n",
      "Epoch 10: Best val_acc: 0.3238\n",
      "Epoch 10: Best val_acc: 0.3238\n",
      "Epoch 10: train_loss: 0.9559 train_acc: 0.7238 | val_loss: 2.4073 val_acc: 0.3238\n",
      "00:02:29.10\n",
      "train_size: 1581\n",
      "Epoch 11: Best val_acc: 0.3238\n",
      "Epoch 11: train_loss: 0.8397 train_acc: 0.7589 | val_loss: 2.4812 val_acc: 0.3238\n",
      "00:02:28.64\n",
      "train_size: 1581\n",
      "Epoch 12: Best val_acc: 0.3525\n",
      "Epoch 12: Best val_acc: 0.3525\n",
      "Epoch 12: train_loss: 0.7325 train_acc: 0.7962 | val_loss: 2.4816 val_acc: 0.3525\n",
      "00:02:28.60\n",
      "train_size: 1581\n",
      "Epoch 13: train_loss: 0.6434 train_acc: 0.8229 | val_loss: 2.4951 val_acc: 0.3402\n",
      "00:02:27.41\n",
      "train_size: 1581\n",
      "Epoch 14: train_loss: 0.5663 train_acc: 0.8504 | val_loss: 2.5997 val_acc: 0.3238\n",
      "00:02:27.66\n",
      "train_size: 1581\n",
      "Epoch 15: train_loss: 0.4954 train_acc: 0.8645 | val_loss: 2.6410 val_acc: 0.3279\n",
      "00:02:27.12\n",
      "train_size: 1581\n",
      "Epoch 16: train_loss: 0.4237 train_acc: 0.8918 | val_loss: 2.7213 val_acc: 0.3279\n",
      "00:02:26.76\n",
      "train_size: 1581\n",
      "Epoch 17: train_loss: 0.3808 train_acc: 0.8985 | val_loss: 2.6645 val_acc: 0.3443\n",
      "00:02:26.90\n",
      "train_size: 1581\n",
      "Epoch 18: train_loss: 0.3188 train_acc: 0.9205 | val_loss: 2.9139 val_acc: 0.3115\n",
      "00:02:26.49\n",
      "train_size: 1581\n",
      "Epoch 19: train_loss: 0.2721 train_acc: 0.9334 | val_loss: 2.9314 val_acc: 0.3156\n",
      "00:02:26.66\n",
      "train_size: 1581\n",
      "Epoch 20: train_loss: 0.2397 train_acc: 0.9410 | val_loss: 2.8953 val_acc: 0.3402\n",
      "00:02:26.93\n",
      "train_size: 1581\n",
      "Epoch 21: train_loss: 0.1978 train_acc: 0.9559 | val_loss: 2.9585 val_acc: 0.3197\n",
      "00:02:27.09\n",
      "train_size: 1581\n",
      "Epoch 22: train_loss: 0.1772 train_acc: 0.9638 | val_loss: 3.0327 val_acc: 0.3443\n",
      "00:02:26.38\n",
      "train_size: 1581\n",
      "Epoch 23: train_loss: 0.1572 train_acc: 0.9646 | val_loss: 2.9575 val_acc: 0.3402\n",
      "00:02:26.62\n",
      "train_size: 1581\n",
      "Epoch 24: train_loss: 0.1297 train_acc: 0.9752 | val_loss: 3.0298 val_acc: 0.3443\n",
      "00:02:26.21\n",
      "train_size: 1581\n",
      "Epoch 25: train_loss: 0.1212 train_acc: 0.9764 | val_loss: 3.0584 val_acc: 0.3484\n",
      "00:02:26.13\n",
      "train_size: 1581\n",
      "Epoch 26: train_loss: 0.1030 train_acc: 0.9799 | val_loss: 3.2205 val_acc: 0.3320\n",
      "00:02:26.69\n",
      "train_size: 1581\n",
      "Epoch 27: train_loss: 0.0805 train_acc: 0.9866 | val_loss: 3.3058 val_acc: 0.3361\n",
      "00:02:26.54\n",
      "train_size: 1581\n",
      "Epoch 28: train_loss: 0.0759 train_acc: 0.9866 | val_loss: 3.3124 val_acc: 0.3320\n",
      "00:02:25.82\n",
      "train_size: 1581\n",
      "Epoch 29: Best val_acc: 0.3525\n",
      "Epoch 29: train_loss: 0.0698 train_acc: 0.9875 | val_loss: 3.3211 val_acc: 0.3525\n",
      "00:02:26.29\n",
      "train_size: 1581\n",
      "Epoch 30: train_loss: 0.0651 train_acc: 0.9881 | val_loss: 3.3991 val_acc: 0.3443\n",
      "00:02:25.42\n",
      "train_size: 1581\n",
      "Epoch 31: train_loss: 0.0542 train_acc: 0.9897 | val_loss: 3.3202 val_acc: 0.3484\n",
      "00:02:25.51\n",
      "train_size: 1581\n"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "from sklearn.exceptions import DataConversionWarning\n",
    "with warnings.catch_warnings():\n",
    "    warnings.simplefilter(\"ignore\")\n",
    "\n",
    "    train(model, train_loader, val_loader, optimizer, scheduler, rev_label_dict) #6m30s 13m"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### latest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/VD/kaveri/anaconda3/envs/py310/lib/python3.10/site-packages/transformers/modeling_utils.py:768: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test_loss: 4.0352 test_acc: 0.2615\n",
      "00:00:00.83\n",
      "                precision    recall  f1-score   support\n",
      "\n",
      "    antithesis       0.75      0.17      0.27        18\n",
      "    background       0.15      0.18      0.16        17\n",
      "         cause       0.13      1.00      0.24         2\n",
      "  circumstance       0.11      0.07      0.08        15\n",
      "    concession       0.46      0.46      0.46        13\n",
      "     condition       0.29      0.44      0.35         9\n",
      "   conjunction       0.56      0.71      0.63         7\n",
      "      contrast       0.15      0.25      0.19         8\n",
      " e-elaboration       0.46      0.55      0.50        11\n",
      "   elaboration       0.09      0.10      0.10        10\n",
      "    evaluation       0.19      0.20      0.20        20\n",
      "      evidence       0.11      0.10      0.11        10\n",
      "interpretation       0.10      0.17      0.12        12\n",
      "         joint       0.27      0.48      0.35        29\n",
      "          list       0.36      0.19      0.25        26\n",
      "         means       0.00      0.00      0.00         2\n",
      "   preparation       0.33      0.25      0.29         4\n",
      "       purpose       1.00      0.33      0.50         3\n",
      "        reason       0.39      0.21      0.27        34\n",
      "   restatement       0.00      0.00      0.00         1\n",
      "      sequence       0.00      0.00      0.00         7\n",
      "  solutionhood       0.00      0.00      0.00         1\n",
      "       summary       0.00      0.00      0.00         1\n",
      "\n",
      "      accuracy                           0.26       260\n",
      "     macro avg       0.26      0.25      0.22       260\n",
      "  weighted avg       0.30      0.26      0.25       260\n",
      "\n",
      "Test Loss: 4.035 |  Test Acc: 26.15%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/VD/kaveri/anaconda3/envs/py310/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1334: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/VD/kaveri/anaconda3/envs/py310/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1334: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/VD/kaveri/anaconda3/envs/py310/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1334: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def validate(model, test_loader, optimizer, rev_label_dict, label_dict):\n",
    "  start = time.time()\n",
    "  test_acc, test_loss, cr, model, optimizer = evaluate_accuracy(model, optimizer, test_loader, rev_label_dict, label_dict, is_training=False)\n",
    "  end = time.time()\n",
    "  hours, rem = divmod(end-start, 3600)\n",
    "  minutes, seconds = divmod(rem, 60)\n",
    "\n",
    "  print(f'Test_loss: {test_loss:.4f} test_acc: {test_acc:.4f}')\n",
    "  print(\"{:0>2}:{:0>2}:{:05.2f}\".format(int(hours),int(minutes),seconds))\n",
    "  print(cr)\n",
    "\n",
    "  return test_loss, test_acc\n",
    "\n",
    "\n",
    "test_loss, test_acc = validate(model, test_loader, optimizer, rev_label_dict, label_dict)\n",
    "writer.add_scalar('test_loss_latest', test_loss, 1)\n",
    "writer.add_scalar('test_acc_latest', test_acc, 1)\n",
    "print(f'Test Loss: {test_loss:.3f} |  Test Acc: {test_acc*100:.2f}%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### best earliest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/VD/kaveri/anaconda3/envs/py310/lib/python3.10/site-packages/transformers/modeling_utils.py:768: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test_loss: 2.8750 test_acc: 0.2692\n",
      "00:00:00.80\n",
      "                precision    recall  f1-score   support\n",
      "\n",
      "    antithesis       0.50      0.22      0.31        18\n",
      "    background       0.23      0.29      0.26        17\n",
      "         cause       0.18      1.00      0.31         2\n",
      "  circumstance       0.17      0.07      0.10        15\n",
      "    concession       0.44      0.54      0.48        13\n",
      "     condition       0.25      0.44      0.32         9\n",
      "   conjunction       0.50      0.71      0.59         7\n",
      "      contrast       0.00      0.00      0.00         8\n",
      " e-elaboration       0.50      0.73      0.59        11\n",
      "   elaboration       0.13      0.20      0.16        10\n",
      "    evaluation       0.20      0.15      0.17        20\n",
      "      evidence       0.00      0.00      0.00        10\n",
      "interpretation       0.11      0.17      0.13        12\n",
      "         joint       0.24      0.34      0.28        29\n",
      "          list       0.23      0.19      0.21        26\n",
      "         means       0.00      0.00      0.00         2\n",
      "   preparation       0.33      0.50      0.40         4\n",
      "       purpose       1.00      0.33      0.50         3\n",
      "        reason       0.36      0.26      0.31        34\n",
      "   restatement       0.00      0.00      0.00         1\n",
      "      sequence       0.00      0.00      0.00         7\n",
      "  solutionhood       0.00      0.00      0.00         1\n",
      "       summary       0.00      0.00      0.00         1\n",
      "\n",
      "      accuracy                           0.27       260\n",
      "     macro avg       0.23      0.27      0.22       260\n",
      "  weighted avg       0.26      0.27      0.25       260\n",
      "\n",
      "Latest Test Loss: 2.875 |  Latest Test Acc: 26.92%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/VD/kaveri/anaconda3/envs/py310/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1334: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/VD/kaveri/anaconda3/envs/py310/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1334: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/VD/kaveri/anaconda3/envs/py310/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1334: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "model.load_state_dict(torch.load('run1/'+save_path_suffix+'_best.pt'))\n",
    "test_loss, test_acc = validate(model, test_loader, optimizer, rev_label_dict, label_dict)\n",
    "writer.add_scalar('test_loss_best_earliest', test_loss, 1)\n",
    "writer.add_scalar('test_acc_best_earliest', test_acc, 1)\n",
    "print(f'Latest Test Loss: {test_loss:.3f} |  Latest Test Acc: {test_acc*100:.2f}%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### best lastest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/VD/kaveri/anaconda3/envs/py310/lib/python3.10/site-packages/transformers/modeling_utils.py:768: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test_loss: 3.8539 test_acc: 0.2846\n",
      "00:00:00.83\n",
      "                precision    recall  f1-score   support\n",
      "\n",
      "    antithesis       0.38      0.17      0.23        18\n",
      "    background       0.16      0.18      0.17        17\n",
      "         cause       0.10      0.50      0.17         2\n",
      "  circumstance       0.17      0.13      0.15        15\n",
      "    concession       0.39      0.54      0.45        13\n",
      "     condition       0.35      0.67      0.46         9\n",
      "   conjunction       0.50      0.71      0.59         7\n",
      "      contrast       0.13      0.25      0.17         8\n",
      " e-elaboration       0.54      0.64      0.58        11\n",
      "   elaboration       0.23      0.30      0.26        10\n",
      "    evaluation       0.38      0.30      0.33        20\n",
      "      evidence       0.14      0.20      0.17        10\n",
      "interpretation       0.15      0.17      0.16        12\n",
      "         joint       0.25      0.45      0.33        29\n",
      "          list       0.36      0.15      0.22        26\n",
      "         means       0.00      0.00      0.00         2\n",
      "   preparation       0.40      0.50      0.44         4\n",
      "       purpose       1.00      0.33      0.50         3\n",
      "        reason       0.38      0.15      0.21        34\n",
      "   restatement       0.00      0.00      0.00         1\n",
      "      sequence       0.00      0.00      0.00         7\n",
      "  solutionhood       0.00      0.00      0.00         1\n",
      "       summary       0.00      0.00      0.00         1\n",
      "\n",
      "      accuracy                           0.28       260\n",
      "     macro avg       0.26      0.28      0.24       260\n",
      "  weighted avg       0.30      0.28      0.27       260\n",
      "\n",
      "Best Test Loss: 3.854 |  Best Test Acc: 28.46%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/VD/kaveri/anaconda3/envs/py310/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1334: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/VD/kaveri/anaconda3/envs/py310/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1334: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/VD/kaveri/anaconda3/envs/py310/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1334: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "model.load_state_dict(torch.load('run1/'+save_path_suffix+'_best_latest.pt'))\n",
    "test_loss, test_acc = validate(model, test_loader, optimizer, rev_label_dict, label_dict)\n",
    "writer.add_scalar('test_loss_best_latest', test_loss, 1)\n",
    "writer.add_scalar('test_acc_best_latest', test_acc, 1)\n",
    "print(f'Best Test Loss: {test_loss:.3f} |  Best Test Acc: {test_acc*100:.2f}%')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### best val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/VD/kaveri/anaconda3/envs/py310/lib/python3.10/site-packages/transformers/modeling_utils.py:768: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test_loss: 3.3995 test_acc: 0.3402\n",
      "00:00:00.80\n",
      "                precision    recall  f1-score   support\n",
      "\n",
      "    antithesis       0.31      0.36      0.33        11\n",
      "    background       0.22      0.24      0.23        17\n",
      "         cause       0.25      0.14      0.18         7\n",
      "  circumstance       0.33      0.31      0.32        13\n",
      "    concession       0.38      0.73      0.50        11\n",
      "     condition       0.70      0.88      0.78         8\n",
      "   conjunction       0.50      0.50      0.50         8\n",
      "      contrast       0.00      0.00      0.00         3\n",
      " e-elaboration       0.60      0.46      0.52        13\n",
      "   elaboration       0.33      0.21      0.26        28\n",
      "    evaluation       0.00      0.00      0.00        13\n",
      "      evidence       0.27      0.38      0.32         8\n",
      "interpretation       0.18      0.15      0.17        13\n",
      "         joint       0.20      0.50      0.29        18\n",
      "          list       0.38      0.28      0.32        18\n",
      "         means       0.00      0.00      0.00         1\n",
      "   preparation       0.88      0.64      0.74        11\n",
      "       purpose       0.75      0.60      0.67         5\n",
      "        reason       0.45      0.32      0.38        28\n",
      "   restatement       0.00      0.00      0.00         1\n",
      "        result       0.50      0.33      0.40         3\n",
      "  solutionhood       0.00      0.00      0.00         1\n",
      "       summary       0.00      0.00      0.00         2\n",
      "\n",
      "      accuracy                           0.34       241\n",
      "     macro avg       0.32      0.31      0.30       241\n",
      "  weighted avg       0.36      0.34      0.34       241\n",
      "\n",
      "Val Loss: 3.399 |  Val Acc: 34.02%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/VD/kaveri/anaconda3/envs/py310/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1334: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/VD/kaveri/anaconda3/envs/py310/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1334: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/VD/kaveri/anaconda3/envs/py310/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1334: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "model.load_state_dict(torch.load('run1/'+save_path_suffix+'_best_latest.pt'))\n",
    "test_loss, test_acc = validate(model, val_loader, optimizer, rev_label_dict, label_dict)\n",
    "writer.add_scalar('val_loss_best_latest', test_loss, 1)\n",
    "writer.add_scalar('val_acc_best_latest', test_acc, 1)\n",
    "print(f'Val Loss: {test_loss:.3f} |  Val Acc: {test_acc*100:.2f}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>unit1_txt</th>\n",
       "      <th>unit1_sent</th>\n",
       "      <th>unit2_txt</th>\n",
       "      <th>unit2_sent</th>\n",
       "      <th>dir</th>\n",
       "      <th>distance</th>\n",
       "      <th>u1_depdir</th>\n",
       "      <th>u2_depdir</th>\n",
       "      <th>u2_func</th>\n",
       "      <th>u1_position</th>\n",
       "      <th>...</th>\n",
       "      <th>sat_children</th>\n",
       "      <th>nuc_children</th>\n",
       "      <th>genre</th>\n",
       "      <th>unit1_case</th>\n",
       "      <th>unit2_case</th>\n",
       "      <th>u1_discontinuous</th>\n",
       "      <th>u2_discontinuous</th>\n",
       "      <th>same_speaker</th>\n",
       "      <th>lex_overlap_length</th>\n",
       "      <th>u1_func</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>label</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>background</th>\n",
       "      <td>701</td>\n",
       "      <td>701</td>\n",
       "      <td>701</td>\n",
       "      <td>701</td>\n",
       "      <td>701</td>\n",
       "      <td>701</td>\n",
       "      <td>701</td>\n",
       "      <td>701</td>\n",
       "      <td>701</td>\n",
       "      <td>701</td>\n",
       "      <td>...</td>\n",
       "      <td>701</td>\n",
       "      <td>701</td>\n",
       "      <td>701</td>\n",
       "      <td>701</td>\n",
       "      <td>701</td>\n",
       "      <td>701</td>\n",
       "      <td>701</td>\n",
       "      <td>701</td>\n",
       "      <td>701</td>\n",
       "      <td>701</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>cause</th>\n",
       "      <td>493</td>\n",
       "      <td>493</td>\n",
       "      <td>493</td>\n",
       "      <td>493</td>\n",
       "      <td>493</td>\n",
       "      <td>493</td>\n",
       "      <td>493</td>\n",
       "      <td>493</td>\n",
       "      <td>493</td>\n",
       "      <td>493</td>\n",
       "      <td>...</td>\n",
       "      <td>493</td>\n",
       "      <td>493</td>\n",
       "      <td>493</td>\n",
       "      <td>493</td>\n",
       "      <td>493</td>\n",
       "      <td>493</td>\n",
       "      <td>493</td>\n",
       "      <td>493</td>\n",
       "      <td>493</td>\n",
       "      <td>493</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>condition</th>\n",
       "      <td>265</td>\n",
       "      <td>265</td>\n",
       "      <td>265</td>\n",
       "      <td>265</td>\n",
       "      <td>265</td>\n",
       "      <td>265</td>\n",
       "      <td>265</td>\n",
       "      <td>265</td>\n",
       "      <td>265</td>\n",
       "      <td>265</td>\n",
       "      <td>...</td>\n",
       "      <td>265</td>\n",
       "      <td>265</td>\n",
       "      <td>265</td>\n",
       "      <td>265</td>\n",
       "      <td>265</td>\n",
       "      <td>265</td>\n",
       "      <td>265</td>\n",
       "      <td>265</td>\n",
       "      <td>265</td>\n",
       "      <td>265</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>contrast</th>\n",
       "      <td>203</td>\n",
       "      <td>203</td>\n",
       "      <td>203</td>\n",
       "      <td>203</td>\n",
       "      <td>203</td>\n",
       "      <td>203</td>\n",
       "      <td>203</td>\n",
       "      <td>203</td>\n",
       "      <td>203</td>\n",
       "      <td>203</td>\n",
       "      <td>...</td>\n",
       "      <td>203</td>\n",
       "      <td>203</td>\n",
       "      <td>203</td>\n",
       "      <td>203</td>\n",
       "      <td>203</td>\n",
       "      <td>203</td>\n",
       "      <td>203</td>\n",
       "      <td>203</td>\n",
       "      <td>203</td>\n",
       "      <td>203</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>elaboration</th>\n",
       "      <td>889</td>\n",
       "      <td>889</td>\n",
       "      <td>889</td>\n",
       "      <td>889</td>\n",
       "      <td>889</td>\n",
       "      <td>889</td>\n",
       "      <td>889</td>\n",
       "      <td>889</td>\n",
       "      <td>889</td>\n",
       "      <td>889</td>\n",
       "      <td>...</td>\n",
       "      <td>889</td>\n",
       "      <td>889</td>\n",
       "      <td>889</td>\n",
       "      <td>889</td>\n",
       "      <td>889</td>\n",
       "      <td>889</td>\n",
       "      <td>889</td>\n",
       "      <td>889</td>\n",
       "      <td>889</td>\n",
       "      <td>889</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>evaluation</th>\n",
       "      <td>455</td>\n",
       "      <td>455</td>\n",
       "      <td>455</td>\n",
       "      <td>455</td>\n",
       "      <td>455</td>\n",
       "      <td>455</td>\n",
       "      <td>455</td>\n",
       "      <td>455</td>\n",
       "      <td>455</td>\n",
       "      <td>455</td>\n",
       "      <td>...</td>\n",
       "      <td>455</td>\n",
       "      <td>455</td>\n",
       "      <td>455</td>\n",
       "      <td>455</td>\n",
       "      <td>455</td>\n",
       "      <td>455</td>\n",
       "      <td>455</td>\n",
       "      <td>455</td>\n",
       "      <td>455</td>\n",
       "      <td>455</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>joint</th>\n",
       "      <td>1076</td>\n",
       "      <td>1076</td>\n",
       "      <td>1076</td>\n",
       "      <td>1076</td>\n",
       "      <td>1076</td>\n",
       "      <td>1076</td>\n",
       "      <td>1076</td>\n",
       "      <td>1076</td>\n",
       "      <td>1076</td>\n",
       "      <td>1076</td>\n",
       "      <td>...</td>\n",
       "      <td>1076</td>\n",
       "      <td>1076</td>\n",
       "      <td>1076</td>\n",
       "      <td>1076</td>\n",
       "      <td>1076</td>\n",
       "      <td>1076</td>\n",
       "      <td>1076</td>\n",
       "      <td>1076</td>\n",
       "      <td>1076</td>\n",
       "      <td>1076</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>means</th>\n",
       "      <td>43</td>\n",
       "      <td>43</td>\n",
       "      <td>43</td>\n",
       "      <td>43</td>\n",
       "      <td>43</td>\n",
       "      <td>43</td>\n",
       "      <td>43</td>\n",
       "      <td>43</td>\n",
       "      <td>43</td>\n",
       "      <td>43</td>\n",
       "      <td>...</td>\n",
       "      <td>43</td>\n",
       "      <td>43</td>\n",
       "      <td>43</td>\n",
       "      <td>43</td>\n",
       "      <td>43</td>\n",
       "      <td>43</td>\n",
       "      <td>43</td>\n",
       "      <td>43</td>\n",
       "      <td>43</td>\n",
       "      <td>43</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>summary</th>\n",
       "      <td>32</td>\n",
       "      <td>32</td>\n",
       "      <td>32</td>\n",
       "      <td>32</td>\n",
       "      <td>32</td>\n",
       "      <td>32</td>\n",
       "      <td>32</td>\n",
       "      <td>32</td>\n",
       "      <td>32</td>\n",
       "      <td>32</td>\n",
       "      <td>...</td>\n",
       "      <td>32</td>\n",
       "      <td>32</td>\n",
       "      <td>32</td>\n",
       "      <td>32</td>\n",
       "      <td>32</td>\n",
       "      <td>32</td>\n",
       "      <td>32</td>\n",
       "      <td>32</td>\n",
       "      <td>32</td>\n",
       "      <td>32</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>9 rows × 21 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             unit1_txt  unit1_sent  unit2_txt  unit2_sent   dir  distance  \\\n",
       "label                                                                       \n",
       "background         701         701        701         701   701       701   \n",
       "cause              493         493        493         493   493       493   \n",
       "condition          265         265        265         265   265       265   \n",
       "contrast           203         203        203         203   203       203   \n",
       "elaboration        889         889        889         889   889       889   \n",
       "evaluation         455         455        455         455   455       455   \n",
       "joint             1076        1076       1076        1076  1076      1076   \n",
       "means               43          43         43          43    43        43   \n",
       "summary             32          32         32          32    32        32   \n",
       "\n",
       "             u1_depdir  u2_depdir  u2_func  u1_position  ...  sat_children  \\\n",
       "label                                                    ...                 \n",
       "background         701        701      701          701  ...           701   \n",
       "cause              493        493      493          493  ...           493   \n",
       "condition          265        265      265          265  ...           265   \n",
       "contrast           203        203      203          203  ...           203   \n",
       "elaboration        889        889      889          889  ...           889   \n",
       "evaluation         455        455      455          455  ...           455   \n",
       "joint             1076       1076     1076         1076  ...          1076   \n",
       "means               43         43       43           43  ...            43   \n",
       "summary             32         32       32           32  ...            32   \n",
       "\n",
       "             nuc_children  genre  unit1_case  unit2_case  u1_discontinuous  \\\n",
       "label                                                                        \n",
       "background            701    701         701         701               701   \n",
       "cause                 493    493         493         493               493   \n",
       "condition             265    265         265         265               265   \n",
       "contrast              203    203         203         203               203   \n",
       "elaboration           889    889         889         889               889   \n",
       "evaluation            455    455         455         455               455   \n",
       "joint                1076   1076        1076        1076              1076   \n",
       "means                  43     43          43          43                43   \n",
       "summary                32     32          32          32                32   \n",
       "\n",
       "             u2_discontinuous  same_speaker  lex_overlap_length  u1_func  \n",
       "label                                                                     \n",
       "background                701           701                 701      701  \n",
       "cause                     493           493                 493      493  \n",
       "condition                 265           265                 265      265  \n",
       "contrast                  203           203                 203      203  \n",
       "elaboration               889           889                 889      889  \n",
       "evaluation                455           455                 455      455  \n",
       "joint                    1076          1076                1076     1076  \n",
       "means                      43            43                  43       43  \n",
       "summary                    32            32                  32       32  \n",
       "\n",
       "[9 rows x 21 columns]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df_en.groupby(['label']).count()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py310",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "e3409ea685db85227fbd9509d1b1ace14d085473eb2d57f3ba9dd0302d25f838"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

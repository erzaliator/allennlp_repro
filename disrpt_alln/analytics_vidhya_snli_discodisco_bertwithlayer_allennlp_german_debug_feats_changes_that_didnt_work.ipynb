{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "#seeding for comparing experiment in part 2\n",
    "import torch\n",
    "import json\n",
    "SEED = 1111\n",
    "torch.manual_seed(SEED)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "\n",
    "device = torch.device('cuda:6')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SNLI Bert\n",
    "## Second Tutorial\n",
    "https://towardsdatascience.com/fine-tuning-pre-trained-transformer-models-for-sentence-entailment-d87caf9ec9db\n",
    "Check his Github code for complete notebook. I never referred to it. Medium was enough.\n",
    "BERT in keras-tf: https://towardsdatascience.com/bert-in-keras-with-tensorflow-hub-76bcbc9417b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define macros\n",
    "BERT_MODEL = 'bert-base-german-cased'\n",
    "\n",
    "batch_size = 4\n",
    "batches_per_epoch = 541"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## load the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# custom reader needed to handle quotechars\n",
    "def read_df_custom(file):\n",
    "    header = 'doc     unit1_toks      unit2_toks      unit1_txt       unit2_txt       s1_toks s2_toks unit1_sent      unit2_sent      dir     nuc_children    sat_children    genre   u1_discontinuous        u2_discontinuous       u1_issent        u2_issent       u1_length       u2_length       length_ratio    u1_speaker      u2_speaker      same_speaker    u1_func u1_pos  u1_depdir       u2_func u2_pos  u2_depdir       doclen  u1_position      u2_position     percent_distance        distance        lex_overlap_words       lex_overlap_length      unit1_case      unit2_case      label'\n",
    "    extracted_columns = ['unit1_txt', 'unit1_sent', 'unit2_txt', 'unit2_sent', 'dir', 'label', 'distance', 'u1_depdir', 'u2_depdir', 'u2_func', 'u1_position', 'u2_position', 'sat_children', 'nuc_children']\n",
    "    header = header.split()\n",
    "    df = pd.DataFrame(columns=extracted_columns)\n",
    "    file = open(file, 'r')\n",
    "\n",
    "    rows = []\n",
    "    count = 0 \n",
    "    for line in file:\n",
    "        line = line[:-1].split('\\t')\n",
    "        count+=1\n",
    "        if count ==1: continue\n",
    "        row = {}\n",
    "        for column in extracted_columns:\n",
    "            index = header.index(column)\n",
    "            row[column] = line[index]\n",
    "        rows.append(row)\n",
    "\n",
    "    df = pd.concat([df, pd.DataFrame.from_records(rows)])\n",
    "    return df\n",
    "\n",
    "# we only need specific columns\n",
    "# train_df = read_df_custom('../processed/nld.rst.nldt_train_enriched.rels')\n",
    "# test_df = read_df_custom('../processed/nld.rst.nldt_test_enriched.rels')\n",
    "# val_df = read_df_custom('../processed/nld.rst.nldt_dev_enriched.rels')\n",
    "# train_df = read_df_custom('../processed/fas.rst.prstc_train_enriched.rels')\n",
    "# test_df = read_df_custom('../processed/fas.rst.prstc_test_enriched.rels')\n",
    "# val_df = read_df_custom('../processed/fas.rst.prstc_dev_enriched.rels')\n",
    "train_df = read_df_custom('../processed/deu.rst.pcc_train_enriched.rels')\n",
    "test_df = read_df_custom('../processed/deu.rst.pcc_test_enriched.rels')\n",
    "val_df = read_df_custom('../processed/deu.rst.pcc_dev_enriched.rels')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clean the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dropping any empty values\n",
    "train_df.dropna(inplace=True)\n",
    "val_df.dropna(inplace=True)\n",
    "test_df.dropna(inplace=True)\n",
    "\n",
    "# train_df = train_df[:1500]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare a dataset handler class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>unit1_txt</th>\n",
       "      <th>unit1_sent</th>\n",
       "      <th>unit2_txt</th>\n",
       "      <th>unit2_sent</th>\n",
       "      <th>dir</th>\n",
       "      <th>label</th>\n",
       "      <th>distance</th>\n",
       "      <th>u1_depdir</th>\n",
       "      <th>u2_depdir</th>\n",
       "      <th>u2_func</th>\n",
       "      <th>u1_position</th>\n",
       "      <th>u2_position</th>\n",
       "      <th>sat_children</th>\n",
       "      <th>nuc_children</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Dagmar Ziegler sitzt in der Schuldenfalle .</td>\n",
       "      <td>Dagmar Ziegler sitzt in der Schuldenfalle .</td>\n",
       "      <td>hat sie jetzt eine seit mehr als einem Jahr er...</td>\n",
       "      <td>Auf Grund der dramatischen Kassenlage in Brand...</td>\n",
       "      <td>1&gt;2</td>\n",
       "      <td>interpretation</td>\n",
       "      <td>2</td>\n",
       "      <td>ROOT</td>\n",
       "      <td>ROOT</td>\n",
       "      <td>root</td>\n",
       "      <td>0.02</td>\n",
       "      <td>0.092</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Auf Grund der dramatischen Kassenlage in Brand...</td>\n",
       "      <td>Auf Grund der dramatischen Kassenlage in Brand...</td>\n",
       "      <td>hat sie jetzt eine seit mehr als einem Jahr er...</td>\n",
       "      <td>Auf Grund der dramatischen Kassenlage in Brand...</td>\n",
       "      <td>1&gt;2</td>\n",
       "      <td>cause</td>\n",
       "      <td>1</td>\n",
       "      <td>RIGHT</td>\n",
       "      <td>ROOT</td>\n",
       "      <td>root</td>\n",
       "      <td>0.056</td>\n",
       "      <td>0.092</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>hat sie jetzt eine seit mehr als einem Jahr er...</td>\n",
       "      <td>Auf Grund der dramatischen Kassenlage in Brand...</td>\n",
       "      <td>Der Rückzieher der Finanzministerin ist aber v...</td>\n",
       "      <td>Der Rückzieher der Finanzministerin ist aber v...</td>\n",
       "      <td>1&gt;2</td>\n",
       "      <td>evaluation-n</td>\n",
       "      <td>4</td>\n",
       "      <td>ROOT</td>\n",
       "      <td>ROOT</td>\n",
       "      <td>root</td>\n",
       "      <td>0.092</td>\n",
       "      <td>0.286</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>hat sie jetzt eine seit mehr als einem Jahr er...</td>\n",
       "      <td>Auf Grund der dramatischen Kassenlage in Brand...</td>\n",
       "      <td>und vorgeschlagen , erst 2003 darüber zu entsc...</td>\n",
       "      <td>Auf Grund der dramatischen Kassenlage in Brand...</td>\n",
       "      <td>1&lt;2</td>\n",
       "      <td>conjunction</td>\n",
       "      <td>1</td>\n",
       "      <td>ROOT</td>\n",
       "      <td>LEFT</td>\n",
       "      <td>conj</td>\n",
       "      <td>0.092</td>\n",
       "      <td>0.168</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>hat sie jetzt eine seit mehr als einem Jahr er...</td>\n",
       "      <td>Auf Grund der dramatischen Kassenlage in Brand...</td>\n",
       "      <td>Überraschend ,</td>\n",
       "      <td>Überraschend , weil das Finanz- und das Bildun...</td>\n",
       "      <td>1&lt;2</td>\n",
       "      <td>interpretation</td>\n",
       "      <td>2</td>\n",
       "      <td>ROOT</td>\n",
       "      <td>ROOT</td>\n",
       "      <td>root</td>\n",
       "      <td>0.092</td>\n",
       "      <td>0.214</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                           unit1_txt  \\\n",
       "0        Dagmar Ziegler sitzt in der Schuldenfalle .   \n",
       "1  Auf Grund der dramatischen Kassenlage in Brand...   \n",
       "2  hat sie jetzt eine seit mehr als einem Jahr er...   \n",
       "3  hat sie jetzt eine seit mehr als einem Jahr er...   \n",
       "4  hat sie jetzt eine seit mehr als einem Jahr er...   \n",
       "\n",
       "                                          unit1_sent  \\\n",
       "0        Dagmar Ziegler sitzt in der Schuldenfalle .   \n",
       "1  Auf Grund der dramatischen Kassenlage in Brand...   \n",
       "2  Auf Grund der dramatischen Kassenlage in Brand...   \n",
       "3  Auf Grund der dramatischen Kassenlage in Brand...   \n",
       "4  Auf Grund der dramatischen Kassenlage in Brand...   \n",
       "\n",
       "                                           unit2_txt  \\\n",
       "0  hat sie jetzt eine seit mehr als einem Jahr er...   \n",
       "1  hat sie jetzt eine seit mehr als einem Jahr er...   \n",
       "2  Der Rückzieher der Finanzministerin ist aber v...   \n",
       "3  und vorgeschlagen , erst 2003 darüber zu entsc...   \n",
       "4                                     Überraschend ,   \n",
       "\n",
       "                                          unit2_sent  dir           label  \\\n",
       "0  Auf Grund der dramatischen Kassenlage in Brand...  1>2  interpretation   \n",
       "1  Auf Grund der dramatischen Kassenlage in Brand...  1>2           cause   \n",
       "2  Der Rückzieher der Finanzministerin ist aber v...  1>2    evaluation-n   \n",
       "3  Auf Grund der dramatischen Kassenlage in Brand...  1<2     conjunction   \n",
       "4  Überraschend , weil das Finanz- und das Bildun...  1<2  interpretation   \n",
       "\n",
       "  distance u1_depdir u2_depdir u2_func u1_position u2_position sat_children  \\\n",
       "0        2      ROOT      ROOT    root        0.02       0.092            0   \n",
       "1        1     RIGHT      ROOT    root       0.056       0.092            0   \n",
       "2        4      ROOT      ROOT    root       0.092       0.286            4   \n",
       "3        1      ROOT      LEFT    conj       0.092       0.168            0   \n",
       "4        2      ROOT      ROOT    root       0.092       0.214            1   \n",
       "\n",
       "  nuc_children  \n",
       "0            4  \n",
       "1            4  \n",
       "2            3  \n",
       "3            4  \n",
       "4            4  "
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from lib2to3.pgen2 import token\n",
    "from multiprocessing.sharedctypes import Value\n",
    "import torch\n",
    "from torch.utils.data import Dataset, TensorDataset, DataLoader\n",
    "from sys import path\n",
    "path.append('/home/VD/kaveri/anaconda3/envs/py310/lib/python3.10/site-packages/allennlp/data/data_loaders/')\n",
    "from allennlp.data import allennlp_collate#, DataLoader\n",
    "# from allennlp.data.data_loaders.simple_data_loader import SimpleDataLoader\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from transformers import BertTokenizer\n",
    "import pandas as pd\n",
    "\n",
    "class MNLIDataBert(Dataset):\n",
    "\n",
    "  def __init__(self, train_df, val_df, test_df):\n",
    "    self.num_labels = -1\n",
    "    self.train_df = train_df\n",
    "    self.val_df = val_df\n",
    "    self.test_df = test_df\n",
    "\n",
    "    self.tokenizer = BertTokenizer.from_pretrained(BERT_MODEL, do_lower_case=True) # Using a pre-trained BERT tokenizer to encode sentences\n",
    "    self.train_data = None\n",
    "    self.val_data = None\n",
    "    self.test_data = None\n",
    "    self.train_idx = None\n",
    "    self.val_idx = None\n",
    "    self.test_idx = None\n",
    "    self.init_data()\n",
    "\n",
    "  def init_data(self):\n",
    "    self.get_label_mapping()\n",
    "    self.train_data, self.train_idx = self.load_data(self.train_df)\n",
    "    self.val_data, self.val_idx = self.load_data(self.val_df)\n",
    "    self.test_data, self.test_idx = self.load_data(self.test_df)\n",
    "\n",
    "  def get_label_mapping(self):\n",
    "    labels = {}\n",
    "    labels_list = list(set(list(self.train_df['label'].unique()) + list(self.test_df['label'].unique()) + list(self.val_df['label'].unique())))\n",
    "    for i in range(len(labels_list)):\n",
    "        labels[labels_list[i]] = i\n",
    "    self.label_dict = labels# {'entailment': 0, 'contradiction': 1, 'neutral': 2}\n",
    "    # needed later for classification report object to generate precision and recall on test dataset\n",
    "    self.rev_label_dict = {self.label_dict[k]:k for k in self.label_dict.keys()} \n",
    "  \n",
    "  def add_directionality(self, premise, hypothesis, dir):\n",
    "    # if dir == \"1<2\":\n",
    "    #     hypothesis = 'left ' + hypothesis + ' {'\n",
    "    #     # hypothesis = '< ' + hypothesis + ' {'\n",
    "    #     # hypothesis = '{ ' + hypothesis + ' {'\n",
    "    # else:\n",
    "    #     premise = '} ' + premise + ' right'\n",
    "    #     # premise = '} ' + premise + ' >'\n",
    "    #     # premise = '} ' + premise + ' }'\n",
    "    return premise, hypothesis\n",
    "\n",
    "  def get_distance(self, d):\n",
    "    if d<-8: return -2.0\n",
    "    elif d>=-8 and d<-2: return -1.0\n",
    "    elif d>=-2 and d<0: return 0.0\n",
    "    elif d>=0 and d<2: return 1.0\n",
    "    elif d>=2 and d<8: return 2.0\n",
    "    elif d>=8: return 3.0\n",
    "\n",
    "  def get_dep(self, d):\n",
    "    if d=='ROOT': return 0.0\n",
    "    elif d=='RIGHT': return 1.0\n",
    "    elif d=='LEFT': return -1.0\n",
    "    else: raise ValueError()\n",
    "\n",
    "  def get_u2_func(self, u):\n",
    "    u2_dict = {'root':0.0, 'conj':1.0, 'advcl':2.0, 'acl':3.0, 'xcomp':4.0, 'obl':5.0, 'ccomp':6.0,\n",
    "       'parataxis':7.0, 'advmod':8.0, 'dep':9.0, 'csubj':10.0, 'nmod':11.0, 'punct':12.0, 'cc':13.0,\n",
    "       'appos':14.0, 'aux':15.0, 'obj':16.0, 'iobj':17.0, 'nsubj':18.0, 'nsubj:pass':19.0, 'csubj:pass':20.0}\n",
    "    return u2_dict[u]\n",
    "\n",
    "  def get_u_position(self, u):\n",
    "    if u>=0.0 and u<0.1: return -5.0\n",
    "    elif u>=0.1 and u<0.2: return -4.0\n",
    "    elif u>=0.2 and u<0.3: return -3.0\n",
    "    elif u<=0.3 and u<0.4: return -2.0\n",
    "    elif u<=0.4 and u<0.5: return -1.0\n",
    "    elif u<=0.5 and u<0.6: return 0.0\n",
    "    elif u<=0.6 and u<0.7: return 1.0\n",
    "    elif u<=0.7 and u<0.8: return 2.0\n",
    "    elif u<=0.8 and u<0.9: return 3.0\n",
    "    elif u<=0.9 and u<1.0: return 4.0\n",
    "    elif u<=1.0 and u<1e9: return 5.0\n",
    "\n",
    "  def get_feature(self, features):\n",
    "    distance = self.get_distance(float(features[0]))\n",
    "    u1_depdir = self.get_dep(features[1])\n",
    "    u2_depdir = self.get_dep(features[2])\n",
    "    u2_func = self.get_u2_func(features[3])\n",
    "    u1_position = self.get_u_position(float(features[4]))\n",
    "    u2_position = self.get_u_position(float(features[5]))\n",
    "    sat_children = float(features[6])\n",
    "    nuc_children = float(features[6])\n",
    "    return [distance, u1_depdir, u2_depdir, u2_func, u1_position, u2_position, sat_children, nuc_children]\n",
    "\n",
    "  def load_data2(self, df):\n",
    "    MAX_LEN = 256 # dont need to enforce this now because snli is a sanitized dataset where sentence lenghts are reasonable. otherwise the beert model doesn't have enough parameters to handle long length sentences\n",
    "    token_ids = []\n",
    "    mask_ids = []\n",
    "    seg_ids = []\n",
    "    y = []\n",
    "    idx = []\n",
    "    idx_map = {}\n",
    "    # self.reach = 'reach'\n",
    "\n",
    "    premise_list = df['unit1_txt'].to_list()\n",
    "    hypothesis_list = df['unit2_txt'].to_list()\n",
    "    label_list = df['label'].to_list()\n",
    "    dir_list = df['dir'].to_list()\n",
    "    \n",
    "    # self.num_labels = max(self.num_labels, len(df['label'].unique()))\n",
    "    self.num_labels = len(self.label_dict)\n",
    "\n",
    "    count=0\n",
    "    for (premise, hypothesis, label, dir) in zip(premise_list, hypothesis_list, label_list, dir_list):\n",
    "      print('old: ', premise, hypothesis)\n",
    "      premise, hypothesis = self.add_directionality(premise, hypothesis, dir)\n",
    "      print('new:', premise, hypothesis, '\\n')\n",
    "    \n",
    "\n",
    "  def load_data(self, df):\n",
    "    MAX_LEN = 256 # dont need to enforce this now because snli is a sanitized dataset where sentence lenghts are reasonable. otherwise the beert model doesn't have enough parameters to handle long length sentences\n",
    "    token_ids = []\n",
    "    mask_ids = []\n",
    "    seg_ids = []\n",
    "    y = []\n",
    "    feats = []\n",
    "    idx = []\n",
    "    idx_map = {}\n",
    "\n",
    "    #\"u1_depdir\", \"u2_depdir\", \"u2_func\", \"u1_position\", \"u2_position\",\"sat_children\", \"nuc_children\"\n",
    "    \n",
    "    self.num_labels = max(self.num_labels, len(df['label'].unique()))\n",
    "\n",
    "    count=0\n",
    "    # for (premise, hypothesis, label, dir, distance, u1_depdir) in zip(premise_list, hypothesis_list, label_list, dir_list, u1_depdir_list, feat_list):\n",
    "    for row in df.iterrows():\n",
    "      row = row[1]\n",
    "      premise = row['unit1_txt']#'Das ist Frühstück .'\n",
    "      hypothesis = row['unit2_txt']#'Das ist Frühstück .'\n",
    "      label = row['label']\n",
    "      dir = row['dir']\n",
    "\n",
    "      distance = row['distance']\n",
    "      u1_depdir = row['u1_depdir']\n",
    "      u2_depdir = row['u2_depdir']\n",
    "      u2_func = row['u2_func']\n",
    "      u1_position = row['u1_position']\n",
    "      u2_position = row['u2_position']\n",
    "      sat_children = row['sat_children']\n",
    "      nuc_children = row['nuc_children']\n",
    "      features = [distance, u1_depdir, u2_depdir, u2_func, u1_position, u2_position, sat_children, nuc_children]\n",
    "\n",
    "      #old----------------------------------------\n",
    "      premise, hypothesis = self.add_directionality(premise, hypothesis, dir)\n",
    "      premise_id = self.tokenizer.encode(premise, add_special_tokens = False, max_length=MAX_LEN, truncation=True)\n",
    "      hypothesis_id = self.tokenizer.encode(hypothesis, add_special_tokens = False, max_length=MAX_LEN, truncation=True)\n",
    "      # pair_token_ids = [self.tokenizer.cls_token_id] + premise_id + [self.tokenizer.sep_token_id] + hypothesis_id + [self.tokenizer.sep_token_id]\n",
    "      pair_token_ids = self.tokenizer.encode(premise, hypothesis, add_special_tokens = True, max_length=MAX_LEN*2, truncation=True)\n",
    "      \n",
    "      premise_len = len(premise_id)\n",
    "      hypothesis_len = len(hypothesis_id)\n",
    "\n",
    "      segment_ids = torch.tensor([0] * (premise_len + 2) + [1] * (hypothesis_len + 1))  # sentence 0 and sentence 1\n",
    "      attention_mask_ids = torch.tensor([1] * (premise_len + hypothesis_len + 3))  # mask padded values\n",
    "\n",
    "      #new---------------------------------------\n",
    "      # encoded = self.tokenizer.encode_plus(premise, hypothesis, add_special_tokens = True, max_length=MAX_LEN*2, truncation=True, padding='max_length')\n",
    "      # pair_token_ids = torch.tensor(encoded['input_ids'])\n",
    "      # segment_ids = torch.tensor(encoded['token_type_ids'])\n",
    "      # attention_mask_ids = torch.tensor(encoded['attention_mask'])\n",
    "      #new--------------------------------------\n",
    "\n",
    "      token_ids.append(torch.tensor(pair_token_ids))\n",
    "      seg_ids.append(segment_ids)\n",
    "      mask_ids.append(attention_mask_ids)\n",
    "      y.append(self.label_dict[label])\n",
    "      feats.append(self.get_feature(features))\n",
    "\n",
    "      idx_map[count] = [premise, hypothesis]\n",
    "      idx.append(count)\n",
    "      count+=1\n",
    "    \n",
    "    # token_ids = torch.stack(token_ids)\n",
    "    # mask_ids = torch.stack(mask_ids)\n",
    "    # seg_ids = torch.stack(seg_ids)\n",
    "\n",
    "    token_ids = pad_sequence(token_ids, batch_first=True)\n",
    "    mask_ids = pad_sequence(mask_ids, batch_first=True)\n",
    "    seg_ids = pad_sequence(seg_ids, batch_first=True)\n",
    "\n",
    "    y = torch.tensor(y)\n",
    "    idx = torch.tensor(idx)\n",
    "    feats = torch.tensor(feats)\n",
    "    dataset = TensorDataset(token_ids, mask_ids, seg_ids, feats, y, idx)\n",
    "    return dataset, idx_map\n",
    "\n",
    "  def get_data_loaders(self, batch_size=4, batches_per_epoch=402, shuffle=True): #1609 samples / 64:25=1600 / 402:4=1608\n",
    "    train_loader_torch = DataLoader(\n",
    "      self.train_data,\n",
    "      shuffle=shuffle,\n",
    "      batch_size=batch_size,\n",
    "    )\n",
    "\n",
    "    val_loader_torch = DataLoader(\n",
    "      self.val_data,\n",
    "      shuffle=shuffle,\n",
    "      batch_size=batch_size,\n",
    "    )\n",
    "\n",
    "    test_loader_torch = DataLoader(\n",
    "      self.test_data,\n",
    "      shuffle=False,#shuffle,\n",
    "      batch_size=batch_size,\n",
    "    )\n",
    "    \n",
    "    train_loader = LoaderWrapper(train_loader_torch, n_step=batches_per_epoch)\n",
    "    val_loader = LoaderWrapper(val_loader_torch, n_step=batches_per_epoch)\n",
    "    test_loader = LoaderWrapper(test_loader_torch, n_step=batches_per_epoch)\n",
    "\n",
    "    return train_loader, val_loader_torch, test_loader_torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LoaderWrapper:\n",
    "    def __init__(self, loader, n_step):\n",
    "        self.step = n_step\n",
    "        self.idx = 0\n",
    "        self.iter_loader = iter(loader)\n",
    "        self.loader = loader\n",
    "    \n",
    "    def __iter__(self):\n",
    "        return self\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.step\n",
    "\n",
    "    def __next__(self):\n",
    "        # if reached number of steps desired, stop\n",
    "        if self.idx == self.step:\n",
    "            self.idx = 0\n",
    "            raise StopIteration\n",
    "        else:\n",
    "            self.idx += 1\n",
    "        # while True\n",
    "        try:\n",
    "            return next(self.iter_loader)\n",
    "        except StopIteration:\n",
    "            # reinstate iter_loader, then continue\n",
    "            self.iter_loader = iter(self.loader)\n",
    "            return next(self.iter_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "mnli_dataset = MNLIDataBert(train_df, val_df, test_df)\n",
    "\n",
    "train_loader, val_loader, test_loader = mnli_dataset.get_data_loaders(batch_size=batch_size, batches_per_epoch=batches_per_epoch) #64X250\n",
    "label_dict = mnli_dataset.label_dict # required by custom func to calculate accuracy, bert model\n",
    "rev_label_dict = mnli_dataset.rev_label_dict # required by custom func to calculate accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'list': 0, 'joint': 1, 'evaluation-n': 2, 'summary': 3, 'concession': 4, 'reason': 5, 'cause': 6, 'restatement': 7, 'condition': 8, 'interpretation': 9, 'evaluation-s': 10, 'purpose': 11, 'result': 12, 'solutionhood': 13, 'conjunction': 14, 'preparation': 15, 'background': 16, 'circumstance': 17, 'antithesis': 18, 'contrast': 19, 'sequence': 20, 'evidence': 21, 'means': 22, 'disjunction': 23, 'elaboration': 24, 'e-elaboration': 25}\n"
     ]
    }
   ],
   "source": [
    "print(label_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## load pretrained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertForSequenceClassification, AdamW\n",
    "from torch import optim\n",
    "\n",
    "# model = BertForSequenceClassification.from_pretrained(BERT_MODEL, num_labels=len(label_dict)).to(device)\n",
    "# optimizer = AdamW(model.parameters(), lr=2e-5, correct_bias=False)\n",
    "# scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, factor=0.6, mode='max', patience=2, min_lr=5e-7, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.3333333333333333\n",
      "1.0\n"
     ]
    }
   ],
   "source": [
    "from CategoricalAccuracy import CategoricalAccuracy as CA\n",
    "import numpy as np\n",
    "\n",
    "ca = CA()\n",
    "\n",
    "x = torch.tensor(np.array([[[1,0,0], [1,0,0], [1,0,0]]]))\n",
    "y1 = torch.tensor(np.array([[0], [1], [1]]))\n",
    "y2 = torch.tensor(np.array([[0], [0], [0]]))\n",
    "\n",
    "ca(x,y1)\n",
    "print(ca.get_metric(reset=True))\n",
    "ca(x,y2)\n",
    "print(ca.get_metric(reset=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## define evaulation metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to evaluate model for train and test. And also use classification report for testing\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# helper function to calculate the batch accuracy\n",
    "def multi_acc(y_pred, y_test, allennlp=False):\n",
    "  if allennlp==False:\n",
    "    acc = (torch.log_softmax(y_pred, dim=1).argmax(dim=1) == y_test).sum().float() / float(y_test.size(0))\n",
    "  elif allennlp==True:\n",
    "    ca(y_pred,y_test)\n",
    "    y_pred = F.one_hot(y_pred.argmax(dim=1), num_classes=mnli_dataset.num_labels)\n",
    "    acc = torch.tensor(ca.get_metric(reset=True))\n",
    "  return acc\n",
    "\n",
    "def save_cm(y_true, y_pred, target_names, display_zeros=True):\n",
    "  confusion_matrix = pd.crosstab(y_true, y_pred)\n",
    "  print(confusion_matrix)\n",
    "  target_namesx = confusion_matrix.columns\n",
    "  target_namesy = confusion_matrix.index.values\n",
    "\n",
    "  fig, ax = plt.subplots(figsize=(10,10))\n",
    "  s = sns.heatmap(confusion_matrix, xticklabels=target_namesx, yticklabels=target_namesy, annot=True, fmt = '.5g')\n",
    "\n",
    "  plt.title('CM predicted v actual values')\n",
    "  plt.xlabel('Pred')\n",
    "  plt.ylabel('True')\n",
    "  plt.tight_layout()\n",
    "  # plt.show()\n",
    "  # plt.savefig(image_file+exp+'.png')\n",
    "  # print(image_file+exp+'.png')\n",
    "\n",
    "# freeze model weights and measure validation / test \n",
    "def evaluate_accuracy(model, optimizer, data_loader, rev_label_dict, label_dict, save_path, is_training=True):\n",
    "  model.eval()\n",
    "  total_val_acc  = 0\n",
    "  total_val_loss = 0\n",
    "  \n",
    "  #for classification report\n",
    "  y_true = []\n",
    "  y_pred = []\n",
    "  idx_list = []\n",
    "  premise_list = []\n",
    "  hypo_list = []\n",
    "  idx_map = mnli_dataset.val_idx if is_training else mnli_dataset.test_idx\n",
    "\n",
    "  with torch.no_grad():\n",
    "    for batch_idx, (pair_token_ids, mask_ids, seg_ids, feat, y, idx) in enumerate(data_loader):      \n",
    "      optimizer.zero_grad()\n",
    "      pair_token_ids = pair_token_ids.to(device)\n",
    "      mask_ids = mask_ids.to(device)\n",
    "      seg_ids = seg_ids.to(device)\n",
    "      labels = y.to(device)\n",
    "      feat = feat.to(device)\n",
    "      \n",
    "      # loss, prediction = model(pair_token_ids, \n",
    "      #                       token_type_ids=seg_ids, \n",
    "      #                       attention_mask=mask_ids, \n",
    "      #                       labels=labels).values()\n",
    "      # acc = multi_acc(prediction, labels)\n",
    "\n",
    "      ############new code#####################\n",
    "      outputs = model(pair_token_ids, \n",
    "                            token_type_ids=seg_ids, \n",
    "                            attention_mask=mask_ids,#['logits']\n",
    "                            feat=feat)\n",
    "      # act1 = nn.Softmax()\n",
    "      # outputs = act1(outputs)\n",
    "      # probs = F.softmax(outputs, dim=1)\n",
    "      # max_idx = torch.max(outputs, 1).indices\n",
    "      # one_hot = F.one_hot(max_idx, outputs.shape[1])\n",
    "\n",
    "      criterion = nn.CrossEntropyLoss()\n",
    "      loss = criterion(outputs, labels)\n",
    "      acc = multi_acc(outputs, labels)\n",
    "      ########################################\n",
    "\n",
    "      total_val_loss += loss.item()\n",
    "      total_val_acc  += acc.item()\n",
    "\n",
    "      # log predictions for classification report\n",
    "      argmax_predictions = torch.argmax(outputs,dim=1).tolist()\n",
    "      labels_list = labels.tolist()\n",
    "      assert(len(labels_list)==len(argmax_predictions))\n",
    "      for p in argmax_predictions: y_pred.append(rev_label_dict[int(p)])\n",
    "      for l in labels_list: y_true.append(rev_label_dict[l])\n",
    "      for i in idx.tolist():\n",
    "        idx_list.append(i)\n",
    "        premise_list.append(idx_map[i][0])\n",
    "        hypo_list.append(idx_map[i][1])\n",
    "\n",
    "  val_acc  = total_val_acc/len(data_loader)\n",
    "  val_loss = total_val_loss/len(data_loader)\n",
    "  cr = classification_report(y_true, y_pred)\n",
    "\n",
    "  idx_json = {'idx': idx_list, 'gold_label': y_true, 'pred_label': y_pred, 'premise': premise_list, 'hypothesis': hypo_list}\n",
    "  \n",
    "  return val_acc, val_loss, cr, model, optimizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## define bert custom model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ASSIGN: 26\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-german-cased were not used when initializing BertModel: ['cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "/home/VD/kaveri/anaconda3/envs/py310/lib/python3.10/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertModel, AutoTokenizer\n",
    "import torch.nn as nn\n",
    "class CustomBERTModel(nn.Module):\n",
    "    #https://stackoverflow.com/questions/64156202/add-dense-layer-on-top-of-huggingface-bert-model\n",
    "    def __init__(self, num_labels):\n",
    "          super(CustomBERTModel, self).__init__()\n",
    "          self.num_classes = num_labels # zero indexed classes\n",
    "          print('ASSIGN:', self.num_classes)\n",
    "          self.bert = BertModel.from_pretrained(BERT_MODEL)\n",
    "          ### New layers:\n",
    "          self.linear1 = nn.Linear(768, self.num_classes)#776\n",
    "        #   self.linear2 = nn.Linear(512, 256)\n",
    "        #   self.linear3 = nn.Linear(256, 128)\n",
    "        #   self.linear4 = nn.Linear(128, self.num_classes)\n",
    "          self.act1 = nn.ReLU() # can i use the same activation object everywhere?\n",
    "        #   self.act2 = nn.ReLU()\n",
    "        #   self.act3 = nn.ReLU()\n",
    "          # self.drop = nn.Dropout(0.1) \n",
    "\n",
    "    def forward(self, pair_token_ids, token_type_ids, attention_mask, feat):\n",
    "        sequence_output, pooled_output = self.bert(input_ids=pair_token_ids, \n",
    "                        token_type_ids=token_type_ids, \n",
    "                        attention_mask=attention_mask).values()\n",
    "\n",
    "        # feat_concat = torch.concat((sequence_output[:,0,:].view(-1,768), feat),-1)\n",
    "\n",
    "        # sequence_output has the following shape: (batch_size, sequence_length, 768)\n",
    "        # linear1_output = self.linear1(feat_concat) ## extract the 1st token's embeddings\n",
    "        linear1_output = self.linear1(sequence_output[:,0,:].view(-1,768)) ## extract the 1st token's embeddings\n",
    "        linear1_output = self.act1(linear1_output)\n",
    "        # linear2_output = self.linear2(linear1_output)\n",
    "        # linear2_output = self.act2(linear2_output)\n",
    "        # linear3_output = self.linear3(linear2_output)\n",
    "        # linear3_output = self.act3(linear3_output)\n",
    "        # linear4_output = self.linear4(linear3_output)\n",
    "        # drop_output = self.drop(linear4_output)\n",
    "        return linear1_output# loss, outputs\n",
    "\n",
    "# tokenizer = AutoTokenizer.from_pretrained(BERT_MODEL)\n",
    "model = CustomBERTModel(mnli_dataset.num_labels)\n",
    "# model = BertForSequenceClassification.from_pretrained(BERT_MODEL, num_labels=mnli_dataset.num_labels)\n",
    "model.to(device) ## can be gpu\n",
    "optimizer = AdamW(model.parameters(), lr=5e-5, correct_bias=False)#2e-5\n",
    "scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, factor=0.6, mode='max', patience=6, min_lr=5e-7, verbose=True)#factor=0.6"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## define training regime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CustomBERTModel(\n",
      "  (bert): BertModel(\n",
      "    (embeddings): BertEmbeddings(\n",
      "      (word_embeddings): Embedding(30000, 768, padding_idx=0)\n",
      "      (position_embeddings): Embedding(512, 768)\n",
      "      (token_type_embeddings): Embedding(2, 768)\n",
      "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "    (encoder): BertEncoder(\n",
      "      (layer): ModuleList(\n",
      "        (0): BertLayer(\n",
      "          (attention): BertAttention(\n",
      "            (self): BertSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): BertSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): BertIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            (intermediate_act_fn): GELUActivation()\n",
      "          )\n",
      "          (output): BertOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (1): BertLayer(\n",
      "          (attention): BertAttention(\n",
      "            (self): BertSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): BertSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): BertIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            (intermediate_act_fn): GELUActivation()\n",
      "          )\n",
      "          (output): BertOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (2): BertLayer(\n",
      "          (attention): BertAttention(\n",
      "            (self): BertSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): BertSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): BertIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            (intermediate_act_fn): GELUActivation()\n",
      "          )\n",
      "          (output): BertOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (3): BertLayer(\n",
      "          (attention): BertAttention(\n",
      "            (self): BertSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): BertSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): BertIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            (intermediate_act_fn): GELUActivation()\n",
      "          )\n",
      "          (output): BertOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (4): BertLayer(\n",
      "          (attention): BertAttention(\n",
      "            (self): BertSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): BertSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): BertIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            (intermediate_act_fn): GELUActivation()\n",
      "          )\n",
      "          (output): BertOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (5): BertLayer(\n",
      "          (attention): BertAttention(\n",
      "            (self): BertSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): BertSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): BertIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            (intermediate_act_fn): GELUActivation()\n",
      "          )\n",
      "          (output): BertOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (6): BertLayer(\n",
      "          (attention): BertAttention(\n",
      "            (self): BertSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): BertSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): BertIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            (intermediate_act_fn): GELUActivation()\n",
      "          )\n",
      "          (output): BertOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (7): BertLayer(\n",
      "          (attention): BertAttention(\n",
      "            (self): BertSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): BertSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): BertIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            (intermediate_act_fn): GELUActivation()\n",
      "          )\n",
      "          (output): BertOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (8): BertLayer(\n",
      "          (attention): BertAttention(\n",
      "            (self): BertSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): BertSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): BertIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            (intermediate_act_fn): GELUActivation()\n",
      "          )\n",
      "          (output): BertOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (9): BertLayer(\n",
      "          (attention): BertAttention(\n",
      "            (self): BertSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): BertSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): BertIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            (intermediate_act_fn): GELUActivation()\n",
      "          )\n",
      "          (output): BertOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (10): BertLayer(\n",
      "          (attention): BertAttention(\n",
      "            (self): BertSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): BertSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): BertIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            (intermediate_act_fn): GELUActivation()\n",
      "          )\n",
      "          (output): BertOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (11): BertLayer(\n",
      "          (attention): BertAttention(\n",
      "            (self): BertSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): BertSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): BertIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            (intermediate_act_fn): GELUActivation()\n",
      "          )\n",
      "          (output): BertOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (pooler): BertPooler(\n",
      "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "      (activation): Tanh()\n",
      "    )\n",
      "  )\n",
      "  (linear1): Linear(in_features=768, out_features=26, bias=True)\n",
      "  (act1): ReLU()\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "### MODIFIED\n",
    "import time\n",
    "import traceback\n",
    "import torch.nn.functional as F\n",
    "from typing import Optional, Iterable, Dict, Any\n",
    "from EarlyStopperUtil import MetricTracker\n",
    "\n",
    "\n",
    "from sklearn.metrics import classification_report\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "EPOCHS = 100\n",
    "\n",
    "def train(model, train_loader, val_loader, optimizer, scheduler, rev_label_dict):  \n",
    "  writer = SummaryWriter()\n",
    "  \n",
    "  EarlyStopper = MetricTracker(patience=12, metric_name='+accuracy')\n",
    "\n",
    "  for epoch in range(EPOCHS):\n",
    "    start = time.time()\n",
    "    model.train()\n",
    "    total_train_loss = 0\n",
    "    total_train_acc  = 0\n",
    "\n",
    "    # logging for scheduler\n",
    "    losses = []\n",
    "    accuracies= []\n",
    "\n",
    "    train_size = 0\n",
    "\n",
    "    for batch_idx, (pair_token_ids, mask_ids, seg_ids, feat, y, idx) in enumerate(train_loader):\n",
    "      train_size+=1\n",
    "      optimizer.zero_grad()\n",
    "      pair_token_ids = pair_token_ids.to(device)\n",
    "      mask_ids = mask_ids.to(device)\n",
    "      seg_ids = seg_ids.to(device)\n",
    "      labels = y.to(device)\n",
    "      feat = feat.to(device)\n",
    "\n",
    "      ############new code#####################\n",
    "      # assert pair_token_ids.shape==(batch_size, 512) and seg_ids.shape==(batch_size, 512) and mask_ids.shape==(batch_size, 512)\n",
    "      outputs = model(pair_token_ids, \n",
    "                            token_type_ids=seg_ids, \n",
    "                            attention_mask=mask_ids,#['logits']\n",
    "                            feat=feat)\n",
    "      # act1 = nn.Softmax()\n",
    "      # outputs = act1(outputs)\n",
    "      # outputs = model(pair_token_ids, \n",
    "      #                       token_type_ids=seg_ids, \n",
    "      #                       attention_mask=mask_ids,\n",
    "      #                       feat=feat)\n",
    "      # print(outputs)\n",
    "      # outputs = F.log_softmax(outputs, dim=1) # log prob\n",
    "      # outputs = snp.argmax(prob, axis=1) # preds\n",
    "      # https://stackoverflow.com/questions/43672047/convert-probability-vector-into-target-vector-in-python\n",
    "      # https://pytorch.org/tutorials/beginner/blitz/cifar10_tutorial.html\n",
    "      criterion = nn.CrossEntropyLoss()\n",
    "      loss = criterion(outputs, labels)\n",
    "      loss.backward()\n",
    "      acc = multi_acc(outputs, labels, allennlp=True)\n",
    "      optimizer.step()\n",
    "      ################old code#################\n",
    "\n",
    "      # loss, prediction = model(pair_token_ids, \n",
    "      #                       token_type_ids=seg_ids, \n",
    "      #                       attention_mask=mask_ids, \n",
    "      #                       labels=labels).values()\n",
    "\n",
    "      # acc = multi_acc(prediction, labels)\n",
    "      # loss.backward()\n",
    "      # optimizer.step()\n",
    "\n",
    "      ########################################\n",
    "      total_train_loss += loss.item()\n",
    "      total_train_acc  += acc.item()\n",
    "\n",
    "      # log losses for scheduler\n",
    "      losses.append(loss)\n",
    "      accuracies.append(acc)\n",
    "    \n",
    "    mean_loss = sum(losses)/len(losses)\n",
    "    scheduler.step(mean_loss)\n",
    "\n",
    "    train_acc  = total_train_acc/len(train_loader)\n",
    "    train_loss = total_train_loss/len(train_loader)\n",
    "\n",
    "    val_acc, val_loss, cr, model, optimizer = evaluate_accuracy(model, optimizer, val_loader, rev_label_dict, label_dict, None)\n",
    "    EarlyStopper.add_metric(val_acc)\n",
    "    if EarlyStopper.should_stop_early(): break\n",
    "\n",
    "    writer.add_scalar('Loss/train', train_loss, epoch)\n",
    "    writer.add_scalar('Loss/val', val_loss, epoch)\n",
    "    writer.add_scalar('Accuracy/train', train_acc, epoch)\n",
    "    writer.add_scalar('Accuracy/val', val_acc, epoch)\n",
    "\n",
    "    end = time.time()\n",
    "    hours, rem = divmod(end-start, 3600)\n",
    "    minutes, seconds = divmod(rem, 60)\n",
    "\n",
    "    print(f'Epoch {epoch+1}: train_loss: {train_loss:.4f} train_acc: {train_acc:.4f} | val_loss: {val_loss:.4f} val_acc: {val_acc:.4f}')\n",
    "    print(\"{:0>2}:{:0>2}:{:05.2f}\".format(int(hours),int(minutes),seconds))\n",
    "    print(f'train_size: {train_size}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: train_loss: 3.2621 train_acc: 0.0767 | val_loss: 3.2581 val_acc: 0.0738\n",
      "00:00:30.98\n",
      "train_size: 541\n",
      "Epoch 2: train_loss: 3.2581 train_acc: 0.0790 | val_loss: 3.2581 val_acc: 0.0738\n",
      "00:00:31.19\n",
      "train_size: 541\n",
      "Epoch 3: train_loss: 3.2581 train_acc: 0.0790 | val_loss: 3.2581 val_acc: 0.0738\n",
      "00:00:31.44\n",
      "train_size: 541\n",
      "Epoch 4: train_loss: 3.2581 train_acc: 0.0790 | val_loss: 3.2581 val_acc: 0.0738\n",
      "00:00:31.92\n",
      "train_size: 541\n",
      "Epoch 5: train_loss: 3.2581 train_acc: 0.0790 | val_loss: 3.2581 val_acc: 0.0738\n",
      "00:00:31.43\n",
      "train_size: 541\n",
      "Epoch 6: train_loss: 3.2581 train_acc: 0.0790 | val_loss: 3.2581 val_acc: 0.0738\n",
      "00:00:30.58\n",
      "train_size: 541\n",
      "Epoch 7: train_loss: 3.2581 train_acc: 0.0790 | val_loss: 3.2581 val_acc: 0.0861\n",
      "00:00:31.77\n",
      "train_size: 541\n",
      "Epoch 00008: reducing learning rate of group 0 to 3.0000e-05.\n",
      "Epoch 8: train_loss: 3.2581 train_acc: 0.0790 | val_loss: 3.2581 val_acc: 0.0738\n",
      "00:00:32.38\n",
      "train_size: 541\n",
      "Epoch 9: train_loss: 3.2581 train_acc: 0.0790 | val_loss: 3.2581 val_acc: 0.0738\n",
      "00:00:33.37\n",
      "train_size: 541\n",
      "Epoch 10: train_loss: 3.2581 train_acc: 0.0790 | val_loss: 3.2581 val_acc: 0.0738\n",
      "00:00:32.57\n",
      "train_size: 541\n",
      "Epoch 11: train_loss: 3.2581 train_acc: 0.0790 | val_loss: 3.2581 val_acc: 0.0738\n",
      "00:00:33.54\n",
      "train_size: 541\n",
      "Epoch 12: train_loss: 3.2581 train_acc: 0.0790 | val_loss: 3.2581 val_acc: 0.0738\n",
      "00:00:32.32\n",
      "train_size: 541\n",
      "Epoch 13: train_loss: 3.2581 train_acc: 0.0790 | val_loss: 3.2581 val_acc: 0.0738\n",
      "00:00:32.76\n",
      "train_size: 541\n",
      "Epoch 14: train_loss: 3.2581 train_acc: 0.0790 | val_loss: 3.2581 val_acc: 0.0861\n",
      "00:00:31.20\n",
      "train_size: 541\n",
      "Epoch 00015: reducing learning rate of group 0 to 1.8000e-05.\n",
      "Epoch 15: train_loss: 3.2581 train_acc: 0.0790 | val_loss: 3.2581 val_acc: 0.0738\n",
      "00:00:32.16\n",
      "train_size: 541\n",
      "Epoch 16: train_loss: 3.2581 train_acc: 0.0790 | val_loss: 3.2581 val_acc: 0.0738\n",
      "00:00:31.75\n",
      "train_size: 541\n",
      "Epoch 17: train_loss: 3.2581 train_acc: 0.0790 | val_loss: 3.2581 val_acc: 0.0738\n",
      "00:00:31.23\n",
      "train_size: 541\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/home/VD/kaveri/bert_categorical_tutorial/allennlp_repro/disrpt_alln/analytics_vidhya_snli_discodisco_bertwithlayer_allennlp_german_debug_feats.ipynb Cell 26\u001b[0m in \u001b[0;36m<cell line: 3>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell://attached-container%2B7b22636f6e7461696e65724e616d65223a222f636f6c695f6b61766572695f74657374222c2273657474696e6773223a7b22686f7374223a227373683a2f2f6d795f746f6e795f736572766572227d7d/home/VD/kaveri/bert_categorical_tutorial/allennlp_repro/disrpt_alln/analytics_vidhya_snli_discodisco_bertwithlayer_allennlp_german_debug_feats.ipynb#X34sdnNjb2RlLXJlbW90ZQ%3D%3D?line=2'>3</a>\u001b[0m \u001b[39mwith\u001b[39;00m warnings\u001b[39m.\u001b[39mcatch_warnings():\n\u001b[1;32m      <a href='vscode-notebook-cell://attached-container%2B7b22636f6e7461696e65724e616d65223a222f636f6c695f6b61766572695f74657374222c2273657474696e6773223a7b22686f7374223a227373683a2f2f6d795f746f6e795f736572766572227d7d/home/VD/kaveri/bert_categorical_tutorial/allennlp_repro/disrpt_alln/analytics_vidhya_snli_discodisco_bertwithlayer_allennlp_german_debug_feats.ipynb#X34sdnNjb2RlLXJlbW90ZQ%3D%3D?line=3'>4</a>\u001b[0m     warnings\u001b[39m.\u001b[39msimplefilter(\u001b[39m\"\u001b[39m\u001b[39mignore\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m----> <a href='vscode-notebook-cell://attached-container%2B7b22636f6e7461696e65724e616d65223a222f636f6c695f6b61766572695f74657374222c2273657474696e6773223a7b22686f7374223a227373683a2f2f6d795f746f6e795f736572766572227d7d/home/VD/kaveri/bert_categorical_tutorial/allennlp_repro/disrpt_alln/analytics_vidhya_snli_discodisco_bertwithlayer_allennlp_german_debug_feats.ipynb#X34sdnNjb2RlLXJlbW90ZQ%3D%3D?line=5'>6</a>\u001b[0m     train(model, train_loader, val_loader, optimizer, scheduler, rev_label_dict)\n",
      "\u001b[1;32m/home/VD/kaveri/bert_categorical_tutorial/allennlp_repro/disrpt_alln/analytics_vidhya_snli_discodisco_bertwithlayer_allennlp_german_debug_feats.ipynb Cell 26\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(model, train_loader, val_loader, optimizer, scheduler, rev_label_dict)\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell://attached-container%2B7b22636f6e7461696e65724e616d65223a222f636f6c695f6b61766572695f74657374222c2273657474696e6773223a7b22686f7374223a227373683a2f2f6d795f746f6e795f736572766572227d7d/home/VD/kaveri/bert_categorical_tutorial/allennlp_repro/disrpt_alln/analytics_vidhya_snli_discodisco_bertwithlayer_allennlp_german_debug_feats.ipynb#X34sdnNjb2RlLXJlbW90ZQ%3D%3D?line=57'>58</a>\u001b[0m loss \u001b[39m=\u001b[39m criterion(outputs, labels)\n\u001b[1;32m     <a href='vscode-notebook-cell://attached-container%2B7b22636f6e7461696e65724e616d65223a222f636f6c695f6b61766572695f74657374222c2273657474696e6773223a7b22686f7374223a227373683a2f2f6d795f746f6e795f736572766572227d7d/home/VD/kaveri/bert_categorical_tutorial/allennlp_repro/disrpt_alln/analytics_vidhya_snli_discodisco_bertwithlayer_allennlp_german_debug_feats.ipynb#X34sdnNjb2RlLXJlbW90ZQ%3D%3D?line=58'>59</a>\u001b[0m loss\u001b[39m.\u001b[39mbackward()\n\u001b[0;32m---> <a href='vscode-notebook-cell://attached-container%2B7b22636f6e7461696e65724e616d65223a222f636f6c695f6b61766572695f74657374222c2273657474696e6773223a7b22686f7374223a227373683a2f2f6d795f746f6e795f736572766572227d7d/home/VD/kaveri/bert_categorical_tutorial/allennlp_repro/disrpt_alln/analytics_vidhya_snli_discodisco_bertwithlayer_allennlp_german_debug_feats.ipynb#X34sdnNjb2RlLXJlbW90ZQ%3D%3D?line=59'>60</a>\u001b[0m acc \u001b[39m=\u001b[39m multi_acc(outputs, labels, allennlp\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n\u001b[1;32m     <a href='vscode-notebook-cell://attached-container%2B7b22636f6e7461696e65724e616d65223a222f636f6c695f6b61766572695f74657374222c2273657474696e6773223a7b22686f7374223a227373683a2f2f6d795f746f6e795f736572766572227d7d/home/VD/kaveri/bert_categorical_tutorial/allennlp_repro/disrpt_alln/analytics_vidhya_snli_discodisco_bertwithlayer_allennlp_german_debug_feats.ipynb#X34sdnNjb2RlLXJlbW90ZQ%3D%3D?line=60'>61</a>\u001b[0m optimizer\u001b[39m.\u001b[39mstep()\n\u001b[1;32m     <a href='vscode-notebook-cell://attached-container%2B7b22636f6e7461696e65724e616d65223a222f636f6c695f6b61766572695f74657374222c2273657474696e6773223a7b22686f7374223a227373683a2f2f6d795f746f6e795f736572766572227d7d/home/VD/kaveri/bert_categorical_tutorial/allennlp_repro/disrpt_alln/analytics_vidhya_snli_discodisco_bertwithlayer_allennlp_german_debug_feats.ipynb#X34sdnNjb2RlLXJlbW90ZQ%3D%3D?line=61'>62</a>\u001b[0m \u001b[39m################old code#################\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell://attached-container%2B7b22636f6e7461696e65724e616d65223a222f636f6c695f6b61766572695f74657374222c2273657474696e6773223a7b22686f7374223a227373683a2f2f6d795f746f6e795f736572766572227d7d/home/VD/kaveri/bert_categorical_tutorial/allennlp_repro/disrpt_alln/analytics_vidhya_snli_discodisco_bertwithlayer_allennlp_german_debug_feats.ipynb#X34sdnNjb2RlLXJlbW90ZQ%3D%3D?line=62'>63</a>\u001b[0m \n\u001b[1;32m     <a href='vscode-notebook-cell://attached-container%2B7b22636f6e7461696e65724e616d65223a222f636f6c695f6b61766572695f74657374222c2273657474696e6773223a7b22686f7374223a227373683a2f2f6d795f746f6e795f736572766572227d7d/home/VD/kaveri/bert_categorical_tutorial/allennlp_repro/disrpt_alln/analytics_vidhya_snli_discodisco_bertwithlayer_allennlp_german_debug_feats.ipynb#X34sdnNjb2RlLXJlbW90ZQ%3D%3D?line=63'>64</a>\u001b[0m \u001b[39m# loss, prediction = model(pair_token_ids, \u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell://attached-container%2B7b22636f6e7461696e65724e616d65223a222f636f6c695f6b61766572695f74657374222c2273657474696e6773223a7b22686f7374223a227373683a2f2f6d795f746f6e795f736572766572227d7d/home/VD/kaveri/bert_categorical_tutorial/allennlp_repro/disrpt_alln/analytics_vidhya_snli_discodisco_bertwithlayer_allennlp_german_debug_feats.ipynb#X34sdnNjb2RlLXJlbW90ZQ%3D%3D?line=71'>72</a>\u001b[0m \n\u001b[1;32m     <a href='vscode-notebook-cell://attached-container%2B7b22636f6e7461696e65724e616d65223a222f636f6c695f6b61766572695f74657374222c2273657474696e6773223a7b22686f7374223a227373683a2f2f6d795f746f6e795f736572766572227d7d/home/VD/kaveri/bert_categorical_tutorial/allennlp_repro/disrpt_alln/analytics_vidhya_snli_discodisco_bertwithlayer_allennlp_german_debug_feats.ipynb#X34sdnNjb2RlLXJlbW90ZQ%3D%3D?line=72'>73</a>\u001b[0m \u001b[39m########################################\u001b[39;00m\n",
      "\u001b[1;32m/home/VD/kaveri/bert_categorical_tutorial/allennlp_repro/disrpt_alln/analytics_vidhya_snli_discodisco_bertwithlayer_allennlp_german_debug_feats.ipynb Cell 26\u001b[0m in \u001b[0;36mmulti_acc\u001b[0;34m(y_pred, y_test, allennlp)\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell://attached-container%2B7b22636f6e7461696e65724e616d65223a222f636f6c695f6b61766572695f74657374222c2273657474696e6773223a7b22686f7374223a227373683a2f2f6d795f746f6e795f736572766572227d7d/home/VD/kaveri/bert_categorical_tutorial/allennlp_repro/disrpt_alln/analytics_vidhya_snli_discodisco_bertwithlayer_allennlp_german_debug_feats.ipynb#X34sdnNjb2RlLXJlbW90ZQ%3D%3D?line=9'>10</a>\u001b[0m   acc \u001b[39m=\u001b[39m (torch\u001b[39m.\u001b[39mlog_softmax(y_pred, dim\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m)\u001b[39m.\u001b[39margmax(dim\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m) \u001b[39m==\u001b[39m y_test)\u001b[39m.\u001b[39msum()\u001b[39m.\u001b[39mfloat() \u001b[39m/\u001b[39m \u001b[39mfloat\u001b[39m(y_test\u001b[39m.\u001b[39msize(\u001b[39m0\u001b[39m))\n\u001b[1;32m     <a href='vscode-notebook-cell://attached-container%2B7b22636f6e7461696e65724e616d65223a222f636f6c695f6b61766572695f74657374222c2273657474696e6773223a7b22686f7374223a227373683a2f2f6d795f746f6e795f736572766572227d7d/home/VD/kaveri/bert_categorical_tutorial/allennlp_repro/disrpt_alln/analytics_vidhya_snli_discodisco_bertwithlayer_allennlp_german_debug_feats.ipynb#X34sdnNjb2RlLXJlbW90ZQ%3D%3D?line=10'>11</a>\u001b[0m \u001b[39melif\u001b[39;00m allennlp\u001b[39m==\u001b[39m\u001b[39mTrue\u001b[39;00m:\n\u001b[0;32m---> <a href='vscode-notebook-cell://attached-container%2B7b22636f6e7461696e65724e616d65223a222f636f6c695f6b61766572695f74657374222c2273657474696e6773223a7b22686f7374223a227373683a2f2f6d795f746f6e795f736572766572227d7d/home/VD/kaveri/bert_categorical_tutorial/allennlp_repro/disrpt_alln/analytics_vidhya_snli_discodisco_bertwithlayer_allennlp_german_debug_feats.ipynb#X34sdnNjb2RlLXJlbW90ZQ%3D%3D?line=11'>12</a>\u001b[0m   ca(y_pred,y_test)\n\u001b[1;32m     <a href='vscode-notebook-cell://attached-container%2B7b22636f6e7461696e65724e616d65223a222f636f6c695f6b61766572695f74657374222c2273657474696e6773223a7b22686f7374223a227373683a2f2f6d795f746f6e795f736572766572227d7d/home/VD/kaveri/bert_categorical_tutorial/allennlp_repro/disrpt_alln/analytics_vidhya_snli_discodisco_bertwithlayer_allennlp_german_debug_feats.ipynb#X34sdnNjb2RlLXJlbW90ZQ%3D%3D?line=12'>13</a>\u001b[0m   y_pred \u001b[39m=\u001b[39m F\u001b[39m.\u001b[39mone_hot(y_pred\u001b[39m.\u001b[39margmax(dim\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m), num_classes\u001b[39m=\u001b[39mmnli_dataset\u001b[39m.\u001b[39mnum_labels)\n\u001b[1;32m     <a href='vscode-notebook-cell://attached-container%2B7b22636f6e7461696e65724e616d65223a222f636f6c695f6b61766572695f74657374222c2273657474696e6773223a7b22686f7374223a227373683a2f2f6d795f746f6e795f736572766572227d7d/home/VD/kaveri/bert_categorical_tutorial/allennlp_repro/disrpt_alln/analytics_vidhya_snli_discodisco_bertwithlayer_allennlp_german_debug_feats.ipynb#X34sdnNjb2RlLXJlbW90ZQ%3D%3D?line=13'>14</a>\u001b[0m   acc \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mtensor(ca\u001b[39m.\u001b[39mget_metric(reset\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m))\n",
      "File \u001b[0;32m/home/VD/kaveri/bert_categorical_tutorial/allennlp_repro/disrpt_alln/CategoricalAccuracy.py:60\u001b[0m, in \u001b[0;36mCategoricalAccuracy.__call__\u001b[0;34m(self, predictions, gold_labels, mask)\u001b[0m\n\u001b[1;32m     55\u001b[0m \u001b[39mif\u001b[39;00m gold_labels\u001b[39m.\u001b[39mdim() \u001b[39m!=\u001b[39m predictions\u001b[39m.\u001b[39mdim() \u001b[39m-\u001b[39m \u001b[39m1\u001b[39m:\n\u001b[1;32m     56\u001b[0m     \u001b[39mraise\u001b[39;00m ConfigurationError(\n\u001b[1;32m     57\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mgold_labels must have dimension == predictions.size() - 1 but \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m     58\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mfound tensor of shape: \u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(predictions\u001b[39m.\u001b[39msize())\n\u001b[1;32m     59\u001b[0m     )\n\u001b[0;32m---> 60\u001b[0m \u001b[39mif\u001b[39;00m (gold_labels \u001b[39m>\u001b[39m\u001b[39m=\u001b[39m num_classes)\u001b[39m.\u001b[39many():\n\u001b[1;32m     61\u001b[0m     \u001b[39mraise\u001b[39;00m ConfigurationError(\n\u001b[1;32m     62\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mA gold label passed to Categorical Accuracy contains an id >= \u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m, \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m     63\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mthe number of classes.\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(num_classes)\n\u001b[1;32m     64\u001b[0m     )\n\u001b[1;32m     66\u001b[0m predictions \u001b[39m=\u001b[39m predictions\u001b[39m.\u001b[39mview((\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m, num_classes))\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "from sklearn.exceptions import DataConversionWarning\n",
    "with warnings.catch_warnings():\n",
    "    warnings.simplefilter(\"ignore\")\n",
    "\n",
    "    train(model, train_loader, val_loader, optimizer, scheduler, rev_label_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_path = 'bert-disrpt_deu_'+str(batch_size)+'X'+str(batches_per_epoch)+'.pt'\n",
    "# torch.save(model.state_dict(), save_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test_loss: 2.9749 test_acc: 0.1308\n",
      "00:00:03.14\n",
      "                precision    recall  f1-score   support\n",
      "\n",
      "    antithesis       0.00      0.00      0.00        18\n",
      "    background       0.00      0.00      0.00        17\n",
      "         cause       0.00      0.00      0.00         2\n",
      "  circumstance       0.00      0.00      0.00        15\n",
      "    concession       0.00      0.00      0.00        13\n",
      "     condition       0.00      0.00      0.00         9\n",
      "   conjunction       0.00      0.00      0.00         7\n",
      "      contrast       0.00      0.00      0.00         8\n",
      " e-elaboration       0.00      0.00      0.00        11\n",
      "   elaboration       0.00      0.00      0.00        10\n",
      "  evaluation-n       0.00      0.00      0.00         3\n",
      "  evaluation-s       0.00      0.00      0.00        17\n",
      "      evidence       0.00      0.00      0.00        10\n",
      "interpretation       0.00      0.00      0.00        12\n",
      "         joint       0.00      0.00      0.00        29\n",
      "          list       0.00      0.00      0.00        26\n",
      "         means       0.00      0.00      0.00         2\n",
      "   preparation       0.00      0.00      0.00         4\n",
      "       purpose       0.00      0.00      0.00         3\n",
      "        reason       0.13      1.00      0.23        34\n",
      "   restatement       0.00      0.00      0.00         1\n",
      "      sequence       0.00      0.00      0.00         7\n",
      "  solutionhood       0.00      0.00      0.00         1\n",
      "       summary       0.00      0.00      0.00         1\n",
      "\n",
      "      accuracy                           0.13       260\n",
      "     macro avg       0.01      0.04      0.01       260\n",
      "  weighted avg       0.02      0.13      0.03       260\n",
      "\n",
      "Test Loss: 2.975 |  Test Acc: 13.08%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/VD/kaveri/anaconda3/envs/py310/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1327: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/VD/kaveri/anaconda3/envs/py310/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1327: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/VD/kaveri/anaconda3/envs/py310/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1327: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "def validate(model, test_loader, optimizer, rev_label_dict, label_dict, save_path):\n",
    "  start = time.time()\n",
    "  test_acc, test_loss, cr, model, optimizer = evaluate_accuracy(model, optimizer, test_loader, rev_label_dict, label_dict, save_path.replace('.pt', '.json'), is_training=False)\n",
    "  end = time.time()\n",
    "  hours, rem = divmod(end-start, 3600)\n",
    "  minutes, seconds = divmod(rem, 60)\n",
    "\n",
    "  print(f'Test_loss: {test_loss:.4f} test_acc: {test_acc:.4f}')\n",
    "  print(\"{:0>2}:{:0>2}:{:05.2f}\".format(int(hours),int(minutes),seconds))\n",
    "  print(cr)\n",
    "\n",
    "  return test_loss, test_acc\n",
    "\n",
    "\n",
    "# model.load_state_dict(torch.load(save_path))\n",
    "test_loss, test_acc = validate(model, test_loader, optimizer, rev_label_dict, label_dict, save_path)\n",
    "print(f'Test Loss: {test_loss:.3f} |  Test Acc: {test_acc*100:.2f}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "e3409ea685db85227fbd9509d1b1ace14d085473eb2d57f3ba9dd0302d25f838"
  },
  "kernelspec": {
   "display_name": "Python 3.10.4 ('py310')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

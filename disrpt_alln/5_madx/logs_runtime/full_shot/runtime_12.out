-------------------------------------------------------------------
Lang1:  deu.rst.pcc    Lang2:  tur.pdtb.tdb
Saving run to:  runs/full_shot/FullShot=v4_deu.rst.pcc_tur.pdtb.tdb_lrfunc=Adafactor_lr=2e-05_plm=bert-base-multilingual-cased
Running with params: BERT_MODEL=bert-base-multilingual-cased lr=2e-05
read 2164 examples
read 241 examples
read 260 examples
read 2451 examples
read 312 examples
read 422 examples
Total prediction labels:  49
BertAdapterModel(
  (shared_parameters): ModuleDict()
  (bert): BertModel(
    (shared_parameters): ModuleDict()
    (invertible_adapters): ModuleDict(
      (de): NICECouplingBlock(
        (F): Sequential(
          (0): Linear(in_features=384, out_features=192, bias=True)
          (1): Activation_Function_Class(
            (f): NewGELUActivation()
          )
          (2): Linear(in_features=192, out_features=384, bias=True)
        )
        (G): Sequential(
          (0): Linear(in_features=384, out_features=192, bias=True)
          (1): Activation_Function_Class(
            (f): NewGELUActivation()
          )
          (2): Linear(in_features=192, out_features=384, bias=True)
        )
      )
      (tr): NICECouplingBlock(
        (F): Sequential(
          (0): Linear(in_features=384, out_features=192, bias=True)
          (1): Activation_Function_Class(
            (f): NewGELUActivation()
          )
          (2): Linear(in_features=192, out_features=384, bias=True)
        )
        (G): Sequential(
          (0): Linear(in_features=384, out_features=192, bias=True)
          (1): Activation_Function_Class(
            (f): NewGELUActivation()
          )
          (2): Linear(in_features=192, out_features=384, bias=True)
        )
      )
    )
    (embeddings): BertEmbeddings(
      (word_embeddings): Embedding(119547, 768, padding_idx=0)
      (position_embeddings): Embedding(512, 768)
      (token_type_embeddings): Embedding(2, 768)
      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
      (dropout): Dropout(p=0.1, inplace=False)
    )
    (encoder): BertEncoder(
      (layer): ModuleList(
        (0): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (de): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (tr): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (1): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (de): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (tr): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (2): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (de): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (tr): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (3): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (de): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (tr): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (4): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (de): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (tr): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (5): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (de): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (tr): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (6): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (de): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (tr): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (7): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (de): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (tr): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (8): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (de): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (tr): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (9): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (de): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (tr): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (10): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (de): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (tr): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (11): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (de): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (tr): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
      )
    )
    (pooler): BertPooler(
      (dense): Linear(in_features=768, out_features=768, bias=True)
      (activation): Tanh()
    )
    (prefix_tuning): PrefixTuningPool(
      (prefix_tunings): ModuleDict()
    )
  )
  (heads): ModuleDict(
    (tr): BertStyleMaskedLMHead(
      (0): Linear(in_features=768, out_features=768, bias=True)
      (1): Activation_Function_Class(
        (f): GELUActivation()
      )
      (2): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
      (3): Linear(in_features=768, out_features=119547, bias=True)
    )
    (disrpt): ClassificationHead(
      (0): Dropout(p=0.1, inplace=False)
      (1): Linear(in_features=768, out_features=768, bias=True)
      (2): Activation_Function_Class(
        (f): Tanh()
      )
      (3): Dropout(p=0.1, inplace=False)
      (4): Linear(in_features=768, out_features=49, bias=True)
    )
  )
)
{'train@deu.rst.pcc_loss': 3.373821496963501, 'train@deu.rst.pcc_accuracy@deu.rst.pcc': 0.10859519408502773, 'train@deu.rst.pcc_f1@deu.rst.pcc': 0.020089803759054378, 'train@deu.rst.pcc_precision@deu.rst.pcc': 0.026516524511202517, 'train@deu.rst.pcc_recall@deu.rst.pcc': 0.04379627039105419, 'train@deu.rst.pcc_loss@deu.rst.pcc': 3.373821258544922, 'train@deu.rst.pcc_runtime': 26.7781, 'train@deu.rst.pcc_samples_per_second': 80.812, 'train@deu.rst.pcc_steps_per_second': 2.539, 'epoch': 1.0}
{'loss': 3.6438, 'learning_rate': 1.8333333333333333e-05, 'epoch': 1.0}
{'eval_loss': 3.3950533866882324, 'eval_accuracy@deu.rst.pcc': 0.12033195020746888, 'eval_f1@deu.rst.pcc': 0.018462684242294438, 'eval_precision@deu.rst.pcc': 0.059897350029120555, 'eval_recall@deu.rst.pcc': 0.04480820105820107, 'eval_loss@deu.rst.pcc': 3.3950531482696533, 'eval_runtime': 3.3279, 'eval_samples_per_second': 72.417, 'eval_steps_per_second': 2.404, 'epoch': 1.0}
{'train@deu.rst.pcc_loss': 3.032642364501953, 'train@deu.rst.pcc_accuracy@deu.rst.pcc': 0.11506469500924214, 'train@deu.rst.pcc_f1@deu.rst.pcc': 0.021450463138633833, 'train@deu.rst.pcc_precision@deu.rst.pcc': 0.028171073172439942, 'train@deu.rst.pcc_recall@deu.rst.pcc': 0.04675360425556812, 'train@deu.rst.pcc_loss@deu.rst.pcc': 3.032642126083374, 'train@deu.rst.pcc_runtime': 26.865, 'train@deu.rst.pcc_samples_per_second': 80.551, 'train@deu.rst.pcc_steps_per_second': 2.531, 'epoch': 2.0}
{'loss': 3.1839, 'learning_rate': 1.6666666666666667e-05, 'epoch': 2.0}
{'eval_loss': 3.0685720443725586, 'eval_accuracy@deu.rst.pcc': 0.11203319502074689, 'eval_f1@deu.rst.pcc': 0.01613636363636364, 'eval_precision@deu.rst.pcc': 0.017330109126984128, 'eval_recall@deu.rst.pcc': 0.04183201058201058, 'eval_loss@deu.rst.pcc': 3.0685718059539795, 'eval_runtime': 3.3718, 'eval_samples_per_second': 71.475, 'eval_steps_per_second': 2.373, 'epoch': 2.0}
{'train@deu.rst.pcc_loss': 2.9523158073425293, 'train@deu.rst.pcc_accuracy@deu.rst.pcc': 0.12292051756007394, 'train@deu.rst.pcc_f1@deu.rst.pcc': 0.02508645342381577, 'train@deu.rst.pcc_precision@deu.rst.pcc': 0.022960495190118186, 'train@deu.rst.pcc_recall@deu.rst.pcc': 0.05086038904646098, 'train@deu.rst.pcc_loss@deu.rst.pcc': 2.9523160457611084, 'train@deu.rst.pcc_runtime': 26.741, 'train@deu.rst.pcc_samples_per_second': 80.924, 'train@deu.rst.pcc_steps_per_second': 2.543, 'epoch': 3.0}
{'loss': 3.0202, 'learning_rate': 1.5000000000000002e-05, 'epoch': 3.0}
{'eval_loss': 3.000418186187744, 'eval_accuracy@deu.rst.pcc': 0.12033195020746888, 'eval_f1@deu.rst.pcc': 0.021066413443830565, 'eval_precision@deu.rst.pcc': 0.014135688997821352, 'eval_recall@deu.rst.pcc': 0.050722425722425725, 'eval_loss@deu.rst.pcc': 3.0004186630249023, 'eval_runtime': 3.3539, 'eval_samples_per_second': 71.857, 'eval_steps_per_second': 2.385, 'epoch': 3.0}
{'train@deu.rst.pcc_loss': 2.903130054473877, 'train@deu.rst.pcc_accuracy@deu.rst.pcc': 0.13354898336414048, 'train@deu.rst.pcc_f1@deu.rst.pcc': 0.03393825821281263, 'train@deu.rst.pcc_precision@deu.rst.pcc': 0.04502093186938772, 'train@deu.rst.pcc_recall@deu.rst.pcc': 0.05717369219196467, 'train@deu.rst.pcc_loss@deu.rst.pcc': 2.903129816055298, 'train@deu.rst.pcc_runtime': 26.7292, 'train@deu.rst.pcc_samples_per_second': 80.96, 'train@deu.rst.pcc_steps_per_second': 2.544, 'epoch': 4.0}
{'loss': 2.9574, 'learning_rate': 1.3333333333333333e-05, 'epoch': 4.0}
{'eval_loss': 2.961678981781006, 'eval_accuracy@deu.rst.pcc': 0.12863070539419086, 'eval_f1@deu.rst.pcc': 0.024023945342534367, 'eval_precision@deu.rst.pcc': 0.01891025641025641, 'eval_recall@deu.rst.pcc': 0.05363502238502238, 'eval_loss@deu.rst.pcc': 2.961679220199585, 'eval_runtime': 3.3597, 'eval_samples_per_second': 71.734, 'eval_steps_per_second': 2.381, 'epoch': 4.0}
{'train@deu.rst.pcc_loss': 2.862736225128174, 'train@deu.rst.pcc_accuracy@deu.rst.pcc': 0.16358595194085027, 'train@deu.rst.pcc_f1@deu.rst.pcc': 0.06379217464171899, 'train@deu.rst.pcc_precision@deu.rst.pcc': 0.08993921032064267, 'train@deu.rst.pcc_recall@deu.rst.pcc': 0.08079395707290785, 'train@deu.rst.pcc_loss@deu.rst.pcc': 2.862736225128174, 'train@deu.rst.pcc_runtime': 26.7067, 'train@deu.rst.pcc_samples_per_second': 81.028, 'train@deu.rst.pcc_steps_per_second': 2.546, 'epoch': 5.0}
{'loss': 2.9126, 'learning_rate': 1.1666666666666668e-05, 'epoch': 5.0}
{'eval_loss': 2.9286611080169678, 'eval_accuracy@deu.rst.pcc': 0.14937759336099585, 'eval_f1@deu.rst.pcc': 0.05522346650524617, 'eval_precision@deu.rst.pcc': 0.11674335359511755, 'eval_recall@deu.rst.pcc': 0.07767348392348392, 'eval_loss@deu.rst.pcc': 2.928661584854126, 'eval_runtime': 3.3682, 'eval_samples_per_second': 71.551, 'eval_steps_per_second': 2.375, 'epoch': 5.0}
{'train@deu.rst.pcc_loss': 2.825692653656006, 'train@deu.rst.pcc_accuracy@deu.rst.pcc': 0.20748613678373382, 'train@deu.rst.pcc_f1@deu.rst.pcc': 0.0875129625946748, 'train@deu.rst.pcc_precision@deu.rst.pcc': 0.10060673848402599, 'train@deu.rst.pcc_recall@deu.rst.pcc': 0.11912873345520561, 'train@deu.rst.pcc_loss@deu.rst.pcc': 2.825692892074585, 'train@deu.rst.pcc_runtime': 26.7021, 'train@deu.rst.pcc_samples_per_second': 81.042, 'train@deu.rst.pcc_steps_per_second': 2.547, 'epoch': 6.0}
{'loss': 2.8735, 'learning_rate': 1e-05, 'epoch': 6.0}
{'eval_loss': 2.8974199295043945, 'eval_accuracy@deu.rst.pcc': 0.14937759336099585, 'eval_f1@deu.rst.pcc': 0.06359370223939308, 'eval_precision@deu.rst.pcc': 0.07400703576933085, 'eval_recall@deu.rst.pcc': 0.09239545177045178, 'eval_loss@deu.rst.pcc': 2.8974196910858154, 'eval_runtime': 3.3711, 'eval_samples_per_second': 71.489, 'eval_steps_per_second': 2.373, 'epoch': 6.0}
{'train@deu.rst.pcc_loss': 2.7967379093170166, 'train@deu.rst.pcc_accuracy@deu.rst.pcc': 0.21903881700554528, 'train@deu.rst.pcc_f1@deu.rst.pcc': 0.09045079340350112, 'train@deu.rst.pcc_precision@deu.rst.pcc': 0.10664632239283588, 'train@deu.rst.pcc_recall@deu.rst.pcc': 0.12926355515529592, 'train@deu.rst.pcc_loss@deu.rst.pcc': 2.7967379093170166, 'train@deu.rst.pcc_runtime': 26.6763, 'train@deu.rst.pcc_samples_per_second': 81.121, 'train@deu.rst.pcc_steps_per_second': 2.549, 'epoch': 7.0}
{'loss': 2.8494, 'learning_rate': 8.333333333333334e-06, 'epoch': 7.0}
{'eval_loss': 2.874138593673706, 'eval_accuracy@deu.rst.pcc': 0.15767634854771784, 'eval_f1@deu.rst.pcc': 0.06708002421398668, 'eval_precision@deu.rst.pcc': 0.07422513636548164, 'eval_recall@deu.rst.pcc': 0.1097120472120472, 'eval_loss@deu.rst.pcc': 2.874138355255127, 'eval_runtime': 3.3739, 'eval_samples_per_second': 71.43, 'eval_steps_per_second': 2.371, 'epoch': 7.0}
{'train@deu.rst.pcc_loss': 2.773865222930908, 'train@deu.rst.pcc_accuracy@deu.rst.pcc': 0.21626617375231053, 'train@deu.rst.pcc_f1@deu.rst.pcc': 0.08877914553735716, 'train@deu.rst.pcc_precision@deu.rst.pcc': 0.09458636353717591, 'train@deu.rst.pcc_recall@deu.rst.pcc': 0.12998694410265352, 'train@deu.rst.pcc_loss@deu.rst.pcc': 2.773865222930908, 'train@deu.rst.pcc_runtime': 26.6771, 'train@deu.rst.pcc_samples_per_second': 81.118, 'train@deu.rst.pcc_steps_per_second': 2.549, 'epoch': 8.0}
{'loss': 2.8189, 'learning_rate': 6.666666666666667e-06, 'epoch': 8.0}
{'eval_loss': 2.8572309017181396, 'eval_accuracy@deu.rst.pcc': 0.16182572614107885, 'eval_f1@deu.rst.pcc': 0.06897535003824774, 'eval_precision@deu.rst.pcc': 0.07104637845432703, 'eval_recall@deu.rst.pcc': 0.11552452177452177, 'eval_loss@deu.rst.pcc': 2.8572309017181396, 'eval_runtime': 3.3478, 'eval_samples_per_second': 71.987, 'eval_steps_per_second': 2.39, 'epoch': 8.0}
{'train@deu.rst.pcc_loss': 2.755708694458008, 'train@deu.rst.pcc_accuracy@deu.rst.pcc': 0.22365988909426987, 'train@deu.rst.pcc_f1@deu.rst.pcc': 0.09201043429073576, 'train@deu.rst.pcc_precision@deu.rst.pcc': 0.09122330505233568, 'train@deu.rst.pcc_recall@deu.rst.pcc': 0.13476757654478264, 'train@deu.rst.pcc_loss@deu.rst.pcc': 2.755708694458008, 'train@deu.rst.pcc_runtime': 26.6838, 'train@deu.rst.pcc_samples_per_second': 81.098, 'train@deu.rst.pcc_steps_per_second': 2.548, 'epoch': 9.0}
{'loss': 2.8015, 'learning_rate': 5e-06, 'epoch': 9.0}
{'eval_loss': 2.8453545570373535, 'eval_accuracy@deu.rst.pcc': 0.15767634854771784, 'eval_f1@deu.rst.pcc': 0.06871318186035005, 'eval_precision@deu.rst.pcc': 0.0698518435587401, 'eval_recall@deu.rst.pcc': 0.11603963166463167, 'eval_loss@deu.rst.pcc': 2.8453543186187744, 'eval_runtime': 3.3465, 'eval_samples_per_second': 72.016, 'eval_steps_per_second': 2.391, 'epoch': 9.0}
{'train@deu.rst.pcc_loss': 2.743556499481201, 'train@deu.rst.pcc_accuracy@deu.rst.pcc': 0.22504621072088724, 'train@deu.rst.pcc_f1@deu.rst.pcc': 0.09266428518566666, 'train@deu.rst.pcc_precision@deu.rst.pcc': 0.1039365831302837, 'train@deu.rst.pcc_recall@deu.rst.pcc': 0.13577072693262296, 'train@deu.rst.pcc_loss@deu.rst.pcc': 2.743556499481201, 'train@deu.rst.pcc_runtime': 26.7243, 'train@deu.rst.pcc_samples_per_second': 80.975, 'train@deu.rst.pcc_steps_per_second': 2.544, 'epoch': 10.0}
{'loss': 2.7841, 'learning_rate': 3.3333333333333333e-06, 'epoch': 10.0}
{'eval_loss': 2.8333122730255127, 'eval_accuracy@deu.rst.pcc': 0.16597510373443983, 'eval_f1@deu.rst.pcc': 0.0712600112217918, 'eval_precision@deu.rst.pcc': 0.0903950609595771, 'eval_recall@deu.rst.pcc': 0.11901582214082214, 'eval_loss@deu.rst.pcc': 2.8333122730255127, 'eval_runtime': 3.3654, 'eval_samples_per_second': 71.611, 'eval_steps_per_second': 2.377, 'epoch': 10.0}
{'train@deu.rst.pcc_loss': 2.736131429672241, 'train@deu.rst.pcc_accuracy@deu.rst.pcc': 0.2208872458410351, 'train@deu.rst.pcc_f1@deu.rst.pcc': 0.09143024493560058, 'train@deu.rst.pcc_precision@deu.rst.pcc': 0.1021980979279436, 'train@deu.rst.pcc_recall@deu.rst.pcc': 0.13396736058127406, 'train@deu.rst.pcc_loss@deu.rst.pcc': 2.7361316680908203, 'train@deu.rst.pcc_runtime': 26.6861, 'train@deu.rst.pcc_samples_per_second': 81.091, 'train@deu.rst.pcc_steps_per_second': 2.548, 'epoch': 11.0}
{'loss': 2.7727, 'learning_rate': 1.6666666666666667e-06, 'epoch': 11.0}
{'eval_loss': 2.8273675441741943, 'eval_accuracy@deu.rst.pcc': 0.17427385892116182, 'eval_f1@deu.rst.pcc': 0.07137445887445888, 'eval_precision@deu.rst.pcc': 0.09016544117647059, 'eval_recall@deu.rst.pcc': 0.12199201261701263, 'eval_loss@deu.rst.pcc': 2.8273677825927734, 'eval_runtime': 3.7696, 'eval_samples_per_second': 63.933, 'eval_steps_per_second': 2.122, 'epoch': 11.0}
{'train@deu.rst.pcc_loss': 2.733633041381836, 'train@deu.rst.pcc_accuracy@deu.rst.pcc': 0.22412199630314233, 'train@deu.rst.pcc_f1@deu.rst.pcc': 0.09248545223710296, 'train@deu.rst.pcc_precision@deu.rst.pcc': 0.10338820454216502, 'train@deu.rst.pcc_recall@deu.rst.pcc': 0.13530180035964975, 'train@deu.rst.pcc_loss@deu.rst.pcc': 2.733633041381836, 'train@deu.rst.pcc_runtime': 26.9237, 'train@deu.rst.pcc_samples_per_second': 80.375, 'train@deu.rst.pcc_steps_per_second': 2.526, 'epoch': 12.0}
{'loss': 2.7667, 'learning_rate': 0.0, 'epoch': 12.0}
{'eval_loss': 2.824856996536255, 'eval_accuracy@deu.rst.pcc': 0.17427385892116182, 'eval_f1@deu.rst.pcc': 0.07137293543543544, 'eval_precision@deu.rst.pcc': 0.09030257936507936, 'eval_recall@deu.rst.pcc': 0.12199201261701263, 'eval_loss@deu.rst.pcc': 2.824856758117676, 'eval_runtime': 3.3519, 'eval_samples_per_second': 71.899, 'eval_steps_per_second': 2.387, 'epoch': 12.0}
{'train_runtime': 1027.9813, 'train_samples_per_second': 25.261, 'train_steps_per_second': 0.794, 'train_loss': 2.94872553208295, 'epoch': 12.0}
***** train_log metrics *****
  epoch                    =       12.0
  train_loss               =     2.9487
  train_runtime            = 0:17:07.98
  train_samples_per_second =     25.261
  train_steps_per_second   =      0.794
{'train@tur.pdtb.tdb_loss': 2.628789186477661, 'train@tur.pdtb.tdb_accuracy@tur.pdtb.tdb': 0.25050999592003265, 'train@tur.pdtb.tdb_f1@tur.pdtb.tdb': 0.01741967515426626, 'train@tur.pdtb.tdb_precision@tur.pdtb.tdb': 0.010891738953044898, 'train@tur.pdtb.tdb_recall@tur.pdtb.tdb': 0.043478260869565216, 'train@tur.pdtb.tdb_loss@tur.pdtb.tdb': 2.628789186477661, 'train@tur.pdtb.tdb_runtime': 30.1969, 'train@tur.pdtb.tdb_samples_per_second': 81.167, 'train@tur.pdtb.tdb_steps_per_second': 2.55, 'epoch': 1.0}
{'loss': 3.0919, 'learning_rate': 1.8333333333333333e-05, 'epoch': 1.0}
{'eval_loss': 2.5698673725128174, 'eval_accuracy@tur.pdtb.tdb': 0.266025641025641, 'eval_f1@tur.pdtb.tdb': 0.019102416570771, 'eval_precision@tur.pdtb.tdb': 0.012092074592074592, 'eval_recall@tur.pdtb.tdb': 0.045454545454545456, 'eval_loss@tur.pdtb.tdb': 2.5698673725128174, 'eval_runtime': 4.2247, 'eval_samples_per_second': 73.851, 'eval_steps_per_second': 2.367, 'epoch': 1.0}
{'train@tur.pdtb.tdb_loss': 2.401594400405884, 'train@tur.pdtb.tdb_accuracy@tur.pdtb.tdb': 0.25050999592003265, 'train@tur.pdtb.tdb_f1@tur.pdtb.tdb': 0.01741967515426626, 'train@tur.pdtb.tdb_precision@tur.pdtb.tdb': 0.010891738953044898, 'train@tur.pdtb.tdb_recall@tur.pdtb.tdb': 0.043478260869565216, 'train@tur.pdtb.tdb_loss@tur.pdtb.tdb': 2.401594400405884, 'train@tur.pdtb.tdb_runtime': 30.1533, 'train@tur.pdtb.tdb_samples_per_second': 81.285, 'train@tur.pdtb.tdb_steps_per_second': 2.554, 'epoch': 2.0}
{'loss': 2.5053, 'learning_rate': 1.6666666666666667e-05, 'epoch': 2.0}
{'eval_loss': 2.3257124423980713, 'eval_accuracy@tur.pdtb.tdb': 0.266025641025641, 'eval_f1@tur.pdtb.tdb': 0.019102416570771, 'eval_precision@tur.pdtb.tdb': 0.012092074592074592, 'eval_recall@tur.pdtb.tdb': 0.045454545454545456, 'eval_loss@tur.pdtb.tdb': 2.3257126808166504, 'eval_runtime': 4.2371, 'eval_samples_per_second': 73.636, 'eval_steps_per_second': 2.36, 'epoch': 2.0}
{'train@tur.pdtb.tdb_loss': 2.34554123878479, 'train@tur.pdtb.tdb_accuracy@tur.pdtb.tdb': 0.2521419828641371, 'train@tur.pdtb.tdb_f1@tur.pdtb.tdb': 0.019149804451843694, 'train@tur.pdtb.tdb_precision@tur.pdtb.tdb': 0.026464418883822644, 'train@tur.pdtb.tdb_recall@tur.pdtb.tdb': 0.04429476081609531, 'train@tur.pdtb.tdb_loss@tur.pdtb.tdb': 2.345541000366211, 'train@tur.pdtb.tdb_runtime': 30.1334, 'train@tur.pdtb.tdb_samples_per_second': 81.338, 'train@tur.pdtb.tdb_steps_per_second': 2.555, 'epoch': 3.0}
{'loss': 2.3976, 'learning_rate': 1.5000000000000002e-05, 'epoch': 3.0}
{'eval_loss': 2.2916195392608643, 'eval_accuracy@tur.pdtb.tdb': 0.266025641025641, 'eval_f1@tur.pdtb.tdb': 0.01915089986155976, 'eval_precision@tur.pdtb.tdb': 0.012130955860859398, 'eval_recall@tur.pdtb.tdb': 0.045454545454545456, 'eval_loss@tur.pdtb.tdb': 2.291619300842285, 'eval_runtime': 4.239, 'eval_samples_per_second': 73.602, 'eval_steps_per_second': 2.359, 'epoch': 3.0}
{'train@tur.pdtb.tdb_loss': 2.2840118408203125, 'train@tur.pdtb.tdb_accuracy@tur.pdtb.tdb': 0.27866177070583437, 'train@tur.pdtb.tdb_f1@tur.pdtb.tdb': 0.03695279774827367, 'train@tur.pdtb.tdb_precision@tur.pdtb.tdb': 0.03745244654335564, 'train@tur.pdtb.tdb_recall@tur.pdtb.tdb': 0.05730197202801245, 'train@tur.pdtb.tdb_loss@tur.pdtb.tdb': 2.2840118408203125, 'train@tur.pdtb.tdb_runtime': 30.1338, 'train@tur.pdtb.tdb_samples_per_second': 81.337, 'train@tur.pdtb.tdb_steps_per_second': 2.555, 'epoch': 4.0}
{'loss': 2.3428, 'learning_rate': 1.3333333333333333e-05, 'epoch': 4.0}
{'eval_loss': 2.254298210144043, 'eval_accuracy@tur.pdtb.tdb': 0.2724358974358974, 'eval_f1@tur.pdtb.tdb': 0.027623004235907467, 'eval_precision@tur.pdtb.tdb': 0.029334371894925527, 'eval_recall@tur.pdtb.tdb': 0.04955830451932507, 'eval_loss@tur.pdtb.tdb': 2.254298210144043, 'eval_runtime': 4.2107, 'eval_samples_per_second': 74.096, 'eval_steps_per_second': 2.375, 'epoch': 4.0}
{'train@tur.pdtb.tdb_loss': 2.2318122386932373, 'train@tur.pdtb.tdb_accuracy@tur.pdtb.tdb': 0.2978376172990616, 'train@tur.pdtb.tdb_f1@tur.pdtb.tdb': 0.05054583198298914, 'train@tur.pdtb.tdb_precision@tur.pdtb.tdb': 0.09275017374012895, 'train@tur.pdtb.tdb_recall@tur.pdtb.tdb': 0.06937689237814287, 'train@tur.pdtb.tdb_loss@tur.pdtb.tdb': 2.2318122386932373, 'train@tur.pdtb.tdb_runtime': 30.1262, 'train@tur.pdtb.tdb_samples_per_second': 81.358, 'train@tur.pdtb.tdb_steps_per_second': 2.556, 'epoch': 5.0}
{'loss': 2.2943, 'learning_rate': 1.1666666666666668e-05, 'epoch': 5.0}
{'eval_loss': 2.21478009223938, 'eval_accuracy@tur.pdtb.tdb': 0.28525641025641024, 'eval_f1@tur.pdtb.tdb': 0.05747395850878794, 'eval_precision@tur.pdtb.tdb': 0.12269782356680752, 'eval_recall@tur.pdtb.tdb': 0.06910553163596025, 'eval_loss@tur.pdtb.tdb': 2.214779853820801, 'eval_runtime': 4.2072, 'eval_samples_per_second': 74.158, 'eval_steps_per_second': 2.377, 'epoch': 5.0}
{'train@tur.pdtb.tdb_loss': 2.187286853790283, 'train@tur.pdtb.tdb_accuracy@tur.pdtb.tdb': 0.3215014279885761, 'train@tur.pdtb.tdb_f1@tur.pdtb.tdb': 0.08120979448410187, 'train@tur.pdtb.tdb_precision@tur.pdtb.tdb': 0.1164384468804472, 'train@tur.pdtb.tdb_recall@tur.pdtb.tdb': 0.09654851168430045, 'train@tur.pdtb.tdb_loss@tur.pdtb.tdb': 2.187286853790283, 'train@tur.pdtb.tdb_runtime': 30.133, 'train@tur.pdtb.tdb_samples_per_second': 81.339, 'train@tur.pdtb.tdb_steps_per_second': 2.555, 'epoch': 6.0}
{'loss': 2.2573, 'learning_rate': 1e-05, 'epoch': 6.0}
{'eval_loss': 2.1864163875579834, 'eval_accuracy@tur.pdtb.tdb': 0.30128205128205127, 'eval_f1@tur.pdtb.tdb': 0.07568502645058077, 'eval_precision@tur.pdtb.tdb': 0.08762261122318388, 'eval_recall@tur.pdtb.tdb': 0.10224969474114393, 'eval_loss@tur.pdtb.tdb': 2.1864163875579834, 'eval_runtime': 4.2459, 'eval_samples_per_second': 73.482, 'eval_steps_per_second': 2.355, 'epoch': 6.0}
{'train@tur.pdtb.tdb_loss': 2.151823043823242, 'train@tur.pdtb.tdb_accuracy@tur.pdtb.tdb': 0.3329253365973072, 'train@tur.pdtb.tdb_f1@tur.pdtb.tdb': 0.08940547062119375, 'train@tur.pdtb.tdb_precision@tur.pdtb.tdb': 0.10044566138594499, 'train@tur.pdtb.tdb_recall@tur.pdtb.tdb': 0.10878232578354342, 'train@tur.pdtb.tdb_loss@tur.pdtb.tdb': 2.151822566986084, 'train@tur.pdtb.tdb_runtime': 30.1315, 'train@tur.pdtb.tdb_samples_per_second': 81.343, 'train@tur.pdtb.tdb_steps_per_second': 2.555, 'epoch': 7.0}
{'loss': 2.2179, 'learning_rate': 8.333333333333334e-06, 'epoch': 7.0}
{'eval_loss': 2.161855936050415, 'eval_accuracy@tur.pdtb.tdb': 0.32051282051282054, 'eval_f1@tur.pdtb.tdb': 0.08501841336131738, 'eval_precision@tur.pdtb.tdb': 0.1349087105363256, 'eval_recall@tur.pdtb.tdb': 0.11010257837872879, 'eval_loss@tur.pdtb.tdb': 2.161855936050415, 'eval_runtime': 4.2198, 'eval_samples_per_second': 73.937, 'eval_steps_per_second': 2.37, 'epoch': 7.0}
{'train@tur.pdtb.tdb_loss': 2.1301121711730957, 'train@tur.pdtb.tdb_accuracy@tur.pdtb.tdb': 0.3398612811097511, 'train@tur.pdtb.tdb_f1@tur.pdtb.tdb': 0.09321644697311124, 'train@tur.pdtb.tdb_precision@tur.pdtb.tdb': 0.10257014871161718, 'train@tur.pdtb.tdb_recall@tur.pdtb.tdb': 0.11172659223569337, 'train@tur.pdtb.tdb_loss@tur.pdtb.tdb': 2.1301121711730957, 'train@tur.pdtb.tdb_runtime': 30.1843, 'train@tur.pdtb.tdb_samples_per_second': 81.201, 'train@tur.pdtb.tdb_steps_per_second': 2.551, 'epoch': 8.0}
{'loss': 2.1923, 'learning_rate': 6.666666666666667e-06, 'epoch': 8.0}
{'eval_loss': 2.1476404666900635, 'eval_accuracy@tur.pdtb.tdb': 0.3173076923076923, 'eval_f1@tur.pdtb.tdb': 0.08613783427958334, 'eval_precision@tur.pdtb.tdb': 0.13205764823411884, 'eval_recall@tur.pdtb.tdb': 0.10937219034593419, 'eval_loss@tur.pdtb.tdb': 2.1476404666900635, 'eval_runtime': 4.2507, 'eval_samples_per_second': 73.4, 'eval_steps_per_second': 2.353, 'epoch': 8.0}
{'train@tur.pdtb.tdb_loss': 2.1085762977600098, 'train@tur.pdtb.tdb_accuracy@tur.pdtb.tdb': 0.3419012647898817, 'train@tur.pdtb.tdb_f1@tur.pdtb.tdb': 0.0947525391799652, 'train@tur.pdtb.tdb_precision@tur.pdtb.tdb': 0.11313927551734039, 'train@tur.pdtb.tdb_recall@tur.pdtb.tdb': 0.11446847270666068, 'train@tur.pdtb.tdb_loss@tur.pdtb.tdb': 2.1085760593414307, 'train@tur.pdtb.tdb_runtime': 30.1597, 'train@tur.pdtb.tdb_samples_per_second': 81.267, 'train@tur.pdtb.tdb_steps_per_second': 2.553, 'epoch': 9.0}
{'loss': 2.1644, 'learning_rate': 5e-06, 'epoch': 9.0}
{'eval_loss': 2.131903648376465, 'eval_accuracy@tur.pdtb.tdb': 0.3173076923076923, 'eval_f1@tur.pdtb.tdb': 0.0856332325082325, 'eval_precision@tur.pdtb.tdb': 0.12372778417082216, 'eval_recall@tur.pdtb.tdb': 0.10955493325277041, 'eval_loss@tur.pdtb.tdb': 2.1319031715393066, 'eval_runtime': 4.4571, 'eval_samples_per_second': 70.0, 'eval_steps_per_second': 2.244, 'epoch': 9.0}
{'train@tur.pdtb.tdb_loss': 2.0935378074645996, 'train@tur.pdtb.tdb_accuracy@tur.pdtb.tdb': 0.3463892288861689, 'train@tur.pdtb.tdb_f1@tur.pdtb.tdb': 0.09861528747073738, 'train@tur.pdtb.tdb_precision@tur.pdtb.tdb': 0.11362508212995974, 'train@tur.pdtb.tdb_recall@tur.pdtb.tdb': 0.11697203413004212, 'train@tur.pdtb.tdb_loss@tur.pdtb.tdb': 2.0935375690460205, 'train@tur.pdtb.tdb_runtime': 30.1728, 'train@tur.pdtb.tdb_samples_per_second': 81.232, 'train@tur.pdtb.tdb_steps_per_second': 2.552, 'epoch': 10.0}
{'loss': 2.1433, 'learning_rate': 3.3333333333333333e-06, 'epoch': 10.0}
{'eval_loss': 2.121262788772583, 'eval_accuracy@tur.pdtb.tdb': 0.3141025641025641, 'eval_f1@tur.pdtb.tdb': 0.08329870168190075, 'eval_precision@tur.pdtb.tdb': 0.116608547925608, 'eval_recall@tur.pdtb.tdb': 0.10964799713038426, 'eval_loss@tur.pdtb.tdb': 2.121262788772583, 'eval_runtime': 4.2136, 'eval_samples_per_second': 74.046, 'eval_steps_per_second': 2.373, 'epoch': 10.0}
{'train@tur.pdtb.tdb_loss': 2.0864980220794678, 'train@tur.pdtb.tdb_accuracy@tur.pdtb.tdb': 0.3463892288861689, 'train@tur.pdtb.tdb_f1@tur.pdtb.tdb': 0.09883778709752353, 'train@tur.pdtb.tdb_precision@tur.pdtb.tdb': 0.1146035693101851, 'train@tur.pdtb.tdb_recall@tur.pdtb.tdb': 0.11702006779749217, 'train@tur.pdtb.tdb_loss@tur.pdtb.tdb': 2.0864980220794678, 'train@tur.pdtb.tdb_runtime': 30.2211, 'train@tur.pdtb.tdb_samples_per_second': 81.102, 'train@tur.pdtb.tdb_steps_per_second': 2.548, 'epoch': 11.0}
{'loss': 2.1441, 'learning_rate': 1.6666666666666667e-06, 'epoch': 11.0}
{'eval_loss': 2.1183829307556152, 'eval_accuracy@tur.pdtb.tdb': 0.3108974358974359, 'eval_f1@tur.pdtb.tdb': 0.08375381119745619, 'eval_precision@tur.pdtb.tdb': 0.11727371763388574, 'eval_recall@tur.pdtb.tdb': 0.10823682660156667, 'eval_loss@tur.pdtb.tdb': 2.118382692337036, 'eval_runtime': 4.2564, 'eval_samples_per_second': 73.301, 'eval_steps_per_second': 2.349, 'epoch': 11.0}
{'train@tur.pdtb.tdb_loss': 2.083590030670166, 'train@tur.pdtb.tdb_accuracy@tur.pdtb.tdb': 0.3484292125662995, 'train@tur.pdtb.tdb_f1@tur.pdtb.tdb': 0.09987068749560615, 'train@tur.pdtb.tdb_precision@tur.pdtb.tdb': 0.11188763863605514, 'train@tur.pdtb.tdb_recall@tur.pdtb.tdb': 0.11795547448825956, 'train@tur.pdtb.tdb_loss@tur.pdtb.tdb': 2.083590030670166, 'train@tur.pdtb.tdb_runtime': 30.1917, 'train@tur.pdtb.tdb_samples_per_second': 81.181, 'train@tur.pdtb.tdb_steps_per_second': 2.55, 'epoch': 12.0}
{'loss': 2.1322, 'learning_rate': 0.0, 'epoch': 12.0}
{'eval_loss': 2.116257667541504, 'eval_accuracy@tur.pdtb.tdb': 0.30128205128205127, 'eval_f1@tur.pdtb.tdb': 0.08103839151397663, 'eval_precision@tur.pdtb.tdb': 0.11066597978823683, 'eval_recall@tur.pdtb.tdb': 0.10659389122369153, 'eval_loss@tur.pdtb.tdb': 2.116257667541504, 'eval_runtime': 4.2387, 'eval_samples_per_second': 73.608, 'eval_steps_per_second': 2.359, 'epoch': 12.0}
{'train_runtime': 1167.5023, 'train_samples_per_second': 25.192, 'train_steps_per_second': 0.791, 'train_loss': 2.3236231122698103, 'epoch': 12.0}
***** train_logger metrics *****
  epoch                    =       12.0
  train_loss               =     2.9487
  train_runtime            = 0:17:07.98
  train_samples_per_second =     25.261
  train_steps_per_second   =      0.794
-------------------------------------------------------------------
Lang1:  eng.pdtb.pdtb    Lang2:  tur.pdtb.tdb
Saving run to:  runs/full_shot/FullShot=v4_eng.pdtb.pdtb_tur.pdtb.tdb_lrfunc=Adafactor_lr=2e-05_plm=bert-base-multilingual-cased
Running with params: BERT_MODEL=bert-base-multilingual-cased lr=2e-05
read 43920 examples
read 1674 examples
read 2257 examples
read 2451 examples
read 312 examples
read 422 examples
Total prediction labels:  25
BertAdapterModel(
  (shared_parameters): ModuleDict()
  (bert): BertModel(
    (shared_parameters): ModuleDict()
    (invertible_adapters): ModuleDict(
      (en): NICECouplingBlock(
        (F): Sequential(
          (0): Linear(in_features=384, out_features=192, bias=True)
          (1): Activation_Function_Class(
            (f): ReLU()
          )
          (2): Linear(in_features=192, out_features=384, bias=True)
        )
        (G): Sequential(
          (0): Linear(in_features=384, out_features=192, bias=True)
          (1): Activation_Function_Class(
            (f): ReLU()
          )
          (2): Linear(in_features=192, out_features=384, bias=True)
        )
      )
      (tr): NICECouplingBlock(
        (F): Sequential(
          (0): Linear(in_features=384, out_features=192, bias=True)
          (1): Activation_Function_Class(
            (f): NewGELUActivation()
          )
          (2): Linear(in_features=192, out_features=384, bias=True)
        )
        (G): Sequential(
          (0): Linear(in_features=384, out_features=192, bias=True)
          (1): Activation_Function_Class(
            (f): NewGELUActivation()
          )
          (2): Linear(in_features=192, out_features=384, bias=True)
        )
      )
    )
    (embeddings): BertEmbeddings(
      (word_embeddings): Embedding(119547, 768, padding_idx=0)
      (position_embeddings): Embedding(512, 768)
      (token_type_embeddings): Embedding(2, 768)
      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
      (dropout): Dropout(p=0.1, inplace=False)
    )
    (encoder): BertEncoder(
      (layer): ModuleList(
        (0): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (en): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (tr): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (1): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (en): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (tr): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (2): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (en): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (tr): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (3): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (en): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (tr): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (4): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (en): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (tr): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (5): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (en): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (tr): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (6): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (en): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (tr): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (7): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (en): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (tr): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (8): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (en): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (tr): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (9): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (en): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (tr): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (10): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (en): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (tr): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (11): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (en): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (tr): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
      )
    )
    (pooler): BertPooler(
      (dense): Linear(in_features=768, out_features=768, bias=True)
      (activation): Tanh()
    )
    (prefix_tuning): PrefixTuningPool(
      (prefix_tunings): ModuleDict()
    )
  )
  (heads): ModuleDict(
    (tr): BertStyleMaskedLMHead(
      (0): Linear(in_features=768, out_features=768, bias=True)
      (1): Activation_Function_Class(
        (f): GELUActivation()
      )
      (2): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
      (3): Linear(in_features=768, out_features=119547, bias=True)
    )
    (disrpt): ClassificationHead(
      (0): Dropout(p=0.1, inplace=False)
      (1): Linear(in_features=768, out_features=768, bias=True)
      (2): Activation_Function_Class(
        (f): Tanh()
      )
      (3): Dropout(p=0.1, inplace=False)
      (4): Linear(in_features=768, out_features=25, bias=True)
    )
  )
)
{'train@eng.pdtb.pdtb_loss': 1.272189974784851, 'train@eng.pdtb.pdtb_accuracy@eng.pdtb.pdtb': 0.5930555555555556, 'train@eng.pdtb.pdtb_f1@eng.pdtb.pdtb': 0.2615433944877454, 'train@eng.pdtb.pdtb_precision@eng.pdtb.pdtb': 0.3528023896549098, 'train@eng.pdtb.pdtb_recall@eng.pdtb.pdtb': 0.25702689220747477, 'train@eng.pdtb.pdtb_loss@eng.pdtb.pdtb': 1.2721900939941406, 'train@eng.pdtb.pdtb_runtime': 525.0979, 'train@eng.pdtb.pdtb_samples_per_second': 83.642, 'train@eng.pdtb.pdtb_steps_per_second': 2.615, 'epoch': 1.0}
{'loss': 1.7962, 'learning_rate': 1.8333333333333333e-05, 'epoch': 1.0}
{'eval_loss': 1.1885179281234741, 'eval_accuracy@eng.pdtb.pdtb': 0.6248506571087217, 'eval_f1@eng.pdtb.pdtb': 0.29872642282019857, 'eval_precision@eng.pdtb.pdtb': 0.32504256884624433, 'eval_recall@eng.pdtb.pdtb': 0.30181413580744354, 'eval_loss@eng.pdtb.pdtb': 1.1885180473327637, 'eval_runtime': 20.2833, 'eval_samples_per_second': 82.531, 'eval_steps_per_second': 2.613, 'epoch': 1.0}
{'train@eng.pdtb.pdtb_loss': 1.1038485765457153, 'train@eng.pdtb.pdtb_accuracy@eng.pdtb.pdtb': 0.6351548269581057, 'train@eng.pdtb.pdtb_f1@eng.pdtb.pdtb': 0.34537033397600303, 'train@eng.pdtb.pdtb_precision@eng.pdtb.pdtb': 0.4710260617051447, 'train@eng.pdtb.pdtb_recall@eng.pdtb.pdtb': 0.334271257306456, 'train@eng.pdtb.pdtb_loss@eng.pdtb.pdtb': 1.1038485765457153, 'train@eng.pdtb.pdtb_runtime': 524.9079, 'train@eng.pdtb.pdtb_samples_per_second': 83.672, 'train@eng.pdtb.pdtb_steps_per_second': 2.616, 'epoch': 2.0}
{'loss': 1.2286, 'learning_rate': 1.6666666666666667e-05, 'epoch': 2.0}
{'eval_loss': 1.037585735321045, 'eval_accuracy@eng.pdtb.pdtb': 0.6565113500597372, 'eval_f1@eng.pdtb.pdtb': 0.38628353783081537, 'eval_precision@eng.pdtb.pdtb': 0.43073320037569357, 'eval_recall@eng.pdtb.pdtb': 0.3799278498311836, 'eval_loss@eng.pdtb.pdtb': 1.037585735321045, 'eval_runtime': 20.265, 'eval_samples_per_second': 82.606, 'eval_steps_per_second': 2.615, 'epoch': 2.0}
{'train@eng.pdtb.pdtb_loss': 1.0542453527450562, 'train@eng.pdtb.pdtb_accuracy@eng.pdtb.pdtb': 0.6504553734061931, 'train@eng.pdtb.pdtb_f1@eng.pdtb.pdtb': 0.42335692571733846, 'train@eng.pdtb.pdtb_precision@eng.pdtb.pdtb': 0.46228172954630464, 'train@eng.pdtb.pdtb_recall@eng.pdtb.pdtb': 0.4051343090433917, 'train@eng.pdtb.pdtb_loss@eng.pdtb.pdtb': 1.0542452335357666, 'train@eng.pdtb.pdtb_runtime': 525.2296, 'train@eng.pdtb.pdtb_samples_per_second': 83.621, 'train@eng.pdtb.pdtb_steps_per_second': 2.614, 'epoch': 3.0}
{'loss': 1.1255, 'learning_rate': 1.5000000000000002e-05, 'epoch': 3.0}
{'eval_loss': 0.9956722855567932, 'eval_accuracy@eng.pdtb.pdtb': 0.6744324970131422, 'eval_f1@eng.pdtb.pdtb': 0.4671969177668022, 'eval_precision@eng.pdtb.pdtb': 0.5188672767403276, 'eval_recall@eng.pdtb.pdtb': 0.4486408786118979, 'eval_loss@eng.pdtb.pdtb': 0.995672345161438, 'eval_runtime': 20.3018, 'eval_samples_per_second': 82.456, 'eval_steps_per_second': 2.611, 'epoch': 3.0}
{'train@eng.pdtb.pdtb_loss': 1.0047699213027954, 'train@eng.pdtb.pdtb_accuracy@eng.pdtb.pdtb': 0.666188524590164, 'train@eng.pdtb.pdtb_f1@eng.pdtb.pdtb': 0.44623283838566213, 'train@eng.pdtb.pdtb_precision@eng.pdtb.pdtb': 0.5180671131616643, 'train@eng.pdtb.pdtb_recall@eng.pdtb.pdtb': 0.43534201914685094, 'train@eng.pdtb.pdtb_loss@eng.pdtb.pdtb': 1.0047698020935059, 'train@eng.pdtb.pdtb_runtime': 525.2461, 'train@eng.pdtb.pdtb_samples_per_second': 83.618, 'train@eng.pdtb.pdtb_steps_per_second': 2.614, 'epoch': 4.0}
{'loss': 1.0778, 'learning_rate': 1.3333333333333333e-05, 'epoch': 4.0}
{'eval_loss': 0.9525158405303955, 'eval_accuracy@eng.pdtb.pdtb': 0.6881720430107527, 'eval_f1@eng.pdtb.pdtb': 0.5051459029090386, 'eval_precision@eng.pdtb.pdtb': 0.5651899550247839, 'eval_recall@eng.pdtb.pdtb': 0.47881712852637415, 'eval_loss@eng.pdtb.pdtb': 0.9525159001350403, 'eval_runtime': 20.3095, 'eval_samples_per_second': 82.424, 'eval_steps_per_second': 2.61, 'epoch': 4.0}
{'train@eng.pdtb.pdtb_loss': 0.9819191098213196, 'train@eng.pdtb.pdtb_accuracy@eng.pdtb.pdtb': 0.6720628415300547, 'train@eng.pdtb.pdtb_f1@eng.pdtb.pdtb': 0.4543253215313943, 'train@eng.pdtb.pdtb_precision@eng.pdtb.pdtb': 0.5199992536070822, 'train@eng.pdtb.pdtb_recall@eng.pdtb.pdtb': 0.4480089692575524, 'train@eng.pdtb.pdtb_loss@eng.pdtb.pdtb': 0.9819191694259644, 'train@eng.pdtb.pdtb_runtime': 524.4382, 'train@eng.pdtb.pdtb_samples_per_second': 83.747, 'train@eng.pdtb.pdtb_steps_per_second': 2.618, 'epoch': 5.0}
{'loss': 1.0486, 'learning_rate': 1.1666666666666668e-05, 'epoch': 5.0}
{'eval_loss': 0.9346547722816467, 'eval_accuracy@eng.pdtb.pdtb': 0.6899641577060932, 'eval_f1@eng.pdtb.pdtb': 0.5079549004305699, 'eval_precision@eng.pdtb.pdtb': 0.5450705431187389, 'eval_recall@eng.pdtb.pdtb': 0.4939929090699442, 'eval_loss@eng.pdtb.pdtb': 0.9346545934677124, 'eval_runtime': 20.2766, 'eval_samples_per_second': 82.558, 'eval_steps_per_second': 2.614, 'epoch': 5.0}
{'train@eng.pdtb.pdtb_loss': 0.9617270827293396, 'train@eng.pdtb.pdtb_accuracy@eng.pdtb.pdtb': 0.6789162112932605, 'train@eng.pdtb.pdtb_f1@eng.pdtb.pdtb': 0.46283374435096547, 'train@eng.pdtb.pdtb_precision@eng.pdtb.pdtb': 0.5263713165733046, 'train@eng.pdtb.pdtb_recall@eng.pdtb.pdtb': 0.45571824483378726, 'train@eng.pdtb.pdtb_loss@eng.pdtb.pdtb': 0.9617272019386292, 'train@eng.pdtb.pdtb_runtime': 524.5462, 'train@eng.pdtb.pdtb_samples_per_second': 83.73, 'train@eng.pdtb.pdtb_steps_per_second': 2.618, 'epoch': 6.0}
{'loss': 1.0255, 'learning_rate': 1e-05, 'epoch': 6.0}
{'eval_loss': 0.9261037111282349, 'eval_accuracy@eng.pdtb.pdtb': 0.6899641577060932, 'eval_f1@eng.pdtb.pdtb': 0.5273298968744401, 'eval_precision@eng.pdtb.pdtb': 0.6023570124457439, 'eval_recall@eng.pdtb.pdtb': 0.51169691357675, 'eval_loss@eng.pdtb.pdtb': 0.9261037707328796, 'eval_runtime': 20.248, 'eval_samples_per_second': 82.675, 'eval_steps_per_second': 2.618, 'epoch': 6.0}
{'train@eng.pdtb.pdtb_loss': 0.9500576257705688, 'train@eng.pdtb.pdtb_accuracy@eng.pdtb.pdtb': 0.6806693989071039, 'train@eng.pdtb.pdtb_f1@eng.pdtb.pdtb': 0.46601720223450976, 'train@eng.pdtb.pdtb_precision@eng.pdtb.pdtb': 0.5316862833869057, 'train@eng.pdtb.pdtb_recall@eng.pdtb.pdtb': 0.45599563309420593, 'train@eng.pdtb.pdtb_loss@eng.pdtb.pdtb': 0.9500576257705688, 'train@eng.pdtb.pdtb_runtime': 524.4887, 'train@eng.pdtb.pdtb_samples_per_second': 83.739, 'train@eng.pdtb.pdtb_steps_per_second': 2.618, 'epoch': 7.0}
{'loss': 1.0101, 'learning_rate': 8.333333333333334e-06, 'epoch': 7.0}
{'eval_loss': 0.9104763865470886, 'eval_accuracy@eng.pdtb.pdtb': 0.6899641577060932, 'eval_f1@eng.pdtb.pdtb': 0.5359905515677361, 'eval_precision@eng.pdtb.pdtb': 0.6265630581543299, 'eval_recall@eng.pdtb.pdtb': 0.5112394511614238, 'eval_loss@eng.pdtb.pdtb': 0.9104763865470886, 'eval_runtime': 20.2884, 'eval_samples_per_second': 82.51, 'eval_steps_per_second': 2.612, 'epoch': 7.0}
{'train@eng.pdtb.pdtb_loss': 0.938462495803833, 'train@eng.pdtb.pdtb_accuracy@eng.pdtb.pdtb': 0.6822859744990892, 'train@eng.pdtb.pdtb_f1@eng.pdtb.pdtb': 0.47138891482644735, 'train@eng.pdtb.pdtb_precision@eng.pdtb.pdtb': 0.5201546754122481, 'train@eng.pdtb.pdtb_recall@eng.pdtb.pdtb': 0.467344821307564, 'train@eng.pdtb.pdtb_loss@eng.pdtb.pdtb': 0.938462495803833, 'train@eng.pdtb.pdtb_runtime': 524.8281, 'train@eng.pdtb.pdtb_samples_per_second': 83.685, 'train@eng.pdtb.pdtb_steps_per_second': 2.616, 'epoch': 8.0}
{'loss': 0.999, 'learning_rate': 6.666666666666667e-06, 'epoch': 8.0}
{'eval_loss': 0.9072310328483582, 'eval_accuracy@eng.pdtb.pdtb': 0.6923536439665472, 'eval_f1@eng.pdtb.pdtb': 0.5341552921915704, 'eval_precision@eng.pdtb.pdtb': 0.5733086039199718, 'eval_recall@eng.pdtb.pdtb': 0.5204849465814979, 'eval_loss@eng.pdtb.pdtb': 0.9072309136390686, 'eval_runtime': 20.2851, 'eval_samples_per_second': 82.523, 'eval_steps_per_second': 2.613, 'epoch': 8.0}
{'train@eng.pdtb.pdtb_loss': 0.9297934174537659, 'train@eng.pdtb.pdtb_accuracy@eng.pdtb.pdtb': 0.6862021857923497, 'train@eng.pdtb.pdtb_f1@eng.pdtb.pdtb': 0.47453189326838185, 'train@eng.pdtb.pdtb_precision@eng.pdtb.pdtb': 0.5284885949954986, 'train@eng.pdtb.pdtb_recall@eng.pdtb.pdtb': 0.46868826670721875, 'train@eng.pdtb.pdtb_loss@eng.pdtb.pdtb': 0.9297935366630554, 'train@eng.pdtb.pdtb_runtime': 525.1641, 'train@eng.pdtb.pdtb_samples_per_second': 83.631, 'train@eng.pdtb.pdtb_steps_per_second': 2.614, 'epoch': 9.0}
{'loss': 0.9853, 'learning_rate': 5e-06, 'epoch': 9.0}
{'eval_loss': 0.9047718644142151, 'eval_accuracy@eng.pdtb.pdtb': 0.7007168458781362, 'eval_f1@eng.pdtb.pdtb': 0.5545041665552449, 'eval_precision@eng.pdtb.pdtb': 0.6166687394824371, 'eval_recall@eng.pdtb.pdtb': 0.5421066138808097, 'eval_loss@eng.pdtb.pdtb': 0.9047718048095703, 'eval_runtime': 20.2928, 'eval_samples_per_second': 82.492, 'eval_steps_per_second': 2.612, 'epoch': 9.0}
{'train@eng.pdtb.pdtb_loss': 0.925957977771759, 'train@eng.pdtb.pdtb_accuracy@eng.pdtb.pdtb': 0.6872040072859745, 'train@eng.pdtb.pdtb_f1@eng.pdtb.pdtb': 0.4748564039839733, 'train@eng.pdtb.pdtb_precision@eng.pdtb.pdtb': 0.5344770117357457, 'train@eng.pdtb.pdtb_recall@eng.pdtb.pdtb': 0.46885108154327065, 'train@eng.pdtb.pdtb_loss@eng.pdtb.pdtb': 0.9259581565856934, 'train@eng.pdtb.pdtb_runtime': 524.5372, 'train@eng.pdtb.pdtb_samples_per_second': 83.731, 'train@eng.pdtb.pdtb_steps_per_second': 2.618, 'epoch': 10.0}
{'loss': 0.9831, 'learning_rate': 3.3333333333333333e-06, 'epoch': 10.0}
{'eval_loss': 0.8984290361404419, 'eval_accuracy@eng.pdtb.pdtb': 0.6929510155316607, 'eval_f1@eng.pdtb.pdtb': 0.5504499963715787, 'eval_precision@eng.pdtb.pdtb': 0.6156790344474976, 'eval_recall@eng.pdtb.pdtb': 0.5372995006803183, 'eval_loss@eng.pdtb.pdtb': 0.8984290957450867, 'eval_runtime': 20.2631, 'eval_samples_per_second': 82.613, 'eval_steps_per_second': 2.616, 'epoch': 10.0}
{'train@eng.pdtb.pdtb_loss': 0.9221647381782532, 'train@eng.pdtb.pdtb_accuracy@eng.pdtb.pdtb': 0.6878415300546448, 'train@eng.pdtb.pdtb_f1@eng.pdtb.pdtb': 0.4777496755642607, 'train@eng.pdtb.pdtb_precision@eng.pdtb.pdtb': 0.5244741521592621, 'train@eng.pdtb.pdtb_recall@eng.pdtb.pdtb': 0.4742272280009191, 'train@eng.pdtb.pdtb_loss@eng.pdtb.pdtb': 0.9221647381782532, 'train@eng.pdtb.pdtb_runtime': 524.4647, 'train@eng.pdtb.pdtb_samples_per_second': 83.743, 'train@eng.pdtb.pdtb_steps_per_second': 2.618, 'epoch': 11.0}
{'loss': 0.9751, 'learning_rate': 1.6666666666666667e-06, 'epoch': 11.0}
{'eval_loss': 0.8992217183113098, 'eval_accuracy@eng.pdtb.pdtb': 0.6989247311827957, 'eval_f1@eng.pdtb.pdtb': 0.5551068411509268, 'eval_precision@eng.pdtb.pdtb': 0.5901656275112553, 'eval_recall@eng.pdtb.pdtb': 0.5450628152774173, 'eval_loss@eng.pdtb.pdtb': 0.8992215394973755, 'eval_runtime': 20.2338, 'eval_samples_per_second': 82.733, 'eval_steps_per_second': 2.619, 'epoch': 11.0}
{'train@eng.pdtb.pdtb_loss': 0.9208411574363708, 'train@eng.pdtb.pdtb_accuracy@eng.pdtb.pdtb': 0.6876366120218579, 'train@eng.pdtb.pdtb_f1@eng.pdtb.pdtb': 0.4767202757030464, 'train@eng.pdtb.pdtb_precision@eng.pdtb.pdtb': 0.5295720231801389, 'train@eng.pdtb.pdtb_recall@eng.pdtb.pdtb': 0.4714603978108591, 'train@eng.pdtb.pdtb_loss@eng.pdtb.pdtb': 0.9208411574363708, 'train@eng.pdtb.pdtb_runtime': 524.5419, 'train@eng.pdtb.pdtb_samples_per_second': 83.73, 'train@eng.pdtb.pdtb_steps_per_second': 2.618, 'epoch': 12.0}
{'loss': 0.9726, 'learning_rate': 0.0, 'epoch': 12.0}
{'eval_loss': 0.8976103663444519, 'eval_accuracy@eng.pdtb.pdtb': 0.6977299880525687, 'eval_f1@eng.pdtb.pdtb': 0.5515654184989387, 'eval_precision@eng.pdtb.pdtb': 0.6127463037002068, 'eval_recall@eng.pdtb.pdtb': 0.5398243528304565, 'eval_loss@eng.pdtb.pdtb': 0.8976104259490967, 'eval_runtime': 20.2656, 'eval_samples_per_second': 82.603, 'eval_steps_per_second': 2.615, 'epoch': 12.0}
{'train_runtime': 19822.384, 'train_samples_per_second': 26.588, 'train_steps_per_second': 0.831, 'train_loss': 1.1022760660273845, 'epoch': 12.0}
***** train_log metrics *****
  epoch                    =       12.0
  train_loss               =     1.1023
  train_runtime            = 5:30:22.38
  train_samples_per_second =     26.588
  train_steps_per_second   =      0.831
{'train@tur.pdtb.tdb_loss': 1.9575239419937134, 'train@tur.pdtb.tdb_accuracy@tur.pdtb.tdb': 0.40677274581803347, 'train@tur.pdtb.tdb_f1@tur.pdtb.tdb': 0.1622900407890573, 'train@tur.pdtb.tdb_precision@tur.pdtb.tdb': 0.23046815474835358, 'train@tur.pdtb.tdb_recall@tur.pdtb.tdb': 0.17283888315260276, 'train@tur.pdtb.tdb_loss@tur.pdtb.tdb': 1.9575239419937134, 'train@tur.pdtb.tdb_runtime': 30.0212, 'train@tur.pdtb.tdb_samples_per_second': 81.642, 'train@tur.pdtb.tdb_steps_per_second': 2.565, 'epoch': 1.0}
{'loss': 2.159, 'learning_rate': 1.8333333333333333e-05, 'epoch': 1.0}
{'eval_loss': 2.075749158859253, 'eval_accuracy@tur.pdtb.tdb': 0.36538461538461536, 'eval_f1@tur.pdtb.tdb': 0.10104120637054245, 'eval_precision@tur.pdtb.tdb': 0.11427657487440096, 'eval_recall@tur.pdtb.tdb': 0.10365649715317625, 'eval_loss@tur.pdtb.tdb': 2.075749158859253, 'eval_runtime': 4.0672, 'eval_samples_per_second': 76.712, 'eval_steps_per_second': 2.459, 'epoch': 1.0}
{'train@tur.pdtb.tdb_loss': 1.8081387281417847, 'train@tur.pdtb.tdb_accuracy@tur.pdtb.tdb': 0.4394124847001224, 'train@tur.pdtb.tdb_f1@tur.pdtb.tdb': 0.19394820262724846, 'train@tur.pdtb.tdb_precision@tur.pdtb.tdb': 0.2855991506015761, 'train@tur.pdtb.tdb_recall@tur.pdtb.tdb': 0.18741731629266142, 'train@tur.pdtb.tdb_loss@tur.pdtb.tdb': 1.8081384897232056, 'train@tur.pdtb.tdb_runtime': 30.0328, 'train@tur.pdtb.tdb_samples_per_second': 81.611, 'train@tur.pdtb.tdb_steps_per_second': 2.564, 'epoch': 2.0}
{'loss': 1.953, 'learning_rate': 1.6666666666666667e-05, 'epoch': 2.0}
{'eval_loss': 1.9882391691207886, 'eval_accuracy@tur.pdtb.tdb': 0.38782051282051283, 'eval_f1@tur.pdtb.tdb': 0.13544440070735397, 'eval_precision@tur.pdtb.tdb': 0.1569004089520589, 'eval_recall@tur.pdtb.tdb': 0.1313368075671883, 'eval_loss@tur.pdtb.tdb': 1.9882391691207886, 'eval_runtime': 4.0686, 'eval_samples_per_second': 76.686, 'eval_steps_per_second': 2.458, 'epoch': 2.0}
{'train@tur.pdtb.tdb_loss': 1.725893497467041, 'train@tur.pdtb.tdb_accuracy@tur.pdtb.tdb': 0.4614443084455324, 'train@tur.pdtb.tdb_f1@tur.pdtb.tdb': 0.20413180911346404, 'train@tur.pdtb.tdb_precision@tur.pdtb.tdb': 0.3441840808544264, 'train@tur.pdtb.tdb_recall@tur.pdtb.tdb': 0.19421260338753457, 'train@tur.pdtb.tdb_loss@tur.pdtb.tdb': 1.725893497467041, 'train@tur.pdtb.tdb_runtime': 30.0241, 'train@tur.pdtb.tdb_samples_per_second': 81.635, 'train@tur.pdtb.tdb_steps_per_second': 2.565, 'epoch': 3.0}
{'loss': 1.8499, 'learning_rate': 1.5000000000000002e-05, 'epoch': 3.0}
{'eval_loss': 1.9657498598098755, 'eval_accuracy@tur.pdtb.tdb': 0.40064102564102566, 'eval_f1@tur.pdtb.tdb': 0.14488960313407964, 'eval_precision@tur.pdtb.tdb': 0.16095330747519818, 'eval_recall@tur.pdtb.tdb': 0.1447611198247083, 'eval_loss@tur.pdtb.tdb': 1.9657498598098755, 'eval_runtime': 4.1009, 'eval_samples_per_second': 76.08, 'eval_steps_per_second': 2.438, 'epoch': 3.0}
{'train@tur.pdtb.tdb_loss': 1.6490893363952637, 'train@tur.pdtb.tdb_accuracy@tur.pdtb.tdb': 0.4949000407996736, 'train@tur.pdtb.tdb_f1@tur.pdtb.tdb': 0.26994543427285306, 'train@tur.pdtb.tdb_precision@tur.pdtb.tdb': 0.38249797124471757, 'train@tur.pdtb.tdb_recall@tur.pdtb.tdb': 0.24653416849045062, 'train@tur.pdtb.tdb_loss@tur.pdtb.tdb': 1.6490893363952637, 'train@tur.pdtb.tdb_runtime': 30.0335, 'train@tur.pdtb.tdb_samples_per_second': 81.609, 'train@tur.pdtb.tdb_steps_per_second': 2.564, 'epoch': 4.0}
{'loss': 1.7704, 'learning_rate': 1.3333333333333333e-05, 'epoch': 4.0}
{'eval_loss': 1.933740496635437, 'eval_accuracy@tur.pdtb.tdb': 0.4230769230769231, 'eval_f1@tur.pdtb.tdb': 0.1882674472951911, 'eval_precision@tur.pdtb.tdb': 0.2022343273391668, 'eval_recall@tur.pdtb.tdb': 0.19074995846928963, 'eval_loss@tur.pdtb.tdb': 1.933740496635437, 'eval_runtime': 4.0761, 'eval_samples_per_second': 76.544, 'eval_steps_per_second': 2.453, 'epoch': 4.0}
{'train@tur.pdtb.tdb_loss': 1.6026017665863037, 'train@tur.pdtb.tdb_accuracy@tur.pdtb.tdb': 0.510811913504692, 'train@tur.pdtb.tdb_f1@tur.pdtb.tdb': 0.2919868485972062, 'train@tur.pdtb.tdb_precision@tur.pdtb.tdb': 0.38463150938697593, 'train@tur.pdtb.tdb_recall@tur.pdtb.tdb': 0.27097612236172525, 'train@tur.pdtb.tdb_loss@tur.pdtb.tdb': 1.6026017665863037, 'train@tur.pdtb.tdb_runtime': 30.1, 'train@tur.pdtb.tdb_samples_per_second': 81.429, 'train@tur.pdtb.tdb_steps_per_second': 2.558, 'epoch': 5.0}
{'loss': 1.7136, 'learning_rate': 1.1666666666666668e-05, 'epoch': 5.0}
{'eval_loss': 1.9218311309814453, 'eval_accuracy@tur.pdtb.tdb': 0.40705128205128205, 'eval_f1@tur.pdtb.tdb': 0.19347382110446984, 'eval_precision@tur.pdtb.tdb': 0.20081409293997768, 'eval_recall@tur.pdtb.tdb': 0.20398595469607253, 'eval_loss@tur.pdtb.tdb': 1.9218310117721558, 'eval_runtime': 4.0679, 'eval_samples_per_second': 76.698, 'eval_steps_per_second': 2.458, 'epoch': 5.0}
{'train@tur.pdtb.tdb_loss': 1.5688841342926025, 'train@tur.pdtb.tdb_accuracy@tur.pdtb.tdb': 0.5201958384332925, 'train@tur.pdtb.tdb_f1@tur.pdtb.tdb': 0.31060283607322453, 'train@tur.pdtb.tdb_precision@tur.pdtb.tdb': 0.38954787422834636, 'train@tur.pdtb.tdb_recall@tur.pdtb.tdb': 0.29109317950393976, 'train@tur.pdtb.tdb_loss@tur.pdtb.tdb': 1.568884015083313, 'train@tur.pdtb.tdb_runtime': 30.084, 'train@tur.pdtb.tdb_samples_per_second': 81.472, 'train@tur.pdtb.tdb_steps_per_second': 2.559, 'epoch': 6.0}
{'loss': 1.6683, 'learning_rate': 1e-05, 'epoch': 6.0}
{'eval_loss': 1.9131646156311035, 'eval_accuracy@tur.pdtb.tdb': 0.40705128205128205, 'eval_f1@tur.pdtb.tdb': 0.20676263252331697, 'eval_precision@tur.pdtb.tdb': 0.2472379762763534, 'eval_recall@tur.pdtb.tdb': 0.2110107480845023, 'eval_loss@tur.pdtb.tdb': 1.9131643772125244, 'eval_runtime': 4.116, 'eval_samples_per_second': 75.802, 'eval_steps_per_second': 2.43, 'epoch': 6.0}
{'train@tur.pdtb.tdb_loss': 1.542934536933899, 'train@tur.pdtb.tdb_accuracy@tur.pdtb.tdb': 0.5287637698898409, 'train@tur.pdtb.tdb_f1@tur.pdtb.tdb': 0.31219860757445855, 'train@tur.pdtb.tdb_precision@tur.pdtb.tdb': 0.37986478878230867, 'train@tur.pdtb.tdb_recall@tur.pdtb.tdb': 0.2960779206613202, 'train@tur.pdtb.tdb_loss@tur.pdtb.tdb': 1.542934536933899, 'train@tur.pdtb.tdb_runtime': 30.0847, 'train@tur.pdtb.tdb_samples_per_second': 81.47, 'train@tur.pdtb.tdb_steps_per_second': 2.559, 'epoch': 7.0}
{'loss': 1.6443, 'learning_rate': 8.333333333333334e-06, 'epoch': 7.0}
{'eval_loss': 1.8939810991287231, 'eval_accuracy@tur.pdtb.tdb': 0.40384615384615385, 'eval_f1@tur.pdtb.tdb': 0.2054795097084481, 'eval_precision@tur.pdtb.tdb': 0.2417219144491872, 'eval_recall@tur.pdtb.tdb': 0.21160111233153922, 'eval_loss@tur.pdtb.tdb': 1.8939810991287231, 'eval_runtime': 4.071, 'eval_samples_per_second': 76.639, 'eval_steps_per_second': 2.456, 'epoch': 7.0}
{'train@tur.pdtb.tdb_loss': 1.5265488624572754, 'train@tur.pdtb.tdb_accuracy@tur.pdtb.tdb': 0.529579763361893, 'train@tur.pdtb.tdb_f1@tur.pdtb.tdb': 0.31977208056475975, 'train@tur.pdtb.tdb_precision@tur.pdtb.tdb': 0.38556860079291133, 'train@tur.pdtb.tdb_recall@tur.pdtb.tdb': 0.3058175200831216, 'train@tur.pdtb.tdb_loss@tur.pdtb.tdb': 1.5265487432479858, 'train@tur.pdtb.tdb_runtime': 30.0273, 'train@tur.pdtb.tdb_samples_per_second': 81.626, 'train@tur.pdtb.tdb_steps_per_second': 2.564, 'epoch': 8.0}
{'loss': 1.62, 'learning_rate': 6.666666666666667e-06, 'epoch': 8.0}
{'eval_loss': 1.8881539106369019, 'eval_accuracy@tur.pdtb.tdb': 0.40064102564102566, 'eval_f1@tur.pdtb.tdb': 0.2045975245500862, 'eval_precision@tur.pdtb.tdb': 0.2410504555375856, 'eval_recall@tur.pdtb.tdb': 0.20991545783258553, 'eval_loss@tur.pdtb.tdb': 1.8881539106369019, 'eval_runtime': 4.0684, 'eval_samples_per_second': 76.689, 'eval_steps_per_second': 2.458, 'epoch': 8.0}
{'train@tur.pdtb.tdb_loss': 1.5140553712844849, 'train@tur.pdtb.tdb_accuracy@tur.pdtb.tdb': 0.5348837209302325, 'train@tur.pdtb.tdb_f1@tur.pdtb.tdb': 0.323389946031332, 'train@tur.pdtb.tdb_precision@tur.pdtb.tdb': 0.3938682209264934, 'train@tur.pdtb.tdb_recall@tur.pdtb.tdb': 0.3068222728315262, 'train@tur.pdtb.tdb_loss@tur.pdtb.tdb': 1.5140552520751953, 'train@tur.pdtb.tdb_runtime': 29.9968, 'train@tur.pdtb.tdb_samples_per_second': 81.709, 'train@tur.pdtb.tdb_steps_per_second': 2.567, 'epoch': 9.0}
{'loss': 1.6012, 'learning_rate': 5e-06, 'epoch': 9.0}
{'eval_loss': 1.8836487531661987, 'eval_accuracy@tur.pdtb.tdb': 0.41025641025641024, 'eval_f1@tur.pdtb.tdb': 0.20919838033457647, 'eval_precision@tur.pdtb.tdb': 0.25015964883977304, 'eval_recall@tur.pdtb.tdb': 0.21111789604392894, 'eval_loss@tur.pdtb.tdb': 1.8836487531661987, 'eval_runtime': 4.1213, 'eval_samples_per_second': 75.705, 'eval_steps_per_second': 2.426, 'epoch': 9.0}
{'train@tur.pdtb.tdb_loss': 1.503613829612732, 'train@tur.pdtb.tdb_accuracy@tur.pdtb.tdb': 0.5377396980824154, 'train@tur.pdtb.tdb_f1@tur.pdtb.tdb': 0.32551278348153917, 'train@tur.pdtb.tdb_precision@tur.pdtb.tdb': 0.39494096445131843, 'train@tur.pdtb.tdb_recall@tur.pdtb.tdb': 0.3105528665512152, 'train@tur.pdtb.tdb_loss@tur.pdtb.tdb': 1.503613829612732, 'train@tur.pdtb.tdb_runtime': 30.0383, 'train@tur.pdtb.tdb_samples_per_second': 81.596, 'train@tur.pdtb.tdb_steps_per_second': 2.563, 'epoch': 10.0}
{'loss': 1.5905, 'learning_rate': 3.3333333333333333e-06, 'epoch': 10.0}
{'eval_loss': 1.8752456903457642, 'eval_accuracy@tur.pdtb.tdb': 0.40384615384615385, 'eval_f1@tur.pdtb.tdb': 0.20508860443920449, 'eval_precision@tur.pdtb.tdb': 0.24301188156509668, 'eval_recall@tur.pdtb.tdb': 0.21002260579201215, 'eval_loss@tur.pdtb.tdb': 1.875245451927185, 'eval_runtime': 4.0713, 'eval_samples_per_second': 76.635, 'eval_steps_per_second': 2.456, 'epoch': 10.0}
{'train@tur.pdtb.tdb_loss': 1.4982080459594727, 'train@tur.pdtb.tdb_accuracy@tur.pdtb.tdb': 0.5385556915544676, 'train@tur.pdtb.tdb_f1@tur.pdtb.tdb': 0.3266116866997735, 'train@tur.pdtb.tdb_precision@tur.pdtb.tdb': 0.3906194733187068, 'train@tur.pdtb.tdb_recall@tur.pdtb.tdb': 0.31231229535334576, 'train@tur.pdtb.tdb_loss@tur.pdtb.tdb': 1.4982080459594727, 'train@tur.pdtb.tdb_runtime': 30.0059, 'train@tur.pdtb.tdb_samples_per_second': 81.684, 'train@tur.pdtb.tdb_steps_per_second': 2.566, 'epoch': 11.0}
{'loss': 1.5765, 'learning_rate': 1.6666666666666667e-06, 'epoch': 11.0}
{'eval_loss': 1.8758865594863892, 'eval_accuracy@tur.pdtb.tdb': 0.40384615384615385, 'eval_f1@tur.pdtb.tdb': 0.20532408773280528, 'eval_precision@tur.pdtb.tdb': 0.24396046030602922, 'eval_recall@tur.pdtb.tdb': 0.21002260579201215, 'eval_loss@tur.pdtb.tdb': 1.87588632106781, 'eval_runtime': 4.0487, 'eval_samples_per_second': 77.061, 'eval_steps_per_second': 2.47, 'epoch': 11.0}
{'train@tur.pdtb.tdb_loss': 1.496154546737671, 'train@tur.pdtb.tdb_accuracy@tur.pdtb.tdb': 0.5369237046103631, 'train@tur.pdtb.tdb_f1@tur.pdtb.tdb': 0.33276568598722384, 'train@tur.pdtb.tdb_precision@tur.pdtb.tdb': 0.39669069249141914, 'train@tur.pdtb.tdb_recall@tur.pdtb.tdb': 0.3177047789489958, 'train@tur.pdtb.tdb_loss@tur.pdtb.tdb': 1.4961546659469604, 'train@tur.pdtb.tdb_runtime': 30.0262, 'train@tur.pdtb.tdb_samples_per_second': 81.629, 'train@tur.pdtb.tdb_steps_per_second': 2.564, 'epoch': 12.0}
{'loss': 1.5922, 'learning_rate': 0.0, 'epoch': 12.0}
{'eval_loss': 1.8747762441635132, 'eval_accuracy@tur.pdtb.tdb': 0.40064102564102566, 'eval_f1@tur.pdtb.tdb': 0.2044454863072772, 'eval_precision@tur.pdtb.tdb': 0.2414630462396656, 'eval_recall@tur.pdtb.tdb': 0.20947496066605378, 'eval_loss@tur.pdtb.tdb': 1.874776005744934, 'eval_runtime': 4.0525, 'eval_samples_per_second': 76.99, 'eval_steps_per_second': 2.468, 'epoch': 12.0}
{'train_runtime': 1164.6868, 'train_samples_per_second': 25.253, 'train_steps_per_second': 0.793, 'train_loss': 1.728240388812441, 'epoch': 12.0}
***** train_logger metrics *****
  epoch                    =       12.0
  train_loss               =     1.1023
  train_runtime            = 5:30:22.38
  train_samples_per_second =     26.588
  train_steps_per_second   =      0.831
-------------------------------------------------------------------
Lang1:  eng.rst.gum    Lang2:  tur.pdtb.tdb
Saving run to:  runs/full_shot/FullShot=v4_eng.rst.gum_tur.pdtb.tdb_lrfunc=Adafactor_lr=2e-05_plm=bert-base-multilingual-cased
Running with params: BERT_MODEL=bert-base-multilingual-cased lr=2e-05
read 13897 examples
read 2149 examples
read 2091 examples
read 2451 examples
read 312 examples
read 422 examples
Total prediction labels:  46
BertAdapterModel(
  (shared_parameters): ModuleDict()
  (bert): BertModel(
    (shared_parameters): ModuleDict()
    (invertible_adapters): ModuleDict(
      (en): NICECouplingBlock(
        (F): Sequential(
          (0): Linear(in_features=384, out_features=192, bias=True)
          (1): Activation_Function_Class(
            (f): ReLU()
          )
          (2): Linear(in_features=192, out_features=384, bias=True)
        )
        (G): Sequential(
          (0): Linear(in_features=384, out_features=192, bias=True)
          (1): Activation_Function_Class(
            (f): ReLU()
          )
          (2): Linear(in_features=192, out_features=384, bias=True)
        )
      )
      (tr): NICECouplingBlock(
        (F): Sequential(
          (0): Linear(in_features=384, out_features=192, bias=True)
          (1): Activation_Function_Class(
            (f): NewGELUActivation()
          )
          (2): Linear(in_features=192, out_features=384, bias=True)
        )
        (G): Sequential(
          (0): Linear(in_features=384, out_features=192, bias=True)
          (1): Activation_Function_Class(
            (f): NewGELUActivation()
          )
          (2): Linear(in_features=192, out_features=384, bias=True)
        )
      )
    )
    (embeddings): BertEmbeddings(
      (word_embeddings): Embedding(119547, 768, padding_idx=0)
      (position_embeddings): Embedding(512, 768)
      (token_type_embeddings): Embedding(2, 768)
      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
      (dropout): Dropout(p=0.1, inplace=False)
    )
    (encoder): BertEncoder(
      (layer): ModuleList(
        (0): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (en): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (tr): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (1): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (en): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (tr): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (2): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (en): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (tr): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (3): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (en): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (tr): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (4): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (en): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (tr): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (5): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (en): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (tr): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (6): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (en): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (tr): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (7): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (en): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (tr): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (8): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (en): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (tr): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (9): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (en): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (tr): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (10): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (en): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (tr): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (11): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (en): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (tr): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
      )
    )
    (pooler): BertPooler(
      (dense): Linear(in_features=768, out_features=768, bias=True)
      (activation): Tanh()
    )
    (prefix_tuning): PrefixTuningPool(
      (prefix_tunings): ModuleDict()
    )
  )
  (heads): ModuleDict(
    (tr): BertStyleMaskedLMHead(
      (0): Linear(in_features=768, out_features=768, bias=True)
      (1): Activation_Function_Class(
        (f): GELUActivation()
      )
      (2): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
      (3): Linear(in_features=768, out_features=119547, bias=True)
    )
    (disrpt): ClassificationHead(
      (0): Dropout(p=0.1, inplace=False)
      (1): Linear(in_features=768, out_features=768, bias=True)
      (2): Activation_Function_Class(
        (f): Tanh()
      )
      (3): Dropout(p=0.1, inplace=False)
      (4): Linear(in_features=768, out_features=46, bias=True)
    )
  )
)
{'train@eng.rst.gum_loss': 2.555878162384033, 'train@eng.rst.gum_accuracy@eng.rst.gum': 0.241490969273944, 'train@eng.rst.gum_f1@eng.rst.gum': 0.03229987252323197, 'train@eng.rst.gum_precision@eng.rst.gum': 0.07156327665232644, 'train@eng.rst.gum_recall@eng.rst.gum': 0.05289135779445035, 'train@eng.rst.gum_loss@eng.rst.gum': 2.555878162384033, 'train@eng.rst.gum_runtime': 166.4588, 'train@eng.rst.gum_samples_per_second': 83.486, 'train@eng.rst.gum_steps_per_second': 2.613, 'epoch': 1.0}
{'loss': 2.8728, 'learning_rate': 1.8333333333333333e-05, 'epoch': 1.0}
{'eval_loss': 2.6330864429473877, 'eval_accuracy@eng.rst.gum': 0.2415076779897627, 'eval_f1@eng.rst.gum': 0.04031572498940463, 'eval_precision@eng.rst.gum': 0.08195272966318295, 'eval_recall@eng.rst.gum': 0.0584616819622109, 'eval_loss@eng.rst.gum': 2.6330864429473877, 'eval_runtime': 26.1272, 'eval_samples_per_second': 82.252, 'eval_steps_per_second': 2.603, 'epoch': 1.0}
{'train@eng.rst.gum_loss': 2.1108484268188477, 'train@eng.rst.gum_accuracy@eng.rst.gum': 0.3905878966683457, 'train@eng.rst.gum_f1@eng.rst.gum': 0.15767811531904247, 'train@eng.rst.gum_precision@eng.rst.gum': 0.21985684817883497, 'train@eng.rst.gum_recall@eng.rst.gum': 0.16820255964705882, 'train@eng.rst.gum_loss@eng.rst.gum': 2.1108484268188477, 'train@eng.rst.gum_runtime': 166.5857, 'train@eng.rst.gum_samples_per_second': 83.423, 'train@eng.rst.gum_steps_per_second': 2.611, 'epoch': 2.0}
{'loss': 2.4012, 'learning_rate': 1.6666666666666667e-05, 'epoch': 2.0}
{'eval_loss': 2.2010700702667236, 'eval_accuracy@eng.rst.gum': 0.3652861796184272, 'eval_f1@eng.rst.gum': 0.15240214132115734, 'eval_precision@eng.rst.gum': 0.23855821615833878, 'eval_recall@eng.rst.gum': 0.1660227663419572, 'eval_loss@eng.rst.gum': 2.201070547103882, 'eval_runtime': 26.1005, 'eval_samples_per_second': 82.336, 'eval_steps_per_second': 2.605, 'epoch': 2.0}
{'train@eng.rst.gum_loss': 1.8247549533843994, 'train@eng.rst.gum_accuracy@eng.rst.gum': 0.4736274015974671, 'train@eng.rst.gum_f1@eng.rst.gum': 0.25833778667500507, 'train@eng.rst.gum_precision@eng.rst.gum': 0.3691721866413709, 'train@eng.rst.gum_recall@eng.rst.gum': 0.27064079929727397, 'train@eng.rst.gum_loss@eng.rst.gum': 1.8247549533843994, 'train@eng.rst.gum_runtime': 166.4545, 'train@eng.rst.gum_samples_per_second': 83.488, 'train@eng.rst.gum_steps_per_second': 2.613, 'epoch': 3.0}
{'loss': 2.0301, 'learning_rate': 1.5000000000000002e-05, 'epoch': 3.0}
{'eval_loss': 1.9511528015136719, 'eval_accuracy@eng.rst.gum': 0.43322475570032576, 'eval_f1@eng.rst.gum': 0.24636829796153403, 'eval_precision@eng.rst.gum': 0.3371280282800978, 'eval_recall@eng.rst.gum': 0.26034614636919945, 'eval_loss@eng.rst.gum': 1.951153039932251, 'eval_runtime': 26.1052, 'eval_samples_per_second': 82.321, 'eval_steps_per_second': 2.605, 'epoch': 3.0}
{'train@eng.rst.gum_loss': 1.6972752809524536, 'train@eng.rst.gum_accuracy@eng.rst.gum': 0.502626466143772, 'train@eng.rst.gum_f1@eng.rst.gum': 0.2899906604580553, 'train@eng.rst.gum_precision@eng.rst.gum': 0.4006770022148583, 'train@eng.rst.gum_recall@eng.rst.gum': 0.3030638571651845, 'train@eng.rst.gum_loss@eng.rst.gum': 1.6972752809524536, 'train@eng.rst.gum_runtime': 166.5375, 'train@eng.rst.gum_samples_per_second': 83.447, 'train@eng.rst.gum_steps_per_second': 2.612, 'epoch': 4.0}
{'loss': 1.8334, 'learning_rate': 1.3333333333333333e-05, 'epoch': 4.0}
{'eval_loss': 1.8585000038146973, 'eval_accuracy@eng.rst.gum': 0.46579804560260585, 'eval_f1@eng.rst.gum': 0.2770019208081922, 'eval_precision@eng.rst.gum': 0.3318882370547243, 'eval_recall@eng.rst.gum': 0.2934318546345427, 'eval_loss@eng.rst.gum': 1.8585000038146973, 'eval_runtime': 26.1397, 'eval_samples_per_second': 82.212, 'eval_steps_per_second': 2.601, 'epoch': 4.0}
{'train@eng.rst.gum_loss': 1.618678092956543, 'train@eng.rst.gum_accuracy@eng.rst.gum': 0.5152191120385695, 'train@eng.rst.gum_f1@eng.rst.gum': 0.319440936580805, 'train@eng.rst.gum_precision@eng.rst.gum': 0.40448187844920314, 'train@eng.rst.gum_recall@eng.rst.gum': 0.33478087786906335, 'train@eng.rst.gum_loss@eng.rst.gum': 1.618678092956543, 'train@eng.rst.gum_runtime': 166.6169, 'train@eng.rst.gum_samples_per_second': 83.407, 'train@eng.rst.gum_steps_per_second': 2.611, 'epoch': 5.0}
{'loss': 1.731, 'learning_rate': 1.1666666666666668e-05, 'epoch': 5.0}
{'eval_loss': 1.7998818159103394, 'eval_accuracy@eng.rst.gum': 0.4806886924150768, 'eval_f1@eng.rst.gum': 0.30586332668500005, 'eval_precision@eng.rst.gum': 0.32143692964212295, 'eval_recall@eng.rst.gum': 0.3250329529105637, 'eval_loss@eng.rst.gum': 1.7998816967010498, 'eval_runtime': 26.1304, 'eval_samples_per_second': 82.241, 'eval_steps_per_second': 2.602, 'epoch': 5.0}
{'train@eng.rst.gum_loss': 1.564801573753357, 'train@eng.rst.gum_accuracy@eng.rst.gum': 0.5308339929481183, 'train@eng.rst.gum_f1@eng.rst.gum': 0.35230117043986475, 'train@eng.rst.gum_precision@eng.rst.gum': 0.4500154078843826, 'train@eng.rst.gum_recall@eng.rst.gum': 0.35952158442719434, 'train@eng.rst.gum_loss@eng.rst.gum': 1.564801573753357, 'train@eng.rst.gum_runtime': 166.5249, 'train@eng.rst.gum_samples_per_second': 83.453, 'train@eng.rst.gum_steps_per_second': 2.612, 'epoch': 6.0}
{'loss': 1.6708, 'learning_rate': 1e-05, 'epoch': 6.0}
{'eval_loss': 1.7577241659164429, 'eval_accuracy@eng.rst.gum': 0.49418334108887857, 'eval_f1@eng.rst.gum': 0.3322224737106853, 'eval_precision@eng.rst.gum': 0.3962274062763854, 'eval_recall@eng.rst.gum': 0.345704594511155, 'eval_loss@eng.rst.gum': 1.7577242851257324, 'eval_runtime': 26.1447, 'eval_samples_per_second': 82.196, 'eval_steps_per_second': 2.601, 'epoch': 6.0}
{'train@eng.rst.gum_loss': 1.5292495489120483, 'train@eng.rst.gum_accuracy@eng.rst.gum': 0.5374541267899546, 'train@eng.rst.gum_f1@eng.rst.gum': 0.3690089465918312, 'train@eng.rst.gum_precision@eng.rst.gum': 0.46784063029158846, 'train@eng.rst.gum_recall@eng.rst.gum': 0.37674660817571565, 'train@eng.rst.gum_loss@eng.rst.gum': 1.529249668121338, 'train@eng.rst.gum_runtime': 166.4229, 'train@eng.rst.gum_samples_per_second': 83.504, 'train@eng.rst.gum_steps_per_second': 2.614, 'epoch': 7.0}
{'loss': 1.6225, 'learning_rate': 8.333333333333334e-06, 'epoch': 7.0}
{'eval_loss': 1.735572338104248, 'eval_accuracy@eng.rst.gum': 0.49976733364355513, 'eval_f1@eng.rst.gum': 0.3586457610660175, 'eval_precision@eng.rst.gum': 0.4295479097521476, 'eval_recall@eng.rst.gum': 0.37012817310178503, 'eval_loss@eng.rst.gum': 1.7355724573135376, 'eval_runtime': 26.0885, 'eval_samples_per_second': 82.373, 'eval_steps_per_second': 2.607, 'epoch': 7.0}
{'train@eng.rst.gum_loss': 1.502845048904419, 'train@eng.rst.gum_accuracy@eng.rst.gum': 0.5465927898107505, 'train@eng.rst.gum_f1@eng.rst.gum': 0.3860533263472434, 'train@eng.rst.gum_precision@eng.rst.gum': 0.5052782429402629, 'train@eng.rst.gum_recall@eng.rst.gum': 0.38943551453183256, 'train@eng.rst.gum_loss@eng.rst.gum': 1.5028448104858398, 'train@eng.rst.gum_runtime': 166.3928, 'train@eng.rst.gum_samples_per_second': 83.519, 'train@eng.rst.gum_steps_per_second': 2.614, 'epoch': 8.0}
{'loss': 1.5917, 'learning_rate': 6.666666666666667e-06, 'epoch': 8.0}
{'eval_loss': 1.7166199684143066, 'eval_accuracy@eng.rst.gum': 0.5072126570497906, 'eval_f1@eng.rst.gum': 0.3709559638732009, 'eval_precision@eng.rst.gum': 0.4112923737067751, 'eval_recall@eng.rst.gum': 0.3828522676417133, 'eval_loss@eng.rst.gum': 1.7166200876235962, 'eval_runtime': 26.1409, 'eval_samples_per_second': 82.208, 'eval_steps_per_second': 2.601, 'epoch': 8.0}
{'train@eng.rst.gum_loss': 1.4834083318710327, 'train@eng.rst.gum_accuracy@eng.rst.gum': 0.5548679571130459, 'train@eng.rst.gum_f1@eng.rst.gum': 0.4066081720859933, 'train@eng.rst.gum_precision@eng.rst.gum': 0.508595643568172, 'train@eng.rst.gum_recall@eng.rst.gum': 0.40894962172527, 'train@eng.rst.gum_loss@eng.rst.gum': 1.4834084510803223, 'train@eng.rst.gum_runtime': 166.2164, 'train@eng.rst.gum_samples_per_second': 83.608, 'train@eng.rst.gum_steps_per_second': 2.617, 'epoch': 9.0}
{'loss': 1.5674, 'learning_rate': 5e-06, 'epoch': 9.0}
{'eval_loss': 1.7009484767913818, 'eval_accuracy@eng.rst.gum': 0.5067473243369008, 'eval_f1@eng.rst.gum': 0.3772047109300387, 'eval_precision@eng.rst.gum': 0.4068090663079607, 'eval_recall@eng.rst.gum': 0.3932893275627401, 'eval_loss@eng.rst.gum': 1.7009484767913818, 'eval_runtime': 26.1265, 'eval_samples_per_second': 82.254, 'eval_steps_per_second': 2.603, 'epoch': 9.0}
{'train@eng.rst.gum_loss': 1.46796452999115, 'train@eng.rst.gum_accuracy@eng.rst.gum': 0.5596891415413399, 'train@eng.rst.gum_f1@eng.rst.gum': 0.41273729859303465, 'train@eng.rst.gum_precision@eng.rst.gum': 0.51660076364976, 'train@eng.rst.gum_recall@eng.rst.gum': 0.41114054551982665, 'train@eng.rst.gum_loss@eng.rst.gum': 1.4679646492004395, 'train@eng.rst.gum_runtime': 166.2028, 'train@eng.rst.gum_samples_per_second': 83.615, 'train@eng.rst.gum_steps_per_second': 2.617, 'epoch': 10.0}
{'loss': 1.5513, 'learning_rate': 3.3333333333333333e-06, 'epoch': 10.0}
{'eval_loss': 1.6908224821090698, 'eval_accuracy@eng.rst.gum': 0.5141926477431363, 'eval_f1@eng.rst.gum': 0.38250118277030487, 'eval_precision@eng.rst.gum': 0.41542401717478855, 'eval_recall@eng.rst.gum': 0.393131164179709, 'eval_loss@eng.rst.gum': 1.6908224821090698, 'eval_runtime': 26.0846, 'eval_samples_per_second': 82.386, 'eval_steps_per_second': 2.607, 'epoch': 10.0}
{'train@eng.rst.gum_loss': 1.4612247943878174, 'train@eng.rst.gum_accuracy@eng.rst.gum': 0.5606965532129237, 'train@eng.rst.gum_f1@eng.rst.gum': 0.416425790757849, 'train@eng.rst.gum_precision@eng.rst.gum': 0.5150405412163607, 'train@eng.rst.gum_recall@eng.rst.gum': 0.41699339114501754, 'train@eng.rst.gum_loss@eng.rst.gum': 1.461224913597107, 'train@eng.rst.gum_runtime': 166.2008, 'train@eng.rst.gum_samples_per_second': 83.616, 'train@eng.rst.gum_steps_per_second': 2.617, 'epoch': 11.0}
{'loss': 1.5431, 'learning_rate': 1.6666666666666667e-06, 'epoch': 11.0}
{'eval_loss': 1.6849948167800903, 'eval_accuracy@eng.rst.gum': 0.5127966496044672, 'eval_f1@eng.rst.gum': 0.38605110215501187, 'eval_precision@eng.rst.gum': 0.427503644481728, 'eval_recall@eng.rst.gum': 0.3991033894265647, 'eval_loss@eng.rst.gum': 1.6849946975708008, 'eval_runtime': 26.1172, 'eval_samples_per_second': 82.283, 'eval_steps_per_second': 2.604, 'epoch': 11.0}
{'train@eng.rst.gum_loss': 1.4579793214797974, 'train@eng.rst.gum_accuracy@eng.rst.gum': 0.5615600489314241, 'train@eng.rst.gum_f1@eng.rst.gum': 0.4179427708132361, 'train@eng.rst.gum_precision@eng.rst.gum': 0.5197650932891102, 'train@eng.rst.gum_recall@eng.rst.gum': 0.41740142453035844, 'train@eng.rst.gum_loss@eng.rst.gum': 1.457979679107666, 'train@eng.rst.gum_runtime': 166.0482, 'train@eng.rst.gum_samples_per_second': 83.693, 'train@eng.rst.gum_steps_per_second': 2.62, 'epoch': 12.0}
{'loss': 1.53, 'learning_rate': 0.0, 'epoch': 12.0}
{'eval_loss': 1.6829997301101685, 'eval_accuracy@eng.rst.gum': 0.5146579804560261, 'eval_f1@eng.rst.gum': 0.3877421967096243, 'eval_precision@eng.rst.gum': 0.4310378771888102, 'eval_recall@eng.rst.gum': 0.4001030923058918, 'eval_loss@eng.rst.gum': 1.682999849319458, 'eval_runtime': 26.056, 'eval_samples_per_second': 82.476, 'eval_steps_per_second': 2.61, 'epoch': 12.0}
{'train_runtime': 6518.004, 'train_samples_per_second': 25.585, 'train_steps_per_second': 0.801, 'train_loss': 1.828763647554478, 'epoch': 12.0}
***** train_log metrics *****
  epoch                    =       12.0
  train_loss               =     1.8288
  train_runtime            = 1:48:38.00
  train_samples_per_second =     25.585
  train_steps_per_second   =      0.801
{'train@tur.pdtb.tdb_loss': 2.569973945617676, 'train@tur.pdtb.tdb_accuracy@tur.pdtb.tdb': 0.15136678906568748, 'train@tur.pdtb.tdb_f1@tur.pdtb.tdb': 0.054614986694978954, 'train@tur.pdtb.tdb_precision@tur.pdtb.tdb': 0.06419058374068182, 'train@tur.pdtb.tdb_recall@tur.pdtb.tdb': 0.05971576780184074, 'train@tur.pdtb.tdb_loss@tur.pdtb.tdb': 2.569973945617676, 'train@tur.pdtb.tdb_runtime': 30.139, 'train@tur.pdtb.tdb_samples_per_second': 81.323, 'train@tur.pdtb.tdb_steps_per_second': 2.555, 'epoch': 1.0}
{'loss': 3.0366, 'learning_rate': 1.8333333333333333e-05, 'epoch': 1.0}
{'eval_loss': 2.499068021774292, 'eval_accuracy@tur.pdtb.tdb': 0.21153846153846154, 'eval_f1@tur.pdtb.tdb': 0.07793362227754697, 'eval_precision@tur.pdtb.tdb': 0.09496022505545208, 'eval_recall@tur.pdtb.tdb': 0.08894934150303453, 'eval_loss@tur.pdtb.tdb': 2.499067544937134, 'eval_runtime': 4.1946, 'eval_samples_per_second': 74.382, 'eval_steps_per_second': 2.384, 'epoch': 1.0}
{'train@tur.pdtb.tdb_loss': 2.2767624855041504, 'train@tur.pdtb.tdb_accuracy@tur.pdtb.tdb': 0.2827417380660955, 'train@tur.pdtb.tdb_f1@tur.pdtb.tdb': 0.07461613943657879, 'train@tur.pdtb.tdb_precision@tur.pdtb.tdb': 0.08034428678491577, 'train@tur.pdtb.tdb_recall@tur.pdtb.tdb': 0.09420861037410234, 'train@tur.pdtb.tdb_loss@tur.pdtb.tdb': 2.2767627239227295, 'train@tur.pdtb.tdb_runtime': 30.113, 'train@tur.pdtb.tdb_samples_per_second': 81.393, 'train@tur.pdtb.tdb_steps_per_second': 2.557, 'epoch': 2.0}
{'loss': 2.4254, 'learning_rate': 1.6666666666666667e-05, 'epoch': 2.0}
{'eval_loss': 2.2226624488830566, 'eval_accuracy@tur.pdtb.tdb': 0.28525641025641024, 'eval_f1@tur.pdtb.tdb': 0.07018062904263561, 'eval_precision@tur.pdtb.tdb': 0.07501750700280112, 'eval_recall@tur.pdtb.tdb': 0.0947242819219216, 'eval_loss@tur.pdtb.tdb': 2.2226624488830566, 'eval_runtime': 4.1967, 'eval_samples_per_second': 74.344, 'eval_steps_per_second': 2.383, 'epoch': 2.0}
{'train@tur.pdtb.tdb_loss': 2.159146308898926, 'train@tur.pdtb.tdb_accuracy@tur.pdtb.tdb': 0.317421460628315, 'train@tur.pdtb.tdb_f1@tur.pdtb.tdb': 0.09897815852843618, 'train@tur.pdtb.tdb_precision@tur.pdtb.tdb': 0.11080797806214172, 'train@tur.pdtb.tdb_recall@tur.pdtb.tdb': 0.11410550100213429, 'train@tur.pdtb.tdb_loss@tur.pdtb.tdb': 2.159146308898926, 'train@tur.pdtb.tdb_runtime': 30.158, 'train@tur.pdtb.tdb_samples_per_second': 81.272, 'train@tur.pdtb.tdb_steps_per_second': 2.553, 'epoch': 3.0}
{'loss': 2.2603, 'learning_rate': 1.5000000000000002e-05, 'epoch': 3.0}
{'eval_loss': 2.1552674770355225, 'eval_accuracy@tur.pdtb.tdb': 0.30448717948717946, 'eval_f1@tur.pdtb.tdb': 0.089725357078217, 'eval_precision@tur.pdtb.tdb': 0.11257279306059793, 'eval_recall@tur.pdtb.tdb': 0.10490808463646105, 'eval_loss@tur.pdtb.tdb': 2.1552670001983643, 'eval_runtime': 4.2337, 'eval_samples_per_second': 73.694, 'eval_steps_per_second': 2.362, 'epoch': 3.0}
{'train@tur.pdtb.tdb_loss': 2.0743279457092285, 'train@tur.pdtb.tdb_accuracy@tur.pdtb.tdb': 0.33700530395756834, 'train@tur.pdtb.tdb_f1@tur.pdtb.tdb': 0.10496840383056139, 'train@tur.pdtb.tdb_precision@tur.pdtb.tdb': 0.16325832939666168, 'train@tur.pdtb.tdb_recall@tur.pdtb.tdb': 0.12361680296222306, 'train@tur.pdtb.tdb_loss@tur.pdtb.tdb': 2.0743277072906494, 'train@tur.pdtb.tdb_runtime': 30.1265, 'train@tur.pdtb.tdb_samples_per_second': 81.357, 'train@tur.pdtb.tdb_steps_per_second': 2.556, 'epoch': 4.0}
{'loss': 2.1543, 'learning_rate': 1.3333333333333333e-05, 'epoch': 4.0}
{'eval_loss': 2.1042685508728027, 'eval_accuracy@tur.pdtb.tdb': 0.30128205128205127, 'eval_f1@tur.pdtb.tdb': 0.08838132959897596, 'eval_precision@tur.pdtb.tdb': 0.10942933432687531, 'eval_recall@tur.pdtb.tdb': 0.10606207851454824, 'eval_loss@tur.pdtb.tdb': 2.1042685508728027, 'eval_runtime': 4.2433, 'eval_samples_per_second': 73.527, 'eval_steps_per_second': 2.357, 'epoch': 4.0}
{'train@tur.pdtb.tdb_loss': 2.006725788116455, 'train@tur.pdtb.tdb_accuracy@tur.pdtb.tdb': 0.34720522235822115, 'train@tur.pdtb.tdb_f1@tur.pdtb.tdb': 0.11276767614611884, 'train@tur.pdtb.tdb_precision@tur.pdtb.tdb': 0.16354468244534584, 'train@tur.pdtb.tdb_recall@tur.pdtb.tdb': 0.13184605611918937, 'train@tur.pdtb.tdb_loss@tur.pdtb.tdb': 2.006725788116455, 'train@tur.pdtb.tdb_runtime': 30.9512, 'train@tur.pdtb.tdb_samples_per_second': 79.189, 'train@tur.pdtb.tdb_steps_per_second': 2.488, 'epoch': 5.0}
{'loss': 2.0955, 'learning_rate': 1.1666666666666668e-05, 'epoch': 5.0}
{'eval_loss': 2.0674173831939697, 'eval_accuracy@tur.pdtb.tdb': 0.3076923076923077, 'eval_f1@tur.pdtb.tdb': 0.09840840713547916, 'eval_precision@tur.pdtb.tdb': 0.10803663629750587, 'eval_recall@tur.pdtb.tdb': 0.11320643701317291, 'eval_loss@tur.pdtb.tdb': 2.067417621612549, 'eval_runtime': 4.2174, 'eval_samples_per_second': 73.979, 'eval_steps_per_second': 2.371, 'epoch': 5.0}
{'train@tur.pdtb.tdb_loss': 1.9559638500213623, 'train@tur.pdtb.tdb_accuracy@tur.pdtb.tdb': 0.3700530395756834, 'train@tur.pdtb.tdb_f1@tur.pdtb.tdb': 0.12649786392261414, 'train@tur.pdtb.tdb_precision@tur.pdtb.tdb': 0.16135305199455588, 'train@tur.pdtb.tdb_recall@tur.pdtb.tdb': 0.14407395365348502, 'train@tur.pdtb.tdb_loss@tur.pdtb.tdb': 1.9559636116027832, 'train@tur.pdtb.tdb_runtime': 30.1646, 'train@tur.pdtb.tdb_samples_per_second': 81.254, 'train@tur.pdtb.tdb_steps_per_second': 2.553, 'epoch': 6.0}
{'loss': 2.0393, 'learning_rate': 1e-05, 'epoch': 6.0}
{'eval_loss': 2.044713020324707, 'eval_accuracy@tur.pdtb.tdb': 0.3269230769230769, 'eval_f1@tur.pdtb.tdb': 0.12677147550994275, 'eval_precision@tur.pdtb.tdb': 0.14716123710138332, 'eval_recall@tur.pdtb.tdb': 0.13474535561091505, 'eval_loss@tur.pdtb.tdb': 2.044712781906128, 'eval_runtime': 4.2329, 'eval_samples_per_second': 73.709, 'eval_steps_per_second': 2.362, 'epoch': 6.0}
{'train@tur.pdtb.tdb_loss': 1.917115330696106, 'train@tur.pdtb.tdb_accuracy@tur.pdtb.tdb': 0.3908608731130151, 'train@tur.pdtb.tdb_f1@tur.pdtb.tdb': 0.13662263508950528, 'train@tur.pdtb.tdb_precision@tur.pdtb.tdb': 0.14643382037349778, 'train@tur.pdtb.tdb_recall@tur.pdtb.tdb': 0.15452413703308263, 'train@tur.pdtb.tdb_loss@tur.pdtb.tdb': 1.917115330696106, 'train@tur.pdtb.tdb_runtime': 30.1741, 'train@tur.pdtb.tdb_samples_per_second': 81.228, 'train@tur.pdtb.tdb_steps_per_second': 2.552, 'epoch': 7.0}
{'loss': 2.0051, 'learning_rate': 8.333333333333334e-06, 'epoch': 7.0}
{'eval_loss': 2.01908802986145, 'eval_accuracy@tur.pdtb.tdb': 0.33653846153846156, 'eval_f1@tur.pdtb.tdb': 0.1369317863556131, 'eval_precision@tur.pdtb.tdb': 0.16878245320434201, 'eval_recall@tur.pdtb.tdb': 0.13983725696818478, 'eval_loss@tur.pdtb.tdb': 2.01908802986145, 'eval_runtime': 4.2262, 'eval_samples_per_second': 73.825, 'eval_steps_per_second': 2.366, 'epoch': 7.0}
{'train@tur.pdtb.tdb_loss': 1.8883495330810547, 'train@tur.pdtb.tdb_accuracy@tur.pdtb.tdb': 0.40310077519379844, 'train@tur.pdtb.tdb_f1@tur.pdtb.tdb': 0.1462274695518564, 'train@tur.pdtb.tdb_precision@tur.pdtb.tdb': 0.15983176292781368, 'train@tur.pdtb.tdb_recall@tur.pdtb.tdb': 0.16054148833008955, 'train@tur.pdtb.tdb_loss@tur.pdtb.tdb': 1.8883495330810547, 'train@tur.pdtb.tdb_runtime': 30.1392, 'train@tur.pdtb.tdb_samples_per_second': 81.323, 'train@tur.pdtb.tdb_steps_per_second': 2.555, 'epoch': 8.0}
{'loss': 1.9699, 'learning_rate': 6.666666666666667e-06, 'epoch': 8.0}
{'eval_loss': 2.0029640197753906, 'eval_accuracy@tur.pdtb.tdb': 0.34294871794871795, 'eval_f1@tur.pdtb.tdb': 0.14424092961701768, 'eval_precision@tur.pdtb.tdb': 0.173323178315641, 'eval_recall@tur.pdtb.tdb': 0.1449880235624235, 'eval_loss@tur.pdtb.tdb': 2.0029640197753906, 'eval_runtime': 4.2262, 'eval_samples_per_second': 73.826, 'eval_steps_per_second': 2.366, 'epoch': 8.0}
{'train@tur.pdtb.tdb_loss': 1.8681293725967407, 'train@tur.pdtb.tdb_accuracy@tur.pdtb.tdb': 0.4039167686658507, 'train@tur.pdtb.tdb_f1@tur.pdtb.tdb': 0.14516708283577226, 'train@tur.pdtb.tdb_precision@tur.pdtb.tdb': 0.155836912490023, 'train@tur.pdtb.tdb_recall@tur.pdtb.tdb': 0.1610531812717026, 'train@tur.pdtb.tdb_loss@tur.pdtb.tdb': 1.8681296110153198, 'train@tur.pdtb.tdb_runtime': 30.1207, 'train@tur.pdtb.tdb_samples_per_second': 81.373, 'train@tur.pdtb.tdb_steps_per_second': 2.556, 'epoch': 9.0}
{'loss': 1.9485, 'learning_rate': 5e-06, 'epoch': 9.0}
{'eval_loss': 1.9934347867965698, 'eval_accuracy@tur.pdtb.tdb': 0.3525641025641026, 'eval_f1@tur.pdtb.tdb': 0.13931764087434953, 'eval_precision@tur.pdtb.tdb': 0.1522919125040099, 'eval_recall@tur.pdtb.tdb': 0.14644747681369874, 'eval_loss@tur.pdtb.tdb': 1.9934347867965698, 'eval_runtime': 4.2085, 'eval_samples_per_second': 74.135, 'eval_steps_per_second': 2.376, 'epoch': 9.0}
{'train@tur.pdtb.tdb_loss': 1.851527214050293, 'train@tur.pdtb.tdb_accuracy@tur.pdtb.tdb': 0.40962872297021624, 'train@tur.pdtb.tdb_f1@tur.pdtb.tdb': 0.14961326908999587, 'train@tur.pdtb.tdb_precision@tur.pdtb.tdb': 0.16041710055254763, 'train@tur.pdtb.tdb_recall@tur.pdtb.tdb': 0.16387390572178573, 'train@tur.pdtb.tdb_loss@tur.pdtb.tdb': 1.851527214050293, 'train@tur.pdtb.tdb_runtime': 30.1653, 'train@tur.pdtb.tdb_samples_per_second': 81.252, 'train@tur.pdtb.tdb_steps_per_second': 2.553, 'epoch': 10.0}
{'loss': 1.9237, 'learning_rate': 3.3333333333333333e-06, 'epoch': 10.0}
{'eval_loss': 1.9847979545593262, 'eval_accuracy@tur.pdtb.tdb': 0.3557692307692308, 'eval_f1@tur.pdtb.tdb': 0.1458326218549868, 'eval_precision@tur.pdtb.tdb': 0.15896091939313794, 'eval_recall@tur.pdtb.tdb': 0.15102518648030622, 'eval_loss@tur.pdtb.tdb': 1.9847979545593262, 'eval_runtime': 4.2017, 'eval_samples_per_second': 74.256, 'eval_steps_per_second': 2.38, 'epoch': 10.0}
{'train@tur.pdtb.tdb_loss': 1.8438446521759033, 'train@tur.pdtb.tdb_accuracy@tur.pdtb.tdb': 0.4145246838025296, 'train@tur.pdtb.tdb_f1@tur.pdtb.tdb': 0.15292284003386092, 'train@tur.pdtb.tdb_precision@tur.pdtb.tdb': 0.18473337987358598, 'train@tur.pdtb.tdb_recall@tur.pdtb.tdb': 0.16607893252934583, 'train@tur.pdtb.tdb_loss@tur.pdtb.tdb': 1.8438446521759033, 'train@tur.pdtb.tdb_runtime': 30.1837, 'train@tur.pdtb.tdb_samples_per_second': 81.203, 'train@tur.pdtb.tdb_steps_per_second': 2.551, 'epoch': 11.0}
{'loss': 1.9209, 'learning_rate': 1.6666666666666667e-06, 'epoch': 11.0}
{'eval_loss': 1.9816983938217163, 'eval_accuracy@tur.pdtb.tdb': 0.358974358974359, 'eval_f1@tur.pdtb.tdb': 0.14710766206368217, 'eval_precision@tur.pdtb.tdb': 0.16015230506918485, 'eval_recall@tur.pdtb.tdb': 0.1520133287727963, 'eval_loss@tur.pdtb.tdb': 1.9816986322402954, 'eval_runtime': 4.255, 'eval_samples_per_second': 73.326, 'eval_steps_per_second': 2.35, 'epoch': 11.0}
{'train@tur.pdtb.tdb_loss': 1.840295433998108, 'train@tur.pdtb.tdb_accuracy@tur.pdtb.tdb': 0.41697266421868623, 'train@tur.pdtb.tdb_f1@tur.pdtb.tdb': 0.15519464288738205, 'train@tur.pdtb.tdb_precision@tur.pdtb.tdb': 0.18746125473220637, 'train@tur.pdtb.tdb_recall@tur.pdtb.tdb': 0.1678964180212627, 'train@tur.pdtb.tdb_loss@tur.pdtb.tdb': 1.840295433998108, 'train@tur.pdtb.tdb_runtime': 30.1365, 'train@tur.pdtb.tdb_samples_per_second': 81.33, 'train@tur.pdtb.tdb_steps_per_second': 2.555, 'epoch': 12.0}
{'loss': 1.9095, 'learning_rate': 0.0, 'epoch': 12.0}
{'eval_loss': 1.979616641998291, 'eval_accuracy@tur.pdtb.tdb': 0.358974358974359, 'eval_f1@tur.pdtb.tdb': 0.14701246200695747, 'eval_precision@tur.pdtb.tdb': 0.15925252351793648, 'eval_recall@tur.pdtb.tdb': 0.1520133287727963, 'eval_loss@tur.pdtb.tdb': 1.979616641998291, 'eval_runtime': 4.2135, 'eval_samples_per_second': 74.048, 'eval_steps_per_second': 2.373, 'epoch': 12.0}
{'train_runtime': 1167.7678, 'train_samples_per_second': 25.187, 'train_steps_per_second': 0.791, 'train_loss': 2.140739374862605, 'epoch': 12.0}
***** train_logger metrics *****
  epoch                    =       12.0
  train_loss               =     1.8288
  train_runtime            = 1:48:38.00
  train_samples_per_second =     25.585
  train_steps_per_second   =      0.801
-------------------------------------------------------------------
Lang1:  eng.rst.rstdt    Lang2:  tur.pdtb.tdb
Saving run to:  runs/full_shot/FullShot=v4_eng.rst.rstdt_tur.pdtb.tdb_lrfunc=Adafactor_lr=2e-05_plm=bert-base-multilingual-cased
Running with params: BERT_MODEL=bert-base-multilingual-cased lr=2e-05
read 16002 examples
read 1621 examples
read 2155 examples
read 2451 examples
read 312 examples
read 422 examples
Total prediction labels:  40
BertAdapterModel(
  (shared_parameters): ModuleDict()
  (bert): BertModel(
    (shared_parameters): ModuleDict()
    (invertible_adapters): ModuleDict(
      (en): NICECouplingBlock(
        (F): Sequential(
          (0): Linear(in_features=384, out_features=192, bias=True)
          (1): Activation_Function_Class(
            (f): ReLU()
          )
          (2): Linear(in_features=192, out_features=384, bias=True)
        )
        (G): Sequential(
          (0): Linear(in_features=384, out_features=192, bias=True)
          (1): Activation_Function_Class(
            (f): ReLU()
          )
          (2): Linear(in_features=192, out_features=384, bias=True)
        )
      )
      (tr): NICECouplingBlock(
        (F): Sequential(
          (0): Linear(in_features=384, out_features=192, bias=True)
          (1): Activation_Function_Class(
            (f): NewGELUActivation()
          )
          (2): Linear(in_features=192, out_features=384, bias=True)
        )
        (G): Sequential(
          (0): Linear(in_features=384, out_features=192, bias=True)
          (1): Activation_Function_Class(
            (f): NewGELUActivation()
          )
          (2): Linear(in_features=192, out_features=384, bias=True)
        )
      )
    )
    (embeddings): BertEmbeddings(
      (word_embeddings): Embedding(119547, 768, padding_idx=0)
      (position_embeddings): Embedding(512, 768)
      (token_type_embeddings): Embedding(2, 768)
      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
      (dropout): Dropout(p=0.1, inplace=False)
    )
    (encoder): BertEncoder(
      (layer): ModuleList(
        (0): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (en): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (tr): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (1): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (en): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (tr): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (2): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (en): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (tr): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (3): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (en): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (tr): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (4): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (en): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (tr): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (5): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (en): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (tr): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (6): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (en): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (tr): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (7): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (en): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (tr): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (8): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (en): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (tr): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (9): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (en): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (tr): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (10): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (en): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (tr): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (11): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (en): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (tr): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
      )
    )
    (pooler): BertPooler(
      (dense): Linear(in_features=768, out_features=768, bias=True)
      (activation): Tanh()
    )
    (prefix_tuning): PrefixTuningPool(
      (prefix_tunings): ModuleDict()
    )
  )
  (heads): ModuleDict(
    (tr): BertStyleMaskedLMHead(
      (0): Linear(in_features=768, out_features=768, bias=True)
      (1): Activation_Function_Class(
        (f): GELUActivation()
      )
      (2): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
      (3): Linear(in_features=768, out_features=119547, bias=True)
    )
    (disrpt): ClassificationHead(
      (0): Dropout(p=0.1, inplace=False)
      (1): Linear(in_features=768, out_features=768, bias=True)
      (2): Activation_Function_Class(
        (f): Tanh()
      )
      (3): Dropout(p=0.1, inplace=False)
      (4): Linear(in_features=768, out_features=40, bias=True)
    )
  )
)
{'train@eng.rst.rstdt_loss': 1.7754526138305664, 'train@eng.rst.rstdt_accuracy@eng.rst.rstdt': 0.5105611798525185, 'train@eng.rst.rstdt_f1@eng.rst.rstdt': 0.08361704719362528, 'train@eng.rst.rstdt_precision@eng.rst.rstdt': 0.12723931345472614, 'train@eng.rst.rstdt_recall@eng.rst.rstdt': 0.11025666333570364, 'train@eng.rst.rstdt_loss@eng.rst.rstdt': 1.7754526138305664, 'train@eng.rst.rstdt_runtime': 191.2097, 'train@eng.rst.rstdt_samples_per_second': 83.688, 'train@eng.rst.rstdt_steps_per_second': 2.62, 'epoch': 1.0}
{'loss': 2.2267, 'learning_rate': 1.8333333333333333e-05, 'epoch': 1.0}
{'eval_loss': 1.7481080293655396, 'eval_accuracy@eng.rst.rstdt': 0.5249845774213449, 'eval_f1@eng.rst.rstdt': 0.08428852953737002, 'eval_precision@eng.rst.rstdt': 0.06942494253648335, 'eval_recall@eng.rst.rstdt': 0.10986460606584796, 'eval_loss@eng.rst.rstdt': 1.7481080293655396, 'eval_runtime': 19.6978, 'eval_samples_per_second': 82.293, 'eval_steps_per_second': 2.589, 'epoch': 1.0}
{'train@eng.rst.rstdt_loss': 1.4664400815963745, 'train@eng.rst.rstdt_accuracy@eng.rst.rstdt': 0.582427196600425, 'train@eng.rst.rstdt_f1@eng.rst.rstdt': 0.17222842989841253, 'train@eng.rst.rstdt_precision@eng.rst.rstdt': 0.20982476207317524, 'train@eng.rst.rstdt_recall@eng.rst.rstdt': 0.1809165625308223, 'train@eng.rst.rstdt_loss@eng.rst.rstdt': 1.466440200805664, 'train@eng.rst.rstdt_runtime': 191.2794, 'train@eng.rst.rstdt_samples_per_second': 83.658, 'train@eng.rst.rstdt_steps_per_second': 2.619, 'epoch': 2.0}
{'loss': 1.6518, 'learning_rate': 1.6666666666666667e-05, 'epoch': 2.0}
{'eval_loss': 1.4645648002624512, 'eval_accuracy@eng.rst.rstdt': 0.5965453423812461, 'eval_f1@eng.rst.rstdt': 0.18135660622543925, 'eval_precision@eng.rst.rstdt': 0.20662065391134962, 'eval_recall@eng.rst.rstdt': 0.18636229270860197, 'eval_loss@eng.rst.rstdt': 1.4645646810531616, 'eval_runtime': 19.7156, 'eval_samples_per_second': 82.219, 'eval_steps_per_second': 2.587, 'epoch': 2.0}
{'train@eng.rst.rstdt_loss': 1.348900556564331, 'train@eng.rst.rstdt_accuracy@eng.rst.rstdt': 0.6171103612048494, 'train@eng.rst.rstdt_f1@eng.rst.rstdt': 0.2293161266855003, 'train@eng.rst.rstdt_precision@eng.rst.rstdt': 0.3188551990208909, 'train@eng.rst.rstdt_recall@eng.rst.rstdt': 0.23233484842722968, 'train@eng.rst.rstdt_loss@eng.rst.rstdt': 1.3489006757736206, 'train@eng.rst.rstdt_runtime': 191.167, 'train@eng.rst.rstdt_samples_per_second': 83.707, 'train@eng.rst.rstdt_steps_per_second': 2.621, 'epoch': 3.0}
{'loss': 1.4652, 'learning_rate': 1.5000000000000002e-05, 'epoch': 3.0}
{'eval_loss': 1.3714574575424194, 'eval_accuracy@eng.rst.rstdt': 0.6255397902529303, 'eval_f1@eng.rst.rstdt': 0.2413975405588248, 'eval_precision@eng.rst.rstdt': 0.2953964952588824, 'eval_recall@eng.rst.rstdt': 0.243962845710274, 'eval_loss@eng.rst.rstdt': 1.371457576751709, 'eval_runtime': 19.6943, 'eval_samples_per_second': 82.308, 'eval_steps_per_second': 2.59, 'epoch': 3.0}
{'train@eng.rst.rstdt_loss': 1.2767834663391113, 'train@eng.rst.rstdt_accuracy@eng.rst.rstdt': 0.6379827521559805, 'train@eng.rst.rstdt_f1@eng.rst.rstdt': 0.3063999596251696, 'train@eng.rst.rstdt_precision@eng.rst.rstdt': 0.4304933207158591, 'train@eng.rst.rstdt_recall@eng.rst.rstdt': 0.2914499772741487, 'train@eng.rst.rstdt_loss@eng.rst.rstdt': 1.2767835855484009, 'train@eng.rst.rstdt_runtime': 191.2471, 'train@eng.rst.rstdt_samples_per_second': 83.672, 'train@eng.rst.rstdt_steps_per_second': 2.62, 'epoch': 4.0}
{'loss': 1.3661, 'learning_rate': 1.3333333333333333e-05, 'epoch': 4.0}
{'eval_loss': 1.3242334127426147, 'eval_accuracy@eng.rst.rstdt': 0.6310919185687847, 'eval_f1@eng.rst.rstdt': 0.2994069000829884, 'eval_precision@eng.rst.rstdt': 0.39961055370557524, 'eval_recall@eng.rst.rstdt': 0.2946821818808816, 'eval_loss@eng.rst.rstdt': 1.3242331743240356, 'eval_runtime': 19.7221, 'eval_samples_per_second': 82.192, 'eval_steps_per_second': 2.586, 'epoch': 4.0}
{'train@eng.rst.rstdt_loss': 1.2269846200942993, 'train@eng.rst.rstdt_accuracy@eng.rst.rstdt': 0.6494188226471691, 'train@eng.rst.rstdt_f1@eng.rst.rstdt': 0.3347356540610926, 'train@eng.rst.rstdt_precision@eng.rst.rstdt': 0.4380941912480184, 'train@eng.rst.rstdt_recall@eng.rst.rstdt': 0.3138506296035227, 'train@eng.rst.rstdt_loss@eng.rst.rstdt': 1.2269847393035889, 'train@eng.rst.rstdt_runtime': 191.4485, 'train@eng.rst.rstdt_samples_per_second': 83.584, 'train@eng.rst.rstdt_steps_per_second': 2.617, 'epoch': 5.0}
{'loss': 1.3105, 'learning_rate': 1.1666666666666668e-05, 'epoch': 5.0}
{'eval_loss': 1.2865381240844727, 'eval_accuracy@eng.rst.rstdt': 0.6403454657618753, 'eval_f1@eng.rst.rstdt': 0.3281397419313253, 'eval_precision@eng.rst.rstdt': 0.41780050889873005, 'eval_recall@eng.rst.rstdt': 0.31749653719450827, 'eval_loss@eng.rst.rstdt': 1.2865381240844727, 'eval_runtime': 19.7533, 'eval_samples_per_second': 82.062, 'eval_steps_per_second': 2.582, 'epoch': 5.0}
{'train@eng.rst.rstdt_loss': 1.19289231300354, 'train@eng.rst.rstdt_accuracy@eng.rst.rstdt': 0.6554180727409074, 'train@eng.rst.rstdt_f1@eng.rst.rstdt': 0.3503858470830953, 'train@eng.rst.rstdt_precision@eng.rst.rstdt': 0.49908666216857006, 'train@eng.rst.rstdt_recall@eng.rst.rstdt': 0.324574605946406, 'train@eng.rst.rstdt_loss@eng.rst.rstdt': 1.19289231300354, 'train@eng.rst.rstdt_runtime': 191.3606, 'train@eng.rst.rstdt_samples_per_second': 83.622, 'train@eng.rst.rstdt_steps_per_second': 2.618, 'epoch': 6.0}
{'loss': 1.2623, 'learning_rate': 1e-05, 'epoch': 6.0}
{'eval_loss': 1.260000467300415, 'eval_accuracy@eng.rst.rstdt': 0.6421961752004935, 'eval_f1@eng.rst.rstdt': 0.33175378491127866, 'eval_precision@eng.rst.rstdt': 0.4138037190785011, 'eval_recall@eng.rst.rstdt': 0.31953152114336036, 'eval_loss@eng.rst.rstdt': 1.2600005865097046, 'eval_runtime': 19.7083, 'eval_samples_per_second': 82.25, 'eval_steps_per_second': 2.588, 'epoch': 6.0}
{'train@eng.rst.rstdt_loss': 1.172149658203125, 'train@eng.rst.rstdt_accuracy@eng.rst.rstdt': 0.6597300337457818, 'train@eng.rst.rstdt_f1@eng.rst.rstdt': 0.3647386979310356, 'train@eng.rst.rstdt_precision@eng.rst.rstdt': 0.4990918162302134, 'train@eng.rst.rstdt_recall@eng.rst.rstdt': 0.3340268495179864, 'train@eng.rst.rstdt_loss@eng.rst.rstdt': 1.1721497774124146, 'train@eng.rst.rstdt_runtime': 191.4425, 'train@eng.rst.rstdt_samples_per_second': 83.586, 'train@eng.rst.rstdt_steps_per_second': 2.617, 'epoch': 7.0}
{'loss': 1.2403, 'learning_rate': 8.333333333333334e-06, 'epoch': 7.0}
{'eval_loss': 1.2401202917099, 'eval_accuracy@eng.rst.rstdt': 0.6421961752004935, 'eval_f1@eng.rst.rstdt': 0.327989566399869, 'eval_precision@eng.rst.rstdt': 0.4140577188474883, 'eval_recall@eng.rst.rstdt': 0.3200332770885032, 'eval_loss@eng.rst.rstdt': 1.2401201725006104, 'eval_runtime': 19.7309, 'eval_samples_per_second': 82.156, 'eval_steps_per_second': 2.585, 'epoch': 7.0}
{'train@eng.rst.rstdt_loss': 1.1519619226455688, 'train@eng.rst.rstdt_accuracy@eng.rst.rstdt': 0.6627296587926509, 'train@eng.rst.rstdt_f1@eng.rst.rstdt': 0.37786682987496945, 'train@eng.rst.rstdt_precision@eng.rst.rstdt': 0.5602325450891958, 'train@eng.rst.rstdt_recall@eng.rst.rstdt': 0.34780679913995566, 'train@eng.rst.rstdt_loss@eng.rst.rstdt': 1.1519618034362793, 'train@eng.rst.rstdt_runtime': 191.3426, 'train@eng.rst.rstdt_samples_per_second': 83.63, 'train@eng.rst.rstdt_steps_per_second': 2.618, 'epoch': 8.0}
{'loss': 1.2184, 'learning_rate': 6.666666666666667e-06, 'epoch': 8.0}
{'eval_loss': 1.2348616123199463, 'eval_accuracy@eng.rst.rstdt': 0.6421961752004935, 'eval_f1@eng.rst.rstdt': 0.3426589031759772, 'eval_precision@eng.rst.rstdt': 0.46909276933749594, 'eval_recall@eng.rst.rstdt': 0.3349344339941174, 'eval_loss@eng.rst.rstdt': 1.2348616123199463, 'eval_runtime': 19.7249, 'eval_samples_per_second': 82.18, 'eval_steps_per_second': 2.586, 'epoch': 8.0}
{'train@eng.rst.rstdt_loss': 1.1392046213150024, 'train@eng.rst.rstdt_accuracy@eng.rst.rstdt': 0.6652293463317085, 'train@eng.rst.rstdt_f1@eng.rst.rstdt': 0.3835830158615047, 'train@eng.rst.rstdt_precision@eng.rst.rstdt': 0.6248643549888034, 'train@eng.rst.rstdt_recall@eng.rst.rstdt': 0.3513304403433284, 'train@eng.rst.rstdt_loss@eng.rst.rstdt': 1.139204740524292, 'train@eng.rst.rstdt_runtime': 191.4743, 'train@eng.rst.rstdt_samples_per_second': 83.573, 'train@eng.rst.rstdt_steps_per_second': 2.617, 'epoch': 9.0}
{'loss': 1.2012, 'learning_rate': 5e-06, 'epoch': 9.0}
{'eval_loss': 1.2227864265441895, 'eval_accuracy@eng.rst.rstdt': 0.6446637877853177, 'eval_f1@eng.rst.rstdt': 0.350657074484744, 'eval_precision@eng.rst.rstdt': 0.4847230008196676, 'eval_recall@eng.rst.rstdt': 0.3388834561897558, 'eval_loss@eng.rst.rstdt': 1.2227864265441895, 'eval_runtime': 19.7653, 'eval_samples_per_second': 82.012, 'eval_steps_per_second': 2.58, 'epoch': 9.0}
{'train@eng.rst.rstdt_loss': 1.132575273513794, 'train@eng.rst.rstdt_accuracy@eng.rst.rstdt': 0.6643544556930384, 'train@eng.rst.rstdt_f1@eng.rst.rstdt': 0.397276739419107, 'train@eng.rst.rstdt_precision@eng.rst.rstdt': 0.6039443444413626, 'train@eng.rst.rstdt_recall@eng.rst.rstdt': 0.3644048370048225, 'train@eng.rst.rstdt_loss@eng.rst.rstdt': 1.132575273513794, 'train@eng.rst.rstdt_runtime': 191.1669, 'train@eng.rst.rstdt_samples_per_second': 83.707, 'train@eng.rst.rstdt_steps_per_second': 2.621, 'epoch': 10.0}
{'loss': 1.1878, 'learning_rate': 3.3333333333333333e-06, 'epoch': 10.0}
{'eval_loss': 1.2256208658218384, 'eval_accuracy@eng.rst.rstdt': 0.6409623689080814, 'eval_f1@eng.rst.rstdt': 0.34954465559195813, 'eval_precision@eng.rst.rstdt': 0.45675056976105627, 'eval_recall@eng.rst.rstdt': 0.34160287378134846, 'eval_loss@eng.rst.rstdt': 1.225620985031128, 'eval_runtime': 19.6837, 'eval_samples_per_second': 82.352, 'eval_steps_per_second': 2.591, 'epoch': 10.0}
{'train@eng.rst.rstdt_loss': 1.1255340576171875, 'train@eng.rst.rstdt_accuracy@eng.rst.rstdt': 0.6669791276090489, 'train@eng.rst.rstdt_f1@eng.rst.rstdt': 0.3960996808171501, 'train@eng.rst.rstdt_precision@eng.rst.rstdt': 0.6153028414618059, 'train@eng.rst.rstdt_recall@eng.rst.rstdt': 0.36210051134370846, 'train@eng.rst.rstdt_loss@eng.rst.rstdt': 1.125534176826477, 'train@eng.rst.rstdt_runtime': 191.1893, 'train@eng.rst.rstdt_samples_per_second': 83.697, 'train@eng.rst.rstdt_steps_per_second': 2.62, 'epoch': 11.0}
{'loss': 1.1817, 'learning_rate': 1.6666666666666667e-06, 'epoch': 11.0}
{'eval_loss': 1.217124104499817, 'eval_accuracy@eng.rst.rstdt': 0.6409623689080814, 'eval_f1@eng.rst.rstdt': 0.3481504022961083, 'eval_precision@eng.rst.rstdt': 0.4608323649413894, 'eval_recall@eng.rst.rstdt': 0.3386411417793509, 'eval_loss@eng.rst.rstdt': 1.2171239852905273, 'eval_runtime': 19.7409, 'eval_samples_per_second': 82.114, 'eval_steps_per_second': 2.583, 'epoch': 11.0}
{'train@eng.rst.rstdt_loss': 1.1240514516830444, 'train@eng.rst.rstdt_accuracy@eng.rst.rstdt': 0.6676040494938132, 'train@eng.rst.rstdt_f1@eng.rst.rstdt': 0.39779719967533833, 'train@eng.rst.rstdt_precision@eng.rst.rstdt': 0.607538031031859, 'train@eng.rst.rstdt_recall@eng.rst.rstdt': 0.36296996379843827, 'train@eng.rst.rstdt_loss@eng.rst.rstdt': 1.1240516901016235, 'train@eng.rst.rstdt_runtime': 191.1984, 'train@eng.rst.rstdt_samples_per_second': 83.693, 'train@eng.rst.rstdt_steps_per_second': 2.62, 'epoch': 12.0}
{'loss': 1.1758, 'learning_rate': 0.0, 'epoch': 12.0}
{'eval_loss': 1.215498924255371, 'eval_accuracy@eng.rst.rstdt': 0.6415792720542874, 'eval_f1@eng.rst.rstdt': 0.34834250950723233, 'eval_precision@eng.rst.rstdt': 0.46096135020163487, 'eval_recall@eng.rst.rstdt': 0.33887002710780134, 'eval_loss@eng.rst.rstdt': 1.2154988050460815, 'eval_runtime': 19.6918, 'eval_samples_per_second': 82.319, 'eval_steps_per_second': 2.59, 'epoch': 12.0}
{'train_runtime': 7370.7001, 'train_samples_per_second': 26.052, 'train_steps_per_second': 0.816, 'train_loss': 1.373981417454804, 'epoch': 12.0}
***** train_log metrics *****
  epoch                    =       12.0
  train_loss               =      1.374
  train_runtime            = 2:02:50.70
  train_samples_per_second =     26.052
  train_steps_per_second   =      0.816
{'train@tur.pdtb.tdb_loss': 2.6941733360290527, 'train@tur.pdtb.tdb_accuracy@tur.pdtb.tdb': 0.24969400244798043, 'train@tur.pdtb.tdb_f1@tur.pdtb.tdb': 0.03756767249440971, 'train@tur.pdtb.tdb_precision@tur.pdtb.tdb': 0.039791260100184585, 'train@tur.pdtb.tdb_recall@tur.pdtb.tdb': 0.05304451065758776, 'train@tur.pdtb.tdb_loss@tur.pdtb.tdb': 2.6941733360290527, 'train@tur.pdtb.tdb_runtime': 30.0957, 'train@tur.pdtb.tdb_samples_per_second': 81.44, 'train@tur.pdtb.tdb_steps_per_second': 2.559, 'epoch': 1.0}
{'loss': 3.5217, 'learning_rate': 1.8333333333333333e-05, 'epoch': 1.0}
{'eval_loss': 2.6504967212677, 'eval_accuracy@tur.pdtb.tdb': 0.2724358974358974, 'eval_f1@tur.pdtb.tdb': 0.037759117277189565, 'eval_precision@tur.pdtb.tdb': 0.039887236151014666, 'eval_recall@tur.pdtb.tdb': 0.05681451864008176, 'eval_loss@tur.pdtb.tdb': 2.650496482849121, 'eval_runtime': 4.1583, 'eval_samples_per_second': 75.03, 'eval_steps_per_second': 2.405, 'epoch': 1.0}
{'train@tur.pdtb.tdb_loss': 2.2841923236846924, 'train@tur.pdtb.tdb_accuracy@tur.pdtb.tdb': 0.31048551611587105, 'train@tur.pdtb.tdb_f1@tur.pdtb.tdb': 0.08005088885156207, 'train@tur.pdtb.tdb_precision@tur.pdtb.tdb': 0.13163222195050994, 'train@tur.pdtb.tdb_recall@tur.pdtb.tdb': 0.08399905616374305, 'train@tur.pdtb.tdb_loss@tur.pdtb.tdb': 2.2841920852661133, 'train@tur.pdtb.tdb_runtime': 30.1067, 'train@tur.pdtb.tdb_samples_per_second': 81.41, 'train@tur.pdtb.tdb_steps_per_second': 2.558, 'epoch': 2.0}
{'loss': 2.4824, 'learning_rate': 1.6666666666666667e-05, 'epoch': 2.0}
{'eval_loss': 2.2961249351501465, 'eval_accuracy@tur.pdtb.tdb': 0.3108974358974359, 'eval_f1@tur.pdtb.tdb': 0.08433519696153148, 'eval_precision@tur.pdtb.tdb': 0.12524864024864024, 'eval_recall@tur.pdtb.tdb': 0.08535248667902044, 'eval_loss@tur.pdtb.tdb': 2.2961246967315674, 'eval_runtime': 4.1626, 'eval_samples_per_second': 74.953, 'eval_steps_per_second': 2.402, 'epoch': 2.0}
{'train@tur.pdtb.tdb_loss': 2.1422760486602783, 'train@tur.pdtb.tdb_accuracy@tur.pdtb.tdb': 0.35454916360669114, 'train@tur.pdtb.tdb_f1@tur.pdtb.tdb': 0.11827283980656982, 'train@tur.pdtb.tdb_precision@tur.pdtb.tdb': 0.13844419645257736, 'train@tur.pdtb.tdb_recall@tur.pdtb.tdb': 0.12353946085615367, 'train@tur.pdtb.tdb_loss@tur.pdtb.tdb': 2.142275810241699, 'train@tur.pdtb.tdb_runtime': 30.0817, 'train@tur.pdtb.tdb_samples_per_second': 81.478, 'train@tur.pdtb.tdb_steps_per_second': 2.56, 'epoch': 3.0}
{'loss': 2.261, 'learning_rate': 1.5000000000000002e-05, 'epoch': 3.0}
{'eval_loss': 2.1970412731170654, 'eval_accuracy@tur.pdtb.tdb': 0.3301282051282051, 'eval_f1@tur.pdtb.tdb': 0.10414655830814383, 'eval_precision@tur.pdtb.tdb': 0.11694570840118787, 'eval_recall@tur.pdtb.tdb': 0.11748325730083967, 'eval_loss@tur.pdtb.tdb': 2.1970410346984863, 'eval_runtime': 4.1955, 'eval_samples_per_second': 74.366, 'eval_steps_per_second': 2.384, 'epoch': 3.0}
{'train@tur.pdtb.tdb_loss': 2.051063060760498, 'train@tur.pdtb.tdb_accuracy@tur.pdtb.tdb': 0.3769889840881273, 'train@tur.pdtb.tdb_f1@tur.pdtb.tdb': 0.12930537670286862, 'train@tur.pdtb.tdb_precision@tur.pdtb.tdb': 0.13745260934011191, 'train@tur.pdtb.tdb_recall@tur.pdtb.tdb': 0.13767556094850295, 'train@tur.pdtb.tdb_loss@tur.pdtb.tdb': 2.051063299179077, 'train@tur.pdtb.tdb_runtime': 30.0693, 'train@tur.pdtb.tdb_samples_per_second': 81.512, 'train@tur.pdtb.tdb_steps_per_second': 2.561, 'epoch': 4.0}
{'loss': 2.161, 'learning_rate': 1.3333333333333333e-05, 'epoch': 4.0}
{'eval_loss': 2.1371591091156006, 'eval_accuracy@tur.pdtb.tdb': 0.3557692307692308, 'eval_f1@tur.pdtb.tdb': 0.12304266435423204, 'eval_precision@tur.pdtb.tdb': 0.14348349745221742, 'eval_recall@tur.pdtb.tdb': 0.1339097224906213, 'eval_loss@tur.pdtb.tdb': 2.1371591091156006, 'eval_runtime': 4.1494, 'eval_samples_per_second': 75.192, 'eval_steps_per_second': 2.41, 'epoch': 4.0}
{'train@tur.pdtb.tdb_loss': 1.9953618049621582, 'train@tur.pdtb.tdb_accuracy@tur.pdtb.tdb': 0.4010607915136679, 'train@tur.pdtb.tdb_f1@tur.pdtb.tdb': 0.140564259829247, 'train@tur.pdtb.tdb_precision@tur.pdtb.tdb': 0.14880830719289315, 'train@tur.pdtb.tdb_recall@tur.pdtb.tdb': 0.14851678506798902, 'train@tur.pdtb.tdb_loss@tur.pdtb.tdb': 1.9953618049621582, 'train@tur.pdtb.tdb_runtime': 30.0577, 'train@tur.pdtb.tdb_samples_per_second': 81.543, 'train@tur.pdtb.tdb_steps_per_second': 2.562, 'epoch': 5.0}
{'loss': 2.0876, 'learning_rate': 1.1666666666666668e-05, 'epoch': 5.0}
{'eval_loss': 2.10227108001709, 'eval_accuracy@tur.pdtb.tdb': 0.36538461538461536, 'eval_f1@tur.pdtb.tdb': 0.12453124165543851, 'eval_precision@tur.pdtb.tdb': 0.1384660827791535, 'eval_recall@tur.pdtb.tdb': 0.1351892400744867, 'eval_loss@tur.pdtb.tdb': 2.102271556854248, 'eval_runtime': 4.1476, 'eval_samples_per_second': 75.224, 'eval_steps_per_second': 2.411, 'epoch': 5.0}
{'train@tur.pdtb.tdb_loss': 1.9519531726837158, 'train@tur.pdtb.tdb_accuracy@tur.pdtb.tdb': 0.4112607099143207, 'train@tur.pdtb.tdb_f1@tur.pdtb.tdb': 0.14622584512872133, 'train@tur.pdtb.tdb_precision@tur.pdtb.tdb': 0.14950666742176147, 'train@tur.pdtb.tdb_recall@tur.pdtb.tdb': 0.15566610682895304, 'train@tur.pdtb.tdb_loss@tur.pdtb.tdb': 1.951953411102295, 'train@tur.pdtb.tdb_runtime': 30.071, 'train@tur.pdtb.tdb_samples_per_second': 81.507, 'train@tur.pdtb.tdb_steps_per_second': 2.561, 'epoch': 6.0}
{'loss': 2.0343, 'learning_rate': 1e-05, 'epoch': 6.0}
{'eval_loss': 2.0732223987579346, 'eval_accuracy@tur.pdtb.tdb': 0.3685897435897436, 'eval_f1@tur.pdtb.tdb': 0.1241424912062226, 'eval_precision@tur.pdtb.tdb': 0.13214901040987997, 'eval_recall@tur.pdtb.tdb': 0.13839659791007616, 'eval_loss@tur.pdtb.tdb': 2.0732223987579346, 'eval_runtime': 4.1526, 'eval_samples_per_second': 75.133, 'eval_steps_per_second': 2.408, 'epoch': 6.0}
{'train@tur.pdtb.tdb_loss': 1.9201509952545166, 'train@tur.pdtb.tdb_accuracy@tur.pdtb.tdb': 0.42064463484292125, 'train@tur.pdtb.tdb_f1@tur.pdtb.tdb': 0.15246151217647685, 'train@tur.pdtb.tdb_precision@tur.pdtb.tdb': 0.15379439104130246, 'train@tur.pdtb.tdb_recall@tur.pdtb.tdb': 0.16124776252885556, 'train@tur.pdtb.tdb_loss@tur.pdtb.tdb': 1.9201509952545166, 'train@tur.pdtb.tdb_runtime': 30.0179, 'train@tur.pdtb.tdb_samples_per_second': 81.651, 'train@tur.pdtb.tdb_steps_per_second': 2.565, 'epoch': 7.0}
{'loss': 2.003, 'learning_rate': 8.333333333333334e-06, 'epoch': 7.0}
{'eval_loss': 2.0582990646362305, 'eval_accuracy@tur.pdtb.tdb': 0.375, 'eval_f1@tur.pdtb.tdb': 0.13001936133470973, 'eval_precision@tur.pdtb.tdb': 0.14099688381107361, 'eval_recall@tur.pdtb.tdb': 0.14342978611865365, 'eval_loss@tur.pdtb.tdb': 2.0582990646362305, 'eval_runtime': 4.143, 'eval_samples_per_second': 75.307, 'eval_steps_per_second': 2.414, 'epoch': 7.0}
{'train@tur.pdtb.tdb_loss': 1.8975024223327637, 'train@tur.pdtb.tdb_accuracy@tur.pdtb.tdb': 0.42431660546715627, 'train@tur.pdtb.tdb_f1@tur.pdtb.tdb': 0.15473502945417505, 'train@tur.pdtb.tdb_precision@tur.pdtb.tdb': 0.15394197174354313, 'train@tur.pdtb.tdb_recall@tur.pdtb.tdb': 0.16506574849547348, 'train@tur.pdtb.tdb_loss@tur.pdtb.tdb': 1.8975024223327637, 'train@tur.pdtb.tdb_runtime': 30.039, 'train@tur.pdtb.tdb_samples_per_second': 81.594, 'train@tur.pdtb.tdb_steps_per_second': 2.563, 'epoch': 8.0}
{'loss': 1.9734, 'learning_rate': 6.666666666666667e-06, 'epoch': 8.0}
{'eval_loss': 2.0465855598449707, 'eval_accuracy@tur.pdtb.tdb': 0.3814102564102564, 'eval_f1@tur.pdtb.tdb': 0.1276917233399868, 'eval_precision@tur.pdtb.tdb': 0.1284628565878566, 'eval_recall@tur.pdtb.tdb': 0.144768168939488, 'eval_loss@tur.pdtb.tdb': 2.0465855598449707, 'eval_runtime': 4.17, 'eval_samples_per_second': 74.82, 'eval_steps_per_second': 2.398, 'epoch': 8.0}
{'train@tur.pdtb.tdb_loss': 1.8806343078613281, 'train@tur.pdtb.tdb_accuracy@tur.pdtb.tdb': 0.4288045695634435, 'train@tur.pdtb.tdb_f1@tur.pdtb.tdb': 0.1554803432909741, 'train@tur.pdtb.tdb_precision@tur.pdtb.tdb': 0.1582230579979347, 'train@tur.pdtb.tdb_recall@tur.pdtb.tdb': 0.16365100941303967, 'train@tur.pdtb.tdb_loss@tur.pdtb.tdb': 1.8806343078613281, 'train@tur.pdtb.tdb_runtime': 30.0706, 'train@tur.pdtb.tdb_samples_per_second': 81.508, 'train@tur.pdtb.tdb_steps_per_second': 2.561, 'epoch': 9.0}
{'loss': 1.9617, 'learning_rate': 5e-06, 'epoch': 9.0}
{'eval_loss': 2.043991804122925, 'eval_accuracy@tur.pdtb.tdb': 0.3717948717948718, 'eval_f1@tur.pdtb.tdb': 0.1290787877269862, 'eval_precision@tur.pdtb.tdb': 0.14005085015972243, 'eval_recall@tur.pdtb.tdb': 0.1414521787193595, 'eval_loss@tur.pdtb.tdb': 2.043992280960083, 'eval_runtime': 4.1655, 'eval_samples_per_second': 74.901, 'eval_steps_per_second': 2.401, 'epoch': 9.0}
{'train@tur.pdtb.tdb_loss': 1.8660051822662354, 'train@tur.pdtb.tdb_accuracy@tur.pdtb.tdb': 0.42758057935536514, 'train@tur.pdtb.tdb_f1@tur.pdtb.tdb': 0.15569568744541137, 'train@tur.pdtb.tdb_precision@tur.pdtb.tdb': 0.15645886752359445, 'train@tur.pdtb.tdb_recall@tur.pdtb.tdb': 0.16485319440679758, 'train@tur.pdtb.tdb_loss@tur.pdtb.tdb': 1.8660051822662354, 'train@tur.pdtb.tdb_runtime': 30.075, 'train@tur.pdtb.tdb_samples_per_second': 81.496, 'train@tur.pdtb.tdb_steps_per_second': 2.56, 'epoch': 10.0}
{'loss': 1.9384, 'learning_rate': 3.3333333333333333e-06, 'epoch': 10.0}
{'eval_loss': 2.030881643295288, 'eval_accuracy@tur.pdtb.tdb': 0.375, 'eval_f1@tur.pdtb.tdb': 0.1284757536004684, 'eval_precision@tur.pdtb.tdb': 0.13038126448206275, 'eval_recall@tur.pdtb.tdb': 0.14243989549116123, 'eval_loss@tur.pdtb.tdb': 2.030881643295288, 'eval_runtime': 4.1393, 'eval_samples_per_second': 75.375, 'eval_steps_per_second': 2.416, 'epoch': 10.0}
{'train@tur.pdtb.tdb_loss': 1.859665870666504, 'train@tur.pdtb.tdb_accuracy@tur.pdtb.tdb': 0.4300285597715218, 'train@tur.pdtb.tdb_f1@tur.pdtb.tdb': 0.1568737387734716, 'train@tur.pdtb.tdb_precision@tur.pdtb.tdb': 0.1588644000883666, 'train@tur.pdtb.tdb_recall@tur.pdtb.tdb': 0.16552169085852314, 'train@tur.pdtb.tdb_loss@tur.pdtb.tdb': 1.859666109085083, 'train@tur.pdtb.tdb_runtime': 30.1064, 'train@tur.pdtb.tdb_samples_per_second': 81.411, 'train@tur.pdtb.tdb_steps_per_second': 2.558, 'epoch': 11.0}
{'loss': 1.9282, 'learning_rate': 1.6666666666666667e-06, 'epoch': 11.0}
{'eval_loss': 2.0294008255004883, 'eval_accuracy@tur.pdtb.tdb': 0.3685897435897436, 'eval_f1@tur.pdtb.tdb': 0.12820013393296842, 'eval_precision@tur.pdtb.tdb': 0.13125775924510227, 'eval_recall@tur.pdtb.tdb': 0.14018912693604488, 'eval_loss@tur.pdtb.tdb': 2.0294010639190674, 'eval_runtime': 4.1564, 'eval_samples_per_second': 75.064, 'eval_steps_per_second': 2.406, 'epoch': 11.0}
{'train@tur.pdtb.tdb_loss': 1.85685396194458, 'train@tur.pdtb.tdb_accuracy@tur.pdtb.tdb': 0.4283965728274174, 'train@tur.pdtb.tdb_f1@tur.pdtb.tdb': 0.15655042577452438, 'train@tur.pdtb.tdb_precision@tur.pdtb.tdb': 0.15783395615021947, 'train@tur.pdtb.tdb_recall@tur.pdtb.tdb': 0.16542049111253468, 'train@tur.pdtb.tdb_loss@tur.pdtb.tdb': 1.85685396194458, 'train@tur.pdtb.tdb_runtime': 30.9251, 'train@tur.pdtb.tdb_samples_per_second': 79.256, 'train@tur.pdtb.tdb_steps_per_second': 2.49, 'epoch': 12.0}
{'loss': 1.9194, 'learning_rate': 0.0, 'epoch': 12.0}
{'eval_loss': 2.027024507522583, 'eval_accuracy@tur.pdtb.tdb': 0.375, 'eval_f1@tur.pdtb.tdb': 0.13126864338794111, 'eval_precision@tur.pdtb.tdb': 0.13339264901764902, 'eval_recall@tur.pdtb.tdb': 0.1431548766278291, 'eval_loss@tur.pdtb.tdb': 2.027024507522583, 'eval_runtime': 4.1565, 'eval_samples_per_second': 75.063, 'eval_steps_per_second': 2.406, 'epoch': 12.0}
{'train_runtime': 1165.4162, 'train_samples_per_second': 25.237, 'train_steps_per_second': 0.793, 'train_loss': 2.1893542401202315, 'epoch': 12.0}
***** train_logger metrics *****
  epoch                    =       12.0
  train_loss               =      1.374
  train_runtime            = 2:02:50.70
  train_samples_per_second =     26.052
  train_steps_per_second   =      0.816
-------------------------------------------------------------------
Lang1:  eng.sdrt.stac    Lang2:  tur.pdtb.tdb
Saving run to:  runs/full_shot/FullShot=v4_eng.sdrt.stac_tur.pdtb.tdb_lrfunc=Adafactor_lr=2e-05_plm=bert-base-multilingual-cased
Running with params: BERT_MODEL=bert-base-multilingual-cased lr=2e-05
read 9580 examples
read 1145 examples
read 1510 examples
read 2451 examples
read 312 examples
read 422 examples
Total prediction labels:  39
BertAdapterModel(
  (shared_parameters): ModuleDict()
  (bert): BertModel(
    (shared_parameters): ModuleDict()
    (invertible_adapters): ModuleDict(
      (en): NICECouplingBlock(
        (F): Sequential(
          (0): Linear(in_features=384, out_features=192, bias=True)
          (1): Activation_Function_Class(
            (f): ReLU()
          )
          (2): Linear(in_features=192, out_features=384, bias=True)
        )
        (G): Sequential(
          (0): Linear(in_features=384, out_features=192, bias=True)
          (1): Activation_Function_Class(
            (f): ReLU()
          )
          (2): Linear(in_features=192, out_features=384, bias=True)
        )
      )
      (tr): NICECouplingBlock(
        (F): Sequential(
          (0): Linear(in_features=384, out_features=192, bias=True)
          (1): Activation_Function_Class(
            (f): NewGELUActivation()
          )
          (2): Linear(in_features=192, out_features=384, bias=True)
        )
        (G): Sequential(
          (0): Linear(in_features=384, out_features=192, bias=True)
          (1): Activation_Function_Class(
            (f): NewGELUActivation()
          )
          (2): Linear(in_features=192, out_features=384, bias=True)
        )
      )
    )
    (embeddings): BertEmbeddings(
      (word_embeddings): Embedding(119547, 768, padding_idx=0)
      (position_embeddings): Embedding(512, 768)
      (token_type_embeddings): Embedding(2, 768)
      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
      (dropout): Dropout(p=0.1, inplace=False)
    )
    (encoder): BertEncoder(
      (layer): ModuleList(
        (0): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (en): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (tr): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (1): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (en): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (tr): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (2): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (en): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (tr): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (3): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (en): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (tr): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (4): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (en): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (tr): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (5): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (en): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (tr): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (6): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (en): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (tr): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (7): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (en): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (tr): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (8): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (en): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (tr): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (9): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (en): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (tr): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (10): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (en): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (tr): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (11): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (en): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (tr): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
      )
    )
    (pooler): BertPooler(
      (dense): Linear(in_features=768, out_features=768, bias=True)
      (activation): Tanh()
    )
    (prefix_tuning): PrefixTuningPool(
      (prefix_tunings): ModuleDict()
    )
  )
  (heads): ModuleDict(
    (tr): BertStyleMaskedLMHead(
      (0): Linear(in_features=768, out_features=768, bias=True)
      (1): Activation_Function_Class(
        (f): GELUActivation()
      )
      (2): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
      (3): Linear(in_features=768, out_features=119547, bias=True)
    )
    (disrpt): ClassificationHead(
      (0): Dropout(p=0.1, inplace=False)
      (1): Linear(in_features=768, out_features=768, bias=True)
      (2): Activation_Function_Class(
        (f): Tanh()
      )
      (3): Dropout(p=0.1, inplace=False)
      (4): Linear(in_features=768, out_features=39, bias=True)
    )
  )
)
{'train@eng.sdrt.stac_loss': 2.1216206550598145, 'train@eng.sdrt.stac_accuracy@eng.sdrt.stac': 0.36012526096033404, 'train@eng.sdrt.stac_f1@eng.sdrt.stac': 0.07014730297961629, 'train@eng.sdrt.stac_precision@eng.sdrt.stac': 0.10814634604319473, 'train@eng.sdrt.stac_recall@eng.sdrt.stac': 0.11213225631121605, 'train@eng.sdrt.stac_loss@eng.sdrt.stac': 2.1216206550598145, 'train@eng.sdrt.stac_runtime': 114.5073, 'train@eng.sdrt.stac_samples_per_second': 83.663, 'train@eng.sdrt.stac_steps_per_second': 2.62, 'epoch': 1.0}
{'loss': 2.5833, 'learning_rate': 1.8333333333333333e-05, 'epoch': 1.0}
{'eval_loss': 2.0818560123443604, 'eval_accuracy@eng.sdrt.stac': 0.36943231441048036, 'eval_f1@eng.sdrt.stac': 0.07449724353547907, 'eval_precision@eng.sdrt.stac': 0.08448089566858788, 'eval_recall@eng.sdrt.stac': 0.11465272505391419, 'eval_loss@eng.sdrt.stac': 2.0818560123443604, 'eval_runtime': 14.0182, 'eval_samples_per_second': 81.68, 'eval_steps_per_second': 2.568, 'epoch': 1.0}
{'train@eng.sdrt.stac_loss': 1.8747353553771973, 'train@eng.sdrt.stac_accuracy@eng.sdrt.stac': 0.4292275574112735, 'train@eng.sdrt.stac_f1@eng.sdrt.stac': 0.1329906634120749, 'train@eng.sdrt.stac_precision@eng.sdrt.stac': 0.1350038109677373, 'train@eng.sdrt.stac_recall@eng.sdrt.stac': 0.1777586137496358, 'train@eng.sdrt.stac_loss@eng.sdrt.stac': 1.8747353553771973, 'train@eng.sdrt.stac_runtime': 114.3747, 'train@eng.sdrt.stac_samples_per_second': 83.76, 'train@eng.sdrt.stac_steps_per_second': 2.623, 'epoch': 2.0}
{'loss': 2.0296, 'learning_rate': 1.6666666666666667e-05, 'epoch': 2.0}
{'eval_loss': 1.8286830186843872, 'eval_accuracy@eng.sdrt.stac': 0.43231441048034935, 'eval_f1@eng.sdrt.stac': 0.12924044463320059, 'eval_precision@eng.sdrt.stac': 0.12314682245826028, 'eval_recall@eng.sdrt.stac': 0.17716274223254538, 'eval_loss@eng.sdrt.stac': 1.828682780265808, 'eval_runtime': 13.9937, 'eval_samples_per_second': 81.823, 'eval_steps_per_second': 2.573, 'epoch': 2.0}
{'train@eng.sdrt.stac_loss': 1.7677216529846191, 'train@eng.sdrt.stac_accuracy@eng.sdrt.stac': 0.4489561586638831, 'train@eng.sdrt.stac_f1@eng.sdrt.stac': 0.15649705049512053, 'train@eng.sdrt.stac_precision@eng.sdrt.stac': 0.14018198480802427, 'train@eng.sdrt.stac_recall@eng.sdrt.stac': 0.1942995202139471, 'train@eng.sdrt.stac_loss@eng.sdrt.stac': 1.7677218914031982, 'train@eng.sdrt.stac_runtime': 114.4337, 'train@eng.sdrt.stac_samples_per_second': 83.717, 'train@eng.sdrt.stac_steps_per_second': 2.622, 'epoch': 3.0}
{'loss': 1.8565, 'learning_rate': 1.5000000000000002e-05, 'epoch': 3.0}
{'eval_loss': 1.7232239246368408, 'eval_accuracy@eng.sdrt.stac': 0.44454148471615723, 'eval_f1@eng.sdrt.stac': 0.14533648122258291, 'eval_precision@eng.sdrt.stac': 0.12914691486531493, 'eval_recall@eng.sdrt.stac': 0.18648331014848263, 'eval_loss@eng.sdrt.stac': 1.7232236862182617, 'eval_runtime': 14.0014, 'eval_samples_per_second': 81.778, 'eval_steps_per_second': 2.571, 'epoch': 3.0}
{'train@eng.sdrt.stac_loss': 1.6974010467529297, 'train@eng.sdrt.stac_accuracy@eng.sdrt.stac': 0.4594989561586639, 'train@eng.sdrt.stac_f1@eng.sdrt.stac': 0.16344924556373513, 'train@eng.sdrt.stac_precision@eng.sdrt.stac': 0.20065948044052778, 'train@eng.sdrt.stac_recall@eng.sdrt.stac': 0.19974694079509658, 'train@eng.sdrt.stac_loss@eng.sdrt.stac': 1.6974010467529297, 'train@eng.sdrt.stac_runtime': 114.4812, 'train@eng.sdrt.stac_samples_per_second': 83.682, 'train@eng.sdrt.stac_steps_per_second': 2.621, 'epoch': 4.0}
{'loss': 1.7727, 'learning_rate': 1.3333333333333333e-05, 'epoch': 4.0}
{'eval_loss': 1.6531422138214111, 'eval_accuracy@eng.sdrt.stac': 0.45414847161572053, 'eval_f1@eng.sdrt.stac': 0.15242815275819627, 'eval_precision@eng.sdrt.stac': 0.19940400651974188, 'eval_recall@eng.sdrt.stac': 0.1923360566835057, 'eval_loss@eng.sdrt.stac': 1.6531423330307007, 'eval_runtime': 14.0089, 'eval_samples_per_second': 81.734, 'eval_steps_per_second': 2.57, 'epoch': 4.0}
{'train@eng.sdrt.stac_loss': 1.653223991394043, 'train@eng.sdrt.stac_accuracy@eng.sdrt.stac': 0.474321503131524, 'train@eng.sdrt.stac_f1@eng.sdrt.stac': 0.1892059433744281, 'train@eng.sdrt.stac_precision@eng.sdrt.stac': 0.21951380138428322, 'train@eng.sdrt.stac_recall@eng.sdrt.stac': 0.21642954619519947, 'train@eng.sdrt.stac_loss@eng.sdrt.stac': 1.653223991394043, 'train@eng.sdrt.stac_runtime': 114.4168, 'train@eng.sdrt.stac_samples_per_second': 83.729, 'train@eng.sdrt.stac_steps_per_second': 2.622, 'epoch': 5.0}
{'loss': 1.7193, 'learning_rate': 1.1666666666666668e-05, 'epoch': 5.0}
{'eval_loss': 1.6126000881195068, 'eval_accuracy@eng.sdrt.stac': 0.47161572052401746, 'eval_f1@eng.sdrt.stac': 0.17059250547916283, 'eval_precision@eng.sdrt.stac': 0.22840862378164228, 'eval_recall@eng.sdrt.stac': 0.20500192299683168, 'eval_loss@eng.sdrt.stac': 1.6126002073287964, 'eval_runtime': 14.0596, 'eval_samples_per_second': 81.439, 'eval_steps_per_second': 2.561, 'epoch': 5.0}
{'train@eng.sdrt.stac_loss': 1.6182907819747925, 'train@eng.sdrt.stac_accuracy@eng.sdrt.stac': 0.477035490605428, 'train@eng.sdrt.stac_f1@eng.sdrt.stac': 0.18528176408760677, 'train@eng.sdrt.stac_precision@eng.sdrt.stac': 0.2321076789301, 'train@eng.sdrt.stac_recall@eng.sdrt.stac': 0.21526573571795277, 'train@eng.sdrt.stac_loss@eng.sdrt.stac': 1.6182907819747925, 'train@eng.sdrt.stac_runtime': 114.4824, 'train@eng.sdrt.stac_samples_per_second': 83.681, 'train@eng.sdrt.stac_steps_per_second': 2.62, 'epoch': 6.0}
{'loss': 1.6786, 'learning_rate': 1e-05, 'epoch': 6.0}
{'eval_loss': 1.5957585573196411, 'eval_accuracy@eng.sdrt.stac': 0.46812227074235807, 'eval_f1@eng.sdrt.stac': 0.16976236857263544, 'eval_precision@eng.sdrt.stac': 0.1760579212690458, 'eval_recall@eng.sdrt.stac': 0.20406067064046424, 'eval_loss@eng.sdrt.stac': 1.5957585573196411, 'eval_runtime': 14.0698, 'eval_samples_per_second': 81.38, 'eval_steps_per_second': 2.559, 'epoch': 6.0}
{'train@eng.sdrt.stac_loss': 1.5969945192337036, 'train@eng.sdrt.stac_accuracy@eng.sdrt.stac': 0.4906054279749478, 'train@eng.sdrt.stac_f1@eng.sdrt.stac': 0.2156161442306008, 'train@eng.sdrt.stac_precision@eng.sdrt.stac': 0.22102508049552094, 'train@eng.sdrt.stac_recall@eng.sdrt.stac': 0.23717181423693648, 'train@eng.sdrt.stac_loss@eng.sdrt.stac': 1.596994400024414, 'train@eng.sdrt.stac_runtime': 114.5517, 'train@eng.sdrt.stac_samples_per_second': 83.63, 'train@eng.sdrt.stac_steps_per_second': 2.619, 'epoch': 7.0}
{'loss': 1.6497, 'learning_rate': 8.333333333333334e-06, 'epoch': 7.0}
{'eval_loss': 1.578381896018982, 'eval_accuracy@eng.sdrt.stac': 0.48646288209606986, 'eval_f1@eng.sdrt.stac': 0.20536140428152927, 'eval_precision@eng.sdrt.stac': 0.23261452679210315, 'eval_recall@eng.sdrt.stac': 0.22668782371838414, 'eval_loss@eng.sdrt.stac': 1.578381896018982, 'eval_runtime': 14.0184, 'eval_samples_per_second': 81.679, 'eval_steps_per_second': 2.568, 'epoch': 7.0}
{'train@eng.sdrt.stac_loss': 1.5687530040740967, 'train@eng.sdrt.stac_accuracy@eng.sdrt.stac': 0.501670146137787, 'train@eng.sdrt.stac_f1@eng.sdrt.stac': 0.2329442051055018, 'train@eng.sdrt.stac_precision@eng.sdrt.stac': 0.36936811484847354, 'train@eng.sdrt.stac_recall@eng.sdrt.stac': 0.2529344770433024, 'train@eng.sdrt.stac_loss@eng.sdrt.stac': 1.5687530040740967, 'train@eng.sdrt.stac_runtime': 114.586, 'train@eng.sdrt.stac_samples_per_second': 83.605, 'train@eng.sdrt.stac_steps_per_second': 2.618, 'epoch': 8.0}
{'loss': 1.6313, 'learning_rate': 6.666666666666667e-06, 'epoch': 8.0}
{'eval_loss': 1.5494686365127563, 'eval_accuracy@eng.sdrt.stac': 0.508296943231441, 'eval_f1@eng.sdrt.stac': 0.21632213162491237, 'eval_precision@eng.sdrt.stac': 0.21764228578624997, 'eval_recall@eng.sdrt.stac': 0.24005010847791314, 'eval_loss@eng.sdrt.stac': 1.5494688749313354, 'eval_runtime': 14.0167, 'eval_samples_per_second': 81.688, 'eval_steps_per_second': 2.568, 'epoch': 8.0}
{'train@eng.sdrt.stac_loss': 1.552670955657959, 'train@eng.sdrt.stac_accuracy@eng.sdrt.stac': 0.5075156576200418, 'train@eng.sdrt.stac_f1@eng.sdrt.stac': 0.24743519223812388, 'train@eng.sdrt.stac_precision@eng.sdrt.stac': 0.34697742682570104, 'train@eng.sdrt.stac_recall@eng.sdrt.stac': 0.2641200293043793, 'train@eng.sdrt.stac_loss@eng.sdrt.stac': 1.552670955657959, 'train@eng.sdrt.stac_runtime': 114.6418, 'train@eng.sdrt.stac_samples_per_second': 83.565, 'train@eng.sdrt.stac_steps_per_second': 2.617, 'epoch': 9.0}
{'loss': 1.6093, 'learning_rate': 5e-06, 'epoch': 9.0}
{'eval_loss': 1.5403990745544434, 'eval_accuracy@eng.sdrt.stac': 0.508296943231441, 'eval_f1@eng.sdrt.stac': 0.23620270366096724, 'eval_precision@eng.sdrt.stac': 0.27449620589414936, 'eval_recall@eng.sdrt.stac': 0.25154517314955793, 'eval_loss@eng.sdrt.stac': 1.5403990745544434, 'eval_runtime': 14.0656, 'eval_samples_per_second': 81.404, 'eval_steps_per_second': 2.559, 'epoch': 9.0}
{'train@eng.sdrt.stac_loss': 1.5385767221450806, 'train@eng.sdrt.stac_accuracy@eng.sdrt.stac': 0.5125260960334029, 'train@eng.sdrt.stac_f1@eng.sdrt.stac': 0.2596382440927655, 'train@eng.sdrt.stac_precision@eng.sdrt.stac': 0.34545494885411177, 'train@eng.sdrt.stac_recall@eng.sdrt.stac': 0.2756800377752368, 'train@eng.sdrt.stac_loss@eng.sdrt.stac': 1.538576602935791, 'train@eng.sdrt.stac_runtime': 114.5296, 'train@eng.sdrt.stac_samples_per_second': 83.646, 'train@eng.sdrt.stac_steps_per_second': 2.619, 'epoch': 10.0}
{'loss': 1.594, 'learning_rate': 3.3333333333333333e-06, 'epoch': 10.0}
{'eval_loss': 1.529477596282959, 'eval_accuracy@eng.sdrt.stac': 0.5152838427947598, 'eval_f1@eng.sdrt.stac': 0.24567461169831872, 'eval_precision@eng.sdrt.stac': 0.2582628768898507, 'eval_recall@eng.sdrt.stac': 0.2623101040879997, 'eval_loss@eng.sdrt.stac': 1.529477596282959, 'eval_runtime': 14.0229, 'eval_samples_per_second': 81.652, 'eval_steps_per_second': 2.567, 'epoch': 10.0}
{'train@eng.sdrt.stac_loss': 1.5300613641738892, 'train@eng.sdrt.stac_accuracy@eng.sdrt.stac': 0.5167014613778705, 'train@eng.sdrt.stac_f1@eng.sdrt.stac': 0.2697194644698002, 'train@eng.sdrt.stac_precision@eng.sdrt.stac': 0.40856199136425797, 'train@eng.sdrt.stac_recall@eng.sdrt.stac': 0.2851941335022222, 'train@eng.sdrt.stac_loss@eng.sdrt.stac': 1.5300614833831787, 'train@eng.sdrt.stac_runtime': 114.5054, 'train@eng.sdrt.stac_samples_per_second': 83.664, 'train@eng.sdrt.stac_steps_per_second': 2.62, 'epoch': 11.0}
{'loss': 1.581, 'learning_rate': 1.6666666666666667e-06, 'epoch': 11.0}
{'eval_loss': 1.5237702131271362, 'eval_accuracy@eng.sdrt.stac': 0.5126637554585153, 'eval_f1@eng.sdrt.stac': 0.24322359418916362, 'eval_precision@eng.sdrt.stac': 0.2455096802303691, 'eval_recall@eng.sdrt.stac': 0.2623112184671365, 'eval_loss@eng.sdrt.stac': 1.5237700939178467, 'eval_runtime': 14.0433, 'eval_samples_per_second': 81.534, 'eval_steps_per_second': 2.564, 'epoch': 11.0}
{'train@eng.sdrt.stac_loss': 1.5286445617675781, 'train@eng.sdrt.stac_accuracy@eng.sdrt.stac': 0.5162839248434238, 'train@eng.sdrt.stac_f1@eng.sdrt.stac': 0.2707240353650848, 'train@eng.sdrt.stac_precision@eng.sdrt.stac': 0.4053205118200378, 'train@eng.sdrt.stac_recall@eng.sdrt.stac': 0.28705582300472354, 'train@eng.sdrt.stac_loss@eng.sdrt.stac': 1.5286445617675781, 'train@eng.sdrt.stac_runtime': 114.5105, 'train@eng.sdrt.stac_samples_per_second': 83.66, 'train@eng.sdrt.stac_steps_per_second': 2.62, 'epoch': 12.0}
{'loss': 1.5787, 'learning_rate': 0.0, 'epoch': 12.0}
{'eval_loss': 1.5230097770690918, 'eval_accuracy@eng.sdrt.stac': 0.514410480349345, 'eval_f1@eng.sdrt.stac': 0.24628733999082195, 'eval_precision@eng.sdrt.stac': 0.24908912910844622, 'eval_recall@eng.sdrt.stac': 0.2657343211799998, 'eval_loss@eng.sdrt.stac': 1.5230096578598022, 'eval_runtime': 14.0256, 'eval_samples_per_second': 81.636, 'eval_steps_per_second': 2.567, 'epoch': 12.0}
{'train_runtime': 4436.2586, 'train_samples_per_second': 25.914, 'train_steps_per_second': 0.811, 'train_loss': 1.773660405476888, 'epoch': 12.0}
***** train_log metrics *****
  epoch                    =       12.0
  train_loss               =     1.7737
  train_runtime            = 1:13:56.25
  train_samples_per_second =     25.914
  train_steps_per_second   =      0.811
{'train@tur.pdtb.tdb_loss': 2.8512723445892334, 'train@tur.pdtb.tdb_accuracy@tur.pdtb.tdb': 0.25050999592003265, 'train@tur.pdtb.tdb_f1@tur.pdtb.tdb': 0.01741967515426626, 'train@tur.pdtb.tdb_precision@tur.pdtb.tdb': 0.010891738953044898, 'train@tur.pdtb.tdb_recall@tur.pdtb.tdb': 0.043478260869565216, 'train@tur.pdtb.tdb_loss@tur.pdtb.tdb': 2.8512721061706543, 'train@tur.pdtb.tdb_runtime': 30.0676, 'train@tur.pdtb.tdb_samples_per_second': 81.516, 'train@tur.pdtb.tdb_steps_per_second': 2.561, 'epoch': 1.0}
{'loss': 3.2903, 'learning_rate': 1.8333333333333333e-05, 'epoch': 1.0}
{'eval_loss': 2.807971715927124, 'eval_accuracy@tur.pdtb.tdb': 0.266025641025641, 'eval_f1@tur.pdtb.tdb': 0.019102416570771, 'eval_precision@tur.pdtb.tdb': 0.012092074592074592, 'eval_recall@tur.pdtb.tdb': 0.045454545454545456, 'eval_loss@tur.pdtb.tdb': 2.807971715927124, 'eval_runtime': 4.1655, 'eval_samples_per_second': 74.9, 'eval_steps_per_second': 2.401, 'epoch': 1.0}
{'train@tur.pdtb.tdb_loss': 2.479938268661499, 'train@tur.pdtb.tdb_accuracy@tur.pdtb.tdb': 0.25050999592003265, 'train@tur.pdtb.tdb_f1@tur.pdtb.tdb': 0.017436742112288076, 'train@tur.pdtb.tdb_precision@tur.pdtb.tdb': 0.010905086672350099, 'train@tur.pdtb.tdb_recall@tur.pdtb.tdb': 0.043478260869565216, 'train@tur.pdtb.tdb_loss@tur.pdtb.tdb': 2.479938268661499, 'train@tur.pdtb.tdb_runtime': 30.819, 'train@tur.pdtb.tdb_samples_per_second': 79.529, 'train@tur.pdtb.tdb_steps_per_second': 2.498, 'epoch': 2.0}
{'loss': 2.6665, 'learning_rate': 1.6666666666666667e-05, 'epoch': 2.0}
{'eval_loss': 2.3827500343322754, 'eval_accuracy@tur.pdtb.tdb': 0.266025641025641, 'eval_f1@tur.pdtb.tdb': 0.01915089986155976, 'eval_precision@tur.pdtb.tdb': 0.012130955860859398, 'eval_recall@tur.pdtb.tdb': 0.045454545454545456, 'eval_loss@tur.pdtb.tdb': 2.3827502727508545, 'eval_runtime': 4.1365, 'eval_samples_per_second': 75.426, 'eval_steps_per_second': 2.418, 'epoch': 2.0}
{'train@tur.pdtb.tdb_loss': 2.3324272632598877, 'train@tur.pdtb.tdb_accuracy@tur.pdtb.tdb': 0.2733578131374949, 'train@tur.pdtb.tdb_f1@tur.pdtb.tdb': 0.056105622656127736, 'train@tur.pdtb.tdb_precision@tur.pdtb.tdb': 0.12060807724970128, 'train@tur.pdtb.tdb_recall@tur.pdtb.tdb': 0.0747412263340037, 'train@tur.pdtb.tdb_loss@tur.pdtb.tdb': 2.3324272632598877, 'train@tur.pdtb.tdb_runtime': 30.0957, 'train@tur.pdtb.tdb_samples_per_second': 81.44, 'train@tur.pdtb.tdb_steps_per_second': 2.559, 'epoch': 3.0}
{'loss': 2.4268, 'learning_rate': 1.5000000000000002e-05, 'epoch': 3.0}
{'eval_loss': 2.2641029357910156, 'eval_accuracy@tur.pdtb.tdb': 0.28525641025641024, 'eval_f1@tur.pdtb.tdb': 0.05993437357073721, 'eval_precision@tur.pdtb.tdb': 0.06552277744330724, 'eval_recall@tur.pdtb.tdb': 0.08467023172905526, 'eval_loss@tur.pdtb.tdb': 2.2641031742095947, 'eval_runtime': 4.1856, 'eval_samples_per_second': 74.541, 'eval_steps_per_second': 2.389, 'epoch': 3.0}
{'train@tur.pdtb.tdb_loss': 2.230783462524414, 'train@tur.pdtb.tdb_accuracy@tur.pdtb.tdb': 0.30640554875560994, 'train@tur.pdtb.tdb_f1@tur.pdtb.tdb': 0.07806228969903299, 'train@tur.pdtb.tdb_precision@tur.pdtb.tdb': 0.11683230142278597, 'train@tur.pdtb.tdb_recall@tur.pdtb.tdb': 0.09350295758066877, 'train@tur.pdtb.tdb_loss@tur.pdtb.tdb': 2.230783462524414, 'train@tur.pdtb.tdb_runtime': 30.1453, 'train@tur.pdtb.tdb_samples_per_second': 81.306, 'train@tur.pdtb.tdb_steps_per_second': 2.554, 'epoch': 4.0}
{'loss': 2.3102, 'learning_rate': 1.3333333333333333e-05, 'epoch': 4.0}
{'eval_loss': 2.204538583755493, 'eval_accuracy@tur.pdtb.tdb': 0.2916666666666667, 'eval_f1@tur.pdtb.tdb': 0.06798692838126857, 'eval_precision@tur.pdtb.tdb': 0.06440696984175245, 'eval_recall@tur.pdtb.tdb': 0.09587637537672973, 'eval_loss@tur.pdtb.tdb': 2.204538345336914, 'eval_runtime': 4.1412, 'eval_samples_per_second': 75.341, 'eval_steps_per_second': 2.415, 'epoch': 4.0}
{'train@tur.pdtb.tdb_loss': 2.15500545501709, 'train@tur.pdtb.tdb_accuracy@tur.pdtb.tdb': 0.3312933496532028, 'train@tur.pdtb.tdb_f1@tur.pdtb.tdb': 0.09203849095366633, 'train@tur.pdtb.tdb_precision@tur.pdtb.tdb': 0.10153539220466548, 'train@tur.pdtb.tdb_recall@tur.pdtb.tdb': 0.10985733459075364, 'train@tur.pdtb.tdb_loss@tur.pdtb.tdb': 2.155005931854248, 'train@tur.pdtb.tdb_runtime': 30.0938, 'train@tur.pdtb.tdb_samples_per_second': 81.445, 'train@tur.pdtb.tdb_steps_per_second': 2.559, 'epoch': 5.0}
{'loss': 2.2388, 'learning_rate': 1.1666666666666668e-05, 'epoch': 5.0}
{'eval_loss': 2.1638855934143066, 'eval_accuracy@tur.pdtb.tdb': 0.2980769230769231, 'eval_f1@tur.pdtb.tdb': 0.07594315887695614, 'eval_precision@tur.pdtb.tdb': 0.07954484722161491, 'eval_recall@tur.pdtb.tdb': 0.10133582458174946, 'eval_loss@tur.pdtb.tdb': 2.1638855934143066, 'eval_runtime': 4.166, 'eval_samples_per_second': 74.892, 'eval_steps_per_second': 2.4, 'epoch': 5.0}
{'train@tur.pdtb.tdb_loss': 2.10079288482666, 'train@tur.pdtb.tdb_accuracy@tur.pdtb.tdb': 0.34802121583027334, 'train@tur.pdtb.tdb_f1@tur.pdtb.tdb': 0.10445370642779603, 'train@tur.pdtb.tdb_precision@tur.pdtb.tdb': 0.14421721106982577, 'train@tur.pdtb.tdb_recall@tur.pdtb.tdb': 0.11898721314327787, 'train@tur.pdtb.tdb_loss@tur.pdtb.tdb': 2.100792646408081, 'train@tur.pdtb.tdb_runtime': 30.0715, 'train@tur.pdtb.tdb_samples_per_second': 81.506, 'train@tur.pdtb.tdb_steps_per_second': 2.561, 'epoch': 6.0}
{'loss': 2.1865, 'learning_rate': 1e-05, 'epoch': 6.0}
{'eval_loss': 2.1355974674224854, 'eval_accuracy@tur.pdtb.tdb': 0.3108974358974359, 'eval_f1@tur.pdtb.tdb': 0.09052871625075425, 'eval_precision@tur.pdtb.tdb': 0.12254901960784315, 'eval_recall@tur.pdtb.tdb': 0.11021294957574836, 'eval_loss@tur.pdtb.tdb': 2.1355974674224854, 'eval_runtime': 4.1892, 'eval_samples_per_second': 74.478, 'eval_steps_per_second': 2.387, 'epoch': 6.0}
{'train@tur.pdtb.tdb_loss': 2.0568630695343018, 'train@tur.pdtb.tdb_accuracy@tur.pdtb.tdb': 0.36760505915952674, 'train@tur.pdtb.tdb_f1@tur.pdtb.tdb': 0.1217883644420511, 'train@tur.pdtb.tdb_precision@tur.pdtb.tdb': 0.15413511131259305, 'train@tur.pdtb.tdb_recall@tur.pdtb.tdb': 0.13428306124966605, 'train@tur.pdtb.tdb_loss@tur.pdtb.tdb': 2.0568630695343018, 'train@tur.pdtb.tdb_runtime': 30.1101, 'train@tur.pdtb.tdb_samples_per_second': 81.401, 'train@tur.pdtb.tdb_steps_per_second': 2.557, 'epoch': 7.0}
{'loss': 2.1336, 'learning_rate': 8.333333333333334e-06, 'epoch': 7.0}
{'eval_loss': 2.1182124614715576, 'eval_accuracy@tur.pdtb.tdb': 0.32051282051282054, 'eval_f1@tur.pdtb.tdb': 0.09891704941736547, 'eval_precision@tur.pdtb.tdb': 0.12613954672778202, 'eval_recall@tur.pdtb.tdb': 0.11669738180559425, 'eval_loss@tur.pdtb.tdb': 2.1182122230529785, 'eval_runtime': 4.1505, 'eval_samples_per_second': 75.172, 'eval_steps_per_second': 2.409, 'epoch': 7.0}
{'train@tur.pdtb.tdb_loss': 2.032212972640991, 'train@tur.pdtb.tdb_accuracy@tur.pdtb.tdb': 0.37658098735210116, 'train@tur.pdtb.tdb_f1@tur.pdtb.tdb': 0.12686580173328735, 'train@tur.pdtb.tdb_precision@tur.pdtb.tdb': 0.15657356243180975, 'train@tur.pdtb.tdb_recall@tur.pdtb.tdb': 0.13720469625286333, 'train@tur.pdtb.tdb_loss@tur.pdtb.tdb': 2.0322134494781494, 'train@tur.pdtb.tdb_runtime': 30.1158, 'train@tur.pdtb.tdb_samples_per_second': 81.386, 'train@tur.pdtb.tdb_steps_per_second': 2.557, 'epoch': 8.0}
{'loss': 2.0995, 'learning_rate': 6.666666666666667e-06, 'epoch': 8.0}
{'eval_loss': 2.1035172939300537, 'eval_accuracy@tur.pdtb.tdb': 0.3301282051282051, 'eval_f1@tur.pdtb.tdb': 0.10386167924441952, 'eval_precision@tur.pdtb.tdb': 0.14235676604097655, 'eval_recall@tur.pdtb.tdb': 0.11993629265320074, 'eval_loss@tur.pdtb.tdb': 2.1035172939300537, 'eval_runtime': 4.1711, 'eval_samples_per_second': 74.801, 'eval_steps_per_second': 2.397, 'epoch': 8.0}
{'train@tur.pdtb.tdb_loss': 2.009748697280884, 'train@tur.pdtb.tdb_accuracy@tur.pdtb.tdb': 0.3810689514483884, 'train@tur.pdtb.tdb_f1@tur.pdtb.tdb': 0.12897813030595356, 'train@tur.pdtb.tdb_precision@tur.pdtb.tdb': 0.15289314430134288, 'train@tur.pdtb.tdb_recall@tur.pdtb.tdb': 0.14021956671479915, 'train@tur.pdtb.tdb_loss@tur.pdtb.tdb': 2.009748697280884, 'train@tur.pdtb.tdb_runtime': 30.1004, 'train@tur.pdtb.tdb_samples_per_second': 81.427, 'train@tur.pdtb.tdb_steps_per_second': 2.558, 'epoch': 9.0}
{'loss': 2.0763, 'learning_rate': 5e-06, 'epoch': 9.0}
{'eval_loss': 2.0920989513397217, 'eval_accuracy@tur.pdtb.tdb': 0.32371794871794873, 'eval_f1@tur.pdtb.tdb': 0.10356945923292274, 'eval_precision@tur.pdtb.tdb': 0.13131347652878753, 'eval_recall@tur.pdtb.tdb': 0.11911548637142011, 'eval_loss@tur.pdtb.tdb': 2.0920987129211426, 'eval_runtime': 4.1381, 'eval_samples_per_second': 75.398, 'eval_steps_per_second': 2.417, 'epoch': 9.0}
{'train@tur.pdtb.tdb_loss': 1.9926506280899048, 'train@tur.pdtb.tdb_accuracy@tur.pdtb.tdb': 0.38555691554467564, 'train@tur.pdtb.tdb_f1@tur.pdtb.tdb': 0.1326342346843506, 'train@tur.pdtb.tdb_precision@tur.pdtb.tdb': 0.15226737520592207, 'train@tur.pdtb.tdb_recall@tur.pdtb.tdb': 0.1435773391772196, 'train@tur.pdtb.tdb_loss@tur.pdtb.tdb': 1.9926505088806152, 'train@tur.pdtb.tdb_runtime': 30.0611, 'train@tur.pdtb.tdb_samples_per_second': 81.534, 'train@tur.pdtb.tdb_steps_per_second': 2.561, 'epoch': 10.0}
{'loss': 2.0525, 'learning_rate': 3.3333333333333333e-06, 'epoch': 10.0}
{'eval_loss': 2.082930088043213, 'eval_accuracy@tur.pdtb.tdb': 0.3269230769230769, 'eval_f1@tur.pdtb.tdb': 0.11319934489546207, 'eval_precision@tur.pdtb.tdb': 0.14971728790711927, 'eval_recall@tur.pdtb.tdb': 0.12457095761097631, 'eval_loss@tur.pdtb.tdb': 2.082930088043213, 'eval_runtime': 4.1898, 'eval_samples_per_second': 74.466, 'eval_steps_per_second': 2.387, 'epoch': 10.0}
{'train@tur.pdtb.tdb_loss': 1.9892866611480713, 'train@tur.pdtb.tdb_accuracy@tur.pdtb.tdb': 0.38514891880864954, 'train@tur.pdtb.tdb_f1@tur.pdtb.tdb': 0.13173295520066314, 'train@tur.pdtb.tdb_precision@tur.pdtb.tdb': 0.1542830030526583, 'train@tur.pdtb.tdb_recall@tur.pdtb.tdb': 0.14258541993993767, 'train@tur.pdtb.tdb_loss@tur.pdtb.tdb': 1.9892868995666504, 'train@tur.pdtb.tdb_runtime': 30.0862, 'train@tur.pdtb.tdb_samples_per_second': 81.466, 'train@tur.pdtb.tdb_steps_per_second': 2.559, 'epoch': 11.0}
{'loss': 2.049, 'learning_rate': 1.6666666666666667e-06, 'epoch': 11.0}
{'eval_loss': 2.0814433097839355, 'eval_accuracy@tur.pdtb.tdb': 0.3269230769230769, 'eval_f1@tur.pdtb.tdb': 0.11217778620476065, 'eval_precision@tur.pdtb.tdb': 0.14885327989394972, 'eval_recall@tur.pdtb.tdb': 0.12413046044444456, 'eval_loss@tur.pdtb.tdb': 2.0814433097839355, 'eval_runtime': 4.1624, 'eval_samples_per_second': 74.957, 'eval_steps_per_second': 2.402, 'epoch': 11.0}
{'train@tur.pdtb.tdb_loss': 1.9846346378326416, 'train@tur.pdtb.tdb_accuracy@tur.pdtb.tdb': 0.38922888616891066, 'train@tur.pdtb.tdb_f1@tur.pdtb.tdb': 0.13453989051841522, 'train@tur.pdtb.tdb_precision@tur.pdtb.tdb': 0.15533749119879198, 'train@tur.pdtb.tdb_recall@tur.pdtb.tdb': 0.14490482595916837, 'train@tur.pdtb.tdb_loss@tur.pdtb.tdb': 1.9846346378326416, 'train@tur.pdtb.tdb_runtime': 30.083, 'train@tur.pdtb.tdb_samples_per_second': 81.474, 'train@tur.pdtb.tdb_steps_per_second': 2.56, 'epoch': 12.0}
{'loss': 2.0388, 'learning_rate': 0.0, 'epoch': 12.0}
{'eval_loss': 2.079763412475586, 'eval_accuracy@tur.pdtb.tdb': 0.3269230769230769, 'eval_f1@tur.pdtb.tdb': 0.11309338313777179, 'eval_precision@tur.pdtb.tdb': 0.14927930483755725, 'eval_recall@tur.pdtb.tdb': 0.12457095761097631, 'eval_loss@tur.pdtb.tdb': 2.079763650894165, 'eval_runtime': 4.1729, 'eval_samples_per_second': 74.769, 'eval_steps_per_second': 2.396, 'epoch': 12.0}
{'train_runtime': 1166.7063, 'train_samples_per_second': 25.209, 'train_steps_per_second': 0.792, 'train_loss': 2.2974076209130225, 'epoch': 12.0}
***** train_logger metrics *****
  epoch                    =       12.0
  train_loss               =     1.7737
  train_runtime            = 1:13:56.25
  train_samples_per_second =     25.914
  train_steps_per_second   =      0.811
-------------------------------------------------------------------
Lang1:  fas.rst.prstc    Lang2:  tur.pdtb.tdb
Saving run to:  runs/full_shot/FullShot=v4_fas.rst.prstc_tur.pdtb.tdb_lrfunc=Adafactor_lr=2e-05_plm=bert-base-multilingual-cased
Running with params: BERT_MODEL=bert-base-multilingual-cased lr=2e-05
read 4100 examples
read 499 examples
read 592 examples
read 2451 examples
read 312 examples
read 422 examples
Total prediction labels:  40
BertAdapterModel(
  (shared_parameters): ModuleDict()
  (bert): BertModel(
    (shared_parameters): ModuleDict()
    (invertible_adapters): ModuleDict(
      (ar): NICECouplingBlock(
        (F): Sequential(
          (0): Linear(in_features=384, out_features=192, bias=True)
          (1): Activation_Function_Class(
            (f): ReLU()
          )
          (2): Linear(in_features=192, out_features=384, bias=True)
        )
        (G): Sequential(
          (0): Linear(in_features=384, out_features=192, bias=True)
          (1): Activation_Function_Class(
            (f): ReLU()
          )
          (2): Linear(in_features=192, out_features=384, bias=True)
        )
      )
      (tr): NICECouplingBlock(
        (F): Sequential(
          (0): Linear(in_features=384, out_features=192, bias=True)
          (1): Activation_Function_Class(
            (f): NewGELUActivation()
          )
          (2): Linear(in_features=192, out_features=384, bias=True)
        )
        (G): Sequential(
          (0): Linear(in_features=384, out_features=192, bias=True)
          (1): Activation_Function_Class(
            (f): NewGELUActivation()
          )
          (2): Linear(in_features=192, out_features=384, bias=True)
        )
      )
    )
    (embeddings): BertEmbeddings(
      (word_embeddings): Embedding(119547, 768, padding_idx=0)
      (position_embeddings): Embedding(512, 768)
      (token_type_embeddings): Embedding(2, 768)
      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
      (dropout): Dropout(p=0.1, inplace=False)
    )
    (encoder): BertEncoder(
      (layer): ModuleList(
        (0): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (ar): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (tr): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (1): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (ar): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (tr): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (2): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (ar): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (tr): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (3): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (ar): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (tr): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (4): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (ar): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (tr): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (5): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (ar): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (tr): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (6): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (ar): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (tr): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (7): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (ar): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (tr): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (8): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (ar): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (tr): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (9): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (ar): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (tr): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (10): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (ar): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (tr): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (11): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (ar): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (tr): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
      )
    )
    (pooler): BertPooler(
      (dense): Linear(in_features=768, out_features=768, bias=True)
      (activation): Tanh()
    )
    (prefix_tuning): PrefixTuningPool(
      (prefix_tunings): ModuleDict()
    )
  )
  (heads): ModuleDict(
    (tr): BertStyleMaskedLMHead(
      (0): Linear(in_features=768, out_features=768, bias=True)
      (1): Activation_Function_Class(
        (f): GELUActivation()
      )
      (2): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
      (3): Linear(in_features=768, out_features=119547, bias=True)
    )
    (disrpt): ClassificationHead(
      (0): Dropout(p=0.1, inplace=False)
      (1): Linear(in_features=768, out_features=768, bias=True)
      (2): Activation_Function_Class(
        (f): Tanh()
      )
      (3): Dropout(p=0.1, inplace=False)
      (4): Linear(in_features=768, out_features=40, bias=True)
    )
  )
)
{'train@fas.rst.prstc_loss': 2.4377989768981934, 'train@fas.rst.prstc_accuracy@fas.rst.prstc': 0.23780487804878048, 'train@fas.rst.prstc_f1@fas.rst.prstc': 0.02260214430599826, 'train@fas.rst.prstc_precision@fas.rst.prstc': 0.013988522238163558, 'train@fas.rst.prstc_recall@fas.rst.prstc': 0.058823529411764705, 'train@fas.rst.prstc_loss@fas.rst.prstc': 2.4377989768981934, 'train@fas.rst.prstc_runtime': 49.1025, 'train@fas.rst.prstc_samples_per_second': 83.499, 'train@fas.rst.prstc_steps_per_second': 2.627, 'epoch': 1.0}
{'loss': 2.8792, 'learning_rate': 1.8333333333333333e-05, 'epoch': 1.0}
{'eval_loss': 2.362274646759033, 'eval_accuracy@fas.rst.prstc': 0.24448897795591182, 'eval_f1@fas.rst.prstc': 0.026194310252281264, 'eval_precision@fas.rst.prstc': 0.016299265197060788, 'eval_recall@fas.rst.prstc': 0.06666666666666667, 'eval_loss@fas.rst.prstc': 2.362274646759033, 'eval_runtime': 6.3043, 'eval_samples_per_second': 79.153, 'eval_steps_per_second': 2.538, 'epoch': 1.0}
{'train@fas.rst.prstc_loss': 2.3667168617248535, 'train@fas.rst.prstc_accuracy@fas.rst.prstc': 0.25926829268292684, 'train@fas.rst.prstc_f1@fas.rst.prstc': 0.037634116693156784, 'train@fas.rst.prstc_precision@fas.rst.prstc': 0.03031201310449725, 'train@fas.rst.prstc_recall@fas.rst.prstc': 0.06584384326562048, 'train@fas.rst.prstc_loss@fas.rst.prstc': 2.3667168617248535, 'train@fas.rst.prstc_runtime': 49.1677, 'train@fas.rst.prstc_samples_per_second': 83.388, 'train@fas.rst.prstc_steps_per_second': 2.624, 'epoch': 2.0}
{'loss': 2.4204, 'learning_rate': 1.6666666666666667e-05, 'epoch': 2.0}
{'eval_loss': 2.2803401947021484, 'eval_accuracy@fas.rst.prstc': 0.28256513026052105, 'eval_f1@fas.rst.prstc': 0.04743226377237467, 'eval_precision@fas.rst.prstc': 0.04198687350835322, 'eval_recall@fas.rst.prstc': 0.07801377999524828, 'eval_loss@fas.rst.prstc': 2.2803404331207275, 'eval_runtime': 6.2991, 'eval_samples_per_second': 79.218, 'eval_steps_per_second': 2.54, 'epoch': 2.0}
{'train@fas.rst.prstc_loss': 2.3472824096679688, 'train@fas.rst.prstc_accuracy@fas.rst.prstc': 0.2426829268292683, 'train@fas.rst.prstc_f1@fas.rst.prstc': 0.026345846761468734, 'train@fas.rst.prstc_precision@fas.rst.prstc': 0.031363244598538724, 'train@fas.rst.prstc_recall@fas.rst.prstc': 0.06028775285345998, 'train@fas.rst.prstc_loss@fas.rst.prstc': 2.3472821712493896, 'train@fas.rst.prstc_runtime': 49.1924, 'train@fas.rst.prstc_samples_per_second': 83.346, 'train@fas.rst.prstc_steps_per_second': 2.622, 'epoch': 3.0}
{'loss': 2.3705, 'learning_rate': 1.5000000000000002e-05, 'epoch': 3.0}
{'eval_loss': 2.2613141536712646, 'eval_accuracy@fas.rst.prstc': 0.25250501002004005, 'eval_f1@fas.rst.prstc': 0.03174342105263158, 'eval_precision@fas.rst.prstc': 0.04223910520206817, 'eval_recall@fas.rst.prstc': 0.06901876930387266, 'eval_loss@fas.rst.prstc': 2.2613136768341064, 'eval_runtime': 6.3124, 'eval_samples_per_second': 79.051, 'eval_steps_per_second': 2.535, 'epoch': 3.0}
{'train@fas.rst.prstc_loss': 2.333575963973999, 'train@fas.rst.prstc_accuracy@fas.rst.prstc': 0.23780487804878048, 'train@fas.rst.prstc_f1@fas.rst.prstc': 0.02260214430599826, 'train@fas.rst.prstc_precision@fas.rst.prstc': 0.013988522238163558, 'train@fas.rst.prstc_recall@fas.rst.prstc': 0.058823529411764705, 'train@fas.rst.prstc_loss@fas.rst.prstc': 2.333575963973999, 'train@fas.rst.prstc_runtime': 49.1893, 'train@fas.rst.prstc_samples_per_second': 83.352, 'train@fas.rst.prstc_steps_per_second': 2.623, 'epoch': 4.0}
{'loss': 2.3594, 'learning_rate': 1.3333333333333333e-05, 'epoch': 4.0}
{'eval_loss': 2.252613067626953, 'eval_accuracy@fas.rst.prstc': 0.24448897795591182, 'eval_f1@fas.rst.prstc': 0.026194310252281264, 'eval_precision@fas.rst.prstc': 0.016299265197060788, 'eval_recall@fas.rst.prstc': 0.06666666666666667, 'eval_loss@fas.rst.prstc': 2.252613067626953, 'eval_runtime': 6.291, 'eval_samples_per_second': 79.319, 'eval_steps_per_second': 2.543, 'epoch': 4.0}
{'train@fas.rst.prstc_loss': 2.3187777996063232, 'train@fas.rst.prstc_accuracy@fas.rst.prstc': 0.25926829268292684, 'train@fas.rst.prstc_f1@fas.rst.prstc': 0.03498999964590544, 'train@fas.rst.prstc_precision@fas.rst.prstc': 0.03141494876587083, 'train@fas.rst.prstc_recall@fas.rst.prstc': 0.06524587358129286, 'train@fas.rst.prstc_loss@fas.rst.prstc': 2.3187780380249023, 'train@fas.rst.prstc_runtime': 49.196, 'train@fas.rst.prstc_samples_per_second': 83.34, 'train@fas.rst.prstc_steps_per_second': 2.622, 'epoch': 5.0}
{'loss': 2.3419, 'learning_rate': 1.1666666666666668e-05, 'epoch': 5.0}
{'eval_loss': 2.234968662261963, 'eval_accuracy@fas.rst.prstc': 0.27655310621242485, 'eval_f1@fas.rst.prstc': 0.04357254638944779, 'eval_precision@fas.rst.prstc': 0.04279549877316186, 'eval_recall@fas.rst.prstc': 0.0760750772154906, 'eval_loss@fas.rst.prstc': 2.234968662261963, 'eval_runtime': 6.3127, 'eval_samples_per_second': 79.047, 'eval_steps_per_second': 2.535, 'epoch': 5.0}
{'train@fas.rst.prstc_loss': 2.291578769683838, 'train@fas.rst.prstc_accuracy@fas.rst.prstc': 0.255609756097561, 'train@fas.rst.prstc_f1@fas.rst.prstc': 0.03339678969557311, 'train@fas.rst.prstc_precision@fas.rst.prstc': 0.03028396375788721, 'train@fas.rst.prstc_recall@fas.rst.prstc': 0.06415690553362643, 'train@fas.rst.prstc_loss@fas.rst.prstc': 2.291579008102417, 'train@fas.rst.prstc_runtime': 49.2191, 'train@fas.rst.prstc_samples_per_second': 83.301, 'train@fas.rst.prstc_steps_per_second': 2.621, 'epoch': 6.0}
{'loss': 2.3226, 'learning_rate': 1e-05, 'epoch': 6.0}
{'eval_loss': 2.2106258869171143, 'eval_accuracy@fas.rst.prstc': 0.27655310621242485, 'eval_f1@fas.rst.prstc': 0.04376599010745352, 'eval_precision@fas.rst.prstc': 0.04469236583522298, 'eval_recall@fas.rst.prstc': 0.0760750772154906, 'eval_loss@fas.rst.prstc': 2.2106258869171143, 'eval_runtime': 6.2988, 'eval_samples_per_second': 79.221, 'eval_steps_per_second': 2.54, 'epoch': 6.0}
{'train@fas.rst.prstc_loss': 2.240675210952759, 'train@fas.rst.prstc_accuracy@fas.rst.prstc': 0.3048780487804878, 'train@fas.rst.prstc_f1@fas.rst.prstc': 0.05045340202395609, 'train@fas.rst.prstc_precision@fas.rst.prstc': 0.03731356143666775, 'train@fas.rst.prstc_recall@fas.rst.prstc': 0.0806585154520073, 'train@fas.rst.prstc_loss@fas.rst.prstc': 2.240675210952759, 'train@fas.rst.prstc_runtime': 49.1773, 'train@fas.rst.prstc_samples_per_second': 83.372, 'train@fas.rst.prstc_steps_per_second': 2.623, 'epoch': 7.0}
{'loss': 2.2924, 'learning_rate': 8.333333333333334e-06, 'epoch': 7.0}
{'eval_loss': 2.1443610191345215, 'eval_accuracy@fas.rst.prstc': 0.3286573146292585, 'eval_f1@fas.rst.prstc': 0.06050034312808035, 'eval_precision@fas.rst.prstc': 0.045707406914303465, 'eval_recall@fas.rst.prstc': 0.09224518888096936, 'eval_loss@fas.rst.prstc': 2.1443610191345215, 'eval_runtime': 6.3102, 'eval_samples_per_second': 79.079, 'eval_steps_per_second': 2.536, 'epoch': 7.0}
{'train@fas.rst.prstc_loss': 2.199537992477417, 'train@fas.rst.prstc_accuracy@fas.rst.prstc': 0.3153658536585366, 'train@fas.rst.prstc_f1@fas.rst.prstc': 0.05278539638273663, 'train@fas.rst.prstc_precision@fas.rst.prstc': 0.039841454504741795, 'train@fas.rst.prstc_recall@fas.rst.prstc': 0.0838323545457463, 'train@fas.rst.prstc_loss@fas.rst.prstc': 2.199537992477417, 'train@fas.rst.prstc_runtime': 49.2104, 'train@fas.rst.prstc_samples_per_second': 83.316, 'train@fas.rst.prstc_steps_per_second': 2.621, 'epoch': 8.0}
{'loss': 2.2467, 'learning_rate': 6.666666666666667e-06, 'epoch': 8.0}
{'eval_loss': 2.1058075428009033, 'eval_accuracy@fas.rst.prstc': 0.3406813627254509, 'eval_f1@fas.rst.prstc': 0.06333442164061058, 'eval_precision@fas.rst.prstc': 0.04867733976014231, 'eval_recall@fas.rst.prstc': 0.09572344975053457, 'eval_loss@fas.rst.prstc': 2.1058077812194824, 'eval_runtime': 6.3264, 'eval_samples_per_second': 78.876, 'eval_steps_per_second': 2.529, 'epoch': 8.0}
{'train@fas.rst.prstc_loss': 2.183863878250122, 'train@fas.rst.prstc_accuracy@fas.rst.prstc': 0.3229268292682927, 'train@fas.rst.prstc_f1@fas.rst.prstc': 0.05438924769790507, 'train@fas.rst.prstc_precision@fas.rst.prstc': 0.04152263520729945, 'train@fas.rst.prstc_recall@fas.rst.prstc': 0.08595102852925131, 'train@fas.rst.prstc_loss@fas.rst.prstc': 2.183863401412964, 'train@fas.rst.prstc_runtime': 49.159, 'train@fas.rst.prstc_samples_per_second': 83.403, 'train@fas.rst.prstc_steps_per_second': 2.624, 'epoch': 9.0}
{'loss': 2.2197, 'learning_rate': 5e-06, 'epoch': 9.0}
{'eval_loss': 2.0846047401428223, 'eval_accuracy@fas.rst.prstc': 0.3486973947895792, 'eval_f1@fas.rst.prstc': 0.06504528964665639, 'eval_precision@fas.rst.prstc': 0.05051851851851852, 'eval_recall@fas.rst.prstc': 0.09807555238774056, 'eval_loss@fas.rst.prstc': 2.0846049785614014, 'eval_runtime': 6.3084, 'eval_samples_per_second': 79.101, 'eval_steps_per_second': 2.536, 'epoch': 9.0}
{'train@fas.rst.prstc_loss': 2.1719653606414795, 'train@fas.rst.prstc_accuracy@fas.rst.prstc': 0.33, 'train@fas.rst.prstc_f1@fas.rst.prstc': 0.05608427746477486, 'train@fas.rst.prstc_precision@fas.rst.prstc': 0.04382325656909697, 'train@fas.rst.prstc_recall@fas.rst.prstc': 0.08814222906838676, 'train@fas.rst.prstc_loss@fas.rst.prstc': 2.1719653606414795, 'train@fas.rst.prstc_runtime': 49.1289, 'train@fas.rst.prstc_samples_per_second': 83.454, 'train@fas.rst.prstc_steps_per_second': 2.626, 'epoch': 10.0}
{'loss': 2.2093, 'learning_rate': 3.3333333333333333e-06, 'epoch': 10.0}
{'eval_loss': 2.0710549354553223, 'eval_accuracy@fas.rst.prstc': 0.35470941883767537, 'eval_f1@fas.rst.prstc': 0.06687024803689393, 'eval_precision@fas.rst.prstc': 0.05296552734732754, 'eval_recall@fas.rst.prstc': 0.09981468282252318, 'eval_loss@fas.rst.prstc': 2.0710551738739014, 'eval_runtime': 6.3134, 'eval_samples_per_second': 79.038, 'eval_steps_per_second': 2.534, 'epoch': 10.0}
{'train@fas.rst.prstc_loss': 2.1668546199798584, 'train@fas.rst.prstc_accuracy@fas.rst.prstc': 0.33463414634146343, 'train@fas.rst.prstc_f1@fas.rst.prstc': 0.0568270846503385, 'train@fas.rst.prstc_precision@fas.rst.prstc': 0.044363419764277434, 'train@fas.rst.prstc_recall@fas.rst.prstc': 0.08938972861375864, 'train@fas.rst.prstc_loss@fas.rst.prstc': 2.1668543815612793, 'train@fas.rst.prstc_runtime': 49.1935, 'train@fas.rst.prstc_samples_per_second': 83.344, 'train@fas.rst.prstc_steps_per_second': 2.622, 'epoch': 11.0}
{'loss': 2.2017, 'learning_rate': 1.6666666666666667e-06, 'epoch': 11.0}
{'eval_loss': 2.066596269607544, 'eval_accuracy@fas.rst.prstc': 0.35470941883767537, 'eval_f1@fas.rst.prstc': 0.06687024803689393, 'eval_precision@fas.rst.prstc': 0.05296552734732754, 'eval_recall@fas.rst.prstc': 0.09981468282252318, 'eval_loss@fas.rst.prstc': 2.066596269607544, 'eval_runtime': 6.3137, 'eval_samples_per_second': 79.034, 'eval_steps_per_second': 2.534, 'epoch': 11.0}
{'train@fas.rst.prstc_loss': 2.1668078899383545, 'train@fas.rst.prstc_accuracy@fas.rst.prstc': 0.33439024390243904, 'train@fas.rst.prstc_f1@fas.rst.prstc': 0.056585176755205475, 'train@fas.rst.prstc_precision@fas.rst.prstc': 0.04383528708205615, 'train@fas.rst.prstc_recall@fas.rst.prstc': 0.08926500005348566, 'train@fas.rst.prstc_loss@fas.rst.prstc': 2.1668076515197754, 'train@fas.rst.prstc_runtime': 49.1903, 'train@fas.rst.prstc_samples_per_second': 83.35, 'train@fas.rst.prstc_steps_per_second': 2.622, 'epoch': 12.0}
{'loss': 2.1928, 'learning_rate': 0.0, 'epoch': 12.0}
{'eval_loss': 2.067131996154785, 'eval_accuracy@fas.rst.prstc': 0.3527054108216433, 'eval_f1@fas.rst.prstc': 0.06628524302870985, 'eval_precision@fas.rst.prstc': 0.05205845681676498, 'eval_recall@fas.rst.prstc': 0.09920171062009979, 'eval_loss@fas.rst.prstc': 2.067131996154785, 'eval_runtime': 6.3212, 'eval_samples_per_second': 78.941, 'eval_steps_per_second': 2.531, 'epoch': 12.0}
{'train_runtime': 1904.704, 'train_samples_per_second': 25.831, 'train_steps_per_second': 0.813, 'train_loss': 2.3380410049004765, 'epoch': 12.0}
***** train_log metrics *****
  epoch                    =       12.0
  train_loss               =      2.338
  train_runtime            = 0:31:44.70
  train_samples_per_second =     25.831
  train_steps_per_second   =      0.813
{'train@tur.pdtb.tdb_loss': 2.6426100730895996, 'train@tur.pdtb.tdb_accuracy@tur.pdtb.tdb': 0.25050999592003265, 'train@tur.pdtb.tdb_f1@tur.pdtb.tdb': 0.01741967515426626, 'train@tur.pdtb.tdb_precision@tur.pdtb.tdb': 0.010891738953044898, 'train@tur.pdtb.tdb_recall@tur.pdtb.tdb': 0.043478260869565216, 'train@tur.pdtb.tdb_loss@tur.pdtb.tdb': 2.6426098346710205, 'train@tur.pdtb.tdb_runtime': 30.0797, 'train@tur.pdtb.tdb_samples_per_second': 81.483, 'train@tur.pdtb.tdb_steps_per_second': 2.56, 'epoch': 1.0}
{'loss': 3.1189, 'learning_rate': 1.8333333333333333e-05, 'epoch': 1.0}
{'eval_loss': 2.588257312774658, 'eval_accuracy@tur.pdtb.tdb': 0.266025641025641, 'eval_f1@tur.pdtb.tdb': 0.019102416570771, 'eval_precision@tur.pdtb.tdb': 0.012092074592074592, 'eval_recall@tur.pdtb.tdb': 0.045454545454545456, 'eval_loss@tur.pdtb.tdb': 2.5882575511932373, 'eval_runtime': 4.1552, 'eval_samples_per_second': 75.087, 'eval_steps_per_second': 2.407, 'epoch': 1.0}
{'train@tur.pdtb.tdb_loss': 2.391779899597168, 'train@tur.pdtb.tdb_accuracy@tur.pdtb.tdb': 0.25091799265605874, 'train@tur.pdtb.tdb_f1@tur.pdtb.tdb': 0.017778842222523714, 'train@tur.pdtb.tdb_precision@tur.pdtb.tdb': 0.054374445430346044, 'train@tur.pdtb.tdb_recall@tur.pdtb.tdb': 0.04365572315882875, 'train@tur.pdtb.tdb_loss@tur.pdtb.tdb': 2.391779899597168, 'train@tur.pdtb.tdb_runtime': 30.0721, 'train@tur.pdtb.tdb_samples_per_second': 81.504, 'train@tur.pdtb.tdb_steps_per_second': 2.561, 'epoch': 2.0}
{'loss': 2.504, 'learning_rate': 1.6666666666666667e-05, 'epoch': 2.0}
{'eval_loss': 2.308462381362915, 'eval_accuracy@tur.pdtb.tdb': 0.266025641025641, 'eval_f1@tur.pdtb.tdb': 0.019102416570771, 'eval_precision@tur.pdtb.tdb': 0.012092074592074592, 'eval_recall@tur.pdtb.tdb': 0.045454545454545456, 'eval_loss@tur.pdtb.tdb': 2.308462381362915, 'eval_runtime': 4.1574, 'eval_samples_per_second': 75.047, 'eval_steps_per_second': 2.405, 'epoch': 2.0}
{'train@tur.pdtb.tdb_loss': 2.315359592437744, 'train@tur.pdtb.tdb_accuracy@tur.pdtb.tdb': 0.25418196654426767, 'train@tur.pdtb.tdb_f1@tur.pdtb.tdb': 0.020975343393919144, 'train@tur.pdtb.tdb_precision@tur.pdtb.tdb': 0.049598024736667815, 'train@tur.pdtb.tdb_recall@tur.pdtb.tdb': 0.04523986428544063, 'train@tur.pdtb.tdb_loss@tur.pdtb.tdb': 2.3153598308563232, 'train@tur.pdtb.tdb_runtime': 30.1089, 'train@tur.pdtb.tdb_samples_per_second': 81.404, 'train@tur.pdtb.tdb_steps_per_second': 2.557, 'epoch': 3.0}
{'loss': 2.38, 'learning_rate': 1.5000000000000002e-05, 'epoch': 3.0}
{'eval_loss': 2.262924909591675, 'eval_accuracy@tur.pdtb.tdb': 0.266025641025641, 'eval_f1@tur.pdtb.tdb': 0.01915089986155976, 'eval_precision@tur.pdtb.tdb': 0.012130955860859398, 'eval_recall@tur.pdtb.tdb': 0.045454545454545456, 'eval_loss@tur.pdtb.tdb': 2.262925148010254, 'eval_runtime': 4.1508, 'eval_samples_per_second': 75.167, 'eval_steps_per_second': 2.409, 'epoch': 3.0}
{'train@tur.pdtb.tdb_loss': 2.256831407546997, 'train@tur.pdtb.tdb_accuracy@tur.pdtb.tdb': 0.28804569563443494, 'train@tur.pdtb.tdb_f1@tur.pdtb.tdb': 0.049945412974198494, 'train@tur.pdtb.tdb_precision@tur.pdtb.tdb': 0.10867267766784591, 'train@tur.pdtb.tdb_recall@tur.pdtb.tdb': 0.06477632009331499, 'train@tur.pdtb.tdb_loss@tur.pdtb.tdb': 2.256831645965576, 'train@tur.pdtb.tdb_runtime': 30.0524, 'train@tur.pdtb.tdb_samples_per_second': 81.558, 'train@tur.pdtb.tdb_steps_per_second': 2.562, 'epoch': 4.0}
{'loss': 2.3162, 'learning_rate': 1.3333333333333333e-05, 'epoch': 4.0}
{'eval_loss': 2.228285312652588, 'eval_accuracy@tur.pdtb.tdb': 0.2916666666666667, 'eval_f1@tur.pdtb.tdb': 0.041832437706318594, 'eval_precision@tur.pdtb.tdb': 0.05180488525558948, 'eval_recall@tur.pdtb.tdb': 0.058861112900801066, 'eval_loss@tur.pdtb.tdb': 2.228285312652588, 'eval_runtime': 4.1656, 'eval_samples_per_second': 74.898, 'eval_steps_per_second': 2.401, 'epoch': 4.0}
{'train@tur.pdtb.tdb_loss': 2.2100536823272705, 'train@tur.pdtb.tdb_accuracy@tur.pdtb.tdb': 0.317421460628315, 'train@tur.pdtb.tdb_f1@tur.pdtb.tdb': 0.08569197239905078, 'train@tur.pdtb.tdb_precision@tur.pdtb.tdb': 0.09651347632675292, 'train@tur.pdtb.tdb_recall@tur.pdtb.tdb': 0.09873643824490053, 'train@tur.pdtb.tdb_loss@tur.pdtb.tdb': 2.2100539207458496, 'train@tur.pdtb.tdb_runtime': 30.1219, 'train@tur.pdtb.tdb_samples_per_second': 81.369, 'train@tur.pdtb.tdb_steps_per_second': 2.556, 'epoch': 5.0}
{'loss': 2.2756, 'learning_rate': 1.1666666666666668e-05, 'epoch': 5.0}
{'eval_loss': 2.195465087890625, 'eval_accuracy@tur.pdtb.tdb': 0.30128205128205127, 'eval_f1@tur.pdtb.tdb': 0.07374216636872542, 'eval_precision@tur.pdtb.tdb': 0.08031127677354093, 'eval_recall@tur.pdtb.tdb': 0.09529585442444038, 'eval_loss@tur.pdtb.tdb': 2.195465087890625, 'eval_runtime': 4.1711, 'eval_samples_per_second': 74.801, 'eval_steps_per_second': 2.397, 'epoch': 5.0}
{'train@tur.pdtb.tdb_loss': 2.1662912368774414, 'train@tur.pdtb.tdb_accuracy@tur.pdtb.tdb': 0.3227254181966544, 'train@tur.pdtb.tdb_f1@tur.pdtb.tdb': 0.08905328041530607, 'train@tur.pdtb.tdb_precision@tur.pdtb.tdb': 0.09252027143530475, 'train@tur.pdtb.tdb_recall@tur.pdtb.tdb': 0.10361031352429542, 'train@tur.pdtb.tdb_loss@tur.pdtb.tdb': 2.1662912368774414, 'train@tur.pdtb.tdb_runtime': 30.0732, 'train@tur.pdtb.tdb_samples_per_second': 81.501, 'train@tur.pdtb.tdb_steps_per_second': 2.56, 'epoch': 6.0}
{'loss': 2.2343, 'learning_rate': 1e-05, 'epoch': 6.0}
{'eval_loss': 2.171719789505005, 'eval_accuracy@tur.pdtb.tdb': 0.30128205128205127, 'eval_f1@tur.pdtb.tdb': 0.0774849281324767, 'eval_precision@tur.pdtb.tdb': 0.0802890175981779, 'eval_recall@tur.pdtb.tdb': 0.10232396687423956, 'eval_loss@tur.pdtb.tdb': 2.171719789505005, 'eval_runtime': 4.1849, 'eval_samples_per_second': 74.553, 'eval_steps_per_second': 2.39, 'epoch': 6.0}
{'train@tur.pdtb.tdb_loss': 2.13980770111084, 'train@tur.pdtb.tdb_accuracy@tur.pdtb.tdb': 0.3312933496532028, 'train@tur.pdtb.tdb_f1@tur.pdtb.tdb': 0.09257792563351216, 'train@tur.pdtb.tdb_precision@tur.pdtb.tdb': 0.08734792055735256, 'train@tur.pdtb.tdb_recall@tur.pdtb.tdb': 0.11029186550820735, 'train@tur.pdtb.tdb_loss@tur.pdtb.tdb': 2.139808177947998, 'train@tur.pdtb.tdb_runtime': 30.0933, 'train@tur.pdtb.tdb_samples_per_second': 81.447, 'train@tur.pdtb.tdb_steps_per_second': 2.559, 'epoch': 7.0}
{'loss': 2.2005, 'learning_rate': 8.333333333333334e-06, 'epoch': 7.0}
{'eval_loss': 2.153933525085449, 'eval_accuracy@tur.pdtb.tdb': 0.30448717948717946, 'eval_f1@tur.pdtb.tdb': 0.08180678544795966, 'eval_precision@tur.pdtb.tdb': 0.07805901829247207, 'eval_recall@tur.pdtb.tdb': 0.1069096194123156, 'eval_loss@tur.pdtb.tdb': 2.15393328666687, 'eval_runtime': 4.1788, 'eval_samples_per_second': 74.662, 'eval_steps_per_second': 2.393, 'epoch': 7.0}
{'train@tur.pdtb.tdb_loss': 2.1134350299835205, 'train@tur.pdtb.tdb_accuracy@tur.pdtb.tdb': 0.3361893104855161, 'train@tur.pdtb.tdb_f1@tur.pdtb.tdb': 0.09610119095894001, 'train@tur.pdtb.tdb_precision@tur.pdtb.tdb': 0.09653251307354369, 'train@tur.pdtb.tdb_recall@tur.pdtb.tdb': 0.11278839534066808, 'train@tur.pdtb.tdb_loss@tur.pdtb.tdb': 2.1134350299835205, 'train@tur.pdtb.tdb_runtime': 30.0852, 'train@tur.pdtb.tdb_samples_per_second': 81.469, 'train@tur.pdtb.tdb_steps_per_second': 2.559, 'epoch': 8.0}
{'loss': 2.1698, 'learning_rate': 6.666666666666667e-06, 'epoch': 8.0}
{'eval_loss': 2.135814666748047, 'eval_accuracy@tur.pdtb.tdb': 0.3141025641025641, 'eval_f1@tur.pdtb.tdb': 0.08532368833525819, 'eval_precision@tur.pdtb.tdb': 0.0837404332657497, 'eval_recall@tur.pdtb.tdb': 0.10943354912325423, 'eval_loss@tur.pdtb.tdb': 2.135814666748047, 'eval_runtime': 4.1519, 'eval_samples_per_second': 75.146, 'eval_steps_per_second': 2.409, 'epoch': 8.0}
{'train@tur.pdtb.tdb_loss': 2.092467784881592, 'train@tur.pdtb.tdb_accuracy@tur.pdtb.tdb': 0.33659730722154224, 'train@tur.pdtb.tdb_f1@tur.pdtb.tdb': 0.09561895784854588, 'train@tur.pdtb.tdb_precision@tur.pdtb.tdb': 0.09886023085542614, 'train@tur.pdtb.tdb_recall@tur.pdtb.tdb': 0.11325072890701954, 'train@tur.pdtb.tdb_loss@tur.pdtb.tdb': 2.09246826171875, 'train@tur.pdtb.tdb_runtime': 30.0548, 'train@tur.pdtb.tdb_samples_per_second': 81.551, 'train@tur.pdtb.tdb_steps_per_second': 2.562, 'epoch': 9.0}
{'loss': 2.1561, 'learning_rate': 5e-06, 'epoch': 9.0}
{'eval_loss': 2.123849391937256, 'eval_accuracy@tur.pdtb.tdb': 0.3301282051282051, 'eval_f1@tur.pdtb.tdb': 0.0931713054813141, 'eval_precision@tur.pdtb.tdb': 0.10422803060787475, 'eval_recall@tur.pdtb.tdb': 0.11631825293890435, 'eval_loss@tur.pdtb.tdb': 2.1238491535186768, 'eval_runtime': 4.1697, 'eval_samples_per_second': 74.825, 'eval_steps_per_second': 2.398, 'epoch': 9.0}
{'train@tur.pdtb.tdb_loss': 2.079871654510498, 'train@tur.pdtb.tdb_accuracy@tur.pdtb.tdb': 0.339453284373725, 'train@tur.pdtb.tdb_f1@tur.pdtb.tdb': 0.09599430142190225, 'train@tur.pdtb.tdb_precision@tur.pdtb.tdb': 0.10494687963877025, 'train@tur.pdtb.tdb_recall@tur.pdtb.tdb': 0.11449391589064213, 'train@tur.pdtb.tdb_loss@tur.pdtb.tdb': 2.079871654510498, 'train@tur.pdtb.tdb_runtime': 30.0697, 'train@tur.pdtb.tdb_samples_per_second': 81.511, 'train@tur.pdtb.tdb_steps_per_second': 2.561, 'epoch': 10.0}
{'loss': 2.1289, 'learning_rate': 3.3333333333333333e-06, 'epoch': 10.0}
{'eval_loss': 2.1158077716827393, 'eval_accuracy@tur.pdtb.tdb': 0.32371794871794873, 'eval_f1@tur.pdtb.tdb': 0.08940504953895094, 'eval_precision@tur.pdtb.tdb': 0.08728402386099508, 'eval_recall@tur.pdtb.tdb': 0.11371872828055617, 'eval_loss@tur.pdtb.tdb': 2.11580753326416, 'eval_runtime': 4.1817, 'eval_samples_per_second': 74.61, 'eval_steps_per_second': 2.391, 'epoch': 10.0}
{'train@tur.pdtb.tdb_loss': 2.074284791946411, 'train@tur.pdtb.tdb_accuracy@tur.pdtb.tdb': 0.3402692778457772, 'train@tur.pdtb.tdb_f1@tur.pdtb.tdb': 0.09632241262116084, 'train@tur.pdtb.tdb_precision@tur.pdtb.tdb': 0.10549301291166877, 'train@tur.pdtb.tdb_recall@tur.pdtb.tdb': 0.11494655800206577, 'train@tur.pdtb.tdb_loss@tur.pdtb.tdb': 2.074284791946411, 'train@tur.pdtb.tdb_runtime': 30.1055, 'train@tur.pdtb.tdb_samples_per_second': 81.414, 'train@tur.pdtb.tdb_steps_per_second': 2.558, 'epoch': 11.0}
{'loss': 2.1277, 'learning_rate': 1.6666666666666667e-06, 'epoch': 11.0}
{'eval_loss': 2.111891984939575, 'eval_accuracy@tur.pdtb.tdb': 0.3173076923076923, 'eval_f1@tur.pdtb.tdb': 0.08758948644762511, 'eval_precision@tur.pdtb.tdb': 0.08474100666614036, 'eval_recall@tur.pdtb.tdb': 0.11183418475887587, 'eval_loss@tur.pdtb.tdb': 2.1118922233581543, 'eval_runtime': 4.1822, 'eval_samples_per_second': 74.602, 'eval_steps_per_second': 2.391, 'epoch': 11.0}
{'train@tur.pdtb.tdb_loss': 2.0715126991271973, 'train@tur.pdtb.tdb_accuracy@tur.pdtb.tdb': 0.339453284373725, 'train@tur.pdtb.tdb_f1@tur.pdtb.tdb': 0.09635900244383987, 'train@tur.pdtb.tdb_precision@tur.pdtb.tdb': 0.09874904963976806, 'train@tur.pdtb.tdb_recall@tur.pdtb.tdb': 0.11503426832959249, 'train@tur.pdtb.tdb_loss@tur.pdtb.tdb': 2.0715126991271973, 'train@tur.pdtb.tdb_runtime': 30.0597, 'train@tur.pdtb.tdb_samples_per_second': 81.538, 'train@tur.pdtb.tdb_steps_per_second': 2.562, 'epoch': 12.0}
{'loss': 2.1091, 'learning_rate': 0.0, 'epoch': 12.0}
{'eval_loss': 2.1101036071777344, 'eval_accuracy@tur.pdtb.tdb': 0.32051282051282054, 'eval_f1@tur.pdtb.tdb': 0.0899849234056145, 'eval_precision@tur.pdtb.tdb': 0.09995171393693347, 'eval_recall@tur.pdtb.tdb': 0.11309681102150212, 'eval_loss@tur.pdtb.tdb': 2.1101036071777344, 'eval_runtime': 4.1619, 'eval_samples_per_second': 74.966, 'eval_steps_per_second': 2.403, 'epoch': 12.0}
{'train_runtime': 1165.4466, 'train_samples_per_second': 25.237, 'train_steps_per_second': 0.793, 'train_loss': 2.3101032471760012, 'epoch': 12.0}
***** train_logger metrics *****
  epoch                    =       12.0
  train_loss               =      2.338
  train_runtime            = 0:31:44.70
  train_samples_per_second =     25.831
  train_steps_per_second   =      0.813
-------------------------------------------------------------------
Lang1:  fra.sdrt.annodis    Lang2:  tur.pdtb.tdb
Saving run to:  runs/full_shot/FullShot=v4_fra.sdrt.annodis_tur.pdtb.tdb_lrfunc=Adafactor_lr=2e-05_plm=bert-base-multilingual-cased
Running with params: BERT_MODEL=bert-base-multilingual-cased lr=2e-05
read 2185 examples
read 528 examples
read 625 examples
read 2451 examples
read 312 examples
read 422 examples
Total prediction labels:  41
BertAdapterModel(
  (shared_parameters): ModuleDict()
  (bert): BertModel(
    (shared_parameters): ModuleDict()
    (invertible_adapters): ModuleDict(
      (fr): NICECouplingBlock(
        (F): Sequential(
          (0): Linear(in_features=384, out_features=192, bias=True)
          (1): Activation_Function_Class(
            (f): NewGELUActivation()
          )
          (2): Linear(in_features=192, out_features=384, bias=True)
        )
        (G): Sequential(
          (0): Linear(in_features=384, out_features=192, bias=True)
          (1): Activation_Function_Class(
            (f): NewGELUActivation()
          )
          (2): Linear(in_features=192, out_features=384, bias=True)
        )
      )
      (tr): NICECouplingBlock(
        (F): Sequential(
          (0): Linear(in_features=384, out_features=192, bias=True)
          (1): Activation_Function_Class(
            (f): NewGELUActivation()
          )
          (2): Linear(in_features=192, out_features=384, bias=True)
        )
        (G): Sequential(
          (0): Linear(in_features=384, out_features=192, bias=True)
          (1): Activation_Function_Class(
            (f): NewGELUActivation()
          )
          (2): Linear(in_features=192, out_features=384, bias=True)
        )
      )
    )
    (embeddings): BertEmbeddings(
      (word_embeddings): Embedding(119547, 768, padding_idx=0)
      (position_embeddings): Embedding(512, 768)
      (token_type_embeddings): Embedding(2, 768)
      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
      (dropout): Dropout(p=0.1, inplace=False)
    )
    (encoder): BertEncoder(
      (layer): ModuleList(
        (0): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (fr): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (tr): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (1): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (fr): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (tr): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (2): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (fr): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (tr): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (3): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (fr): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (tr): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (4): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (fr): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (tr): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (5): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (fr): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (tr): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (6): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (fr): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (tr): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (7): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (fr): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (tr): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (8): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (fr): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (tr): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (9): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (fr): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (tr): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (10): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (fr): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (tr): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (11): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (fr): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (tr): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
      )
    )
    (pooler): BertPooler(
      (dense): Linear(in_features=768, out_features=768, bias=True)
      (activation): Tanh()
    )
    (prefix_tuning): PrefixTuningPool(
      (prefix_tunings): ModuleDict()
    )
  )
  (heads): ModuleDict(
    (tr): BertStyleMaskedLMHead(
      (0): Linear(in_features=768, out_features=768, bias=True)
      (1): Activation_Function_Class(
        (f): GELUActivation()
      )
      (2): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
      (3): Linear(in_features=768, out_features=119547, bias=True)
    )
    (disrpt): ClassificationHead(
      (0): Dropout(p=0.1, inplace=False)
      (1): Linear(in_features=768, out_features=768, bias=True)
      (2): Activation_Function_Class(
        (f): Tanh()
      )
      (3): Dropout(p=0.1, inplace=False)
      (4): Linear(in_features=768, out_features=41, bias=True)
    )
  )
)
{'train@fra.sdrt.annodis_loss': 2.7515177726745605, 'train@fra.sdrt.annodis_accuracy@fra.sdrt.annodis': 0.23615560640732267, 'train@fra.sdrt.annodis_f1@fra.sdrt.annodis': 0.04918303744315778, 'train@fra.sdrt.annodis_precision@fra.sdrt.annodis': 0.04267965737789167, 'train@fra.sdrt.annodis_recall@fra.sdrt.annodis': 0.06906024733878463, 'train@fra.sdrt.annodis_loss@fra.sdrt.annodis': 2.7515180110931396, 'train@fra.sdrt.annodis_runtime': 26.7263, 'train@fra.sdrt.annodis_samples_per_second': 81.755, 'train@fra.sdrt.annodis_steps_per_second': 2.582, 'epoch': 1.0}
{'loss': 3.2716, 'learning_rate': 1.8333333333333333e-05, 'epoch': 1.0}
{'eval_loss': 2.7538206577301025, 'eval_accuracy@fra.sdrt.annodis': 0.24053030303030304, 'eval_f1@fra.sdrt.annodis': 0.04674506772315714, 'eval_precision@fra.sdrt.annodis': 0.038521457965902416, 'eval_recall@fra.sdrt.annodis': 0.06689191787841839, 'eval_loss@fra.sdrt.annodis': 2.7538204193115234, 'eval_runtime': 6.7664, 'eval_samples_per_second': 78.032, 'eval_steps_per_second': 2.512, 'epoch': 1.0}
{'train@fra.sdrt.annodis_loss': 2.4114267826080322, 'train@fra.sdrt.annodis_accuracy@fra.sdrt.annodis': 0.2773455377574371, 'train@fra.sdrt.annodis_f1@fra.sdrt.annodis': 0.05980570977317887, 'train@fra.sdrt.annodis_precision@fra.sdrt.annodis': 0.05013653496450718, 'train@fra.sdrt.annodis_recall@fra.sdrt.annodis': 0.08190991363392432, 'train@fra.sdrt.annodis_loss@fra.sdrt.annodis': 2.4114267826080322, 'train@fra.sdrt.annodis_runtime': 26.7942, 'train@fra.sdrt.annodis_samples_per_second': 81.547, 'train@fra.sdrt.annodis_steps_per_second': 2.575, 'epoch': 2.0}
{'loss': 2.5647, 'learning_rate': 1.6666666666666667e-05, 'epoch': 2.0}
{'eval_loss': 2.4127745628356934, 'eval_accuracy@fra.sdrt.annodis': 0.25757575757575757, 'eval_f1@fra.sdrt.annodis': 0.05184377870800024, 'eval_precision@fra.sdrt.annodis': 0.04198101093721675, 'eval_recall@fra.sdrt.annodis': 0.07248820298318395, 'eval_loss@fra.sdrt.annodis': 2.4127743244171143, 'eval_runtime': 6.7663, 'eval_samples_per_second': 78.034, 'eval_steps_per_second': 2.512, 'epoch': 2.0}
{'train@fra.sdrt.annodis_loss': 2.3222265243530273, 'train@fra.sdrt.annodis_accuracy@fra.sdrt.annodis': 0.2778032036613272, 'train@fra.sdrt.annodis_f1@fra.sdrt.annodis': 0.058384097824234406, 'train@fra.sdrt.annodis_precision@fra.sdrt.annodis': 0.056752432335395966, 'train@fra.sdrt.annodis_recall@fra.sdrt.annodis': 0.08044253374424441, 'train@fra.sdrt.annodis_loss@fra.sdrt.annodis': 2.3222262859344482, 'train@fra.sdrt.annodis_runtime': 26.7973, 'train@fra.sdrt.annodis_samples_per_second': 81.538, 'train@fra.sdrt.annodis_steps_per_second': 2.575, 'epoch': 3.0}
{'loss': 2.3939, 'learning_rate': 1.5000000000000002e-05, 'epoch': 3.0}
{'eval_loss': 2.3299248218536377, 'eval_accuracy@fra.sdrt.annodis': 0.2556818181818182, 'eval_f1@fra.sdrt.annodis': 0.051286235593854534, 'eval_precision@fra.sdrt.annodis': 0.04356057134064064, 'eval_recall@fra.sdrt.annodis': 0.07154385260996546, 'eval_loss@fra.sdrt.annodis': 2.329925298690796, 'eval_runtime': 6.864, 'eval_samples_per_second': 76.923, 'eval_steps_per_second': 2.477, 'epoch': 3.0}
{'train@fra.sdrt.annodis_loss': 2.2630672454833984, 'train@fra.sdrt.annodis_accuracy@fra.sdrt.annodis': 0.29016018306636154, 'train@fra.sdrt.annodis_f1@fra.sdrt.annodis': 0.06372721607741477, 'train@fra.sdrt.annodis_precision@fra.sdrt.annodis': 0.07527301075740908, 'train@fra.sdrt.annodis_recall@fra.sdrt.annodis': 0.084967226718057, 'train@fra.sdrt.annodis_loss@fra.sdrt.annodis': 2.2630672454833984, 'train@fra.sdrt.annodis_runtime': 26.8114, 'train@fra.sdrt.annodis_samples_per_second': 81.495, 'train@fra.sdrt.annodis_steps_per_second': 2.574, 'epoch': 4.0}
{'loss': 2.3328, 'learning_rate': 1.3333333333333333e-05, 'epoch': 4.0}
{'eval_loss': 2.276766300201416, 'eval_accuracy@fra.sdrt.annodis': 0.2708333333333333, 'eval_f1@fra.sdrt.annodis': 0.05529326640437751, 'eval_precision@fra.sdrt.annodis': 0.04743135095825658, 'eval_recall@fra.sdrt.annodis': 0.07614138887936049, 'eval_loss@fra.sdrt.annodis': 2.276766538619995, 'eval_runtime': 6.749, 'eval_samples_per_second': 78.234, 'eval_steps_per_second': 2.519, 'epoch': 4.0}
{'train@fra.sdrt.annodis_loss': 2.2146029472351074, 'train@fra.sdrt.annodis_accuracy@fra.sdrt.annodis': 0.33043478260869563, 'train@fra.sdrt.annodis_f1@fra.sdrt.annodis': 0.07266762887166903, 'train@fra.sdrt.annodis_precision@fra.sdrt.annodis': 0.07303211192100081, 'train@fra.sdrt.annodis_recall@fra.sdrt.annodis': 0.10047698186009144, 'train@fra.sdrt.annodis_loss@fra.sdrt.annodis': 2.2146027088165283, 'train@fra.sdrt.annodis_runtime': 26.8109, 'train@fra.sdrt.annodis_samples_per_second': 81.497, 'train@fra.sdrt.annodis_steps_per_second': 2.574, 'epoch': 5.0}
{'loss': 2.2752, 'learning_rate': 1.1666666666666668e-05, 'epoch': 5.0}
{'eval_loss': 2.2356817722320557, 'eval_accuracy@fra.sdrt.annodis': 0.2935606060606061, 'eval_f1@fra.sdrt.annodis': 0.06221440354608638, 'eval_precision@fra.sdrt.annodis': 0.04868728860531137, 'eval_recall@fra.sdrt.annodis': 0.08618812651349861, 'eval_loss@fra.sdrt.annodis': 2.2356820106506348, 'eval_runtime': 6.7885, 'eval_samples_per_second': 77.779, 'eval_steps_per_second': 2.504, 'epoch': 5.0}
{'train@fra.sdrt.annodis_loss': 2.1690356731414795, 'train@fra.sdrt.annodis_accuracy@fra.sdrt.annodis': 0.36292906178489703, 'train@fra.sdrt.annodis_f1@fra.sdrt.annodis': 0.10881051500302766, 'train@fra.sdrt.annodis_precision@fra.sdrt.annodis': 0.1334272180858146, 'train@fra.sdrt.annodis_recall@fra.sdrt.annodis': 0.12432295871881156, 'train@fra.sdrt.annodis_loss@fra.sdrt.annodis': 2.1690359115600586, 'train@fra.sdrt.annodis_runtime': 26.8511, 'train@fra.sdrt.annodis_samples_per_second': 81.375, 'train@fra.sdrt.annodis_steps_per_second': 2.57, 'epoch': 6.0}
{'loss': 2.2278, 'learning_rate': 1e-05, 'epoch': 6.0}
{'eval_loss': 2.1956288814544678, 'eval_accuracy@fra.sdrt.annodis': 0.3181818181818182, 'eval_f1@fra.sdrt.annodis': 0.0777926378249571, 'eval_precision@fra.sdrt.annodis': 0.13559395364982876, 'eval_recall@fra.sdrt.annodis': 0.09856420776800487, 'eval_loss@fra.sdrt.annodis': 2.1956286430358887, 'eval_runtime': 6.7853, 'eval_samples_per_second': 77.816, 'eval_steps_per_second': 2.505, 'epoch': 6.0}
{'train@fra.sdrt.annodis_loss': 2.126405954360962, 'train@fra.sdrt.annodis_accuracy@fra.sdrt.annodis': 0.3871853546910755, 'train@fra.sdrt.annodis_f1@fra.sdrt.annodis': 0.123387261338827, 'train@fra.sdrt.annodis_precision@fra.sdrt.annodis': 0.12734623536515974, 'train@fra.sdrt.annodis_recall@fra.sdrt.annodis': 0.1413251134211408, 'train@fra.sdrt.annodis_loss@fra.sdrt.annodis': 2.126405954360962, 'train@fra.sdrt.annodis_runtime': 26.8721, 'train@fra.sdrt.annodis_samples_per_second': 81.311, 'train@fra.sdrt.annodis_steps_per_second': 2.568, 'epoch': 7.0}
{'loss': 2.1859, 'learning_rate': 8.333333333333334e-06, 'epoch': 7.0}
{'eval_loss': 2.1604220867156982, 'eval_accuracy@fra.sdrt.annodis': 0.32386363636363635, 'eval_f1@fra.sdrt.annodis': 0.08539910678359947, 'eval_precision@fra.sdrt.annodis': 0.11889213290128985, 'eval_recall@fra.sdrt.annodis': 0.10364980667295913, 'eval_loss@fra.sdrt.annodis': 2.1604220867156982, 'eval_runtime': 6.7795, 'eval_samples_per_second': 77.882, 'eval_steps_per_second': 2.508, 'epoch': 7.0}
{'train@fra.sdrt.annodis_loss': 2.089174747467041, 'train@fra.sdrt.annodis_accuracy@fra.sdrt.annodis': 0.41098398169336386, 'train@fra.sdrt.annodis_f1@fra.sdrt.annodis': 0.1336074405633525, 'train@fra.sdrt.annodis_precision@fra.sdrt.annodis': 0.12935253847815073, 'train@fra.sdrt.annodis_recall@fra.sdrt.annodis': 0.15805763999933012, 'train@fra.sdrt.annodis_loss@fra.sdrt.annodis': 2.089174747467041, 'train@fra.sdrt.annodis_runtime': 26.8418, 'train@fra.sdrt.annodis_samples_per_second': 81.403, 'train@fra.sdrt.annodis_steps_per_second': 2.571, 'epoch': 8.0}
{'loss': 2.1522, 'learning_rate': 6.666666666666667e-06, 'epoch': 8.0}
{'eval_loss': 2.1294822692871094, 'eval_accuracy@fra.sdrt.annodis': 0.3465909090909091, 'eval_f1@fra.sdrt.annodis': 0.10083129444026086, 'eval_precision@fra.sdrt.annodis': 0.10869003530517851, 'eval_recall@fra.sdrt.annodis': 0.11894772435216433, 'eval_loss@fra.sdrt.annodis': 2.1294822692871094, 'eval_runtime': 6.7666, 'eval_samples_per_second': 78.03, 'eval_steps_per_second': 2.512, 'epoch': 8.0}
{'train@fra.sdrt.annodis_loss': 2.0605571269989014, 'train@fra.sdrt.annodis_accuracy@fra.sdrt.annodis': 0.41327231121281466, 'train@fra.sdrt.annodis_f1@fra.sdrt.annodis': 0.132385927970886, 'train@fra.sdrt.annodis_precision@fra.sdrt.annodis': 0.1273867030267909, 'train@fra.sdrt.annodis_recall@fra.sdrt.annodis': 0.16115325742187758, 'train@fra.sdrt.annodis_loss@fra.sdrt.annodis': 2.0605571269989014, 'train@fra.sdrt.annodis_runtime': 26.8322, 'train@fra.sdrt.annodis_samples_per_second': 81.432, 'train@fra.sdrt.annodis_steps_per_second': 2.572, 'epoch': 9.0}
{'loss': 2.1174, 'learning_rate': 5e-06, 'epoch': 9.0}
{'eval_loss': 2.104982852935791, 'eval_accuracy@fra.sdrt.annodis': 0.35984848484848486, 'eval_f1@fra.sdrt.annodis': 0.10308310088782606, 'eval_precision@fra.sdrt.annodis': 0.10946350789655633, 'eval_recall@fra.sdrt.annodis': 0.12493782452148208, 'eval_loss@fra.sdrt.annodis': 2.104982852935791, 'eval_runtime': 6.7601, 'eval_samples_per_second': 78.105, 'eval_steps_per_second': 2.515, 'epoch': 9.0}
{'train@fra.sdrt.annodis_loss': 2.039377212524414, 'train@fra.sdrt.annodis_accuracy@fra.sdrt.annodis': 0.41922196796338673, 'train@fra.sdrt.annodis_f1@fra.sdrt.annodis': 0.13468822233948027, 'train@fra.sdrt.annodis_precision@fra.sdrt.annodis': 0.1262987383856778, 'train@fra.sdrt.annodis_recall@fra.sdrt.annodis': 0.16432402409423258, 'train@fra.sdrt.annodis_loss@fra.sdrt.annodis': 2.039377212524414, 'train@fra.sdrt.annodis_runtime': 26.86, 'train@fra.sdrt.annodis_samples_per_second': 81.348, 'train@fra.sdrt.annodis_steps_per_second': 2.569, 'epoch': 10.0}
{'loss': 2.092, 'learning_rate': 3.3333333333333333e-06, 'epoch': 10.0}
{'eval_loss': 2.0877599716186523, 'eval_accuracy@fra.sdrt.annodis': 0.36553030303030304, 'eval_f1@fra.sdrt.annodis': 0.10645331073019726, 'eval_precision@fra.sdrt.annodis': 0.10418283928408181, 'eval_recall@fra.sdrt.annodis': 0.12846541197442712, 'eval_loss@fra.sdrt.annodis': 2.0877602100372314, 'eval_runtime': 6.7816, 'eval_samples_per_second': 77.857, 'eval_steps_per_second': 2.507, 'epoch': 10.0}
{'train@fra.sdrt.annodis_loss': 2.026728630065918, 'train@fra.sdrt.annodis_accuracy@fra.sdrt.annodis': 0.42013729977116704, 'train@fra.sdrt.annodis_f1@fra.sdrt.annodis': 0.1346694961523323, 'train@fra.sdrt.annodis_precision@fra.sdrt.annodis': 0.12516957630161818, 'train@fra.sdrt.annodis_recall@fra.sdrt.annodis': 0.16517904644315737, 'train@fra.sdrt.annodis_loss@fra.sdrt.annodis': 2.026728868484497, 'train@fra.sdrt.annodis_runtime': 26.9041, 'train@fra.sdrt.annodis_samples_per_second': 81.215, 'train@fra.sdrt.annodis_steps_per_second': 2.565, 'epoch': 11.0}
{'loss': 2.0703, 'learning_rate': 1.6666666666666667e-06, 'epoch': 11.0}
{'eval_loss': 2.0765254497528076, 'eval_accuracy@fra.sdrt.annodis': 0.375, 'eval_f1@fra.sdrt.annodis': 0.11102598313893355, 'eval_precision@fra.sdrt.annodis': 0.10710249922344851, 'eval_recall@fra.sdrt.annodis': 0.13286476076988463, 'eval_loss@fra.sdrt.annodis': 2.0765252113342285, 'eval_runtime': 6.7658, 'eval_samples_per_second': 78.04, 'eval_steps_per_second': 2.513, 'epoch': 11.0}
{'train@fra.sdrt.annodis_loss': 2.0223443508148193, 'train@fra.sdrt.annodis_accuracy@fra.sdrt.annodis': 0.42013729977116704, 'train@fra.sdrt.annodis_f1@fra.sdrt.annodis': 0.13492323181599108, 'train@fra.sdrt.annodis_precision@fra.sdrt.annodis': 0.12407147288522744, 'train@fra.sdrt.annodis_recall@fra.sdrt.annodis': 0.1654465113066588, 'train@fra.sdrt.annodis_loss@fra.sdrt.annodis': 2.0223443508148193, 'train@fra.sdrt.annodis_runtime': 26.8805, 'train@fra.sdrt.annodis_samples_per_second': 81.286, 'train@fra.sdrt.annodis_steps_per_second': 2.567, 'epoch': 12.0}
{'loss': 2.0691, 'learning_rate': 0.0, 'epoch': 12.0}
{'eval_loss': 2.073300838470459, 'eval_accuracy@fra.sdrt.annodis': 0.3731060606060606, 'eval_f1@fra.sdrt.annodis': 0.11062181339878713, 'eval_precision@fra.sdrt.annodis': 0.10674338727526773, 'eval_recall@fra.sdrt.annodis': 0.1323455499703, 'eval_loss@fra.sdrt.annodis': 2.073300361633301, 'eval_runtime': 6.7995, 'eval_samples_per_second': 77.653, 'eval_steps_per_second': 2.5, 'epoch': 12.0}
{'train_runtime': 1074.4768, 'train_samples_per_second': 24.403, 'train_steps_per_second': 0.771, 'train_loss': 2.3127450067639925, 'epoch': 12.0}
***** train_log metrics *****
  epoch                    =       12.0
  train_loss               =     2.3127
  train_runtime            = 0:17:54.47
  train_samples_per_second =     24.403
  train_steps_per_second   =      0.771
{'train@tur.pdtb.tdb_loss': 2.623657464981079, 'train@tur.pdtb.tdb_accuracy@tur.pdtb.tdb': 0.25050999592003265, 'train@tur.pdtb.tdb_f1@tur.pdtb.tdb': 0.01741967515426626, 'train@tur.pdtb.tdb_precision@tur.pdtb.tdb': 0.010891738953044898, 'train@tur.pdtb.tdb_recall@tur.pdtb.tdb': 0.043478260869565216, 'train@tur.pdtb.tdb_loss@tur.pdtb.tdb': 2.6236572265625, 'train@tur.pdtb.tdb_runtime': 30.0818, 'train@tur.pdtb.tdb_samples_per_second': 81.478, 'train@tur.pdtb.tdb_steps_per_second': 2.56, 'epoch': 1.0}
{'loss': 3.0627, 'learning_rate': 1.8333333333333333e-05, 'epoch': 1.0}
{'eval_loss': 2.563278913497925, 'eval_accuracy@tur.pdtb.tdb': 0.266025641025641, 'eval_f1@tur.pdtb.tdb': 0.019102416570771, 'eval_precision@tur.pdtb.tdb': 0.012092074592074592, 'eval_recall@tur.pdtb.tdb': 0.045454545454545456, 'eval_loss@tur.pdtb.tdb': 2.563278913497925, 'eval_runtime': 4.169, 'eval_samples_per_second': 74.838, 'eval_steps_per_second': 2.399, 'epoch': 1.0}
{'train@tur.pdtb.tdb_loss': 2.3834168910980225, 'train@tur.pdtb.tdb_accuracy@tur.pdtb.tdb': 0.2529579763361893, 'train@tur.pdtb.tdb_f1@tur.pdtb.tdb': 0.01951243573244216, 'train@tur.pdtb.tdb_precision@tur.pdtb.tdb': 0.05442358771413801, 'train@tur.pdtb.tdb_recall@tur.pdtb.tdb': 0.04452528837622006, 'train@tur.pdtb.tdb_loss@tur.pdtb.tdb': 2.3834164142608643, 'train@tur.pdtb.tdb_runtime': 30.1184, 'train@tur.pdtb.tdb_samples_per_second': 81.379, 'train@tur.pdtb.tdb_steps_per_second': 2.557, 'epoch': 2.0}
{'loss': 2.4973, 'learning_rate': 1.6666666666666667e-05, 'epoch': 2.0}
{'eval_loss': 2.318514585494995, 'eval_accuracy@tur.pdtb.tdb': 0.2692307692307692, 'eval_f1@tur.pdtb.tdb': 0.021724882411905313, 'eval_precision@tur.pdtb.tdb': 0.03489736070381232, 'eval_recall@tur.pdtb.tdb': 0.04679144385026737, 'eval_loss@tur.pdtb.tdb': 2.318514823913574, 'eval_runtime': 4.1609, 'eval_samples_per_second': 74.983, 'eval_steps_per_second': 2.403, 'epoch': 2.0}
{'train@tur.pdtb.tdb_loss': 2.3163912296295166, 'train@tur.pdtb.tdb_accuracy@tur.pdtb.tdb': 0.2672378620971032, 'train@tur.pdtb.tdb_f1@tur.pdtb.tdb': 0.030604797227787108, 'train@tur.pdtb.tdb_precision@tur.pdtb.tdb': 0.038127191824729435, 'train@tur.pdtb.tdb_recall@tur.pdtb.tdb': 0.05179942714941804, 'train@tur.pdtb.tdb_loss@tur.pdtb.tdb': 2.3163909912109375, 'train@tur.pdtb.tdb_runtime': 30.1187, 'train@tur.pdtb.tdb_samples_per_second': 81.378, 'train@tur.pdtb.tdb_steps_per_second': 2.557, 'epoch': 3.0}
{'loss': 2.3845, 'learning_rate': 1.5000000000000002e-05, 'epoch': 3.0}
{'eval_loss': 2.276956796646118, 'eval_accuracy@tur.pdtb.tdb': 0.2692307692307692, 'eval_f1@tur.pdtb.tdb': 0.025796810670969176, 'eval_precision@tur.pdtb.tdb': 0.03658260344306856, 'eval_recall@tur.pdtb.tdb': 0.04822140612360315, 'eval_loss@tur.pdtb.tdb': 2.276956796646118, 'eval_runtime': 4.1899, 'eval_samples_per_second': 74.465, 'eval_steps_per_second': 2.387, 'epoch': 3.0}
{'train@tur.pdtb.tdb_loss': 2.2463607788085938, 'train@tur.pdtb.tdb_accuracy@tur.pdtb.tdb': 0.29661362709098327, 'train@tur.pdtb.tdb_f1@tur.pdtb.tdb': 0.04537874552109113, 'train@tur.pdtb.tdb_precision@tur.pdtb.tdb': 0.04278206123321248, 'train@tur.pdtb.tdb_recall@tur.pdtb.tdb': 0.06689515562633494, 'train@tur.pdtb.tdb_loss@tur.pdtb.tdb': 2.2463605403900146, 'train@tur.pdtb.tdb_runtime': 30.0905, 'train@tur.pdtb.tdb_samples_per_second': 81.454, 'train@tur.pdtb.tdb_steps_per_second': 2.559, 'epoch': 4.0}
{'loss': 2.318, 'learning_rate': 1.3333333333333333e-05, 'epoch': 4.0}
{'eval_loss': 2.234806537628174, 'eval_accuracy@tur.pdtb.tdb': 0.27564102564102566, 'eval_f1@tur.pdtb.tdb': 0.031925328718603936, 'eval_precision@tur.pdtb.tdb': 0.03442793505656175, 'eval_recall@tur.pdtb.tdb': 0.05247370945457409, 'eval_loss@tur.pdtb.tdb': 2.234806537628174, 'eval_runtime': 4.2057, 'eval_samples_per_second': 74.185, 'eval_steps_per_second': 2.378, 'epoch': 4.0}
{'train@tur.pdtb.tdb_loss': 2.1898584365844727, 'train@tur.pdtb.tdb_accuracy@tur.pdtb.tdb': 0.3117095063239494, 'train@tur.pdtb.tdb_f1@tur.pdtb.tdb': 0.07140436387895892, 'train@tur.pdtb.tdb_precision@tur.pdtb.tdb': 0.12003921439068885, 'train@tur.pdtb.tdb_recall@tur.pdtb.tdb': 0.08496877421509177, 'train@tur.pdtb.tdb_loss@tur.pdtb.tdb': 2.1898584365844727, 'train@tur.pdtb.tdb_runtime': 30.0266, 'train@tur.pdtb.tdb_samples_per_second': 81.628, 'train@tur.pdtb.tdb_steps_per_second': 2.564, 'epoch': 5.0}
{'loss': 2.2694, 'learning_rate': 1.1666666666666668e-05, 'epoch': 5.0}
{'eval_loss': 2.193495988845825, 'eval_accuracy@tur.pdtb.tdb': 0.27564102564102566, 'eval_f1@tur.pdtb.tdb': 0.04239136385142665, 'eval_precision@tur.pdtb.tdb': 0.047715053763440866, 'eval_recall@tur.pdtb.tdb': 0.05984544237556286, 'eval_loss@tur.pdtb.tdb': 2.193495750427246, 'eval_runtime': 4.1575, 'eval_samples_per_second': 75.046, 'eval_steps_per_second': 2.405, 'epoch': 5.0}
{'train@tur.pdtb.tdb_loss': 2.140786647796631, 'train@tur.pdtb.tdb_accuracy@tur.pdtb.tdb': 0.32558139534883723, 'train@tur.pdtb.tdb_f1@tur.pdtb.tdb': 0.08451442730396627, 'train@tur.pdtb.tdb_precision@tur.pdtb.tdb': 0.10103302680163562, 'train@tur.pdtb.tdb_recall@tur.pdtb.tdb': 0.10016351843731963, 'train@tur.pdtb.tdb_loss@tur.pdtb.tdb': 2.140786647796631, 'train@tur.pdtb.tdb_runtime': 30.0492, 'train@tur.pdtb.tdb_samples_per_second': 81.566, 'train@tur.pdtb.tdb_steps_per_second': 2.562, 'epoch': 6.0}
{'loss': 2.2192, 'learning_rate': 1e-05, 'epoch': 6.0}
{'eval_loss': 2.161696434020996, 'eval_accuracy@tur.pdtb.tdb': 0.2948717948717949, 'eval_f1@tur.pdtb.tdb': 0.07475183499135594, 'eval_precision@tur.pdtb.tdb': 0.0752399666398842, 'eval_recall@tur.pdtb.tdb': 0.09957589794970007, 'eval_loss@tur.pdtb.tdb': 2.161696434020996, 'eval_runtime': 4.2499, 'eval_samples_per_second': 73.414, 'eval_steps_per_second': 2.353, 'epoch': 6.0}
{'train@tur.pdtb.tdb_loss': 2.1030828952789307, 'train@tur.pdtb.tdb_accuracy@tur.pdtb.tdb': 0.3361893104855161, 'train@tur.pdtb.tdb_f1@tur.pdtb.tdb': 0.09357158175744137, 'train@tur.pdtb.tdb_precision@tur.pdtb.tdb': 0.10211707513851317, 'train@tur.pdtb.tdb_recall@tur.pdtb.tdb': 0.11022735736998113, 'train@tur.pdtb.tdb_loss@tur.pdtb.tdb': 2.1030826568603516, 'train@tur.pdtb.tdb_runtime': 30.0677, 'train@tur.pdtb.tdb_samples_per_second': 81.516, 'train@tur.pdtb.tdb_steps_per_second': 2.561, 'epoch': 7.0}
{'loss': 2.1777, 'learning_rate': 8.333333333333334e-06, 'epoch': 7.0}
{'eval_loss': 2.135376214981079, 'eval_accuracy@tur.pdtb.tdb': 0.30448717948717946, 'eval_f1@tur.pdtb.tdb': 0.08337747363395169, 'eval_precision@tur.pdtb.tdb': 0.07966528457794833, 'eval_recall@tur.pdtb.tdb': 0.10648659117598816, 'eval_loss@tur.pdtb.tdb': 2.135376214981079, 'eval_runtime': 4.17, 'eval_samples_per_second': 74.82, 'eval_steps_per_second': 2.398, 'epoch': 7.0}
{'train@tur.pdtb.tdb_loss': 2.077676296234131, 'train@tur.pdtb.tdb_accuracy@tur.pdtb.tdb': 0.3443492452060384, 'train@tur.pdtb.tdb_f1@tur.pdtb.tdb': 0.100002141869827, 'train@tur.pdtb.tdb_precision@tur.pdtb.tdb': 0.11115415837345956, 'train@tur.pdtb.tdb_recall@tur.pdtb.tdb': 0.11588131239522534, 'train@tur.pdtb.tdb_loss@tur.pdtb.tdb': 2.0776760578155518, 'train@tur.pdtb.tdb_runtime': 30.0337, 'train@tur.pdtb.tdb_samples_per_second': 81.608, 'train@tur.pdtb.tdb_steps_per_second': 2.564, 'epoch': 8.0}
{'loss': 2.1496, 'learning_rate': 6.666666666666667e-06, 'epoch': 8.0}
{'eval_loss': 2.117771625518799, 'eval_accuracy@tur.pdtb.tdb': 0.3301282051282051, 'eval_f1@tur.pdtb.tdb': 0.09885861021773254, 'eval_precision@tur.pdtb.tdb': 0.14342452627416621, 'eval_recall@tur.pdtb.tdb': 0.11750543634239775, 'eval_loss@tur.pdtb.tdb': 2.117771625518799, 'eval_runtime': 4.1624, 'eval_samples_per_second': 74.957, 'eval_steps_per_second': 2.402, 'epoch': 8.0}
{'train@tur.pdtb.tdb_loss': 2.0528948307037354, 'train@tur.pdtb.tdb_accuracy@tur.pdtb.tdb': 0.34516523867809057, 'train@tur.pdtb.tdb_f1@tur.pdtb.tdb': 0.09965611750059866, 'train@tur.pdtb.tdb_precision@tur.pdtb.tdb': 0.11392764785511517, 'train@tur.pdtb.tdb_recall@tur.pdtb.tdb': 0.11663883707281265, 'train@tur.pdtb.tdb_loss@tur.pdtb.tdb': 2.0528948307037354, 'train@tur.pdtb.tdb_runtime': 30.0914, 'train@tur.pdtb.tdb_samples_per_second': 81.452, 'train@tur.pdtb.tdb_steps_per_second': 2.559, 'epoch': 9.0}
{'loss': 2.1204, 'learning_rate': 5e-06, 'epoch': 9.0}
{'eval_loss': 2.098980665206909, 'eval_accuracy@tur.pdtb.tdb': 0.3333333333333333, 'eval_f1@tur.pdtb.tdb': 0.09873738629070478, 'eval_precision@tur.pdtb.tdb': 0.11470695228106295, 'eval_recall@tur.pdtb.tdb': 0.11961411907767898, 'eval_loss@tur.pdtb.tdb': 2.0989809036254883, 'eval_runtime': 4.1608, 'eval_samples_per_second': 74.986, 'eval_steps_per_second': 2.403, 'epoch': 9.0}
{'train@tur.pdtb.tdb_loss': 2.037182331085205, 'train@tur.pdtb.tdb_accuracy@tur.pdtb.tdb': 0.3529171766625867, 'train@tur.pdtb.tdb_f1@tur.pdtb.tdb': 0.10701328040724561, 'train@tur.pdtb.tdb_precision@tur.pdtb.tdb': 0.1432213025473817, 'train@tur.pdtb.tdb_recall@tur.pdtb.tdb': 0.12165511443956985, 'train@tur.pdtb.tdb_loss@tur.pdtb.tdb': 2.037182331085205, 'train@tur.pdtb.tdb_runtime': 30.0805, 'train@tur.pdtb.tdb_samples_per_second': 81.481, 'train@tur.pdtb.tdb_steps_per_second': 2.56, 'epoch': 10.0}
{'loss': 2.1018, 'learning_rate': 3.3333333333333333e-06, 'epoch': 10.0}
{'eval_loss': 2.087343215942383, 'eval_accuracy@tur.pdtb.tdb': 0.32371794871794873, 'eval_f1@tur.pdtb.tdb': 0.0959660309480761, 'eval_precision@tur.pdtb.tdb': 0.09987421435073891, 'eval_recall@tur.pdtb.tdb': 0.11639267716027675, 'eval_loss@tur.pdtb.tdb': 2.087343215942383, 'eval_runtime': 4.175, 'eval_samples_per_second': 74.731, 'eval_steps_per_second': 2.395, 'epoch': 10.0}
{'train@tur.pdtb.tdb_loss': 2.0285534858703613, 'train@tur.pdtb.tdb_accuracy@tur.pdtb.tdb': 0.3578131374949, 'train@tur.pdtb.tdb_f1@tur.pdtb.tdb': 0.11028091156579134, 'train@tur.pdtb.tdb_precision@tur.pdtb.tdb': 0.14989702034069632, 'train@tur.pdtb.tdb_recall@tur.pdtb.tdb': 0.12406031908949729, 'train@tur.pdtb.tdb_loss@tur.pdtb.tdb': 2.0285534858703613, 'train@tur.pdtb.tdb_runtime': 30.0764, 'train@tur.pdtb.tdb_samples_per_second': 81.492, 'train@tur.pdtb.tdb_steps_per_second': 2.56, 'epoch': 11.0}
{'loss': 2.0934, 'learning_rate': 1.6666666666666667e-06, 'epoch': 11.0}
{'eval_loss': 2.0820255279541016, 'eval_accuracy@tur.pdtb.tdb': 0.32371794871794873, 'eval_f1@tur.pdtb.tdb': 0.10128328157167826, 'eval_precision@tur.pdtb.tdb': 0.1476193123899546, 'eval_recall@tur.pdtb.tdb': 0.11855228226105834, 'eval_loss@tur.pdtb.tdb': 2.0820255279541016, 'eval_runtime': 4.1658, 'eval_samples_per_second': 74.895, 'eval_steps_per_second': 2.4, 'epoch': 11.0}
{'train@tur.pdtb.tdb_loss': 2.0255110263824463, 'train@tur.pdtb.tdb_accuracy@tur.pdtb.tdb': 0.35862913096695226, 'train@tur.pdtb.tdb_f1@tur.pdtb.tdb': 0.1121180841833885, 'train@tur.pdtb.tdb_precision@tur.pdtb.tdb': 0.14986754479174583, 'train@tur.pdtb.tdb_recall@tur.pdtb.tdb': 0.1258552312463881, 'train@tur.pdtb.tdb_loss@tur.pdtb.tdb': 2.025510787963867, 'train@tur.pdtb.tdb_runtime': 60.2202, 'train@tur.pdtb.tdb_samples_per_second': 40.701, 'train@tur.pdtb.tdb_steps_per_second': 1.279, 'epoch': 12.0}
{'loss': 2.0819, 'learning_rate': 0.0, 'epoch': 12.0}
{'eval_loss': 2.0796852111816406, 'eval_accuracy@tur.pdtb.tdb': 0.3269230769230769, 'eval_f1@tur.pdtb.tdb': 0.10356750835091287, 'eval_precision@tur.pdtb.tdb': 0.15425049444179229, 'eval_recall@tur.pdtb.tdb': 0.1197406363905889, 'eval_loss@tur.pdtb.tdb': 2.0796852111816406, 'eval_runtime': 30.4826, 'eval_samples_per_second': 10.235, 'eval_steps_per_second': 0.328, 'epoch': 12.0}
{'train_runtime': 1221.4658, 'train_samples_per_second': 24.079, 'train_steps_per_second': 0.756, 'train_loss': 2.2896478661211024, 'epoch': 12.0}
***** train_logger metrics *****
  epoch                    =       12.0
  train_loss               =     2.3127
  train_runtime            = 0:17:54.47
  train_samples_per_second =     24.403
  train_steps_per_second   =      0.771
-------------------------------------------------------------------
Lang1:  nld.rst.nldt    Lang2:  tur.pdtb.tdb
Saving run to:  runs/full_shot/FullShot=v4_nld.rst.nldt_tur.pdtb.tdb_lrfunc=Adafactor_lr=2e-05_plm=bert-base-multilingual-cased
Running with params: BERT_MODEL=bert-base-multilingual-cased lr=2e-05
read 1608 examples
read 331 examples
read 326 examples
read 2451 examples
read 312 examples
read 422 examples
Total prediction labels:  56
BertAdapterModel(
  (shared_parameters): ModuleDict()
  (bert): BertModel(
    (shared_parameters): ModuleDict()
    (invertible_adapters): ModuleDict(
      (de): NICECouplingBlock(
        (F): Sequential(
          (0): Linear(in_features=384, out_features=192, bias=True)
          (1): Activation_Function_Class(
            (f): NewGELUActivation()
          )
          (2): Linear(in_features=192, out_features=384, bias=True)
        )
        (G): Sequential(
          (0): Linear(in_features=384, out_features=192, bias=True)
          (1): Activation_Function_Class(
            (f): NewGELUActivation()
          )
          (2): Linear(in_features=192, out_features=384, bias=True)
        )
      )
      (tr): NICECouplingBlock(
        (F): Sequential(
          (0): Linear(in_features=384, out_features=192, bias=True)
          (1): Activation_Function_Class(
            (f): NewGELUActivation()
          )
          (2): Linear(in_features=192, out_features=384, bias=True)
        )
        (G): Sequential(
          (0): Linear(in_features=384, out_features=192, bias=True)
          (1): Activation_Function_Class(
            (f): NewGELUActivation()
          )
          (2): Linear(in_features=192, out_features=384, bias=True)
        )
      )
    )
    (embeddings): BertEmbeddings(
      (word_embeddings): Embedding(119547, 768, padding_idx=0)
      (position_embeddings): Embedding(512, 768)
      (token_type_embeddings): Embedding(2, 768)
      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
      (dropout): Dropout(p=0.1, inplace=False)
    )
    (encoder): BertEncoder(
      (layer): ModuleList(
        (0): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (de): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (tr): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (1): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (de): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (tr): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (2): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (de): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (tr): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (3): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (de): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (tr): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (4): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (de): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (tr): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (5): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (de): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (tr): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (6): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (de): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (tr): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (7): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (de): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (tr): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (8): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (de): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (tr): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (9): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (de): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (tr): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (10): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (de): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (tr): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (11): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (de): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (tr): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
      )
    )
    (pooler): BertPooler(
      (dense): Linear(in_features=768, out_features=768, bias=True)
      (activation): Tanh()
    )
    (prefix_tuning): PrefixTuningPool(
      (prefix_tunings): ModuleDict()
    )
  )
  (heads): ModuleDict(
    (tr): BertStyleMaskedLMHead(
      (0): Linear(in_features=768, out_features=768, bias=True)
      (1): Activation_Function_Class(
        (f): GELUActivation()
      )
      (2): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
      (3): Linear(in_features=768, out_features=119547, bias=True)
    )
    (disrpt): ClassificationHead(
      (0): Dropout(p=0.1, inplace=False)
      (1): Linear(in_features=768, out_features=768, bias=True)
      (2): Activation_Function_Class(
        (f): Tanh()
      )
      (3): Dropout(p=0.1, inplace=False)
      (4): Linear(in_features=768, out_features=56, bias=True)
    )
  )
)
{'train@nld.rst.nldt_loss': 3.4293811321258545, 'train@nld.rst.nldt_accuracy@nld.rst.nldt': 0.26430348258706465, 'train@nld.rst.nldt_f1@nld.rst.nldt': 0.01306566650270536, 'train@nld.rst.nldt_precision@nld.rst.nldt': 0.00825948383084577, 'train@nld.rst.nldt_recall@nld.rst.nldt': 0.03125, 'train@nld.rst.nldt_loss@nld.rst.nldt': 3.4293811321258545, 'train@nld.rst.nldt_runtime': 19.8016, 'train@nld.rst.nldt_samples_per_second': 81.205, 'train@nld.rst.nldt_steps_per_second': 2.576, 'epoch': 1.0}
{'loss': 3.7299, 'learning_rate': 1.8333333333333333e-05, 'epoch': 1.0}
{'eval_loss': 3.3975250720977783, 'eval_accuracy@nld.rst.nldt': 0.27794561933534745, 'eval_f1@nld.rst.nldt': 0.016110673321075214, 'eval_precision@nld.rst.nldt': 0.010294282197605462, 'eval_recall@nld.rst.nldt': 0.037037037037037035, 'eval_loss@nld.rst.nldt': 3.3975255489349365, 'eval_runtime': 4.5253, 'eval_samples_per_second': 73.145, 'eval_steps_per_second': 2.431, 'epoch': 1.0}
{'train@nld.rst.nldt_loss': 3.0180349349975586, 'train@nld.rst.nldt_accuracy@nld.rst.nldt': 0.26430348258706465, 'train@nld.rst.nldt_f1@nld.rst.nldt': 0.01306566650270536, 'train@nld.rst.nldt_precision@nld.rst.nldt': 0.00825948383084577, 'train@nld.rst.nldt_recall@nld.rst.nldt': 0.03125, 'train@nld.rst.nldt_loss@nld.rst.nldt': 3.0180351734161377, 'train@nld.rst.nldt_runtime': 20.0202, 'train@nld.rst.nldt_samples_per_second': 80.319, 'train@nld.rst.nldt_steps_per_second': 2.547, 'epoch': 2.0}
{'loss': 3.2189, 'learning_rate': 1.6666666666666667e-05, 'epoch': 2.0}
{'eval_loss': 2.9518442153930664, 'eval_accuracy@nld.rst.nldt': 0.27794561933534745, 'eval_f1@nld.rst.nldt': 0.016110673321075214, 'eval_precision@nld.rst.nldt': 0.010294282197605462, 'eval_recall@nld.rst.nldt': 0.037037037037037035, 'eval_loss@nld.rst.nldt': 2.9518442153930664, 'eval_runtime': 4.5421, 'eval_samples_per_second': 72.874, 'eval_steps_per_second': 2.422, 'epoch': 2.0}
{'train@nld.rst.nldt_loss': 2.863081932067871, 'train@nld.rst.nldt_accuracy@nld.rst.nldt': 0.2667910447761194, 'train@nld.rst.nldt_f1@nld.rst.nldt': 0.01541086193226866, 'train@nld.rst.nldt_precision@nld.rst.nldt': 0.026157924107142856, 'train@nld.rst.nldt_recall@nld.rst.nldt': 0.032475490196078434, 'train@nld.rst.nldt_loss@nld.rst.nldt': 2.863081932067871, 'train@nld.rst.nldt_runtime': 20.0314, 'train@nld.rst.nldt_samples_per_second': 80.274, 'train@nld.rst.nldt_steps_per_second': 2.546, 'epoch': 3.0}
{'loss': 2.9653, 'learning_rate': 1.5000000000000002e-05, 'epoch': 3.0}
{'eval_loss': 2.801215171813965, 'eval_accuracy@nld.rst.nldt': 0.2809667673716012, 'eval_f1@nld.rst.nldt': 0.019890912289962173, 'eval_precision@nld.rst.nldt': 0.028875379939209724, 'eval_recall@nld.rst.nldt': 0.03909465020576132, 'eval_loss@nld.rst.nldt': 2.801215171813965, 'eval_runtime': 4.5661, 'eval_samples_per_second': 72.49, 'eval_steps_per_second': 2.409, 'epoch': 3.0}
{'train@nld.rst.nldt_loss': 2.794318437576294, 'train@nld.rst.nldt_accuracy@nld.rst.nldt': 0.2736318407960199, 'train@nld.rst.nldt_f1@nld.rst.nldt': 0.020738968713441525, 'train@nld.rst.nldt_precision@nld.rst.nldt': 0.017736098345588237, 'train@nld.rst.nldt_recall@nld.rst.nldt': 0.03700980392156863, 'train@nld.rst.nldt_loss@nld.rst.nldt': 2.794318437576294, 'train@nld.rst.nldt_runtime': 20.0667, 'train@nld.rst.nldt_samples_per_second': 80.133, 'train@nld.rst.nldt_steps_per_second': 2.542, 'epoch': 4.0}
{'loss': 2.8351, 'learning_rate': 1.3333333333333333e-05, 'epoch': 4.0}
{'eval_loss': 2.738968849182129, 'eval_accuracy@nld.rst.nldt': 0.29003021148036257, 'eval_f1@nld.rst.nldt': 0.025722692389359054, 'eval_precision@nld.rst.nldt': 0.02069370958259847, 'eval_recall@nld.rst.nldt': 0.04526748971193416, 'eval_loss@nld.rst.nldt': 2.73896861076355, 'eval_runtime': 4.5529, 'eval_samples_per_second': 72.7, 'eval_steps_per_second': 2.416, 'epoch': 4.0}
{'train@nld.rst.nldt_loss': 2.7521517276763916, 'train@nld.rst.nldt_accuracy@nld.rst.nldt': 0.28233830845771146, 'train@nld.rst.nldt_f1@nld.rst.nldt': 0.025417624333345212, 'train@nld.rst.nldt_precision@nld.rst.nldt': 0.025022185353342066, 'train@nld.rst.nldt_recall@nld.rst.nldt': 0.04164740896358544, 'train@nld.rst.nldt_loss@nld.rst.nldt': 2.7521517276763916, 'train@nld.rst.nldt_runtime': 20.0381, 'train@nld.rst.nldt_samples_per_second': 80.247, 'train@nld.rst.nldt_steps_per_second': 2.545, 'epoch': 5.0}
{'loss': 2.7946, 'learning_rate': 1.1666666666666668e-05, 'epoch': 5.0}
{'eval_loss': 2.701094627380371, 'eval_accuracy@nld.rst.nldt': 0.29607250755287007, 'eval_f1@nld.rst.nldt': 0.030105173912322706, 'eval_precision@nld.rst.nldt': 0.02325302648927244, 'eval_recall@nld.rst.nldt': 0.05103775272857398, 'eval_loss@nld.rst.nldt': 2.70109486579895, 'eval_runtime': 4.6005, 'eval_samples_per_second': 71.949, 'eval_steps_per_second': 2.391, 'epoch': 5.0}
{'train@nld.rst.nldt_loss': 2.7153639793395996, 'train@nld.rst.nldt_accuracy@nld.rst.nldt': 0.2898009950248756, 'train@nld.rst.nldt_f1@nld.rst.nldt': 0.029544213973799124, 'train@nld.rst.nldt_precision@nld.rst.nldt': 0.027409824563000215, 'train@nld.rst.nldt_recall@nld.rst.nldt': 0.0470109710550887, 'train@nld.rst.nldt_loss@nld.rst.nldt': 2.7153642177581787, 'train@nld.rst.nldt_runtime': 20.0146, 'train@nld.rst.nldt_samples_per_second': 80.341, 'train@nld.rst.nldt_steps_per_second': 2.548, 'epoch': 6.0}
{'loss': 2.7525, 'learning_rate': 1e-05, 'epoch': 6.0}
{'eval_loss': 2.668872356414795, 'eval_accuracy@nld.rst.nldt': 0.30513595166163143, 'eval_f1@nld.rst.nldt': 0.034586243226400325, 'eval_precision@nld.rst.nldt': 0.0405324679518228, 'eval_recall@nld.rst.nldt': 0.05686680469289165, 'eval_loss@nld.rst.nldt': 2.668872117996216, 'eval_runtime': 4.5255, 'eval_samples_per_second': 73.141, 'eval_steps_per_second': 2.431, 'epoch': 6.0}
{'train@nld.rst.nldt_loss': 2.682830333709717, 'train@nld.rst.nldt_accuracy@nld.rst.nldt': 0.2978855721393035, 'train@nld.rst.nldt_f1@nld.rst.nldt': 0.03259329343425453, 'train@nld.rst.nldt_precision@nld.rst.nldt': 0.027914072419693528, 'train@nld.rst.nldt_recall@nld.rst.nldt': 0.051632819794584506, 'train@nld.rst.nldt_loss@nld.rst.nldt': 2.682830333709717, 'train@nld.rst.nldt_runtime': 20.0073, 'train@nld.rst.nldt_samples_per_second': 80.371, 'train@nld.rst.nldt_steps_per_second': 2.549, 'epoch': 7.0}
{'loss': 2.7274, 'learning_rate': 8.333333333333334e-06, 'epoch': 7.0}
{'eval_loss': 2.641589403152466, 'eval_accuracy@nld.rst.nldt': 0.32326283987915405, 'eval_f1@nld.rst.nldt': 0.04158115251175714, 'eval_precision@nld.rst.nldt': 0.038572445043628056, 'eval_recall@nld.rst.nldt': 0.06786928405285893, 'eval_loss@nld.rst.nldt': 2.6415889263153076, 'eval_runtime': 4.5344, 'eval_samples_per_second': 72.998, 'eval_steps_per_second': 2.426, 'epoch': 7.0}
{'train@nld.rst.nldt_loss': 2.6595544815063477, 'train@nld.rst.nldt_accuracy@nld.rst.nldt': 0.30597014925373134, 'train@nld.rst.nldt_f1@nld.rst.nldt': 0.034575391029722, 'train@nld.rst.nldt_precision@nld.rst.nldt': 0.02796226182100251, 'train@nld.rst.nldt_recall@nld.rst.nldt': 0.0561968954248366, 'train@nld.rst.nldt_loss@nld.rst.nldt': 2.6595540046691895, 'train@nld.rst.nldt_runtime': 20.7089, 'train@nld.rst.nldt_samples_per_second': 77.648, 'train@nld.rst.nldt_steps_per_second': 2.463, 'epoch': 8.0}
{'loss': 2.7061, 'learning_rate': 6.666666666666667e-06, 'epoch': 8.0}
{'eval_loss': 2.622995138168335, 'eval_accuracy@nld.rst.nldt': 0.32326283987915405, 'eval_f1@nld.rst.nldt': 0.04108606405525173, 'eval_precision@nld.rst.nldt': 0.037431366676649695, 'eval_recall@nld.rst.nldt': 0.06852490862152698, 'eval_loss@nld.rst.nldt': 2.622995376586914, 'eval_runtime': 4.516, 'eval_samples_per_second': 73.295, 'eval_steps_per_second': 2.436, 'epoch': 8.0}
{'train@nld.rst.nldt_loss': 2.6450395584106445, 'train@nld.rst.nldt_accuracy@nld.rst.nldt': 0.30845771144278605, 'train@nld.rst.nldt_f1@nld.rst.nldt': 0.03587640220261714, 'train@nld.rst.nldt_precision@nld.rst.nldt': 0.028371357325848626, 'train@nld.rst.nldt_recall@nld.rst.nldt': 0.05724673202614379, 'train@nld.rst.nldt_loss@nld.rst.nldt': 2.6450395584106445, 'train@nld.rst.nldt_runtime': 20.0306, 'train@nld.rst.nldt_samples_per_second': 80.277, 'train@nld.rst.nldt_steps_per_second': 2.546, 'epoch': 9.0}
{'loss': 2.6781, 'learning_rate': 5e-06, 'epoch': 9.0}
{'eval_loss': 2.6109347343444824, 'eval_accuracy@nld.rst.nldt': 0.32326283987915405, 'eval_f1@nld.rst.nldt': 0.04129533077301629, 'eval_precision@nld.rst.nldt': 0.035967895930301946, 'eval_recall@nld.rst.nldt': 0.06852490862152698, 'eval_loss@nld.rst.nldt': 2.6109347343444824, 'eval_runtime': 4.5301, 'eval_samples_per_second': 73.067, 'eval_steps_per_second': 2.428, 'epoch': 9.0}
{'train@nld.rst.nldt_loss': 2.635141134262085, 'train@nld.rst.nldt_accuracy@nld.rst.nldt': 0.30659203980099503, 'train@nld.rst.nldt_f1@nld.rst.nldt': 0.03526881117168635, 'train@nld.rst.nldt_precision@nld.rst.nldt': 0.028131576178451176, 'train@nld.rst.nldt_recall@nld.rst.nldt': 0.055861928104575166, 'train@nld.rst.nldt_loss@nld.rst.nldt': 2.635141611099243, 'train@nld.rst.nldt_runtime': 19.9827, 'train@nld.rst.nldt_samples_per_second': 80.47, 'train@nld.rst.nldt_steps_per_second': 2.552, 'epoch': 10.0}
{'loss': 2.6644, 'learning_rate': 3.3333333333333333e-06, 'epoch': 10.0}
{'eval_loss': 2.6020667552948, 'eval_accuracy@nld.rst.nldt': 0.32628398791540786, 'eval_f1@nld.rst.nldt': 0.0417255910292122, 'eval_precision@nld.rst.nldt': 0.035661861277213804, 'eval_recall@nld.rst.nldt': 0.06892748511105999, 'eval_loss@nld.rst.nldt': 2.6020667552948, 'eval_runtime': 4.5357, 'eval_samples_per_second': 72.977, 'eval_steps_per_second': 2.425, 'epoch': 10.0}
{'train@nld.rst.nldt_loss': 2.627241373062134, 'train@nld.rst.nldt_accuracy@nld.rst.nldt': 0.31032338308457713, 'train@nld.rst.nldt_f1@nld.rst.nldt': 0.03636114038715802, 'train@nld.rst.nldt_precision@nld.rst.nldt': 0.028671380144663416, 'train@nld.rst.nldt_recall@nld.rst.nldt': 0.05804913632119515, 'train@nld.rst.nldt_loss@nld.rst.nldt': 2.627241373062134, 'train@nld.rst.nldt_runtime': 20.0041, 'train@nld.rst.nldt_samples_per_second': 80.383, 'train@nld.rst.nldt_steps_per_second': 2.549, 'epoch': 11.0}
{'loss': 2.6554, 'learning_rate': 1.6666666666666667e-06, 'epoch': 11.0}
{'eval_loss': 2.595721483230591, 'eval_accuracy@nld.rst.nldt': 0.32326283987915405, 'eval_f1@nld.rst.nldt': 0.04101867386731895, 'eval_precision@nld.rst.nldt': 0.03504720406681191, 'eval_recall@nld.rst.nldt': 0.06852490862152698, 'eval_loss@nld.rst.nldt': 2.5957212448120117, 'eval_runtime': 4.5047, 'eval_samples_per_second': 73.479, 'eval_steps_per_second': 2.442, 'epoch': 11.0}
{'train@nld.rst.nldt_loss': 2.6242904663085938, 'train@nld.rst.nldt_accuracy@nld.rst.nldt': 0.31156716417910446, 'train@nld.rst.nldt_f1@nld.rst.nldt': 0.03667121934480341, 'train@nld.rst.nldt_precision@nld.rst.nldt': 0.028739964332929382, 'train@nld.rst.nldt_recall@nld.rst.nldt': 0.05883636788048553, 'train@nld.rst.nldt_loss@nld.rst.nldt': 2.624290704727173, 'train@nld.rst.nldt_runtime': 19.9893, 'train@nld.rst.nldt_samples_per_second': 80.443, 'train@nld.rst.nldt_steps_per_second': 2.551, 'epoch': 12.0}
{'loss': 2.6523, 'learning_rate': 0.0, 'epoch': 12.0}
{'eval_loss': 2.5938706398010254, 'eval_accuracy@nld.rst.nldt': 0.32326283987915405, 'eval_f1@nld.rst.nldt': 0.04069134970328104, 'eval_precision@nld.rst.nldt': 0.035413365980123866, 'eval_recall@nld.rst.nldt': 0.06852490862152698, 'eval_loss@nld.rst.nldt': 2.5938704013824463, 'eval_runtime': 4.5424, 'eval_samples_per_second': 72.869, 'eval_steps_per_second': 2.422, 'epoch': 12.0}
{'train_runtime': 789.7414, 'train_samples_per_second': 24.433, 'train_steps_per_second': 0.775, 'train_loss': 2.8649907828935612, 'epoch': 12.0}
***** train_log metrics *****
  epoch                    =       12.0
  train_loss               =      2.865
  train_runtime            = 0:13:09.74
  train_samples_per_second =     24.433
  train_steps_per_second   =      0.775
{'train@tur.pdtb.tdb_loss': 2.6459808349609375, 'train@tur.pdtb.tdb_accuracy@tur.pdtb.tdb': 0.25050999592003265, 'train@tur.pdtb.tdb_f1@tur.pdtb.tdb': 0.01741967515426626, 'train@tur.pdtb.tdb_precision@tur.pdtb.tdb': 0.010891738953044898, 'train@tur.pdtb.tdb_recall@tur.pdtb.tdb': 0.043478260869565216, 'train@tur.pdtb.tdb_loss@tur.pdtb.tdb': 2.6459808349609375, 'train@tur.pdtb.tdb_runtime': 30.2057, 'train@tur.pdtb.tdb_samples_per_second': 81.144, 'train@tur.pdtb.tdb_steps_per_second': 2.549, 'epoch': 1.0}
{'loss': 3.265, 'learning_rate': 1.8333333333333333e-05, 'epoch': 1.0}
{'eval_loss': 2.557119369506836, 'eval_accuracy@tur.pdtb.tdb': 0.266025641025641, 'eval_f1@tur.pdtb.tdb': 0.019102416570771, 'eval_precision@tur.pdtb.tdb': 0.012092074592074592, 'eval_recall@tur.pdtb.tdb': 0.045454545454545456, 'eval_loss@tur.pdtb.tdb': 2.557119369506836, 'eval_runtime': 4.3229, 'eval_samples_per_second': 72.174, 'eval_steps_per_second': 2.313, 'epoch': 1.0}
{'train@tur.pdtb.tdb_loss': 2.4422812461853027, 'train@tur.pdtb.tdb_accuracy@tur.pdtb.tdb': 0.25050999592003265, 'train@tur.pdtb.tdb_f1@tur.pdtb.tdb': 0.01741967515426626, 'train@tur.pdtb.tdb_precision@tur.pdtb.tdb': 0.010891738953044898, 'train@tur.pdtb.tdb_recall@tur.pdtb.tdb': 0.043478260869565216, 'train@tur.pdtb.tdb_loss@tur.pdtb.tdb': 2.442281484603882, 'train@tur.pdtb.tdb_runtime': 30.1782, 'train@tur.pdtb.tdb_samples_per_second': 81.218, 'train@tur.pdtb.tdb_steps_per_second': 2.552, 'epoch': 2.0}
{'loss': 2.5429, 'learning_rate': 1.6666666666666667e-05, 'epoch': 2.0}
{'eval_loss': 2.3542871475219727, 'eval_accuracy@tur.pdtb.tdb': 0.266025641025641, 'eval_f1@tur.pdtb.tdb': 0.019102416570771, 'eval_precision@tur.pdtb.tdb': 0.012092074592074592, 'eval_recall@tur.pdtb.tdb': 0.045454545454545456, 'eval_loss@tur.pdtb.tdb': 2.3542866706848145, 'eval_runtime': 4.3089, 'eval_samples_per_second': 72.408, 'eval_steps_per_second': 2.321, 'epoch': 2.0}
{'train@tur.pdtb.tdb_loss': 2.3861865997314453, 'train@tur.pdtb.tdb_accuracy@tur.pdtb.tdb': 0.25050999592003265, 'train@tur.pdtb.tdb_f1@tur.pdtb.tdb': 0.01741967515426626, 'train@tur.pdtb.tdb_precision@tur.pdtb.tdb': 0.010891738953044898, 'train@tur.pdtb.tdb_recall@tur.pdtb.tdb': 0.043478260869565216, 'train@tur.pdtb.tdb_loss@tur.pdtb.tdb': 2.3861865997314453, 'train@tur.pdtb.tdb_runtime': 30.2509, 'train@tur.pdtb.tdb_samples_per_second': 81.022, 'train@tur.pdtb.tdb_steps_per_second': 2.545, 'epoch': 3.0}
{'loss': 2.4405, 'learning_rate': 1.5000000000000002e-05, 'epoch': 3.0}
{'eval_loss': 2.322437286376953, 'eval_accuracy@tur.pdtb.tdb': 0.266025641025641, 'eval_f1@tur.pdtb.tdb': 0.019102416570771, 'eval_precision@tur.pdtb.tdb': 0.012092074592074592, 'eval_recall@tur.pdtb.tdb': 0.045454545454545456, 'eval_loss@tur.pdtb.tdb': 2.322437286376953, 'eval_runtime': 4.3356, 'eval_samples_per_second': 71.962, 'eval_steps_per_second': 2.306, 'epoch': 3.0}
{'train@tur.pdtb.tdb_loss': 2.3294637203216553, 'train@tur.pdtb.tdb_accuracy@tur.pdtb.tdb': 0.262749898000816, 'train@tur.pdtb.tdb_f1@tur.pdtb.tdb': 0.026921997967835575, 'train@tur.pdtb.tdb_precision@tur.pdtb.tdb': 0.02565059524050469, 'train@tur.pdtb.tdb_recall@tur.pdtb.tdb': 0.049520464523438605, 'train@tur.pdtb.tdb_loss@tur.pdtb.tdb': 2.329463481903076, 'train@tur.pdtb.tdb_runtime': 30.2534, 'train@tur.pdtb.tdb_samples_per_second': 81.016, 'train@tur.pdtb.tdb_steps_per_second': 2.545, 'epoch': 4.0}
{'loss': 2.3855, 'learning_rate': 1.3333333333333333e-05, 'epoch': 4.0}
{'eval_loss': 2.2984864711761475, 'eval_accuracy@tur.pdtb.tdb': 0.266025641025641, 'eval_f1@tur.pdtb.tdb': 0.023356488472767542, 'eval_precision@tur.pdtb.tdb': 0.02347488038277512, 'eval_recall@tur.pdtb.tdb': 0.047033051994072546, 'eval_loss@tur.pdtb.tdb': 2.2984864711761475, 'eval_runtime': 4.311, 'eval_samples_per_second': 72.373, 'eval_steps_per_second': 2.32, 'epoch': 4.0}
{'train@tur.pdtb.tdb_loss': 2.2839701175689697, 'train@tur.pdtb.tdb_accuracy@tur.pdtb.tdb': 0.2839657282741738, 'train@tur.pdtb.tdb_f1@tur.pdtb.tdb': 0.04143284270703161, 'train@tur.pdtb.tdb_precision@tur.pdtb.tdb': 0.05550464164341973, 'train@tur.pdtb.tdb_recall@tur.pdtb.tdb': 0.06073866571701979, 'train@tur.pdtb.tdb_loss@tur.pdtb.tdb': 2.283970355987549, 'train@tur.pdtb.tdb_runtime': 30.2814, 'train@tur.pdtb.tdb_samples_per_second': 80.941, 'train@tur.pdtb.tdb_steps_per_second': 2.543, 'epoch': 5.0}
{'loss': 2.3487, 'learning_rate': 1.1666666666666668e-05, 'epoch': 5.0}
{'eval_loss': 2.272885799407959, 'eval_accuracy@tur.pdtb.tdb': 0.28525641025641024, 'eval_f1@tur.pdtb.tdb': 0.03583869961986305, 'eval_precision@tur.pdtb.tdb': 0.038562538399032835, 'eval_recall@tur.pdtb.tdb': 0.05554660710578499, 'eval_loss@tur.pdtb.tdb': 2.272886037826538, 'eval_runtime': 4.2977, 'eval_samples_per_second': 72.596, 'eval_steps_per_second': 2.327, 'epoch': 5.0}
{'train@tur.pdtb.tdb_loss': 2.2425858974456787, 'train@tur.pdtb.tdb_accuracy@tur.pdtb.tdb': 0.29865361077111385, 'train@tur.pdtb.tdb_f1@tur.pdtb.tdb': 0.04966026645067527, 'train@tur.pdtb.tdb_precision@tur.pdtb.tdb': 0.06628635755295177, 'train@tur.pdtb.tdb_recall@tur.pdtb.tdb': 0.06765878643599439, 'train@tur.pdtb.tdb_loss@tur.pdtb.tdb': 2.2425858974456787, 'train@tur.pdtb.tdb_runtime': 30.2754, 'train@tur.pdtb.tdb_samples_per_second': 80.957, 'train@tur.pdtb.tdb_steps_per_second': 2.543, 'epoch': 6.0}
{'loss': 2.3047, 'learning_rate': 1e-05, 'epoch': 6.0}
{'eval_loss': 2.2520370483398438, 'eval_accuracy@tur.pdtb.tdb': 0.28525641025641024, 'eval_f1@tur.pdtb.tdb': 0.038394868233912166, 'eval_precision@tur.pdtb.tdb': 0.054264957907847515, 'eval_recall@tur.pdtb.tdb': 0.05677635754208028, 'eval_loss@tur.pdtb.tdb': 2.2520368099212646, 'eval_runtime': 4.2971, 'eval_samples_per_second': 72.606, 'eval_steps_per_second': 2.327, 'epoch': 6.0}
{'train@tur.pdtb.tdb_loss': 2.207432985305786, 'train@tur.pdtb.tdb_accuracy@tur.pdtb.tdb': 0.32966136270909835, 'train@tur.pdtb.tdb_f1@tur.pdtb.tdb': 0.08991317239363494, 'train@tur.pdtb.tdb_precision@tur.pdtb.tdb': 0.1175472781307651, 'train@tur.pdtb.tdb_recall@tur.pdtb.tdb': 0.10095654992159447, 'train@tur.pdtb.tdb_loss@tur.pdtb.tdb': 2.207432746887207, 'train@tur.pdtb.tdb_runtime': 30.223, 'train@tur.pdtb.tdb_samples_per_second': 81.097, 'train@tur.pdtb.tdb_steps_per_second': 2.548, 'epoch': 7.0}
{'loss': 2.2695, 'learning_rate': 8.333333333333334e-06, 'epoch': 7.0}
{'eval_loss': 2.237433910369873, 'eval_accuracy@tur.pdtb.tdb': 0.2948717948717949, 'eval_f1@tur.pdtb.tdb': 0.07914625674414831, 'eval_precision@tur.pdtb.tdb': 0.08442732674197412, 'eval_recall@tur.pdtb.tdb': 0.09723163754763299, 'eval_loss@tur.pdtb.tdb': 2.237433910369873, 'eval_runtime': 4.3031, 'eval_samples_per_second': 72.505, 'eval_steps_per_second': 2.324, 'epoch': 7.0}
{'train@tur.pdtb.tdb_loss': 2.181288003921509, 'train@tur.pdtb.tdb_accuracy@tur.pdtb.tdb': 0.33659730722154224, 'train@tur.pdtb.tdb_f1@tur.pdtb.tdb': 0.09640480322138896, 'train@tur.pdtb.tdb_precision@tur.pdtb.tdb': 0.11805355499682478, 'train@tur.pdtb.tdb_recall@tur.pdtb.tdb': 0.1094495829548461, 'train@tur.pdtb.tdb_loss@tur.pdtb.tdb': 2.1812877655029297, 'train@tur.pdtb.tdb_runtime': 30.1987, 'train@tur.pdtb.tdb_samples_per_second': 81.162, 'train@tur.pdtb.tdb_steps_per_second': 2.55, 'epoch': 8.0}
{'loss': 2.2365, 'learning_rate': 6.666666666666667e-06, 'epoch': 8.0}
{'eval_loss': 2.2167954444885254, 'eval_accuracy@tur.pdtb.tdb': 0.2948717948717949, 'eval_f1@tur.pdtb.tdb': 0.07994990845781359, 'eval_precision@tur.pdtb.tdb': 0.07777777777777778, 'eval_recall@tur.pdtb.tdb': 0.10514074433049568, 'eval_loss@tur.pdtb.tdb': 2.2167954444885254, 'eval_runtime': 4.2999, 'eval_samples_per_second': 72.561, 'eval_steps_per_second': 2.326, 'epoch': 8.0}
{'train@tur.pdtb.tdb_loss': 2.1608238220214844, 'train@tur.pdtb.tdb_accuracy@tur.pdtb.tdb': 0.3423092615259078, 'train@tur.pdtb.tdb_f1@tur.pdtb.tdb': 0.0975557837600979, 'train@tur.pdtb.tdb_precision@tur.pdtb.tdb': 0.11727567256088701, 'train@tur.pdtb.tdb_recall@tur.pdtb.tdb': 0.1134715019889219, 'train@tur.pdtb.tdb_loss@tur.pdtb.tdb': 2.1608238220214844, 'train@tur.pdtb.tdb_runtime': 30.269, 'train@tur.pdtb.tdb_samples_per_second': 80.974, 'train@tur.pdtb.tdb_steps_per_second': 2.544, 'epoch': 9.0}
{'loss': 2.2163, 'learning_rate': 5e-06, 'epoch': 9.0}
{'eval_loss': 2.199983596801758, 'eval_accuracy@tur.pdtb.tdb': 0.28525641025641024, 'eval_f1@tur.pdtb.tdb': 0.07487609009348141, 'eval_precision@tur.pdtb.tdb': 0.07135706980717986, 'eval_recall@tur.pdtb.tdb': 0.1007638241098938, 'eval_loss@tur.pdtb.tdb': 2.199983596801758, 'eval_runtime': 4.3101, 'eval_samples_per_second': 72.388, 'eval_steps_per_second': 2.32, 'epoch': 9.0}
{'train@tur.pdtb.tdb_loss': 2.145817756652832, 'train@tur.pdtb.tdb_accuracy@tur.pdtb.tdb': 0.34108527131782945, 'train@tur.pdtb.tdb_f1@tur.pdtb.tdb': 0.09763676181472786, 'train@tur.pdtb.tdb_precision@tur.pdtb.tdb': 0.10984080004520021, 'train@tur.pdtb.tdb_recall@tur.pdtb.tdb': 0.11408257401904075, 'train@tur.pdtb.tdb_loss@tur.pdtb.tdb': 2.145817756652832, 'train@tur.pdtb.tdb_runtime': 30.3102, 'train@tur.pdtb.tdb_samples_per_second': 80.864, 'train@tur.pdtb.tdb_steps_per_second': 2.54, 'epoch': 10.0}
{'loss': 2.1941, 'learning_rate': 3.3333333333333333e-06, 'epoch': 10.0}
{'eval_loss': 2.1882638931274414, 'eval_accuracy@tur.pdtb.tdb': 0.28846153846153844, 'eval_f1@tur.pdtb.tdb': 0.07876536934843645, 'eval_precision@tur.pdtb.tdb': 0.07569896816980592, 'eval_recall@tur.pdtb.tdb': 0.10239267540595615, 'eval_loss@tur.pdtb.tdb': 2.1882641315460205, 'eval_runtime': 4.2994, 'eval_samples_per_second': 72.568, 'eval_steps_per_second': 2.326, 'epoch': 10.0}
{'train@tur.pdtb.tdb_loss': 2.1370041370391846, 'train@tur.pdtb.tdb_accuracy@tur.pdtb.tdb': 0.3427172582619339, 'train@tur.pdtb.tdb_f1@tur.pdtb.tdb': 0.09783783998716414, 'train@tur.pdtb.tdb_precision@tur.pdtb.tdb': 0.10715952751751147, 'train@tur.pdtb.tdb_recall@tur.pdtb.tdb': 0.1146893216324042, 'train@tur.pdtb.tdb_loss@tur.pdtb.tdb': 2.1370041370391846, 'train@tur.pdtb.tdb_runtime': 30.2625, 'train@tur.pdtb.tdb_samples_per_second': 80.991, 'train@tur.pdtb.tdb_steps_per_second': 2.544, 'epoch': 11.0}
{'loss': 2.1913, 'learning_rate': 1.6666666666666667e-06, 'epoch': 11.0}
{'eval_loss': 2.182518243789673, 'eval_accuracy@tur.pdtb.tdb': 0.2948717948717949, 'eval_f1@tur.pdtb.tdb': 0.08174490123207792, 'eval_precision@tur.pdtb.tdb': 0.07853683152596197, 'eval_recall@tur.pdtb.tdb': 0.10499220006430433, 'eval_loss@tur.pdtb.tdb': 2.182518482208252, 'eval_runtime': 4.3143, 'eval_samples_per_second': 72.318, 'eval_steps_per_second': 2.318, 'epoch': 11.0}
{'train@tur.pdtb.tdb_loss': 2.1342408657073975, 'train@tur.pdtb.tdb_accuracy@tur.pdtb.tdb': 0.3419012647898817, 'train@tur.pdtb.tdb_f1@tur.pdtb.tdb': 0.09789297672493127, 'train@tur.pdtb.tdb_precision@tur.pdtb.tdb': 0.10409970581387556, 'train@tur.pdtb.tdb_recall@tur.pdtb.tdb': 0.11479661295386609, 'train@tur.pdtb.tdb_loss@tur.pdtb.tdb': 2.1342413425445557, 'train@tur.pdtb.tdb_runtime': 30.2394, 'train@tur.pdtb.tdb_samples_per_second': 81.053, 'train@tur.pdtb.tdb_steps_per_second': 2.546, 'epoch': 12.0}
{'loss': 2.1773, 'learning_rate': 0.0, 'epoch': 12.0}
{'eval_loss': 2.1804420948028564, 'eval_accuracy@tur.pdtb.tdb': 0.2980769230769231, 'eval_f1@tur.pdtb.tdb': 0.08332362082362083, 'eval_precision@tur.pdtb.tdb': 0.07981852540574673, 'eval_recall@tur.pdtb.tdb': 0.10625482632693059, 'eval_loss@tur.pdtb.tdb': 2.1804420948028564, 'eval_runtime': 4.3237, 'eval_samples_per_second': 72.161, 'eval_steps_per_second': 2.313, 'epoch': 12.0}
{'train_runtime': 1168.8312, 'train_samples_per_second': 25.164, 'train_steps_per_second': 0.791, 'train_loss': 2.3810306565586106, 'epoch': 12.0}
***** train_logger metrics *****
  epoch                    =       12.0
  train_loss               =      2.865
  train_runtime            = 0:13:09.74
  train_samples_per_second =     24.433
  train_steps_per_second   =      0.775
-------------------------------------------------------------------
Lang1:  por.rst.cstn    Lang2:  tur.pdtb.tdb
Saving run to:  runs/full_shot/FullShot=v4_por.rst.cstn_tur.pdtb.tdb_lrfunc=Adafactor_lr=2e-05_plm=bert-base-multilingual-cased
Running with params: BERT_MODEL=bert-base-multilingual-cased lr=2e-05
read 4148 examples
read 573 examples
read 272 examples
read 2451 examples
read 312 examples
read 422 examples
Total prediction labels:  55
BertAdapterModel(
  (shared_parameters): ModuleDict()
  (bert): BertModel(
    (shared_parameters): ModuleDict()
    (invertible_adapters): ModuleDict(
      (pt): NICECouplingBlock(
        (F): Sequential(
          (0): Linear(in_features=384, out_features=192, bias=True)
          (1): Activation_Function_Class(
            (f): NewGELUActivation()
          )
          (2): Linear(in_features=192, out_features=384, bias=True)
        )
        (G): Sequential(
          (0): Linear(in_features=384, out_features=192, bias=True)
          (1): Activation_Function_Class(
            (f): NewGELUActivation()
          )
          (2): Linear(in_features=192, out_features=384, bias=True)
        )
      )
      (tr): NICECouplingBlock(
        (F): Sequential(
          (0): Linear(in_features=384, out_features=192, bias=True)
          (1): Activation_Function_Class(
            (f): NewGELUActivation()
          )
          (2): Linear(in_features=192, out_features=384, bias=True)
        )
        (G): Sequential(
          (0): Linear(in_features=384, out_features=192, bias=True)
          (1): Activation_Function_Class(
            (f): NewGELUActivation()
          )
          (2): Linear(in_features=192, out_features=384, bias=True)
        )
      )
    )
    (embeddings): BertEmbeddings(
      (word_embeddings): Embedding(119547, 768, padding_idx=0)
      (position_embeddings): Embedding(512, 768)
      (token_type_embeddings): Embedding(2, 768)
      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
      (dropout): Dropout(p=0.1, inplace=False)
    )
    (encoder): BertEncoder(
      (layer): ModuleList(
        (0): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (pt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (tr): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (1): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (pt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (tr): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (2): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (pt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (tr): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (3): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (pt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (tr): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (4): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (pt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (tr): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (5): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (pt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (tr): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (6): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (pt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (tr): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (7): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (pt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (tr): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (8): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (pt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (tr): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (9): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (pt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (tr): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (10): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (pt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (tr): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (11): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (pt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (tr): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
      )
    )
    (pooler): BertPooler(
      (dense): Linear(in_features=768, out_features=768, bias=True)
      (activation): Tanh()
    )
    (prefix_tuning): PrefixTuningPool(
      (prefix_tunings): ModuleDict()
    )
  )
  (heads): ModuleDict(
    (pt): BertStyleMaskedLMHead(
      (0): Linear(in_features=768, out_features=768, bias=True)
      (1): Activation_Function_Class(
        (f): GELUActivation()
      )
      (2): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
      (3): Linear(in_features=768, out_features=119547, bias=True)
    )
    (tr): BertStyleMaskedLMHead(
      (0): Linear(in_features=768, out_features=768, bias=True)
      (1): Activation_Function_Class(
        (f): GELUActivation()
      )
      (2): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
      (3): Linear(in_features=768, out_features=119547, bias=True)
    )
    (disrpt): ClassificationHead(
      (0): Dropout(p=0.1, inplace=False)
      (1): Linear(in_features=768, out_features=768, bias=True)
      (2): Activation_Function_Class(
        (f): Tanh()
      )
      (3): Dropout(p=0.1, inplace=False)
      (4): Linear(in_features=768, out_features=55, bias=True)
    )
  )
)
{'train@por.rst.cstn_loss': 2.492722988128662, 'train@por.rst.cstn_accuracy@por.rst.cstn': 0.27820636451301833, 'train@por.rst.cstn_f1@por.rst.cstn': 0.014083364803625377, 'train@por.rst.cstn_precision@por.rst.cstn': 0.039927623642943305, 'train@por.rst.cstn_recall@por.rst.cstn': 0.03150201612903226, 'train@por.rst.cstn_loss@por.rst.cstn': 2.492722988128662, 'train@por.rst.cstn_runtime': 50.6895, 'train@por.rst.cstn_samples_per_second': 81.831, 'train@por.rst.cstn_steps_per_second': 2.565, 'epoch': 1.0}
{'loss': 3.1875, 'learning_rate': 1.8333333333333333e-05, 'epoch': 1.0}
{'eval_loss': 2.610133409500122, 'eval_accuracy@por.rst.cstn': 0.28097731239092494, 'eval_f1@por.rst.cstn': 0.019940549913301957, 'eval_precision@por.rst.cstn': 0.012771696017769315, 'eval_recall@por.rst.cstn': 0.045454545454545456, 'eval_loss@por.rst.cstn': 2.610133409500122, 'eval_runtime': 7.4337, 'eval_samples_per_second': 77.081, 'eval_steps_per_second': 2.421, 'epoch': 1.0}
{'train@por.rst.cstn_loss': 2.2571682929992676, 'train@por.rst.cstn_accuracy@por.rst.cstn': 0.37246865959498554, 'train@por.rst.cstn_f1@por.rst.cstn': 0.053512415876578186, 'train@por.rst.cstn_precision@por.rst.cstn': 0.06799189489895158, 'train@por.rst.cstn_recall@por.rst.cstn': 0.06311219970950334, 'train@por.rst.cstn_loss@por.rst.cstn': 2.2571685314178467, 'train@por.rst.cstn_runtime': 50.7923, 'train@por.rst.cstn_samples_per_second': 81.666, 'train@por.rst.cstn_steps_per_second': 2.559, 'epoch': 2.0}
{'loss': 2.4099, 'learning_rate': 1.6666666666666667e-05, 'epoch': 2.0}
{'eval_loss': 2.366528272628784, 'eval_accuracy@por.rst.cstn': 0.3298429319371728, 'eval_f1@por.rst.cstn': 0.07144462087289108, 'eval_precision@por.rst.cstn': 0.10083090341392924, 'eval_recall@por.rst.cstn': 0.0863469534418151, 'eval_loss@por.rst.cstn': 2.366528272628784, 'eval_runtime': 7.4505, 'eval_samples_per_second': 76.907, 'eval_steps_per_second': 2.416, 'epoch': 2.0}
{'train@por.rst.cstn_loss': 2.0504493713378906, 'train@por.rst.cstn_accuracy@por.rst.cstn': 0.4363548698167792, 'train@por.rst.cstn_f1@por.rst.cstn': 0.0671888621516823, 'train@por.rst.cstn_precision@por.rst.cstn': 0.0759727513183519, 'train@por.rst.cstn_recall@por.rst.cstn': 0.0833122493627, 'train@por.rst.cstn_loss@por.rst.cstn': 2.0504493713378906, 'train@por.rst.cstn_runtime': 50.9138, 'train@por.rst.cstn_samples_per_second': 81.471, 'train@por.rst.cstn_steps_per_second': 2.553, 'epoch': 3.0}
{'loss': 2.2055, 'learning_rate': 1.5000000000000002e-05, 'epoch': 3.0}
{'eval_loss': 2.178560972213745, 'eval_accuracy@por.rst.cstn': 0.35951134380453753, 'eval_f1@por.rst.cstn': 0.0910835165229793, 'eval_precision@por.rst.cstn': 0.08173792741387174, 'eval_recall@por.rst.cstn': 0.11906259090053557, 'eval_loss@por.rst.cstn': 2.178561210632324, 'eval_runtime': 7.4584, 'eval_samples_per_second': 76.826, 'eval_steps_per_second': 2.413, 'epoch': 3.0}
{'train@por.rst.cstn_loss': 1.903642177581787, 'train@por.rst.cstn_accuracy@por.rst.cstn': 0.491321118611379, 'train@por.rst.cstn_f1@por.rst.cstn': 0.08060124973396024, 'train@por.rst.cstn_precision@por.rst.cstn': 0.07254976131966133, 'train@por.rst.cstn_recall@por.rst.cstn': 0.09585653104589943, 'train@por.rst.cstn_loss@por.rst.cstn': 1.9036422967910767, 'train@por.rst.cstn_runtime': 50.7818, 'train@por.rst.cstn_samples_per_second': 81.683, 'train@por.rst.cstn_steps_per_second': 2.56, 'epoch': 4.0}
{'loss': 2.0336, 'learning_rate': 1.3333333333333333e-05, 'epoch': 4.0}
{'eval_loss': 2.0570197105407715, 'eval_accuracy@por.rst.cstn': 0.4118673647469459, 'eval_f1@por.rst.cstn': 0.10675707125958005, 'eval_precision@por.rst.cstn': 0.09638399744936675, 'eval_recall@por.rst.cstn': 0.13660541490268974, 'eval_loss@por.rst.cstn': 2.0570197105407715, 'eval_runtime': 7.4524, 'eval_samples_per_second': 76.888, 'eval_steps_per_second': 2.415, 'epoch': 4.0}
{'train@por.rst.cstn_loss': 1.801929235458374, 'train@por.rst.cstn_accuracy@por.rst.cstn': 0.49349083895853424, 'train@por.rst.cstn_f1@por.rst.cstn': 0.09271936883041132, 'train@por.rst.cstn_precision@por.rst.cstn': 0.1349925871770531, 'train@por.rst.cstn_recall@por.rst.cstn': 0.10389576190626398, 'train@por.rst.cstn_loss@por.rst.cstn': 1.8019293546676636, 'train@por.rst.cstn_runtime': 50.8076, 'train@por.rst.cstn_samples_per_second': 81.641, 'train@por.rst.cstn_steps_per_second': 2.559, 'epoch': 5.0}
{'loss': 1.91, 'learning_rate': 1.1666666666666668e-05, 'epoch': 5.0}
{'eval_loss': 1.9538203477859497, 'eval_accuracy@por.rst.cstn': 0.41012216404886565, 'eval_f1@por.rst.cstn': 0.12653912467376702, 'eval_precision@por.rst.cstn': 0.1324043365598944, 'eval_recall@por.rst.cstn': 0.15328285079689283, 'eval_loss@por.rst.cstn': 1.9538203477859497, 'eval_runtime': 7.4528, 'eval_samples_per_second': 76.883, 'eval_steps_per_second': 2.415, 'epoch': 5.0}
{'train@por.rst.cstn_loss': 1.7290524244308472, 'train@por.rst.cstn_accuracy@por.rst.cstn': 0.5395371263259402, 'train@por.rst.cstn_f1@por.rst.cstn': 0.10907881594663311, 'train@por.rst.cstn_precision@por.rst.cstn': 0.13120404514507272, 'train@por.rst.cstn_recall@por.rst.cstn': 0.1219945885883232, 'train@por.rst.cstn_loss@por.rst.cstn': 1.7290524244308472, 'train@por.rst.cstn_runtime': 50.846, 'train@por.rst.cstn_samples_per_second': 81.58, 'train@por.rst.cstn_steps_per_second': 2.557, 'epoch': 6.0}
{'loss': 1.8271, 'learning_rate': 1e-05, 'epoch': 6.0}
{'eval_loss': 1.8772509098052979, 'eval_accuracy@por.rst.cstn': 0.44502617801047123, 'eval_f1@por.rst.cstn': 0.13836021712029498, 'eval_precision@por.rst.cstn': 0.12417328494642006, 'eval_recall@por.rst.cstn': 0.16918872219475506, 'eval_loss@por.rst.cstn': 1.877251148223877, 'eval_runtime': 7.4615, 'eval_samples_per_second': 76.794, 'eval_steps_per_second': 2.412, 'epoch': 6.0}
{'train@por.rst.cstn_loss': 1.67742919921875, 'train@por.rst.cstn_accuracy@por.rst.cstn': 0.5571359691417551, 'train@por.rst.cstn_f1@por.rst.cstn': 0.11669238870008179, 'train@por.rst.cstn_precision@por.rst.cstn': 0.12984753969349702, 'train@por.rst.cstn_recall@por.rst.cstn': 0.13099262093289799, 'train@por.rst.cstn_loss@por.rst.cstn': 1.67742919921875, 'train@por.rst.cstn_runtime': 50.8579, 'train@por.rst.cstn_samples_per_second': 81.561, 'train@por.rst.cstn_steps_per_second': 2.556, 'epoch': 7.0}
{'loss': 1.7595, 'learning_rate': 8.333333333333334e-06, 'epoch': 7.0}
{'eval_loss': 1.818686604499817, 'eval_accuracy@por.rst.cstn': 0.44851657940663175, 'eval_f1@por.rst.cstn': 0.1513994264839259, 'eval_precision@por.rst.cstn': 0.19236677402416638, 'eval_recall@por.rst.cstn': 0.17596060759156246, 'eval_loss@por.rst.cstn': 1.818686604499817, 'eval_runtime': 7.4805, 'eval_samples_per_second': 76.6, 'eval_steps_per_second': 2.406, 'epoch': 7.0}
{'train@por.rst.cstn_loss': 1.6443800926208496, 'train@por.rst.cstn_accuracy@por.rst.cstn': 0.562198649951784, 'train@por.rst.cstn_f1@por.rst.cstn': 0.12101746642986393, 'train@por.rst.cstn_precision@por.rst.cstn': 0.13399998872018387, 'train@por.rst.cstn_recall@por.rst.cstn': 0.1352636671744057, 'train@por.rst.cstn_loss@por.rst.cstn': 1.6443800926208496, 'train@por.rst.cstn_runtime': 50.8248, 'train@por.rst.cstn_samples_per_second': 81.614, 'train@por.rst.cstn_steps_per_second': 2.558, 'epoch': 8.0}
{'loss': 1.7262, 'learning_rate': 6.666666666666667e-06, 'epoch': 8.0}
{'eval_loss': 1.787883996963501, 'eval_accuracy@por.rst.cstn': 0.4537521815008726, 'eval_f1@por.rst.cstn': 0.1552042094302222, 'eval_precision@por.rst.cstn': 0.17663736690546814, 'eval_recall@por.rst.cstn': 0.17927525575226844, 'eval_loss@por.rst.cstn': 1.7878835201263428, 'eval_runtime': 7.4884, 'eval_samples_per_second': 76.518, 'eval_steps_per_second': 2.404, 'epoch': 8.0}
{'train@por.rst.cstn_loss': 1.6190061569213867, 'train@por.rst.cstn_accuracy@por.rst.cstn': 0.5684667309546769, 'train@por.rst.cstn_f1@por.rst.cstn': 0.12581433764897562, 'train@por.rst.cstn_precision@por.rst.cstn': 0.13677245328194593, 'train@por.rst.cstn_recall@por.rst.cstn': 0.13874328989691664, 'train@por.rst.cstn_loss@por.rst.cstn': 1.6190059185028076, 'train@por.rst.cstn_runtime': 50.8195, 'train@por.rst.cstn_samples_per_second': 81.622, 'train@por.rst.cstn_steps_per_second': 2.558, 'epoch': 9.0}
{'loss': 1.6932, 'learning_rate': 5e-06, 'epoch': 9.0}
{'eval_loss': 1.7616738080978394, 'eval_accuracy@por.rst.cstn': 0.45898778359511344, 'eval_f1@por.rst.cstn': 0.16163299724498262, 'eval_precision@por.rst.cstn': 0.16725197626631375, 'eval_recall@por.rst.cstn': 0.1829050520360069, 'eval_loss@por.rst.cstn': 1.761673927307129, 'eval_runtime': 7.4543, 'eval_samples_per_second': 76.868, 'eval_steps_per_second': 2.415, 'epoch': 9.0}
{'train@por.rst.cstn_loss': 1.6032428741455078, 'train@por.rst.cstn_accuracy@por.rst.cstn': 0.5694310511089682, 'train@por.rst.cstn_f1@por.rst.cstn': 0.12696133547159358, 'train@por.rst.cstn_precision@por.rst.cstn': 0.13083706192211086, 'train@por.rst.cstn_recall@por.rst.cstn': 0.1411623968948264, 'train@por.rst.cstn_loss@por.rst.cstn': 1.6032428741455078, 'train@por.rst.cstn_runtime': 50.7744, 'train@por.rst.cstn_samples_per_second': 81.695, 'train@por.rst.cstn_steps_per_second': 2.56, 'epoch': 10.0}
{'loss': 1.6655, 'learning_rate': 3.3333333333333333e-06, 'epoch': 10.0}
{'eval_loss': 1.7438442707061768, 'eval_accuracy@por.rst.cstn': 0.4781849912739965, 'eval_f1@por.rst.cstn': 0.1721565965293184, 'eval_precision@por.rst.cstn': 0.17647174375206062, 'eval_recall@por.rst.cstn': 0.19483397528436014, 'eval_loss@por.rst.cstn': 1.7438443899154663, 'eval_runtime': 7.4966, 'eval_samples_per_second': 76.435, 'eval_steps_per_second': 2.401, 'epoch': 10.0}
{'train@por.rst.cstn_loss': 1.5932010412216187, 'train@por.rst.cstn_accuracy@por.rst.cstn': 0.5718418514946962, 'train@por.rst.cstn_f1@por.rst.cstn': 0.12817240001608954, 'train@por.rst.cstn_precision@por.rst.cstn': 0.13099229345165012, 'train@por.rst.cstn_recall@por.rst.cstn': 0.14211611089082343, 'train@por.rst.cstn_loss@por.rst.cstn': 1.5932008028030396, 'train@por.rst.cstn_runtime': 50.73, 'train@por.rst.cstn_samples_per_second': 81.766, 'train@por.rst.cstn_steps_per_second': 2.563, 'epoch': 11.0}
{'loss': 1.6535, 'learning_rate': 1.6666666666666667e-06, 'epoch': 11.0}
{'eval_loss': 1.7354241609573364, 'eval_accuracy@por.rst.cstn': 0.48342059336823734, 'eval_f1@por.rst.cstn': 0.17369217265200146, 'eval_precision@por.rst.cstn': 0.17758471009101792, 'eval_recall@por.rst.cstn': 0.19571901441830714, 'eval_loss@por.rst.cstn': 1.7354240417480469, 'eval_runtime': 7.4326, 'eval_samples_per_second': 77.093, 'eval_steps_per_second': 2.422, 'epoch': 11.0}
{'train@por.rst.cstn_loss': 1.589875340461731, 'train@por.rst.cstn_accuracy@por.rst.cstn': 0.5725650916104147, 'train@por.rst.cstn_f1@por.rst.cstn': 0.12925368975572604, 'train@por.rst.cstn_precision@por.rst.cstn': 0.13179373636643793, 'train@por.rst.cstn_recall@por.rst.cstn': 0.14267636190503263, 'train@por.rst.cstn_loss@por.rst.cstn': 1.5898752212524414, 'train@por.rst.cstn_runtime': 50.7082, 'train@por.rst.cstn_samples_per_second': 81.801, 'train@por.rst.cstn_steps_per_second': 2.564, 'epoch': 12.0}
{'loss': 1.6488, 'learning_rate': 0.0, 'epoch': 12.0}
{'eval_loss': 1.7308974266052246, 'eval_accuracy@por.rst.cstn': 0.4816753926701571, 'eval_f1@por.rst.cstn': 0.1738812192973361, 'eval_precision@por.rst.cstn': 0.17866078396652696, 'eval_recall@por.rst.cstn': 0.19441772400218416, 'eval_loss@por.rst.cstn': 1.7308975458145142, 'eval_runtime': 7.4619, 'eval_samples_per_second': 76.79, 'eval_steps_per_second': 2.412, 'epoch': 12.0}
{'train_runtime': 1979.8843, 'train_samples_per_second': 25.141, 'train_steps_per_second': 0.788, 'train_loss': 1.9766961415608724, 'epoch': 12.0}
***** train_log metrics *****
  epoch                    =       12.0
  train_loss               =     1.9767
  train_runtime            = 0:32:59.88
  train_samples_per_second =     25.141
  train_steps_per_second   =      0.788
{'train@tur.pdtb.tdb_loss': 2.737839698791504, 'train@tur.pdtb.tdb_accuracy@tur.pdtb.tdb': 0.24765401876784987, 'train@tur.pdtb.tdb_f1@tur.pdtb.tdb': 0.016628314705237783, 'train@tur.pdtb.tdb_precision@tur.pdtb.tdb': 0.010416666666666666, 'train@tur.pdtb.tdb_recall@tur.pdtb.tdb': 0.04119163952225841, 'train@tur.pdtb.tdb_loss@tur.pdtb.tdb': 2.737839698791504, 'train@tur.pdtb.tdb_runtime': 30.1894, 'train@tur.pdtb.tdb_samples_per_second': 81.188, 'train@tur.pdtb.tdb_steps_per_second': 2.551, 'epoch': 1.0}
{'loss': 3.5413, 'learning_rate': 1.8333333333333333e-05, 'epoch': 1.0}
{'eval_loss': 2.669678211212158, 'eval_accuracy@tur.pdtb.tdb': 0.266025641025641, 'eval_f1@tur.pdtb.tdb': 0.019102416570771, 'eval_precision@tur.pdtb.tdb': 0.012092074592074592, 'eval_recall@tur.pdtb.tdb': 0.045454545454545456, 'eval_loss@tur.pdtb.tdb': 2.669677734375, 'eval_runtime': 4.3029, 'eval_samples_per_second': 72.509, 'eval_steps_per_second': 2.324, 'epoch': 1.0}
{'train@tur.pdtb.tdb_loss': 2.357297420501709, 'train@tur.pdtb.tdb_accuracy@tur.pdtb.tdb': 0.2733578131374949, 'train@tur.pdtb.tdb_f1@tur.pdtb.tdb': 0.036093751761904914, 'train@tur.pdtb.tdb_precision@tur.pdtb.tdb': 0.08347081372553106, 'train@tur.pdtb.tdb_recall@tur.pdtb.tdb': 0.05416299829480497, 'train@tur.pdtb.tdb_loss@tur.pdtb.tdb': 2.357297420501709, 'train@tur.pdtb.tdb_runtime': 30.1733, 'train@tur.pdtb.tdb_samples_per_second': 81.231, 'train@tur.pdtb.tdb_steps_per_second': 2.552, 'epoch': 2.0}
{'loss': 2.5257, 'learning_rate': 1.6666666666666667e-05, 'epoch': 2.0}
{'eval_loss': 2.311038017272949, 'eval_accuracy@tur.pdtb.tdb': 0.25961538461538464, 'eval_f1@tur.pdtb.tdb': 0.021016869728209934, 'eval_precision@tur.pdtb.tdb': 0.01949826130153999, 'eval_recall@tur.pdtb.tdb': 0.04514850847239224, 'eval_loss@tur.pdtb.tdb': 2.31103777885437, 'eval_runtime': 4.2914, 'eval_samples_per_second': 72.704, 'eval_steps_per_second': 2.33, 'epoch': 2.0}
{'train@tur.pdtb.tdb_loss': 2.263939142227173, 'train@tur.pdtb.tdb_accuracy@tur.pdtb.tdb': 0.2953896368829049, 'train@tur.pdtb.tdb_f1@tur.pdtb.tdb': 0.0509633890399936, 'train@tur.pdtb.tdb_precision@tur.pdtb.tdb': 0.06430855165566758, 'train@tur.pdtb.tdb_recall@tur.pdtb.tdb': 0.06674940729575525, 'train@tur.pdtb.tdb_loss@tur.pdtb.tdb': 2.263939142227173, 'train@tur.pdtb.tdb_runtime': 30.2165, 'train@tur.pdtb.tdb_samples_per_second': 81.115, 'train@tur.pdtb.tdb_steps_per_second': 2.548, 'epoch': 3.0}
{'loss': 2.3521, 'learning_rate': 1.5000000000000002e-05, 'epoch': 3.0}
{'eval_loss': 2.2530388832092285, 'eval_accuracy@tur.pdtb.tdb': 0.2916666666666667, 'eval_f1@tur.pdtb.tdb': 0.04384703332436912, 'eval_precision@tur.pdtb.tdb': 0.10334928229665072, 'eval_recall@tur.pdtb.tdb': 0.05915306580114148, 'eval_loss@tur.pdtb.tdb': 2.2530386447906494, 'eval_runtime': 4.2782, 'eval_samples_per_second': 72.928, 'eval_steps_per_second': 2.337, 'epoch': 3.0}
{'train@tur.pdtb.tdb_loss': 2.189600944519043, 'train@tur.pdtb.tdb_accuracy@tur.pdtb.tdb': 0.31252549979600164, 'train@tur.pdtb.tdb_f1@tur.pdtb.tdb': 0.05769222004575757, 'train@tur.pdtb.tdb_precision@tur.pdtb.tdb': 0.07276446800598457, 'train@tur.pdtb.tdb_recall@tur.pdtb.tdb': 0.07517457133019252, 'train@tur.pdtb.tdb_loss@tur.pdtb.tdb': 2.189600706100464, 'train@tur.pdtb.tdb_runtime': 30.2664, 'train@tur.pdtb.tdb_samples_per_second': 80.981, 'train@tur.pdtb.tdb_steps_per_second': 2.544, 'epoch': 4.0}
{'loss': 2.2727, 'learning_rate': 1.3333333333333333e-05, 'epoch': 4.0}
{'eval_loss': 2.2157087326049805, 'eval_accuracy@tur.pdtb.tdb': 0.28846153846153844, 'eval_f1@tur.pdtb.tdb': 0.04246067226154781, 'eval_precision@tur.pdtb.tdb': 0.06162000230441295, 'eval_recall@tur.pdtb.tdb': 0.059268734241001826, 'eval_loss@tur.pdtb.tdb': 2.2157089710235596, 'eval_runtime': 4.2884, 'eval_samples_per_second': 72.754, 'eval_steps_per_second': 2.332, 'epoch': 4.0}
{'train@tur.pdtb.tdb_loss': 2.1351852416992188, 'train@tur.pdtb.tdb_accuracy@tur.pdtb.tdb': 0.33455732354141166, 'train@tur.pdtb.tdb_f1@tur.pdtb.tdb': 0.08262423725765586, 'train@tur.pdtb.tdb_precision@tur.pdtb.tdb': 0.1127405730738518, 'train@tur.pdtb.tdb_recall@tur.pdtb.tdb': 0.09281684428390856, 'train@tur.pdtb.tdb_loss@tur.pdtb.tdb': 2.135185480117798, 'train@tur.pdtb.tdb_runtime': 30.2346, 'train@tur.pdtb.tdb_samples_per_second': 81.066, 'train@tur.pdtb.tdb_steps_per_second': 2.547, 'epoch': 5.0}
{'loss': 2.2177, 'learning_rate': 1.1666666666666668e-05, 'epoch': 5.0}
{'eval_loss': 2.182849645614624, 'eval_accuracy@tur.pdtb.tdb': 0.2916666666666667, 'eval_f1@tur.pdtb.tdb': 0.06268670226293799, 'eval_precision@tur.pdtb.tdb': 0.09727782873562925, 'eval_recall@tur.pdtb.tdb': 0.07065968282959009, 'eval_loss@tur.pdtb.tdb': 2.182849884033203, 'eval_runtime': 4.3131, 'eval_samples_per_second': 72.338, 'eval_steps_per_second': 2.319, 'epoch': 5.0}
{'train@tur.pdtb.tdb_loss': 2.0913712978363037, 'train@tur.pdtb.tdb_accuracy@tur.pdtb.tdb': 0.3671970624235006, 'train@tur.pdtb.tdb_f1@tur.pdtb.tdb': 0.11560034224018931, 'train@tur.pdtb.tdb_precision@tur.pdtb.tdb': 0.14116820095878013, 'train@tur.pdtb.tdb_recall@tur.pdtb.tdb': 0.12691634412327027, 'train@tur.pdtb.tdb_loss@tur.pdtb.tdb': 2.0913712978363037, 'train@tur.pdtb.tdb_runtime': 30.246, 'train@tur.pdtb.tdb_samples_per_second': 81.036, 'train@tur.pdtb.tdb_steps_per_second': 2.546, 'epoch': 6.0}
{'loss': 2.166, 'learning_rate': 1e-05, 'epoch': 6.0}
{'eval_loss': 2.1568846702575684, 'eval_accuracy@tur.pdtb.tdb': 0.33974358974358976, 'eval_f1@tur.pdtb.tdb': 0.11706242355889723, 'eval_precision@tur.pdtb.tdb': 0.20374476665430716, 'eval_recall@tur.pdtb.tdb': 0.12589495217344446, 'eval_loss@tur.pdtb.tdb': 2.1568844318389893, 'eval_runtime': 4.9892, 'eval_samples_per_second': 62.536, 'eval_steps_per_second': 2.004, 'epoch': 6.0}
{'train@tur.pdtb.tdb_loss': 2.0583479404449463, 'train@tur.pdtb.tdb_accuracy@tur.pdtb.tdb': 0.3773969808241534, 'train@tur.pdtb.tdb_f1@tur.pdtb.tdb': 0.12232243670600472, 'train@tur.pdtb.tdb_precision@tur.pdtb.tdb': 0.14440166105550659, 'train@tur.pdtb.tdb_recall@tur.pdtb.tdb': 0.13564484866214618, 'train@tur.pdtb.tdb_loss@tur.pdtb.tdb': 2.0583479404449463, 'train@tur.pdtb.tdb_runtime': 30.2159, 'train@tur.pdtb.tdb_samples_per_second': 81.116, 'train@tur.pdtb.tdb_steps_per_second': 2.548, 'epoch': 7.0}
{'loss': 2.1325, 'learning_rate': 8.333333333333334e-06, 'epoch': 7.0}
{'eval_loss': 2.138545036315918, 'eval_accuracy@tur.pdtb.tdb': 0.3269230769230769, 'eval_f1@tur.pdtb.tdb': 0.11279581687740997, 'eval_precision@tur.pdtb.tdb': 0.17658017658017658, 'eval_recall@tur.pdtb.tdb': 0.12687562091875465, 'eval_loss@tur.pdtb.tdb': 2.138545036315918, 'eval_runtime': 4.2664, 'eval_samples_per_second': 73.129, 'eval_steps_per_second': 2.344, 'epoch': 7.0}
{'train@tur.pdtb.tdb_loss': 2.0350522994995117, 'train@tur.pdtb.tdb_accuracy@tur.pdtb.tdb': 0.3806609547123623, 'train@tur.pdtb.tdb_f1@tur.pdtb.tdb': 0.12630239371789959, 'train@tur.pdtb.tdb_precision@tur.pdtb.tdb': 0.14694587415167443, 'train@tur.pdtb.tdb_recall@tur.pdtb.tdb': 0.14034183129712668, 'train@tur.pdtb.tdb_loss@tur.pdtb.tdb': 2.0350520610809326, 'train@tur.pdtb.tdb_runtime': 30.2435, 'train@tur.pdtb.tdb_samples_per_second': 81.042, 'train@tur.pdtb.tdb_steps_per_second': 2.546, 'epoch': 8.0}
{'loss': 2.1045, 'learning_rate': 6.666666666666667e-06, 'epoch': 8.0}
{'eval_loss': 2.1206605434417725, 'eval_accuracy@tur.pdtb.tdb': 0.3301282051282051, 'eval_f1@tur.pdtb.tdb': 0.11618412934172283, 'eval_precision@tur.pdtb.tdb': 0.1738160873674459, 'eval_recall@tur.pdtb.tdb': 0.12918525471083997, 'eval_loss@tur.pdtb.tdb': 2.1206605434417725, 'eval_runtime': 4.3026, 'eval_samples_per_second': 72.515, 'eval_steps_per_second': 2.324, 'epoch': 8.0}
{'train@tur.pdtb.tdb_loss': 2.0145955085754395, 'train@tur.pdtb.tdb_accuracy@tur.pdtb.tdb': 0.3835169318645451, 'train@tur.pdtb.tdb_f1@tur.pdtb.tdb': 0.12620205036804108, 'train@tur.pdtb.tdb_precision@tur.pdtb.tdb': 0.1472512097870357, 'train@tur.pdtb.tdb_recall@tur.pdtb.tdb': 0.1386027585738226, 'train@tur.pdtb.tdb_loss@tur.pdtb.tdb': 2.0145957469940186, 'train@tur.pdtb.tdb_runtime': 30.2165, 'train@tur.pdtb.tdb_samples_per_second': 81.115, 'train@tur.pdtb.tdb_steps_per_second': 2.548, 'epoch': 9.0}
{'loss': 2.0811, 'learning_rate': 5e-06, 'epoch': 9.0}
{'eval_loss': 2.106041193008423, 'eval_accuracy@tur.pdtb.tdb': 0.33653846153846156, 'eval_f1@tur.pdtb.tdb': 0.1145472768068143, 'eval_precision@tur.pdtb.tdb': 0.17575916147344717, 'eval_recall@tur.pdtb.tdb': 0.12772930302686622, 'eval_loss@tur.pdtb.tdb': 2.106041193008423, 'eval_runtime': 4.2993, 'eval_samples_per_second': 72.57, 'eval_steps_per_second': 2.326, 'epoch': 9.0}
{'train@tur.pdtb.tdb_loss': 2.0008413791656494, 'train@tur.pdtb.tdb_accuracy@tur.pdtb.tdb': 0.3884128926968584, 'train@tur.pdtb.tdb_f1@tur.pdtb.tdb': 0.12887000929515202, 'train@tur.pdtb.tdb_precision@tur.pdtb.tdb': 0.14552621084983056, 'train@tur.pdtb.tdb_recall@tur.pdtb.tdb': 0.14116555459925276, 'train@tur.pdtb.tdb_loss@tur.pdtb.tdb': 2.0008413791656494, 'train@tur.pdtb.tdb_runtime': 30.2119, 'train@tur.pdtb.tdb_samples_per_second': 81.127, 'train@tur.pdtb.tdb_steps_per_second': 2.549, 'epoch': 10.0}
{'loss': 2.0651, 'learning_rate': 3.3333333333333333e-06, 'epoch': 10.0}
{'eval_loss': 2.0971896648406982, 'eval_accuracy@tur.pdtb.tdb': 0.3301282051282051, 'eval_f1@tur.pdtb.tdb': 0.11436527448697201, 'eval_precision@tur.pdtb.tdb': 0.1744296821599453, 'eval_recall@tur.pdtb.tdb': 0.12690849674508559, 'eval_loss@tur.pdtb.tdb': 2.097189426422119, 'eval_runtime': 4.3159, 'eval_samples_per_second': 72.291, 'eval_steps_per_second': 2.317, 'epoch': 10.0}
{'train@tur.pdtb.tdb_loss': 1.9932457208633423, 'train@tur.pdtb.tdb_accuracy@tur.pdtb.tdb': 0.386780905752754, 'train@tur.pdtb.tdb_f1@tur.pdtb.tdb': 0.13091749929634006, 'train@tur.pdtb.tdb_precision@tur.pdtb.tdb': 0.14861285671533264, 'train@tur.pdtb.tdb_recall@tur.pdtb.tdb': 0.14206949019290077, 'train@tur.pdtb.tdb_loss@tur.pdtb.tdb': 1.9932457208633423, 'train@tur.pdtb.tdb_runtime': 30.2228, 'train@tur.pdtb.tdb_samples_per_second': 81.098, 'train@tur.pdtb.tdb_steps_per_second': 2.548, 'epoch': 11.0}
{'loss': 2.055, 'learning_rate': 1.6666666666666667e-06, 'epoch': 11.0}
{'eval_loss': 2.0929017066955566, 'eval_accuracy@tur.pdtb.tdb': 0.3269230769230769, 'eval_f1@tur.pdtb.tdb': 0.11403189772672456, 'eval_precision@tur.pdtb.tdb': 0.17408452207294864, 'eval_recall@tur.pdtb.tdb': 0.12636085161912725, 'eval_loss@tur.pdtb.tdb': 2.0929017066955566, 'eval_runtime': 4.3099, 'eval_samples_per_second': 72.391, 'eval_steps_per_second': 2.32, 'epoch': 11.0}
{'train@tur.pdtb.tdb_loss': 1.9905145168304443, 'train@tur.pdtb.tdb_accuracy@tur.pdtb.tdb': 0.390452876376989, 'train@tur.pdtb.tdb_f1@tur.pdtb.tdb': 0.1316458292963531, 'train@tur.pdtb.tdb_precision@tur.pdtb.tdb': 0.14655011300707949, 'train@tur.pdtb.tdb_recall@tur.pdtb.tdb': 0.1434847282434358, 'train@tur.pdtb.tdb_loss@tur.pdtb.tdb': 1.990514874458313, 'train@tur.pdtb.tdb_runtime': 30.2378, 'train@tur.pdtb.tdb_samples_per_second': 81.057, 'train@tur.pdtb.tdb_steps_per_second': 2.546, 'epoch': 12.0}
{'loss': 2.0427, 'learning_rate': 0.0, 'epoch': 12.0}
{'eval_loss': 2.0909101963043213, 'eval_accuracy@tur.pdtb.tdb': 0.3301282051282051, 'eval_f1@tur.pdtb.tdb': 0.11508119661965636, 'eval_precision@tur.pdtb.tdb': 0.17375080476684754, 'eval_recall@tur.pdtb.tdb': 0.12734899391161736, 'eval_loss@tur.pdtb.tdb': 2.0909106731414795, 'eval_runtime': 4.2928, 'eval_samples_per_second': 72.68, 'eval_steps_per_second': 2.329, 'epoch': 12.0}
{'train_runtime': 1169.6679, 'train_samples_per_second': 25.146, 'train_steps_per_second': 0.79, 'train_loss': 2.2963486543465486, 'epoch': 12.0}
***** train_logger metrics *****
  epoch                    =       12.0
  train_loss               =     1.9767
  train_runtime            = 0:32:59.88
  train_samples_per_second =     25.141
  train_steps_per_second   =      0.788
-------------------------------------------------------------------
Lang1:  rus.rst.rrt    Lang2:  tur.pdtb.tdb
Saving run to:  runs/full_shot/FullShot=v4_rus.rst.rrt_tur.pdtb.tdb_lrfunc=Adafactor_lr=2e-05_plm=bert-base-multilingual-cased
Running with params: BERT_MODEL=bert-base-multilingual-cased lr=2e-05
read 28822 examples
read 2855 examples
read 2843 examples
read 2451 examples
read 312 examples
read 422 examples
Total prediction labels:  45
BertAdapterModel(
  (shared_parameters): ModuleDict()
  (bert): BertModel(
    (shared_parameters): ModuleDict()
    (invertible_adapters): ModuleDict(
      (ru): NICECouplingBlock(
        (F): Sequential(
          (0): Linear(in_features=384, out_features=192, bias=True)
          (1): Activation_Function_Class(
            (f): NewGELUActivation()
          )
          (2): Linear(in_features=192, out_features=384, bias=True)
        )
        (G): Sequential(
          (0): Linear(in_features=384, out_features=192, bias=True)
          (1): Activation_Function_Class(
            (f): NewGELUActivation()
          )
          (2): Linear(in_features=192, out_features=384, bias=True)
        )
      )
      (tr): NICECouplingBlock(
        (F): Sequential(
          (0): Linear(in_features=384, out_features=192, bias=True)
          (1): Activation_Function_Class(
            (f): NewGELUActivation()
          )
          (2): Linear(in_features=192, out_features=384, bias=True)
        )
        (G): Sequential(
          (0): Linear(in_features=384, out_features=192, bias=True)
          (1): Activation_Function_Class(
            (f): NewGELUActivation()
          )
          (2): Linear(in_features=192, out_features=384, bias=True)
        )
      )
    )
    (embeddings): BertEmbeddings(
      (word_embeddings): Embedding(119547, 768, padding_idx=0)
      (position_embeddings): Embedding(512, 768)
      (token_type_embeddings): Embedding(2, 768)
      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
      (dropout): Dropout(p=0.1, inplace=False)
    )
    (encoder): BertEncoder(
      (layer): ModuleList(
        (0): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (ru): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (tr): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (1): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (ru): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (tr): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (2): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (ru): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (tr): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (3): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (ru): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (tr): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (4): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (ru): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (tr): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (5): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (ru): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (tr): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (6): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (ru): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (tr): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (7): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (ru): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (tr): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (8): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (ru): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (tr): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (9): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (ru): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (tr): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (10): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (ru): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (tr): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (11): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (ru): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (tr): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
      )
    )
    (pooler): BertPooler(
      (dense): Linear(in_features=768, out_features=768, bias=True)
      (activation): Tanh()
    )
    (prefix_tuning): PrefixTuningPool(
      (prefix_tunings): ModuleDict()
    )
  )
  (heads): ModuleDict(
    (tr): BertStyleMaskedLMHead(
      (0): Linear(in_features=768, out_features=768, bias=True)
      (1): Activation_Function_Class(
        (f): GELUActivation()
      )
      (2): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
      (3): Linear(in_features=768, out_features=119547, bias=True)
    )
    (disrpt): ClassificationHead(
      (0): Dropout(p=0.1, inplace=False)
      (1): Linear(in_features=768, out_features=768, bias=True)
      (2): Activation_Function_Class(
        (f): Tanh()
      )
      (3): Dropout(p=0.1, inplace=False)
      (4): Linear(in_features=768, out_features=45, bias=True)
    )
  )
)
{'train@rus.rst.rrt_loss': 1.762648105621338, 'train@rus.rst.rrt_accuracy@rus.rst.rrt': 0.4863992783290542, 'train@rus.rst.rrt_f1@rus.rst.rrt': 0.1770205682910749, 'train@rus.rst.rrt_precision@rus.rst.rrt': 0.2282167872694573, 'train@rus.rst.rrt_recall@rus.rst.rrt': 0.19409828334572177, 'train@rus.rst.rrt_loss@rus.rst.rrt': 1.7626478672027588, 'train@rus.rst.rrt_runtime': 350.5061, 'train@rus.rst.rrt_samples_per_second': 82.23, 'train@rus.rst.rrt_steps_per_second': 2.571, 'epoch': 1.0}
{'loss': 2.2418, 'learning_rate': 1.8333333333333333e-05, 'epoch': 1.0}
{'eval_loss': 1.7968131303787231, 'eval_accuracy@rus.rst.rrt': 0.46865148861646233, 'eval_f1@rus.rst.rrt': 0.19592529725281974, 'eval_precision@rus.rst.rrt': 0.2068974962195811, 'eval_recall@rus.rst.rrt': 0.21478692778561462, 'eval_loss@rus.rst.rrt': 1.7968132495880127, 'eval_runtime': 35.0884, 'eval_samples_per_second': 81.366, 'eval_steps_per_second': 2.565, 'epoch': 1.0}
{'train@rus.rst.rrt_loss': 1.5179413557052612, 'train@rus.rst.rrt_accuracy@rus.rst.rrt': 0.5389979876483242, 'train@rus.rst.rrt_f1@rus.rst.rrt': 0.22272451618119735, 'train@rus.rst.rrt_precision@rus.rst.rrt': 0.3169184165845491, 'train@rus.rst.rrt_recall@rus.rst.rrt': 0.22945847193052255, 'train@rus.rst.rrt_loss@rus.rst.rrt': 1.5179413557052612, 'train@rus.rst.rrt_runtime': 350.39, 'train@rus.rst.rrt_samples_per_second': 82.257, 'train@rus.rst.rrt_steps_per_second': 2.571, 'epoch': 2.0}
{'loss': 1.6646, 'learning_rate': 1.6666666666666667e-05, 'epoch': 2.0}
{'eval_loss': 1.5839450359344482, 'eval_accuracy@rus.rst.rrt': 0.5166374781085814, 'eval_f1@rus.rst.rrt': 0.24293821821723718, 'eval_precision@rus.rst.rrt': 0.2868064309680404, 'eval_recall@rus.rst.rrt': 0.2526924647002904, 'eval_loss@rus.rst.rrt': 1.5839450359344482, 'eval_runtime': 35.0535, 'eval_samples_per_second': 81.447, 'eval_steps_per_second': 2.568, 'epoch': 2.0}
{'train@rus.rst.rrt_loss': 1.4355140924453735, 'train@rus.rst.rrt_accuracy@rus.rst.rrt': 0.5626257719797377, 'train@rus.rst.rrt_f1@rus.rst.rrt': 0.27489632423476534, 'train@rus.rst.rrt_precision@rus.rst.rrt': 0.3976929596574893, 'train@rus.rst.rrt_recall@rus.rst.rrt': 0.27162898420512355, 'train@rus.rst.rrt_loss@rus.rst.rrt': 1.4355140924453735, 'train@rus.rst.rrt_runtime': 350.6294, 'train@rus.rst.rrt_samples_per_second': 82.201, 'train@rus.rst.rrt_steps_per_second': 2.57, 'epoch': 3.0}
{'loss': 1.5246, 'learning_rate': 1.5000000000000002e-05, 'epoch': 3.0}
{'eval_loss': 1.5095754861831665, 'eval_accuracy@rus.rst.rrt': 0.5387040280210158, 'eval_f1@rus.rst.rrt': 0.3016566088719363, 'eval_precision@rus.rst.rrt': 0.4113870722324769, 'eval_recall@rus.rst.rrt': 0.29906210074165623, 'eval_loss@rus.rst.rrt': 1.5095754861831665, 'eval_runtime': 35.0839, 'eval_samples_per_second': 81.376, 'eval_steps_per_second': 2.565, 'epoch': 3.0}
{'train@rus.rst.rrt_loss': 1.3791292905807495, 'train@rus.rst.rrt_accuracy@rus.rst.rrt': 0.580147109846645, 'train@rus.rst.rrt_f1@rus.rst.rrt': 0.312802576793576, 'train@rus.rst.rrt_precision@rus.rst.rrt': 0.4344645794792916, 'train@rus.rst.rrt_recall@rus.rst.rrt': 0.29574003299671714, 'train@rus.rst.rrt_loss@rus.rst.rrt': 1.379129409790039, 'train@rus.rst.rrt_runtime': 350.4035, 'train@rus.rst.rrt_samples_per_second': 82.254, 'train@rus.rst.rrt_steps_per_second': 2.571, 'epoch': 4.0}
{'loss': 1.4588, 'learning_rate': 1.3333333333333333e-05, 'epoch': 4.0}
{'eval_loss': 1.4563214778900146, 'eval_accuracy@rus.rst.rrt': 0.5551663747810858, 'eval_f1@rus.rst.rrt': 0.3566953052722847, 'eval_precision@rus.rst.rrt': 0.4952189908940532, 'eval_recall@rus.rst.rrt': 0.3356574277072042, 'eval_loss@rus.rst.rrt': 1.4563214778900146, 'eval_runtime': 35.0918, 'eval_samples_per_second': 81.358, 'eval_steps_per_second': 2.565, 'epoch': 4.0}
{'train@rus.rst.rrt_loss': 1.3449974060058594, 'train@rus.rst.rrt_accuracy@rus.rst.rrt': 0.589376170980501, 'train@rus.rst.rrt_f1@rus.rst.rrt': 0.32768178092869654, 'train@rus.rst.rrt_precision@rus.rst.rrt': 0.43653135023087036, 'train@rus.rst.rrt_recall@rus.rst.rrt': 0.3102315064342115, 'train@rus.rst.rrt_loss@rus.rst.rrt': 1.3449972867965698, 'train@rus.rst.rrt_runtime': 350.4483, 'train@rus.rst.rrt_samples_per_second': 82.243, 'train@rus.rst.rrt_steps_per_second': 2.571, 'epoch': 5.0}
{'loss': 1.4185, 'learning_rate': 1.1666666666666668e-05, 'epoch': 5.0}
{'eval_loss': 1.4309500455856323, 'eval_accuracy@rus.rst.rrt': 0.559369527145359, 'eval_f1@rus.rst.rrt': 0.37551033873637685, 'eval_precision@rus.rst.rrt': 0.4857388845002903, 'eval_recall@rus.rst.rrt': 0.3578310531554272, 'eval_loss@rus.rst.rrt': 1.4309500455856323, 'eval_runtime': 35.0822, 'eval_samples_per_second': 81.38, 'eval_steps_per_second': 2.565, 'epoch': 5.0}
{'train@rus.rst.rrt_loss': 1.320406198501587, 'train@rus.rst.rrt_accuracy@rus.rst.rrt': 0.594927485948234, 'train@rus.rst.rrt_f1@rus.rst.rrt': 0.3401537153148246, 'train@rus.rst.rrt_precision@rus.rst.rrt': 0.49044561071011433, 'train@rus.rst.rrt_recall@rus.rst.rrt': 0.3210635295351574, 'train@rus.rst.rrt_loss@rus.rst.rrt': 1.3204058408737183, 'train@rus.rst.rrt_runtime': 350.7898, 'train@rus.rst.rrt_samples_per_second': 82.163, 'train@rus.rst.rrt_steps_per_second': 2.568, 'epoch': 6.0}
{'loss': 1.3875, 'learning_rate': 1e-05, 'epoch': 6.0}
{'eval_loss': 1.4077507257461548, 'eval_accuracy@rus.rst.rrt': 0.5635726795096322, 'eval_f1@rus.rst.rrt': 0.3841525194064604, 'eval_precision@rus.rst.rrt': 0.4743359205991557, 'eval_recall@rus.rst.rrt': 0.36846567176357253, 'eval_loss@rus.rst.rrt': 1.4077507257461548, 'eval_runtime': 35.0955, 'eval_samples_per_second': 81.349, 'eval_steps_per_second': 2.564, 'epoch': 6.0}
{'train@rus.rst.rrt_loss': 1.304408311843872, 'train@rus.rst.rrt_accuracy@rus.rst.rrt': 0.5983970578030671, 'train@rus.rst.rrt_f1@rus.rst.rrt': 0.3485242712261306, 'train@rus.rst.rrt_precision@rus.rst.rrt': 0.4792404665986785, 'train@rus.rst.rrt_recall@rus.rst.rrt': 0.33025171117402413, 'train@rus.rst.rrt_loss@rus.rst.rrt': 1.3044081926345825, 'train@rus.rst.rrt_runtime': 350.4421, 'train@rus.rst.rrt_samples_per_second': 82.245, 'train@rus.rst.rrt_steps_per_second': 2.571, 'epoch': 7.0}
{'loss': 1.3653, 'learning_rate': 8.333333333333334e-06, 'epoch': 7.0}
{'eval_loss': 1.39828360080719, 'eval_accuracy@rus.rst.rrt': 0.5677758318739055, 'eval_f1@rus.rst.rrt': 0.39333794607148975, 'eval_precision@rus.rst.rrt': 0.4554668600110405, 'eval_recall@rus.rst.rrt': 0.38115385420062753, 'eval_loss@rus.rst.rrt': 1.3982837200164795, 'eval_runtime': 35.0488, 'eval_samples_per_second': 81.458, 'eval_steps_per_second': 2.568, 'epoch': 7.0}
{'train@rus.rst.rrt_loss': 1.290930151939392, 'train@rus.rst.rrt_accuracy@rus.rst.rrt': 0.6036361113038651, 'train@rus.rst.rrt_f1@rus.rst.rrt': 0.3573741298813035, 'train@rus.rst.rrt_precision@rus.rst.rrt': 0.4584587054088971, 'train@rus.rst.rrt_recall@rus.rst.rrt': 0.34068924388323274, 'train@rus.rst.rrt_loss@rus.rst.rrt': 1.2909302711486816, 'train@rus.rst.rrt_runtime': 350.2649, 'train@rus.rst.rrt_samples_per_second': 82.286, 'train@rus.rst.rrt_steps_per_second': 2.572, 'epoch': 8.0}
{'loss': 1.3477, 'learning_rate': 6.666666666666667e-06, 'epoch': 8.0}
{'eval_loss': 1.3862415552139282, 'eval_accuracy@rus.rst.rrt': 0.5740805604203152, 'eval_f1@rus.rst.rrt': 0.40634919152883964, 'eval_precision@rus.rst.rrt': 0.45921042971764897, 'eval_recall@rus.rst.rrt': 0.39465005442105333, 'eval_loss@rus.rst.rrt': 1.3862415552139282, 'eval_runtime': 35.1296, 'eval_samples_per_second': 81.271, 'eval_steps_per_second': 2.562, 'epoch': 8.0}
{'train@rus.rst.rrt_loss': 1.2782032489776611, 'train@rus.rst.rrt_accuracy@rus.rst.rrt': 0.6058913330095066, 'train@rus.rst.rrt_f1@rus.rst.rrt': 0.36191852145785136, 'train@rus.rst.rrt_precision@rus.rst.rrt': 0.46316634537653417, 'train@rus.rst.rrt_recall@rus.rst.rrt': 0.34341943168225547, 'train@rus.rst.rrt_loss@rus.rst.rrt': 1.2782033681869507, 'train@rus.rst.rrt_runtime': 350.1484, 'train@rus.rst.rrt_samples_per_second': 82.314, 'train@rus.rst.rrt_steps_per_second': 2.573, 'epoch': 9.0}
{'loss': 1.3384, 'learning_rate': 5e-06, 'epoch': 9.0}
{'eval_loss': 1.3781923055648804, 'eval_accuracy@rus.rst.rrt': 0.5733800350262697, 'eval_f1@rus.rst.rrt': 0.407308916933401, 'eval_precision@rus.rst.rrt': 0.4685552891884776, 'eval_recall@rus.rst.rrt': 0.39252343940580436, 'eval_loss@rus.rst.rrt': 1.3781923055648804, 'eval_runtime': 35.0689, 'eval_samples_per_second': 81.411, 'eval_steps_per_second': 2.566, 'epoch': 9.0}
{'train@rus.rst.rrt_loss': 1.2715766429901123, 'train@rus.rst.rrt_accuracy@rus.rst.rrt': 0.6083200333078898, 'train@rus.rst.rrt_f1@rus.rst.rrt': 0.36358868138990663, 'train@rus.rst.rrt_precision@rus.rst.rrt': 0.4639718602490767, 'train@rus.rst.rrt_recall@rus.rst.rrt': 0.3423255795084023, 'train@rus.rst.rrt_loss@rus.rst.rrt': 1.2715766429901123, 'train@rus.rst.rrt_runtime': 350.1305, 'train@rus.rst.rrt_samples_per_second': 82.318, 'train@rus.rst.rrt_steps_per_second': 2.573, 'epoch': 10.0}
{'loss': 1.3286, 'learning_rate': 3.3333333333333333e-06, 'epoch': 10.0}
{'eval_loss': 1.3748791217803955, 'eval_accuracy@rus.rst.rrt': 0.5768826619964974, 'eval_f1@rus.rst.rrt': 0.41092844520697863, 'eval_precision@rus.rst.rrt': 0.47604447944854433, 'eval_recall@rus.rst.rrt': 0.39273284377619067, 'eval_loss@rus.rst.rrt': 1.3748791217803955, 'eval_runtime': 35.0446, 'eval_samples_per_second': 81.468, 'eval_steps_per_second': 2.568, 'epoch': 10.0}
{'train@rus.rst.rrt_loss': 1.2673470973968506, 'train@rus.rst.rrt_accuracy@rus.rst.rrt': 0.6079383804038582, 'train@rus.rst.rrt_f1@rus.rst.rrt': 0.3669973838350233, 'train@rus.rst.rrt_precision@rus.rst.rrt': 0.46078761452424266, 'train@rus.rst.rrt_recall@rus.rst.rrt': 0.3476660115036213, 'train@rus.rst.rrt_loss@rus.rst.rrt': 1.2673470973968506, 'train@rus.rst.rrt_runtime': 350.0781, 'train@rus.rst.rrt_samples_per_second': 82.33, 'train@rus.rst.rrt_steps_per_second': 2.574, 'epoch': 11.0}
{'loss': 1.3195, 'learning_rate': 1.6666666666666667e-06, 'epoch': 11.0}
{'eval_loss': 1.3740442991256714, 'eval_accuracy@rus.rst.rrt': 0.5737302977232924, 'eval_f1@rus.rst.rrt': 0.4178847895142531, 'eval_precision@rus.rst.rrt': 0.5097113684827147, 'eval_recall@rus.rst.rrt': 0.3997896784845628, 'eval_loss@rus.rst.rrt': 1.3740441799163818, 'eval_runtime': 35.5389, 'eval_samples_per_second': 80.334, 'eval_steps_per_second': 2.532, 'epoch': 11.0}
{'train@rus.rst.rrt_loss': 1.2661150693893433, 'train@rus.rst.rrt_accuracy@rus.rst.rrt': 0.6092221219901465, 'train@rus.rst.rrt_f1@rus.rst.rrt': 0.3661337035238985, 'train@rus.rst.rrt_precision@rus.rst.rrt': 0.45900019629624106, 'train@rus.rst.rrt_recall@rus.rst.rrt': 0.3471291453295637, 'train@rus.rst.rrt_loss@rus.rst.rrt': 1.2661150693893433, 'train@rus.rst.rrt_runtime': 350.3393, 'train@rus.rst.rrt_samples_per_second': 82.269, 'train@rus.rst.rrt_steps_per_second': 2.572, 'epoch': 12.0}
{'loss': 1.3169, 'learning_rate': 0.0, 'epoch': 12.0}
{'eval_loss': 1.3726017475128174, 'eval_accuracy@rus.rst.rrt': 0.5761821366024519, 'eval_f1@rus.rst.rrt': 0.41403523677213705, 'eval_precision@rus.rst.rrt': 0.4723004654568812, 'eval_recall@rus.rst.rrt': 0.39914752595435543, 'eval_loss@rus.rst.rrt': 1.3726017475128174, 'eval_runtime': 35.0697, 'eval_samples_per_second': 81.409, 'eval_steps_per_second': 2.566, 'epoch': 12.0}
{'train_runtime': 13497.0459, 'train_samples_per_second': 25.625, 'train_steps_per_second': 0.801, 'train_loss': 1.4760332463892838, 'epoch': 12.0}
***** train_log metrics *****
  epoch                    =       12.0
  train_loss               =      1.476
  train_runtime            = 3:44:57.04
  train_samples_per_second =     25.625
  train_steps_per_second   =      0.801
{'train@tur.pdtb.tdb_loss': 2.599125623703003, 'train@tur.pdtb.tdb_accuracy@tur.pdtb.tdb': 0.2460220318237454, 'train@tur.pdtb.tdb_f1@tur.pdtb.tdb': 0.0220501334105468, 'train@tur.pdtb.tdb_precision@tur.pdtb.tdb': 0.021909808415023817, 'train@tur.pdtb.tdb_recall@tur.pdtb.tdb': 0.0441767779618443, 'train@tur.pdtb.tdb_loss@tur.pdtb.tdb': 2.599125623703003, 'train@tur.pdtb.tdb_runtime': 30.1018, 'train@tur.pdtb.tdb_samples_per_second': 81.424, 'train@tur.pdtb.tdb_steps_per_second': 2.558, 'epoch': 1.0}
{'loss': 2.9362, 'learning_rate': 1.8333333333333333e-05, 'epoch': 1.0}
{'eval_loss': 2.543849468231201, 'eval_accuracy@tur.pdtb.tdb': 0.26282051282051283, 'eval_f1@tur.pdtb.tdb': 0.01961722488038278, 'eval_precision@tur.pdtb.tdb': 0.012549739822467097, 'eval_recall@tur.pdtb.tdb': 0.044906900328587074, 'eval_loss@tur.pdtb.tdb': 2.543849468231201, 'eval_runtime': 4.1809, 'eval_samples_per_second': 74.626, 'eval_steps_per_second': 2.392, 'epoch': 1.0}
{'train@tur.pdtb.tdb_loss': 2.3027279376983643, 'train@tur.pdtb.tdb_accuracy@tur.pdtb.tdb': 0.26764585883312936, 'train@tur.pdtb.tdb_f1@tur.pdtb.tdb': 0.04221310136551375, 'train@tur.pdtb.tdb_precision@tur.pdtb.tdb': 0.05871492491886706, 'train@tur.pdtb.tdb_recall@tur.pdtb.tdb': 0.05700616997001706, 'train@tur.pdtb.tdb_loss@tur.pdtb.tdb': 2.3027281761169434, 'train@tur.pdtb.tdb_runtime': 30.0908, 'train@tur.pdtb.tdb_samples_per_second': 81.453, 'train@tur.pdtb.tdb_steps_per_second': 2.559, 'epoch': 2.0}
{'loss': 2.4561, 'learning_rate': 1.6666666666666667e-05, 'epoch': 2.0}
{'eval_loss': 2.251685619354248, 'eval_accuracy@tur.pdtb.tdb': 0.2692307692307692, 'eval_f1@tur.pdtb.tdb': 0.03319196994269338, 'eval_precision@tur.pdtb.tdb': 0.04065369493333555, 'eval_recall@tur.pdtb.tdb': 0.051884820110127745, 'eval_loss@tur.pdtb.tdb': 2.251685619354248, 'eval_runtime': 4.1864, 'eval_samples_per_second': 74.527, 'eval_steps_per_second': 2.389, 'epoch': 2.0}
{'train@tur.pdtb.tdb_loss': 2.1584508419036865, 'train@tur.pdtb.tdb_accuracy@tur.pdtb.tdb': 0.3263973888208894, 'train@tur.pdtb.tdb_f1@tur.pdtb.tdb': 0.07891131720441283, 'train@tur.pdtb.tdb_precision@tur.pdtb.tdb': 0.09300042642155605, 'train@tur.pdtb.tdb_recall@tur.pdtb.tdb': 0.08668335500312752, 'train@tur.pdtb.tdb_loss@tur.pdtb.tdb': 2.1584508419036865, 'train@tur.pdtb.tdb_runtime': 30.0325, 'train@tur.pdtb.tdb_samples_per_second': 81.612, 'train@tur.pdtb.tdb_steps_per_second': 2.564, 'epoch': 3.0}
{'loss': 2.2738, 'learning_rate': 1.5000000000000002e-05, 'epoch': 3.0}
{'eval_loss': 2.1720705032348633, 'eval_accuracy@tur.pdtb.tdb': 0.2980769230769231, 'eval_f1@tur.pdtb.tdb': 0.06416838259402524, 'eval_precision@tur.pdtb.tdb': 0.09802426043612823, 'eval_recall@tur.pdtb.tdb': 0.06960593036190474, 'eval_loss@tur.pdtb.tdb': 2.1720707416534424, 'eval_runtime': 4.181, 'eval_samples_per_second': 74.624, 'eval_steps_per_second': 2.392, 'epoch': 3.0}
{'train@tur.pdtb.tdb_loss': 2.0394387245178223, 'train@tur.pdtb.tdb_accuracy@tur.pdtb.tdb': 0.37821297429620565, 'train@tur.pdtb.tdb_f1@tur.pdtb.tdb': 0.12196447419755882, 'train@tur.pdtb.tdb_precision@tur.pdtb.tdb': 0.14003392162333334, 'train@tur.pdtb.tdb_recall@tur.pdtb.tdb': 0.12429564071961226, 'train@tur.pdtb.tdb_loss@tur.pdtb.tdb': 2.0394387245178223, 'train@tur.pdtb.tdb_runtime': 30.0699, 'train@tur.pdtb.tdb_samples_per_second': 81.51, 'train@tur.pdtb.tdb_steps_per_second': 2.561, 'epoch': 4.0}
{'loss': 2.1553, 'learning_rate': 1.3333333333333333e-05, 'epoch': 4.0}
{'eval_loss': 2.10711669921875, 'eval_accuracy@tur.pdtb.tdb': 0.3141025641025641, 'eval_f1@tur.pdtb.tdb': 0.0851277364947905, 'eval_precision@tur.pdtb.tdb': 0.13788562774956395, 'eval_recall@tur.pdtb.tdb': 0.0856087897063938, 'eval_loss@tur.pdtb.tdb': 2.10711669921875, 'eval_runtime': 4.1747, 'eval_samples_per_second': 74.737, 'eval_steps_per_second': 2.395, 'epoch': 4.0}
{'train@tur.pdtb.tdb_loss': 1.9481353759765625, 'train@tur.pdtb.tdb_accuracy@tur.pdtb.tdb': 0.408812729498164, 'train@tur.pdtb.tdb_f1@tur.pdtb.tdb': 0.1457073686129722, 'train@tur.pdtb.tdb_precision@tur.pdtb.tdb': 0.19107199598662855, 'train@tur.pdtb.tdb_recall@tur.pdtb.tdb': 0.14842627702359262, 'train@tur.pdtb.tdb_loss@tur.pdtb.tdb': 1.9481353759765625, 'train@tur.pdtb.tdb_runtime': 30.096, 'train@tur.pdtb.tdb_samples_per_second': 81.439, 'train@tur.pdtb.tdb_steps_per_second': 2.558, 'epoch': 5.0}
{'loss': 2.0631, 'learning_rate': 1.1666666666666668e-05, 'epoch': 5.0}
{'eval_loss': 2.055806875228882, 'eval_accuracy@tur.pdtb.tdb': 0.3525641025641026, 'eval_f1@tur.pdtb.tdb': 0.12561641337183757, 'eval_precision@tur.pdtb.tdb': 0.1374723558786795, 'eval_recall@tur.pdtb.tdb': 0.13221749064982496, 'eval_loss@tur.pdtb.tdb': 2.0558066368103027, 'eval_runtime': 4.1921, 'eval_samples_per_second': 74.425, 'eval_steps_per_second': 2.385, 'epoch': 5.0}
{'train@tur.pdtb.tdb_loss': 1.8771820068359375, 'train@tur.pdtb.tdb_accuracy@tur.pdtb.tdb': 0.4292125662994696, 'train@tur.pdtb.tdb_f1@tur.pdtb.tdb': 0.17278237945516564, 'train@tur.pdtb.tdb_precision@tur.pdtb.tdb': 0.20475104588645443, 'train@tur.pdtb.tdb_recall@tur.pdtb.tdb': 0.17105289670367546, 'train@tur.pdtb.tdb_loss@tur.pdtb.tdb': 1.8771817684173584, 'train@tur.pdtb.tdb_runtime': 30.0629, 'train@tur.pdtb.tdb_samples_per_second': 81.529, 'train@tur.pdtb.tdb_steps_per_second': 2.561, 'epoch': 6.0}
{'loss': 1.9852, 'learning_rate': 1e-05, 'epoch': 6.0}
{'eval_loss': 2.0119805335998535, 'eval_accuracy@tur.pdtb.tdb': 0.3717948717948718, 'eval_f1@tur.pdtb.tdb': 0.1322284613820664, 'eval_precision@tur.pdtb.tdb': 0.13969234721347884, 'eval_recall@tur.pdtb.tdb': 0.13939282455150157, 'eval_loss@tur.pdtb.tdb': 2.0119805335998535, 'eval_runtime': 4.174, 'eval_samples_per_second': 74.749, 'eval_steps_per_second': 2.396, 'epoch': 6.0}
{'train@tur.pdtb.tdb_loss': 1.829126238822937, 'train@tur.pdtb.tdb_accuracy@tur.pdtb.tdb': 0.4541003671970624, 'train@tur.pdtb.tdb_f1@tur.pdtb.tdb': 0.19079907947131255, 'train@tur.pdtb.tdb_precision@tur.pdtb.tdb': 0.202592622040407, 'train@tur.pdtb.tdb_recall@tur.pdtb.tdb': 0.1951151866903766, 'train@tur.pdtb.tdb_loss@tur.pdtb.tdb': 1.8291261196136475, 'train@tur.pdtb.tdb_runtime': 30.0904, 'train@tur.pdtb.tdb_samples_per_second': 81.454, 'train@tur.pdtb.tdb_steps_per_second': 2.559, 'epoch': 7.0}
{'loss': 1.9357, 'learning_rate': 8.333333333333334e-06, 'epoch': 7.0}
{'eval_loss': 1.9823158979415894, 'eval_accuracy@tur.pdtb.tdb': 0.3974358974358974, 'eval_f1@tur.pdtb.tdb': 0.1435780065697114, 'eval_precision@tur.pdtb.tdb': 0.14946400091136935, 'eval_recall@tur.pdtb.tdb': 0.1512208854582301, 'eval_loss@tur.pdtb.tdb': 1.9823156595230103, 'eval_runtime': 4.1968, 'eval_samples_per_second': 74.342, 'eval_steps_per_second': 2.383, 'epoch': 7.0}
{'train@tur.pdtb.tdb_loss': 1.7924439907073975, 'train@tur.pdtb.tdb_accuracy@tur.pdtb.tdb': 0.463484292125663, 'train@tur.pdtb.tdb_f1@tur.pdtb.tdb': 0.20017072951032536, 'train@tur.pdtb.tdb_precision@tur.pdtb.tdb': 0.2108444032607808, 'train@tur.pdtb.tdb_recall@tur.pdtb.tdb': 0.20677335629982735, 'train@tur.pdtb.tdb_loss@tur.pdtb.tdb': 1.7924439907073975, 'train@tur.pdtb.tdb_runtime': 30.1298, 'train@tur.pdtb.tdb_samples_per_second': 81.348, 'train@tur.pdtb.tdb_steps_per_second': 2.556, 'epoch': 8.0}
{'loss': 1.9015, 'learning_rate': 6.666666666666667e-06, 'epoch': 8.0}
{'eval_loss': 1.9610118865966797, 'eval_accuracy@tur.pdtb.tdb': 0.3974358974358974, 'eval_f1@tur.pdtb.tdb': 0.16734973167588355, 'eval_precision@tur.pdtb.tdb': 0.1951868096253123, 'eval_recall@tur.pdtb.tdb': 0.16653973662045474, 'eval_loss@tur.pdtb.tdb': 1.9610120058059692, 'eval_runtime': 4.212, 'eval_samples_per_second': 74.075, 'eval_steps_per_second': 2.374, 'epoch': 8.0}
{'train@tur.pdtb.tdb_loss': 1.7680672407150269, 'train@tur.pdtb.tdb_accuracy@tur.pdtb.tdb': 0.46878824969400246, 'train@tur.pdtb.tdb_f1@tur.pdtb.tdb': 0.20216254522866087, 'train@tur.pdtb.tdb_precision@tur.pdtb.tdb': 0.21201474212615218, 'train@tur.pdtb.tdb_recall@tur.pdtb.tdb': 0.20931202181764863, 'train@tur.pdtb.tdb_loss@tur.pdtb.tdb': 1.7680672407150269, 'train@tur.pdtb.tdb_runtime': 30.0979, 'train@tur.pdtb.tdb_samples_per_second': 81.434, 'train@tur.pdtb.tdb_steps_per_second': 2.558, 'epoch': 9.0}
{'loss': 1.8628, 'learning_rate': 5e-06, 'epoch': 9.0}
{'eval_loss': 1.954396367073059, 'eval_accuracy@tur.pdtb.tdb': 0.41025641025641024, 'eval_f1@tur.pdtb.tdb': 0.18228909354136216, 'eval_precision@tur.pdtb.tdb': 0.19950573748420639, 'eval_recall@tur.pdtb.tdb': 0.1825449338800815, 'eval_loss@tur.pdtb.tdb': 1.954396367073059, 'eval_runtime': 4.1775, 'eval_samples_per_second': 74.686, 'eval_steps_per_second': 2.394, 'epoch': 9.0}
{'train@tur.pdtb.tdb_loss': 1.750009536743164, 'train@tur.pdtb.tdb_accuracy@tur.pdtb.tdb': 0.47123623011015914, 'train@tur.pdtb.tdb_f1@tur.pdtb.tdb': 0.2028306579059049, 'train@tur.pdtb.tdb_precision@tur.pdtb.tdb': 0.20740337426299132, 'train@tur.pdtb.tdb_recall@tur.pdtb.tdb': 0.21258159605756502, 'train@tur.pdtb.tdb_loss@tur.pdtb.tdb': 1.750009536743164, 'train@tur.pdtb.tdb_runtime': 30.0878, 'train@tur.pdtb.tdb_samples_per_second': 81.462, 'train@tur.pdtb.tdb_steps_per_second': 2.559, 'epoch': 10.0}
{'loss': 1.8416, 'learning_rate': 3.3333333333333333e-06, 'epoch': 10.0}
{'eval_loss': 1.9407172203063965, 'eval_accuracy@tur.pdtb.tdb': 0.3974358974358974, 'eval_f1@tur.pdtb.tdb': 0.1783956495504997, 'eval_precision@tur.pdtb.tdb': 0.19314466395674998, 'eval_recall@tur.pdtb.tdb': 0.1787758468367209, 'eval_loss@tur.pdtb.tdb': 1.940717101097107, 'eval_runtime': 4.1738, 'eval_samples_per_second': 74.752, 'eval_steps_per_second': 2.396, 'epoch': 10.0}
{'train@tur.pdtb.tdb_loss': 1.7407641410827637, 'train@tur.pdtb.tdb_accuracy@tur.pdtb.tdb': 0.4728682170542636, 'train@tur.pdtb.tdb_f1@tur.pdtb.tdb': 0.20530750397354233, 'train@tur.pdtb.tdb_precision@tur.pdtb.tdb': 0.21412085498932368, 'train@tur.pdtb.tdb_recall@tur.pdtb.tdb': 0.21414438542657155, 'train@tur.pdtb.tdb_loss@tur.pdtb.tdb': 1.7407641410827637, 'train@tur.pdtb.tdb_runtime': 30.092, 'train@tur.pdtb.tdb_samples_per_second': 81.45, 'train@tur.pdtb.tdb_steps_per_second': 2.559, 'epoch': 11.0}
{'loss': 1.8321, 'learning_rate': 1.6666666666666667e-06, 'epoch': 11.0}
{'eval_loss': 1.9367120265960693, 'eval_accuracy@tur.pdtb.tdb': 0.3974358974358974, 'eval_f1@tur.pdtb.tdb': 0.17144070036832595, 'eval_precision@tur.pdtb.tdb': 0.17702232858852074, 'eval_recall@tur.pdtb.tdb': 0.17806086570005297, 'eval_loss@tur.pdtb.tdb': 1.9367122650146484, 'eval_runtime': 4.1873, 'eval_samples_per_second': 74.511, 'eval_steps_per_second': 2.388, 'epoch': 11.0}
{'train@tur.pdtb.tdb_loss': 1.7370972633361816, 'train@tur.pdtb.tdb_accuracy@tur.pdtb.tdb': 0.4749082007343941, 'train@tur.pdtb.tdb_f1@tur.pdtb.tdb': 0.20663218704967684, 'train@tur.pdtb.tdb_precision@tur.pdtb.tdb': 0.21456292881674255, 'train@tur.pdtb.tdb_recall@tur.pdtb.tdb': 0.21595651266543778, 'train@tur.pdtb.tdb_loss@tur.pdtb.tdb': 1.7370972633361816, 'train@tur.pdtb.tdb_runtime': 30.0716, 'train@tur.pdtb.tdb_samples_per_second': 81.506, 'train@tur.pdtb.tdb_steps_per_second': 2.561, 'epoch': 12.0}
{'loss': 1.8227, 'learning_rate': 0.0, 'epoch': 12.0}
{'eval_loss': 1.9329614639282227, 'eval_accuracy@tur.pdtb.tdb': 0.3942307692307692, 'eval_f1@tur.pdtb.tdb': 0.17164481919456123, 'eval_precision@tur.pdtb.tdb': 0.17604622961854213, 'eval_recall@tur.pdtb.tdb': 0.1782282017107625, 'eval_loss@tur.pdtb.tdb': 1.9329614639282227, 'eval_runtime': 4.1651, 'eval_samples_per_second': 74.908, 'eval_steps_per_second': 2.401, 'epoch': 12.0}
{'train_runtime': 1164.6096, 'train_samples_per_second': 25.255, 'train_steps_per_second': 0.793, 'train_loss': 2.0888379414876304, 'epoch': 12.0}
***** train_logger metrics *****
  epoch                    =       12.0
  train_loss               =      1.476
  train_runtime            = 3:44:57.04
  train_samples_per_second =     25.625
  train_steps_per_second   =      0.801
-------------------------------------------------------------------
Lang1:  spa.rst.rststb    Lang2:  tur.pdtb.tdb
Saving run to:  runs/full_shot/FullShot=v4_spa.rst.rststb_tur.pdtb.tdb_lrfunc=Adafactor_lr=2e-05_plm=bert-base-multilingual-cased
Running with params: BERT_MODEL=bert-base-multilingual-cased lr=2e-05
read 2240 examples
read 383 examples
read 426 examples
read 2451 examples
read 312 examples
read 422 examples
Total prediction labels:  52
BertAdapterModel(
  (shared_parameters): ModuleDict()
  (bert): BertModel(
    (shared_parameters): ModuleDict()
    (invertible_adapters): ModuleDict(
      (es): NICECouplingBlock(
        (F): Sequential(
          (0): Linear(in_features=384, out_features=192, bias=True)
          (1): Activation_Function_Class(
            (f): NewGELUActivation()
          )
          (2): Linear(in_features=192, out_features=384, bias=True)
        )
        (G): Sequential(
          (0): Linear(in_features=384, out_features=192, bias=True)
          (1): Activation_Function_Class(
            (f): NewGELUActivation()
          )
          (2): Linear(in_features=192, out_features=384, bias=True)
        )
      )
      (tr): NICECouplingBlock(
        (F): Sequential(
          (0): Linear(in_features=384, out_features=192, bias=True)
          (1): Activation_Function_Class(
            (f): NewGELUActivation()
          )
          (2): Linear(in_features=192, out_features=384, bias=True)
        )
        (G): Sequential(
          (0): Linear(in_features=384, out_features=192, bias=True)
          (1): Activation_Function_Class(
            (f): NewGELUActivation()
          )
          (2): Linear(in_features=192, out_features=384, bias=True)
        )
      )
    )
    (embeddings): BertEmbeddings(
      (word_embeddings): Embedding(119547, 768, padding_idx=0)
      (position_embeddings): Embedding(512, 768)
      (token_type_embeddings): Embedding(2, 768)
      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
      (dropout): Dropout(p=0.1, inplace=False)
    )
    (encoder): BertEncoder(
      (layer): ModuleList(
        (0): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (es): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (tr): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (1): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (es): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (tr): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (2): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (es): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (tr): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (3): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (es): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (tr): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (4): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (es): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (tr): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (5): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (es): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (tr): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (6): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (es): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (tr): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (7): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (es): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (tr): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (8): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (es): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (tr): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (9): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (es): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (tr): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (10): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (es): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (tr): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (11): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (es): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (tr): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
      )
    )
    (pooler): BertPooler(
      (dense): Linear(in_features=768, out_features=768, bias=True)
      (activation): Tanh()
    )
    (prefix_tuning): PrefixTuningPool(
      (prefix_tunings): ModuleDict()
    )
  )
  (heads): ModuleDict(
    (tr): BertStyleMaskedLMHead(
      (0): Linear(in_features=768, out_features=768, bias=True)
      (1): Activation_Function_Class(
        (f): GELUActivation()
      )
      (2): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
      (3): Linear(in_features=768, out_features=119547, bias=True)
    )
    (disrpt): ClassificationHead(
      (0): Dropout(p=0.1, inplace=False)
      (1): Linear(in_features=768, out_features=768, bias=True)
      (2): Activation_Function_Class(
        (f): Tanh()
      )
      (3): Dropout(p=0.1, inplace=False)
      (4): Linear(in_features=768, out_features=52, bias=True)
    )
  )
)
{'train@spa.rst.rststb_loss': 3.043095111846924, 'train@spa.rst.rststb_accuracy@spa.rst.rststb': 0.21607142857142858, 'train@spa.rst.rststb_f1@spa.rst.rststb': 0.019951934215239744, 'train@spa.rst.rststb_precision@spa.rst.rststb': 0.04891581455454567, 'train@spa.rst.rststb_recall@spa.rst.rststb': 0.03961319595876538, 'train@spa.rst.rststb_loss@spa.rst.rststb': 3.0430948734283447, 'train@spa.rst.rststb_runtime': 27.5376, 'train@spa.rst.rststb_samples_per_second': 81.343, 'train@spa.rst.rststb_steps_per_second': 2.542, 'epoch': 1.0}
{'loss': 3.5166, 'learning_rate': 1.8333333333333333e-05, 'epoch': 1.0}
{'eval_loss': 3.08113431930542, 'eval_accuracy@spa.rst.rststb': 0.20365535248041775, 'eval_f1@spa.rst.rststb': 0.015924853885361256, 'eval_precision@spa.rst.rststb': 0.013782324524012502, 'eval_recall@spa.rst.rststb': 0.043026474671222864, 'eval_loss@spa.rst.rststb': 3.08113431930542, 'eval_runtime': 5.1202, 'eval_samples_per_second': 74.802, 'eval_steps_per_second': 2.344, 'epoch': 1.0}
{'train@spa.rst.rststb_loss': 2.6698224544525146, 'train@spa.rst.rststb_accuracy@spa.rst.rststb': 0.2629464285714286, 'train@spa.rst.rststb_f1@spa.rst.rststb': 0.037527519228917276, 'train@spa.rst.rststb_precision@spa.rst.rststb': 0.04941184638032164, 'train@spa.rst.rststb_recall@spa.rst.rststb': 0.052693868721441516, 'train@spa.rst.rststb_loss@spa.rst.rststb': 2.6698222160339355, 'train@spa.rst.rststb_runtime': 27.6199, 'train@spa.rst.rststb_samples_per_second': 81.101, 'train@spa.rst.rststb_steps_per_second': 2.534, 'epoch': 2.0}
{'loss': 2.8541, 'learning_rate': 1.6666666666666667e-05, 'epoch': 2.0}
{'eval_loss': 2.775019407272339, 'eval_accuracy@spa.rst.rststb': 0.22193211488250653, 'eval_f1@spa.rst.rststb': 0.03219880291567074, 'eval_precision@spa.rst.rststb': 0.05329388065887252, 'eval_recall@spa.rst.rststb': 0.05101618290694922, 'eval_loss@spa.rst.rststb': 2.775019407272339, 'eval_runtime': 5.0798, 'eval_samples_per_second': 75.397, 'eval_steps_per_second': 2.362, 'epoch': 2.0}
{'train@spa.rst.rststb_loss': 2.524245023727417, 'train@spa.rst.rststb_accuracy@spa.rst.rststb': 0.321875, 'train@spa.rst.rststb_f1@spa.rst.rststb': 0.04819887667403903, 'train@spa.rst.rststb_precision@spa.rst.rststb': 0.039878671415206464, 'train@spa.rst.rststb_recall@spa.rst.rststb': 0.06980734960768024, 'train@spa.rst.rststb_loss@spa.rst.rststb': 2.524245023727417, 'train@spa.rst.rststb_runtime': 27.6259, 'train@spa.rst.rststb_samples_per_second': 81.083, 'train@spa.rst.rststb_steps_per_second': 2.534, 'epoch': 3.0}
{'loss': 2.6301, 'learning_rate': 1.5000000000000002e-05, 'epoch': 3.0}
{'eval_loss': 2.6695914268493652, 'eval_accuracy@spa.rst.rststb': 0.2819843342036554, 'eval_f1@spa.rst.rststb': 0.05127764877808712, 'eval_precision@spa.rst.rststb': 0.045486439821335566, 'eval_recall@spa.rst.rststb': 0.07176325059049575, 'eval_loss@spa.rst.rststb': 2.6695914268493652, 'eval_runtime': 5.0991, 'eval_samples_per_second': 75.112, 'eval_steps_per_second': 2.353, 'epoch': 3.0}
{'train@spa.rst.rststb_loss': 2.422558546066284, 'train@spa.rst.rststb_accuracy@spa.rst.rststb': 0.32589285714285715, 'train@spa.rst.rststb_f1@spa.rst.rststb': 0.049654957925280595, 'train@spa.rst.rststb_precision@spa.rst.rststb': 0.04257779924231131, 'train@spa.rst.rststb_recall@spa.rst.rststb': 0.07033546596980776, 'train@spa.rst.rststb_loss@spa.rst.rststb': 2.422558546066284, 'train@spa.rst.rststb_runtime': 27.5912, 'train@spa.rst.rststb_samples_per_second': 81.185, 'train@spa.rst.rststb_steps_per_second': 2.537, 'epoch': 4.0}
{'loss': 2.516, 'learning_rate': 1.3333333333333333e-05, 'epoch': 4.0}
{'eval_loss': 2.5981714725494385, 'eval_accuracy@spa.rst.rststb': 0.2689295039164491, 'eval_f1@spa.rst.rststb': 0.04988713127843562, 'eval_precision@spa.rst.rststb': 0.04842499914384834, 'eval_recall@spa.rst.rststb': 0.06842003272276233, 'eval_loss@spa.rst.rststb': 2.5981712341308594, 'eval_runtime': 5.1011, 'eval_samples_per_second': 75.082, 'eval_steps_per_second': 2.352, 'epoch': 4.0}
{'train@spa.rst.rststb_loss': 2.335387706756592, 'train@spa.rst.rststb_accuracy@spa.rst.rststb': 0.365625, 'train@spa.rst.rststb_f1@spa.rst.rststb': 0.07883275929589528, 'train@spa.rst.rststb_precision@spa.rst.rststb': 0.09045070089509322, 'train@spa.rst.rststb_recall@spa.rst.rststb': 0.09176057751633236, 'train@spa.rst.rststb_loss@spa.rst.rststb': 2.335387945175171, 'train@spa.rst.rststb_runtime': 27.5772, 'train@spa.rst.rststb_samples_per_second': 81.226, 'train@spa.rst.rststb_steps_per_second': 2.538, 'epoch': 5.0}
{'loss': 2.4232, 'learning_rate': 1.1666666666666668e-05, 'epoch': 5.0}
{'eval_loss': 2.536410093307495, 'eval_accuracy@spa.rst.rststb': 0.3289817232375979, 'eval_f1@spa.rst.rststb': 0.09068231239514059, 'eval_precision@spa.rst.rststb': 0.11786215187337208, 'eval_recall@spa.rst.rststb': 0.1007147983075993, 'eval_loss@spa.rst.rststb': 2.536409854888916, 'eval_runtime': 5.091, 'eval_samples_per_second': 75.23, 'eval_steps_per_second': 2.357, 'epoch': 5.0}
{'train@spa.rst.rststb_loss': 2.2619307041168213, 'train@spa.rst.rststb_accuracy@spa.rst.rststb': 0.3875, 'train@spa.rst.rststb_f1@spa.rst.rststb': 0.09092899616084194, 'train@spa.rst.rststb_precision@spa.rst.rststb': 0.08704287997711911, 'train@spa.rst.rststb_recall@spa.rst.rststb': 0.10980273611527085, 'train@spa.rst.rststb_loss@spa.rst.rststb': 2.2619307041168213, 'train@spa.rst.rststb_runtime': 27.585, 'train@spa.rst.rststb_samples_per_second': 81.204, 'train@spa.rst.rststb_steps_per_second': 2.538, 'epoch': 6.0}
{'loss': 2.3494, 'learning_rate': 1e-05, 'epoch': 6.0}
{'eval_loss': 2.4795331954956055, 'eval_accuracy@spa.rst.rststb': 0.35509138381201044, 'eval_f1@spa.rst.rststb': 0.10629721907049344, 'eval_precision@spa.rst.rststb': 0.10258515504500476, 'eval_recall@spa.rst.rststb': 0.12549044405516038, 'eval_loss@spa.rst.rststb': 2.4795331954956055, 'eval_runtime': 5.0986, 'eval_samples_per_second': 75.118, 'eval_steps_per_second': 2.354, 'epoch': 6.0}
{'train@spa.rst.rststb_loss': 2.199673891067505, 'train@spa.rst.rststb_accuracy@spa.rst.rststb': 0.40580357142857143, 'train@spa.rst.rststb_f1@spa.rst.rststb': 0.09721922268989688, 'train@spa.rst.rststb_precision@spa.rst.rststb': 0.08521696789318414, 'train@spa.rst.rststb_recall@spa.rst.rststb': 0.1234012698366626, 'train@spa.rst.rststb_loss@spa.rst.rststb': 2.199674129486084, 'train@spa.rst.rststb_runtime': 27.5528, 'train@spa.rst.rststb_samples_per_second': 81.299, 'train@spa.rst.rststb_steps_per_second': 2.541, 'epoch': 7.0}
{'loss': 2.2846, 'learning_rate': 8.333333333333334e-06, 'epoch': 7.0}
{'eval_loss': 2.433764934539795, 'eval_accuracy@spa.rst.rststb': 0.3629242819843342, 'eval_f1@spa.rst.rststb': 0.1004575625019506, 'eval_precision@spa.rst.rststb': 0.08834670246861495, 'eval_recall@spa.rst.rststb': 0.12968274794394608, 'eval_loss@spa.rst.rststb': 2.433765172958374, 'eval_runtime': 5.0871, 'eval_samples_per_second': 75.288, 'eval_steps_per_second': 2.359, 'epoch': 7.0}
{'train@spa.rst.rststb_loss': 2.1533918380737305, 'train@spa.rst.rststb_accuracy@spa.rst.rststb': 0.4142857142857143, 'train@spa.rst.rststb_f1@spa.rst.rststb': 0.09982791737426765, 'train@spa.rst.rststb_precision@spa.rst.rststb': 0.08685879645183638, 'train@spa.rst.rststb_recall@spa.rst.rststb': 0.12825724684081904, 'train@spa.rst.rststb_loss@spa.rst.rststb': 2.1533915996551514, 'train@spa.rst.rststb_runtime': 27.5938, 'train@spa.rst.rststb_samples_per_second': 81.178, 'train@spa.rst.rststb_steps_per_second': 2.537, 'epoch': 8.0}
{'loss': 2.2307, 'learning_rate': 6.666666666666667e-06, 'epoch': 8.0}
{'eval_loss': 2.3998351097106934, 'eval_accuracy@spa.rst.rststb': 0.38642297650130547, 'eval_f1@spa.rst.rststb': 0.10629373894227993, 'eval_precision@spa.rst.rststb': 0.09365230265843533, 'eval_recall@spa.rst.rststb': 0.13881456315474996, 'eval_loss@spa.rst.rststb': 2.399834632873535, 'eval_runtime': 5.0991, 'eval_samples_per_second': 75.112, 'eval_steps_per_second': 2.353, 'epoch': 8.0}
{'train@spa.rst.rststb_loss': 2.1176342964172363, 'train@spa.rst.rststb_accuracy@spa.rst.rststb': 0.41875, 'train@spa.rst.rststb_f1@spa.rst.rststb': 0.10113204615051884, 'train@spa.rst.rststb_precision@spa.rst.rststb': 0.12106756280653731, 'train@spa.rst.rststb_recall@spa.rst.rststb': 0.13117362358407816, 'train@spa.rst.rststb_loss@spa.rst.rststb': 2.1176342964172363, 'train@spa.rst.rststb_runtime': 27.5795, 'train@spa.rst.rststb_samples_per_second': 81.22, 'train@spa.rst.rststb_steps_per_second': 2.538, 'epoch': 9.0}
{'loss': 2.191, 'learning_rate': 5e-06, 'epoch': 9.0}
{'eval_loss': 2.369779348373413, 'eval_accuracy@spa.rst.rststb': 0.3838120104438642, 'eval_f1@spa.rst.rststb': 0.1058467460580021, 'eval_precision@spa.rst.rststb': 0.09239067504892799, 'eval_recall@spa.rst.rststb': 0.13846134849059138, 'eval_loss@spa.rst.rststb': 2.369779109954834, 'eval_runtime': 5.0967, 'eval_samples_per_second': 75.147, 'eval_steps_per_second': 2.354, 'epoch': 9.0}
{'train@spa.rst.rststb_loss': 2.092968702316284, 'train@spa.rst.rststb_accuracy@spa.rst.rststb': 0.42098214285714286, 'train@spa.rst.rststb_f1@spa.rst.rststb': 0.10150899854678455, 'train@spa.rst.rststb_precision@spa.rst.rststb': 0.10868262970909488, 'train@spa.rst.rststb_recall@spa.rst.rststb': 0.13197641759686213, 'train@spa.rst.rststb_loss@spa.rst.rststb': 2.092968702316284, 'train@spa.rst.rststb_runtime': 27.5803, 'train@spa.rst.rststb_samples_per_second': 81.217, 'train@spa.rst.rststb_steps_per_second': 2.538, 'epoch': 10.0}
{'loss': 2.1607, 'learning_rate': 3.3333333333333333e-06, 'epoch': 10.0}
{'eval_loss': 2.3531556129455566, 'eval_accuracy@spa.rst.rststb': 0.38903394255874674, 'eval_f1@spa.rst.rststb': 0.1093686702649099, 'eval_precision@spa.rst.rststb': 0.09400753054845126, 'eval_recall@spa.rst.rststb': 0.14464363190018076, 'eval_loss@spa.rst.rststb': 2.3531558513641357, 'eval_runtime': 5.1123, 'eval_samples_per_second': 74.917, 'eval_steps_per_second': 2.347, 'epoch': 10.0}
{'train@spa.rst.rststb_loss': 2.0785069465637207, 'train@spa.rst.rststb_accuracy@spa.rst.rststb': 0.421875, 'train@spa.rst.rststb_f1@spa.rst.rststb': 0.10174930366191878, 'train@spa.rst.rststb_precision@spa.rst.rststb': 0.1089012908360731, 'train@spa.rst.rststb_recall@spa.rst.rststb': 0.13245353106040766, 'train@spa.rst.rststb_loss@spa.rst.rststb': 2.0785071849823, 'train@spa.rst.rststb_runtime': 27.6095, 'train@spa.rst.rststb_samples_per_second': 81.132, 'train@spa.rst.rststb_steps_per_second': 2.535, 'epoch': 11.0}
{'loss': 2.1424, 'learning_rate': 1.6666666666666667e-06, 'epoch': 11.0}
{'eval_loss': 2.341116428375244, 'eval_accuracy@spa.rst.rststb': 0.38903394255874674, 'eval_f1@spa.rst.rststb': 0.10913865960388437, 'eval_precision@spa.rst.rststb': 0.09370054448023453, 'eval_recall@spa.rst.rststb': 0.14464363190018076, 'eval_loss@spa.rst.rststb': 2.3411166667938232, 'eval_runtime': 7.5374, 'eval_samples_per_second': 50.813, 'eval_steps_per_second': 1.592, 'epoch': 11.0}
{'train@spa.rst.rststb_loss': 2.0739834308624268, 'train@spa.rst.rststb_accuracy@spa.rst.rststb': 0.42276785714285714, 'train@spa.rst.rststb_f1@spa.rst.rststb': 0.10238202565422791, 'train@spa.rst.rststb_precision@spa.rst.rststb': 0.10977232482642198, 'train@spa.rst.rststb_recall@spa.rst.rststb': 0.13261155887330275, 'train@spa.rst.rststb_loss@spa.rst.rststb': 2.0739831924438477, 'train@spa.rst.rststb_runtime': 27.6047, 'train@spa.rst.rststb_samples_per_second': 81.146, 'train@spa.rst.rststb_steps_per_second': 2.536, 'epoch': 12.0}
{'loss': 2.1281, 'learning_rate': 0.0, 'epoch': 12.0}
{'eval_loss': 2.338264226913452, 'eval_accuracy@spa.rst.rststb': 0.391644908616188, 'eval_f1@spa.rst.rststb': 0.11035857321545281, 'eval_precision@spa.rst.rststb': 0.09578230058623946, 'eval_recall@spa.rst.rststb': 0.14519398963270688, 'eval_loss@spa.rst.rststb': 2.338263988494873, 'eval_runtime': 5.1154, 'eval_samples_per_second': 74.872, 'eval_steps_per_second': 2.346, 'epoch': 12.0}
{'train_runtime': 1081.9527, 'train_samples_per_second': 24.844, 'train_steps_per_second': 0.776, 'train_loss': 2.4522369748070125, 'epoch': 12.0}
***** train_log metrics *****
  epoch                    =       12.0
  train_loss               =     2.4522
  train_runtime            = 0:18:01.95
  train_samples_per_second =     24.844
  train_steps_per_second   =      0.776
{'train@tur.pdtb.tdb_loss': 2.6715168952941895, 'train@tur.pdtb.tdb_accuracy@tur.pdtb.tdb': 0.262749898000816, 'train@tur.pdtb.tdb_f1@tur.pdtb.tdb': 0.026982984145173812, 'train@tur.pdtb.tdb_precision@tur.pdtb.tdb': 0.02366306005713535, 'train@tur.pdtb.tdb_recall@tur.pdtb.tdb': 0.04982976915451069, 'train@tur.pdtb.tdb_loss@tur.pdtb.tdb': 2.6715168952941895, 'train@tur.pdtb.tdb_runtime': 30.1308, 'train@tur.pdtb.tdb_samples_per_second': 81.345, 'train@tur.pdtb.tdb_steps_per_second': 2.556, 'epoch': 1.0}
{'loss': 3.1467, 'learning_rate': 1.8333333333333333e-05, 'epoch': 1.0}
{'eval_loss': 2.621727466583252, 'eval_accuracy@tur.pdtb.tdb': 0.2692307692307692, 'eval_f1@tur.pdtb.tdb': 0.025079242913708773, 'eval_precision@tur.pdtb.tdb': 0.022803283672848892, 'eval_recall@tur.pdtb.tdb': 0.048369950389794476, 'eval_loss@tur.pdtb.tdb': 2.621727705001831, 'eval_runtime': 4.2395, 'eval_samples_per_second': 73.593, 'eval_steps_per_second': 2.359, 'epoch': 1.0}
{'train@tur.pdtb.tdb_loss': 2.367171049118042, 'train@tur.pdtb.tdb_accuracy@tur.pdtb.tdb': 0.2713178294573643, 'train@tur.pdtb.tdb_f1@tur.pdtb.tdb': 0.030924064143125377, 'train@tur.pdtb.tdb_precision@tur.pdtb.tdb': 0.023719429054246437, 'train@tur.pdtb.tdb_recall@tur.pdtb.tdb': 0.05492536467922391, 'train@tur.pdtb.tdb_loss@tur.pdtb.tdb': 2.367171287536621, 'train@tur.pdtb.tdb_runtime': 30.1518, 'train@tur.pdtb.tdb_samples_per_second': 81.289, 'train@tur.pdtb.tdb_steps_per_second': 2.554, 'epoch': 2.0}
{'loss': 2.5107, 'learning_rate': 1.6666666666666667e-05, 'epoch': 2.0}
{'eval_loss': 2.3043603897094727, 'eval_accuracy@tur.pdtb.tdb': 0.27884615384615385, 'eval_f1@tur.pdtb.tdb': 0.029606222654351, 'eval_precision@tur.pdtb.tdb': 0.025639309144463784, 'eval_recall@tur.pdtb.tdb': 0.05238064557696025, 'eval_loss@tur.pdtb.tdb': 2.3043603897094727, 'eval_runtime': 4.2599, 'eval_samples_per_second': 73.242, 'eval_steps_per_second': 2.347, 'epoch': 2.0}
{'train@tur.pdtb.tdb_loss': 2.298797130584717, 'train@tur.pdtb.tdb_accuracy@tur.pdtb.tdb': 0.28233374133006933, 'train@tur.pdtb.tdb_f1@tur.pdtb.tdb': 0.03839971932216231, 'train@tur.pdtb.tdb_precision@tur.pdtb.tdb': 0.06565909018498227, 'train@tur.pdtb.tdb_recall@tur.pdtb.tdb': 0.05933383837132197, 'train@tur.pdtb.tdb_loss@tur.pdtb.tdb': 2.298797130584717, 'train@tur.pdtb.tdb_runtime': 30.1819, 'train@tur.pdtb.tdb_samples_per_second': 81.208, 'train@tur.pdtb.tdb_steps_per_second': 2.551, 'epoch': 3.0}
{'loss': 2.3697, 'learning_rate': 1.5000000000000002e-05, 'epoch': 3.0}
{'eval_loss': 2.2590372562408447, 'eval_accuracy@tur.pdtb.tdb': 0.27564102564102566, 'eval_f1@tur.pdtb.tdb': 0.02968553459119497, 'eval_precision@tur.pdtb.tdb': 0.026807367110178934, 'eval_recall@tur.pdtb.tdb': 0.05183300045100187, 'eval_loss@tur.pdtb.tdb': 2.2590372562408447, 'eval_runtime': 4.2472, 'eval_samples_per_second': 73.46, 'eval_steps_per_second': 2.354, 'epoch': 3.0}
{'train@tur.pdtb.tdb_loss': 2.2254204750061035, 'train@tur.pdtb.tdb_accuracy@tur.pdtb.tdb': 0.2937576499388005, 'train@tur.pdtb.tdb_f1@tur.pdtb.tdb': 0.04033407501010562, 'train@tur.pdtb.tdb_precision@tur.pdtb.tdb': 0.056472532066154865, 'train@tur.pdtb.tdb_recall@tur.pdtb.tdb': 0.0657662662132353, 'train@tur.pdtb.tdb_loss@tur.pdtb.tdb': 2.2254202365875244, 'train@tur.pdtb.tdb_runtime': 30.1541, 'train@tur.pdtb.tdb_samples_per_second': 81.282, 'train@tur.pdtb.tdb_steps_per_second': 2.554, 'epoch': 4.0}
{'loss': 2.3035, 'learning_rate': 1.3333333333333333e-05, 'epoch': 4.0}
{'eval_loss': 2.2087976932525635, 'eval_accuracy@tur.pdtb.tdb': 0.27884615384615385, 'eval_f1@tur.pdtb.tdb': 0.03045055503156062, 'eval_precision@tur.pdtb.tdb': 0.02315886134067952, 'eval_recall@tur.pdtb.tdb': 0.05395915211648734, 'eval_loss@tur.pdtb.tdb': 2.2087974548339844, 'eval_runtime': 4.2437, 'eval_samples_per_second': 73.52, 'eval_steps_per_second': 2.356, 'epoch': 4.0}
{'train@tur.pdtb.tdb_loss': 2.171621322631836, 'train@tur.pdtb.tdb_accuracy@tur.pdtb.tdb': 0.32966136270909835, 'train@tur.pdtb.tdb_f1@tur.pdtb.tdb': 0.08072658709880258, 'train@tur.pdtb.tdb_precision@tur.pdtb.tdb': 0.11596424238598231, 'train@tur.pdtb.tdb_recall@tur.pdtb.tdb': 0.09289023230104865, 'train@tur.pdtb.tdb_loss@tur.pdtb.tdb': 2.171621084213257, 'train@tur.pdtb.tdb_runtime': 30.1256, 'train@tur.pdtb.tdb_samples_per_second': 81.359, 'train@tur.pdtb.tdb_steps_per_second': 2.556, 'epoch': 5.0}
{'loss': 2.2502, 'learning_rate': 1.1666666666666668e-05, 'epoch': 5.0}
{'eval_loss': 2.169464588165283, 'eval_accuracy@tur.pdtb.tdb': 0.2948717948717949, 'eval_f1@tur.pdtb.tdb': 0.06407254361799818, 'eval_precision@tur.pdtb.tdb': 0.07057116620752985, 'eval_recall@tur.pdtb.tdb': 0.08288345974769418, 'eval_loss@tur.pdtb.tdb': 2.169464588165283, 'eval_runtime': 4.2768, 'eval_samples_per_second': 72.951, 'eval_steps_per_second': 2.338, 'epoch': 5.0}
{'train@tur.pdtb.tdb_loss': 2.124505043029785, 'train@tur.pdtb.tdb_accuracy@tur.pdtb.tdb': 0.34149326805385555, 'train@tur.pdtb.tdb_f1@tur.pdtb.tdb': 0.09142592755523665, 'train@tur.pdtb.tdb_precision@tur.pdtb.tdb': 0.11071892606907775, 'train@tur.pdtb.tdb_recall@tur.pdtb.tdb': 0.10748351468676666, 'train@tur.pdtb.tdb_loss@tur.pdtb.tdb': 2.1245052814483643, 'train@tur.pdtb.tdb_runtime': 30.1304, 'train@tur.pdtb.tdb_samples_per_second': 81.346, 'train@tur.pdtb.tdb_steps_per_second': 2.556, 'epoch': 6.0}
{'loss': 2.1997, 'learning_rate': 1e-05, 'epoch': 6.0}
{'eval_loss': 2.140868663787842, 'eval_accuracy@tur.pdtb.tdb': 0.3108974358974359, 'eval_f1@tur.pdtb.tdb': 0.0795885410305473, 'eval_precision@tur.pdtb.tdb': 0.08818816563459991, 'eval_recall@tur.pdtb.tdb': 0.10865075429984682, 'eval_loss@tur.pdtb.tdb': 2.1408684253692627, 'eval_runtime': 4.286, 'eval_samples_per_second': 72.796, 'eval_steps_per_second': 2.333, 'epoch': 6.0}
{'train@tur.pdtb.tdb_loss': 2.089012622833252, 'train@tur.pdtb.tdb_accuracy@tur.pdtb.tdb': 0.35495716034271724, 'train@tur.pdtb.tdb_f1@tur.pdtb.tdb': 0.09850660530588402, 'train@tur.pdtb.tdb_precision@tur.pdtb.tdb': 0.11219629078038246, 'train@tur.pdtb.tdb_recall@tur.pdtb.tdb': 0.11843012313248877, 'train@tur.pdtb.tdb_loss@tur.pdtb.tdb': 2.089012384414673, 'train@tur.pdtb.tdb_runtime': 30.1395, 'train@tur.pdtb.tdb_samples_per_second': 81.322, 'train@tur.pdtb.tdb_steps_per_second': 2.555, 'epoch': 7.0}
{'loss': 2.1633, 'learning_rate': 8.333333333333334e-06, 'epoch': 7.0}
{'eval_loss': 2.1181652545928955, 'eval_accuracy@tur.pdtb.tdb': 0.32371794871794873, 'eval_f1@tur.pdtb.tdb': 0.08634069373982672, 'eval_precision@tur.pdtb.tdb': 0.11208475125587425, 'eval_recall@tur.pdtb.tdb': 0.11515382618593413, 'eval_loss@tur.pdtb.tdb': 2.1181652545928955, 'eval_runtime': 4.2595, 'eval_samples_per_second': 73.249, 'eval_steps_per_second': 2.348, 'epoch': 7.0}
{'train@tur.pdtb.tdb_loss': 2.063549280166626, 'train@tur.pdtb.tdb_accuracy@tur.pdtb.tdb': 0.3639330885352917, 'train@tur.pdtb.tdb_f1@tur.pdtb.tdb': 0.10326687054708215, 'train@tur.pdtb.tdb_precision@tur.pdtb.tdb': 0.11015650895526698, 'train@tur.pdtb.tdb_recall@tur.pdtb.tdb': 0.12268229366694167, 'train@tur.pdtb.tdb_loss@tur.pdtb.tdb': 2.063549041748047, 'train@tur.pdtb.tdb_runtime': 30.1675, 'train@tur.pdtb.tdb_samples_per_second': 81.246, 'train@tur.pdtb.tdb_steps_per_second': 2.552, 'epoch': 8.0}
{'loss': 2.136, 'learning_rate': 6.666666666666667e-06, 'epoch': 8.0}
{'eval_loss': 2.101365327835083, 'eval_accuracy@tur.pdtb.tdb': 0.3333333333333333, 'eval_f1@tur.pdtb.tdb': 0.09077857102825093, 'eval_precision@tur.pdtb.tdb': 0.10499413359226444, 'eval_recall@tur.pdtb.tdb': 0.1181182530634045, 'eval_loss@tur.pdtb.tdb': 2.101365566253662, 'eval_runtime': 4.2671, 'eval_samples_per_second': 73.117, 'eval_steps_per_second': 2.344, 'epoch': 8.0}
{'train@tur.pdtb.tdb_loss': 2.041469097137451, 'train@tur.pdtb.tdb_accuracy@tur.pdtb.tdb': 0.3635250917992656, 'train@tur.pdtb.tdb_f1@tur.pdtb.tdb': 0.10259773920242908, 'train@tur.pdtb.tdb_precision@tur.pdtb.tdb': 0.11382550857145778, 'train@tur.pdtb.tdb_recall@tur.pdtb.tdb': 0.12204056590195898, 'train@tur.pdtb.tdb_loss@tur.pdtb.tdb': 2.041469097137451, 'train@tur.pdtb.tdb_runtime': 30.1293, 'train@tur.pdtb.tdb_samples_per_second': 81.349, 'train@tur.pdtb.tdb_steps_per_second': 2.556, 'epoch': 9.0}
{'loss': 2.1099, 'learning_rate': 5e-06, 'epoch': 9.0}
{'eval_loss': 2.087693929672241, 'eval_accuracy@tur.pdtb.tdb': 0.3269230769230769, 'eval_f1@tur.pdtb.tdb': 0.08909397015518805, 'eval_precision@tur.pdtb.tdb': 0.10773594946269784, 'eval_recall@tur.pdtb.tdb': 0.11641645244856041, 'eval_loss@tur.pdtb.tdb': 2.087693929672241, 'eval_runtime': 4.2746, 'eval_samples_per_second': 72.99, 'eval_steps_per_second': 2.339, 'epoch': 9.0}
{'train@tur.pdtb.tdb_loss': 2.0259664058685303, 'train@tur.pdtb.tdb_accuracy@tur.pdtb.tdb': 0.37250101999184004, 'train@tur.pdtb.tdb_f1@tur.pdtb.tdb': 0.1098919213072512, 'train@tur.pdtb.tdb_precision@tur.pdtb.tdb': 0.12204214873548955, 'train@tur.pdtb.tdb_recall@tur.pdtb.tdb': 0.12767611939565535, 'train@tur.pdtb.tdb_loss@tur.pdtb.tdb': 2.025966167449951, 'train@tur.pdtb.tdb_runtime': 30.1248, 'train@tur.pdtb.tdb_samples_per_second': 81.362, 'train@tur.pdtb.tdb_steps_per_second': 2.556, 'epoch': 10.0}
{'loss': 2.0893, 'learning_rate': 3.3333333333333333e-06, 'epoch': 10.0}
{'eval_loss': 2.0773766040802, 'eval_accuracy@tur.pdtb.tdb': 0.3333333333333333, 'eval_f1@tur.pdtb.tdb': 0.09534913239806478, 'eval_precision@tur.pdtb.tdb': 0.144014193353816, 'eval_recall@tur.pdtb.tdb': 0.11877319823706625, 'eval_loss@tur.pdtb.tdb': 2.0773766040802, 'eval_runtime': 4.2294, 'eval_samples_per_second': 73.769, 'eval_steps_per_second': 2.364, 'epoch': 10.0}
{'train@tur.pdtb.tdb_loss': 2.017662286758423, 'train@tur.pdtb.tdb_accuracy@tur.pdtb.tdb': 0.3737250101999184, 'train@tur.pdtb.tdb_f1@tur.pdtb.tdb': 0.11076678013866545, 'train@tur.pdtb.tdb_precision@tur.pdtb.tdb': 0.12253978712337252, 'train@tur.pdtb.tdb_recall@tur.pdtb.tdb': 0.12824944143932543, 'train@tur.pdtb.tdb_loss@tur.pdtb.tdb': 2.017662286758423, 'train@tur.pdtb.tdb_runtime': 30.1986, 'train@tur.pdtb.tdb_samples_per_second': 81.163, 'train@tur.pdtb.tdb_steps_per_second': 2.55, 'epoch': 11.0}
{'loss': 2.0854, 'learning_rate': 1.6666666666666667e-06, 'epoch': 11.0}
{'eval_loss': 2.0730483531951904, 'eval_accuracy@tur.pdtb.tdb': 0.3301282051282051, 'eval_f1@tur.pdtb.tdb': 0.0951326881057271, 'eval_precision@tur.pdtb.tdb': 0.14568627193500802, 'eval_recall@tur.pdtb.tdb': 0.11805953991471231, 'eval_loss@tur.pdtb.tdb': 2.0730483531951904, 'eval_runtime': 4.2669, 'eval_samples_per_second': 73.121, 'eval_steps_per_second': 2.344, 'epoch': 11.0}
{'train@tur.pdtb.tdb_loss': 2.014726161956787, 'train@tur.pdtb.tdb_accuracy@tur.pdtb.tdb': 0.3773969808241534, 'train@tur.pdtb.tdb_f1@tur.pdtb.tdb': 0.11291664433351699, 'train@tur.pdtb.tdb_precision@tur.pdtb.tdb': 0.12236747272192554, 'train@tur.pdtb.tdb_recall@tur.pdtb.tdb': 0.130108860873935, 'train@tur.pdtb.tdb_loss@tur.pdtb.tdb': 2.014725923538208, 'train@tur.pdtb.tdb_runtime': 30.15, 'train@tur.pdtb.tdb_samples_per_second': 81.294, 'train@tur.pdtb.tdb_steps_per_second': 2.554, 'epoch': 12.0}
{'loss': 2.0749, 'learning_rate': 0.0, 'epoch': 12.0}
{'eval_loss': 2.070680856704712, 'eval_accuracy@tur.pdtb.tdb': 0.33653846153846156, 'eval_f1@tur.pdtb.tdb': 0.09760063047868317, 'eval_precision@tur.pdtb.tdb': 0.12617536551498815, 'eval_recall@tur.pdtb.tdb': 0.12003582449969254, 'eval_loss@tur.pdtb.tdb': 2.070680618286133, 'eval_runtime': 4.2908, 'eval_samples_per_second': 72.713, 'eval_steps_per_second': 2.331, 'epoch': 12.0}
{'train_runtime': 1166.2377, 'train_samples_per_second': 25.22, 'train_steps_per_second': 0.792, 'train_loss': 2.2865952661027125, 'epoch': 12.0}
***** train_logger metrics *****
  epoch                    =       12.0
  train_loss               =     2.4522
  train_runtime            = 0:18:01.95
  train_samples_per_second =     24.844
  train_steps_per_second   =      0.776
-------------------------------------------------------------------
Lang1:  spa.rst.sctb    Lang2:  tur.pdtb.tdb
Saving run to:  runs/full_shot/FullShot=v4_spa.rst.sctb_tur.pdtb.tdb_lrfunc=Adafactor_lr=2e-05_plm=bert-base-multilingual-cased
Running with params: BERT_MODEL=bert-base-multilingual-cased lr=2e-05
read 439 examples
read 94 examples
read 159 examples
read 2451 examples
read 312 examples
read 422 examples
Total prediction labels:  48
BertAdapterModel(
  (shared_parameters): ModuleDict()
  (bert): BertModel(
    (shared_parameters): ModuleDict()
    (invertible_adapters): ModuleDict(
      (es): NICECouplingBlock(
        (F): Sequential(
          (0): Linear(in_features=384, out_features=192, bias=True)
          (1): Activation_Function_Class(
            (f): NewGELUActivation()
          )
          (2): Linear(in_features=192, out_features=384, bias=True)
        )
        (G): Sequential(
          (0): Linear(in_features=384, out_features=192, bias=True)
          (1): Activation_Function_Class(
            (f): NewGELUActivation()
          )
          (2): Linear(in_features=192, out_features=384, bias=True)
        )
      )
      (tr): NICECouplingBlock(
        (F): Sequential(
          (0): Linear(in_features=384, out_features=192, bias=True)
          (1): Activation_Function_Class(
            (f): NewGELUActivation()
          )
          (2): Linear(in_features=192, out_features=384, bias=True)
        )
        (G): Sequential(
          (0): Linear(in_features=384, out_features=192, bias=True)
          (1): Activation_Function_Class(
            (f): NewGELUActivation()
          )
          (2): Linear(in_features=192, out_features=384, bias=True)
        )
      )
    )
    (embeddings): BertEmbeddings(
      (word_embeddings): Embedding(119547, 768, padding_idx=0)
      (position_embeddings): Embedding(512, 768)
      (token_type_embeddings): Embedding(2, 768)
      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
      (dropout): Dropout(p=0.1, inplace=False)
    )
    (encoder): BertEncoder(
      (layer): ModuleList(
        (0): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (es): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (tr): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (1): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (es): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (tr): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (2): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (es): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (tr): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (3): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (es): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (tr): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (4): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (es): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (tr): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (5): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (es): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (tr): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (6): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (es): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (tr): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (7): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (es): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (tr): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (8): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (es): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (tr): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (9): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (es): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (tr): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (10): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (es): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (tr): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (11): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (es): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (tr): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
      )
    )
    (pooler): BertPooler(
      (dense): Linear(in_features=768, out_features=768, bias=True)
      (activation): Tanh()
    )
    (prefix_tuning): PrefixTuningPool(
      (prefix_tunings): ModuleDict()
    )
  )
  (heads): ModuleDict(
    (tr): BertStyleMaskedLMHead(
      (0): Linear(in_features=768, out_features=768, bias=True)
      (1): Activation_Function_Class(
        (f): GELUActivation()
      )
      (2): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
      (3): Linear(in_features=768, out_features=119547, bias=True)
    )
    (disrpt): ClassificationHead(
      (0): Dropout(p=0.1, inplace=False)
      (1): Linear(in_features=768, out_features=768, bias=True)
      (2): Activation_Function_Class(
        (f): Tanh()
      )
      (3): Dropout(p=0.1, inplace=False)
      (4): Linear(in_features=768, out_features=48, bias=True)
    )
  )
)
{'train@spa.rst.sctb_loss': 3.6444244384765625, 'train@spa.rst.sctb_accuracy@spa.rst.sctb': 0.34851936218678814, 'train@spa.rst.sctb_f1@spa.rst.sctb': 0.02414835164835165, 'train@spa.rst.sctb_precision@spa.rst.sctb': 0.04007638702760653, 'train@spa.rst.sctb_recall@spa.rst.sctb': 0.04018546796462429, 'train@spa.rst.sctb_loss@spa.rst.sctb': 3.6444246768951416, 'train@spa.rst.sctb_runtime': 5.6976, 'train@spa.rst.sctb_samples_per_second': 77.051, 'train@spa.rst.sctb_steps_per_second': 2.457, 'epoch': 1.0}
{'loss': 3.806, 'learning_rate': 1.8333333333333333e-05, 'epoch': 1.0}
{'eval_loss': 3.626422643661499, 'eval_accuracy@spa.rst.sctb': 0.3723404255319149, 'eval_f1@spa.rst.sctb': 0.038497884054398294, 'eval_precision@spa.rst.sctb': 0.07214665878178593, 'eval_recall@spa.rst.sctb': 0.05817174515235456, 'eval_loss@spa.rst.sctb': 3.626422643661499, 'eval_runtime': 1.5626, 'eval_samples_per_second': 60.154, 'eval_steps_per_second': 1.92, 'epoch': 1.0}
{'train@spa.rst.sctb_loss': 3.376213550567627, 'train@spa.rst.sctb_accuracy@spa.rst.sctb': 0.35079726651480636, 'train@spa.rst.sctb_f1@spa.rst.sctb': 0.02699212329715474, 'train@spa.rst.sctb_precision@spa.rst.sctb': 0.07016850974196946, 'train@spa.rst.sctb_recall@spa.rst.sctb': 0.04415253653156879, 'train@spa.rst.sctb_loss@spa.rst.sctb': 3.376213788986206, 'train@spa.rst.sctb_runtime': 5.7262, 'train@spa.rst.sctb_samples_per_second': 76.665, 'train@spa.rst.sctb_steps_per_second': 2.445, 'epoch': 2.0}
{'loss': 3.5323, 'learning_rate': 1.6666666666666667e-05, 'epoch': 2.0}
{'eval_loss': 3.357238531112671, 'eval_accuracy@spa.rst.sctb': 0.3829787234042553, 'eval_f1@spa.rst.sctb': 0.04690911359241469, 'eval_precision@spa.rst.sctb': 0.06568627450980392, 'eval_recall@spa.rst.sctb': 0.06811145510835914, 'eval_loss@spa.rst.sctb': 3.3572380542755127, 'eval_runtime': 1.5477, 'eval_samples_per_second': 60.735, 'eval_steps_per_second': 1.938, 'epoch': 2.0}
{'train@spa.rst.sctb_loss': 3.1065986156463623, 'train@spa.rst.sctb_accuracy@spa.rst.sctb': 0.3530751708428246, 'train@spa.rst.sctb_f1@spa.rst.sctb': 0.02625291375291375, 'train@spa.rst.sctb_precision@spa.rst.sctb': 0.029417572716290308, 'train@spa.rst.sctb_recall@spa.rst.sctb': 0.04407706093189964, 'train@spa.rst.sctb_loss@spa.rst.sctb': 3.1065988540649414, 'train@spa.rst.sctb_runtime': 5.7164, 'train@spa.rst.sctb_samples_per_second': 76.797, 'train@spa.rst.sctb_steps_per_second': 2.449, 'epoch': 3.0}
{'loss': 3.2824, 'learning_rate': 1.5000000000000002e-05, 'epoch': 3.0}
{'eval_loss': 3.0903635025024414, 'eval_accuracy@spa.rst.sctb': 0.39361702127659576, 'eval_f1@spa.rst.sctb': 0.05143040822886532, 'eval_precision@spa.rst.sctb': 0.06886979510905486, 'eval_recall@spa.rst.sctb': 0.07120743034055728, 'eval_loss@spa.rst.sctb': 3.0903635025024414, 'eval_runtime': 1.553, 'eval_samples_per_second': 60.528, 'eval_steps_per_second': 1.932, 'epoch': 3.0}
{'train@spa.rst.sctb_loss': 2.8611419200897217, 'train@spa.rst.sctb_accuracy@spa.rst.sctb': 0.35079726651480636, 'train@spa.rst.sctb_f1@spa.rst.sctb': 0.024831425782982874, 'train@spa.rst.sctb_precision@spa.rst.sctb': 0.029754318889832907, 'train@spa.rst.sctb_recall@spa.rst.sctb': 0.04345878136200717, 'train@spa.rst.sctb_loss@spa.rst.sctb': 2.8611414432525635, 'train@spa.rst.sctb_runtime': 5.7315, 'train@spa.rst.sctb_samples_per_second': 76.595, 'train@spa.rst.sctb_steps_per_second': 2.443, 'epoch': 4.0}
{'loss': 3.02, 'learning_rate': 1.3333333333333333e-05, 'epoch': 4.0}
{'eval_loss': 2.855186700820923, 'eval_accuracy@spa.rst.sctb': 0.3723404255319149, 'eval_f1@spa.rst.sctb': 0.042004485078488876, 'eval_precision@spa.rst.sctb': 0.06054729584141348, 'eval_recall@spa.rst.sctb': 0.06501547987616098, 'eval_loss@spa.rst.sctb': 2.855186700820923, 'eval_runtime': 1.5765, 'eval_samples_per_second': 59.626, 'eval_steps_per_second': 1.903, 'epoch': 4.0}
{'train@spa.rst.sctb_loss': 2.6634950637817383, 'train@spa.rst.sctb_accuracy@spa.rst.sctb': 0.3439635535307517, 'train@spa.rst.sctb_f1@spa.rst.sctb': 0.022226627896730994, 'train@spa.rst.sctb_precision@spa.rst.sctb': 0.02478448275862069, 'train@spa.rst.sctb_recall@spa.rst.sctb': 0.042114695340501794, 'train@spa.rst.sctb_loss@spa.rst.sctb': 2.6634953022003174, 'train@spa.rst.sctb_runtime': 5.7548, 'train@spa.rst.sctb_samples_per_second': 76.284, 'train@spa.rst.sctb_steps_per_second': 2.433, 'epoch': 5.0}
{'loss': 2.8003, 'learning_rate': 1.1666666666666668e-05, 'epoch': 5.0}
{'eval_loss': 2.667593240737915, 'eval_accuracy@spa.rst.sctb': 0.3617021276595745, 'eval_f1@spa.rst.sctb': 0.0366610644257703, 'eval_precision@spa.rst.sctb': 0.05051150895140665, 'eval_recall@spa.rst.sctb': 0.06191950464396284, 'eval_loss@spa.rst.sctb': 2.6675937175750732, 'eval_runtime': 1.568, 'eval_samples_per_second': 59.95, 'eval_steps_per_second': 1.913, 'epoch': 5.0}
{'train@spa.rst.sctb_loss': 2.525631904602051, 'train@spa.rst.sctb_accuracy@spa.rst.sctb': 0.3439635535307517, 'train@spa.rst.sctb_f1@spa.rst.sctb': 0.02219911357603337, 'train@spa.rst.sctb_precision@spa.rst.sctb': 0.02822375127420999, 'train@spa.rst.sctb_recall@spa.rst.sctb': 0.042114695340501794, 'train@spa.rst.sctb_loss@spa.rst.sctb': 2.5256316661834717, 'train@spa.rst.sctb_runtime': 5.7342, 'train@spa.rst.sctb_samples_per_second': 76.558, 'train@spa.rst.sctb_steps_per_second': 2.441, 'epoch': 6.0}
{'loss': 2.6446, 'learning_rate': 1e-05, 'epoch': 6.0}
{'eval_loss': 2.5390994548797607, 'eval_accuracy@spa.rst.sctb': 0.35106382978723405, 'eval_f1@spa.rst.sctb': 0.03056970819823993, 'eval_precision@spa.rst.sctb': 0.02065081351689612, 'eval_recall@spa.rst.sctb': 0.058823529411764705, 'eval_loss@spa.rst.sctb': 2.539099931716919, 'eval_runtime': 1.5565, 'eval_samples_per_second': 60.392, 'eval_steps_per_second': 1.927, 'epoch': 6.0}
{'train@spa.rst.sctb_loss': 2.437227249145508, 'train@spa.rst.sctb_accuracy@spa.rst.sctb': 0.3462414578587699, 'train@spa.rst.sctb_f1@spa.rst.sctb': 0.023124325011117466, 'train@spa.rst.sctb_precision@spa.rst.sctb': 0.028323069027457015, 'train@spa.rst.sctb_recall@spa.rst.sctb': 0.04256272401433692, 'train@spa.rst.sctb_loss@spa.rst.sctb': 2.4372270107269287, 'train@spa.rst.sctb_runtime': 5.727, 'train@spa.rst.sctb_samples_per_second': 76.655, 'train@spa.rst.sctb_steps_per_second': 2.445, 'epoch': 7.0}
{'loss': 2.5392, 'learning_rate': 8.333333333333334e-06, 'epoch': 7.0}
{'eval_loss': 2.4586234092712402, 'eval_accuracy@spa.rst.sctb': 0.35106382978723405, 'eval_f1@spa.rst.sctb': 0.03056970819823993, 'eval_precision@spa.rst.sctb': 0.02065081351689612, 'eval_recall@spa.rst.sctb': 0.058823529411764705, 'eval_loss@spa.rst.sctb': 2.4586234092712402, 'eval_runtime': 1.5557, 'eval_samples_per_second': 60.423, 'eval_steps_per_second': 1.928, 'epoch': 7.0}
{'train@spa.rst.sctb_loss': 2.3846373558044434, 'train@spa.rst.sctb_accuracy@spa.rst.sctb': 0.3462414578587699, 'train@spa.rst.sctb_f1@spa.rst.sctb': 0.023124325011117466, 'train@spa.rst.sctb_precision@spa.rst.sctb': 0.028323069027457015, 'train@spa.rst.sctb_recall@spa.rst.sctb': 0.04256272401433692, 'train@spa.rst.sctb_loss@spa.rst.sctb': 2.3846373558044434, 'train@spa.rst.sctb_runtime': 5.7486, 'train@spa.rst.sctb_samples_per_second': 76.366, 'train@spa.rst.sctb_steps_per_second': 2.435, 'epoch': 8.0}
{'loss': 2.4428, 'learning_rate': 6.666666666666667e-06, 'epoch': 8.0}
{'eval_loss': 2.4115407466888428, 'eval_accuracy@spa.rst.sctb': 0.3617021276595745, 'eval_f1@spa.rst.sctb': 0.0366610644257703, 'eval_precision@spa.rst.sctb': 0.05051150895140665, 'eval_recall@spa.rst.sctb': 0.06191950464396284, 'eval_loss@spa.rst.sctb': 2.4115405082702637, 'eval_runtime': 1.5364, 'eval_samples_per_second': 61.18, 'eval_steps_per_second': 1.953, 'epoch': 8.0}
{'train@spa.rst.sctb_loss': 2.350386381149292, 'train@spa.rst.sctb_accuracy@spa.rst.sctb': 0.35079726651480636, 'train@spa.rst.sctb_f1@spa.rst.sctb': 0.024854395232290397, 'train@spa.rst.sctb_precision@spa.rst.sctb': 0.02661064425770308, 'train@spa.rst.sctb_recall@spa.rst.sctb': 0.04345878136200717, 'train@spa.rst.sctb_loss@spa.rst.sctb': 2.350386381149292, 'train@spa.rst.sctb_runtime': 5.7598, 'train@spa.rst.sctb_samples_per_second': 76.218, 'train@spa.rst.sctb_steps_per_second': 2.431, 'epoch': 9.0}
{'loss': 2.4157, 'learning_rate': 5e-06, 'epoch': 9.0}
{'eval_loss': 2.380855083465576, 'eval_accuracy@spa.rst.sctb': 0.3617021276595745, 'eval_f1@spa.rst.sctb': 0.0366610644257703, 'eval_precision@spa.rst.sctb': 0.05051150895140665, 'eval_recall@spa.rst.sctb': 0.06191950464396284, 'eval_loss@spa.rst.sctb': 2.3808555603027344, 'eval_runtime': 1.5701, 'eval_samples_per_second': 59.871, 'eval_steps_per_second': 1.911, 'epoch': 9.0}
{'train@spa.rst.sctb_loss': 2.3287320137023926, 'train@spa.rst.sctb_accuracy@spa.rst.sctb': 0.3553530751708428, 'train@spa.rst.sctb_f1@spa.rst.sctb': 0.026977287702825134, 'train@spa.rst.sctb_precision@spa.rst.sctb': 0.02874136097820308, 'train@spa.rst.sctb_recall@spa.rst.sctb': 0.044525089605734765, 'train@spa.rst.sctb_loss@spa.rst.sctb': 2.3287317752838135, 'train@spa.rst.sctb_runtime': 5.7279, 'train@spa.rst.sctb_samples_per_second': 76.643, 'train@spa.rst.sctb_steps_per_second': 2.444, 'epoch': 10.0}
{'loss': 2.3879, 'learning_rate': 3.3333333333333333e-06, 'epoch': 10.0}
{'eval_loss': 2.3623411655426025, 'eval_accuracy@spa.rst.sctb': 0.3723404255319149, 'eval_f1@spa.rst.sctb': 0.042004485078488876, 'eval_precision@spa.rst.sctb': 0.06054729584141348, 'eval_recall@spa.rst.sctb': 0.06501547987616098, 'eval_loss@spa.rst.sctb': 2.3623411655426025, 'eval_runtime': 1.5624, 'eval_samples_per_second': 60.166, 'eval_steps_per_second': 1.92, 'epoch': 10.0}
{'train@spa.rst.sctb_loss': 2.316995859146118, 'train@spa.rst.sctb_accuracy@spa.rst.sctb': 0.357630979498861, 'train@spa.rst.sctb_f1@spa.rst.sctb': 0.02769598445926948, 'train@spa.rst.sctb_precision@spa.rst.sctb': 0.03003960467989245, 'train@spa.rst.sctb_recall@spa.rst.sctb': 0.044973118279569894, 'train@spa.rst.sctb_loss@spa.rst.sctb': 2.316995859146118, 'train@spa.rst.sctb_runtime': 5.7572, 'train@spa.rst.sctb_samples_per_second': 76.253, 'train@spa.rst.sctb_steps_per_second': 2.432, 'epoch': 11.0}
{'loss': 2.3697, 'learning_rate': 1.6666666666666667e-06, 'epoch': 11.0}
{'eval_loss': 2.3518834114074707, 'eval_accuracy@spa.rst.sctb': 0.3723404255319149, 'eval_f1@spa.rst.sctb': 0.042004485078488876, 'eval_precision@spa.rst.sctb': 0.06054729584141348, 'eval_recall@spa.rst.sctb': 0.06501547987616098, 'eval_loss@spa.rst.sctb': 2.35188364982605, 'eval_runtime': 1.5736, 'eval_samples_per_second': 59.735, 'eval_steps_per_second': 1.906, 'epoch': 11.0}
{'train@spa.rst.sctb_loss': 2.3131489753723145, 'train@spa.rst.sctb_accuracy@spa.rst.sctb': 0.357630979498861, 'train@spa.rst.sctb_f1@spa.rst.sctb': 0.02769598445926948, 'train@spa.rst.sctb_precision@spa.rst.sctb': 0.03003960467989245, 'train@spa.rst.sctb_recall@spa.rst.sctb': 0.044973118279569894, 'train@spa.rst.sctb_loss@spa.rst.sctb': 2.3131484985351562, 'train@spa.rst.sctb_runtime': 5.7238, 'train@spa.rst.sctb_samples_per_second': 76.697, 'train@spa.rst.sctb_steps_per_second': 2.446, 'epoch': 12.0}
{'loss': 2.3469, 'learning_rate': 0.0, 'epoch': 12.0}
{'eval_loss': 2.3487117290496826, 'eval_accuracy@spa.rst.sctb': 0.3723404255319149, 'eval_f1@spa.rst.sctb': 0.042004485078488876, 'eval_precision@spa.rst.sctb': 0.06054729584141348, 'eval_recall@spa.rst.sctb': 0.06501547987616098, 'eval_loss@spa.rst.sctb': 2.3487117290496826, 'eval_runtime': 1.5578, 'eval_samples_per_second': 60.343, 'eval_steps_per_second': 1.926, 'epoch': 12.0}
{'train_runtime': 222.1362, 'train_samples_per_second': 23.715, 'train_steps_per_second': 0.756, 'train_loss': 2.7989706993103027, 'epoch': 12.0}
***** train_log metrics *****
  epoch                    =       12.0
  train_loss               =      2.799
  train_runtime            = 0:03:42.13
  train_samples_per_second =     23.715
  train_steps_per_second   =      0.756
{'train@tur.pdtb.tdb_loss': 2.6242849826812744, 'train@tur.pdtb.tdb_accuracy@tur.pdtb.tdb': 0.25050999592003265, 'train@tur.pdtb.tdb_f1@tur.pdtb.tdb': 0.017665368605233437, 'train@tur.pdtb.tdb_precision@tur.pdtb.tdb': 0.032622010758606, 'train@tur.pdtb.tdb_recall@tur.pdtb.tdb': 0.04353960213837816, 'train@tur.pdtb.tdb_loss@tur.pdtb.tdb': 2.6242852210998535, 'train@tur.pdtb.tdb_runtime': 30.0774, 'train@tur.pdtb.tdb_samples_per_second': 81.49, 'train@tur.pdtb.tdb_steps_per_second': 2.56, 'epoch': 1.0}
{'loss': 3.1649, 'learning_rate': 1.8333333333333333e-05, 'epoch': 1.0}
{'eval_loss': 2.546700954437256, 'eval_accuracy@tur.pdtb.tdb': 0.266025641025641, 'eval_f1@tur.pdtb.tdb': 0.019102416570771, 'eval_precision@tur.pdtb.tdb': 0.012092074592074592, 'eval_recall@tur.pdtb.tdb': 0.045454545454545456, 'eval_loss@tur.pdtb.tdb': 2.5467007160186768, 'eval_runtime': 4.2105, 'eval_samples_per_second': 74.101, 'eval_steps_per_second': 2.375, 'epoch': 1.0}
{'train@tur.pdtb.tdb_loss': 2.4174506664276123, 'train@tur.pdtb.tdb_accuracy@tur.pdtb.tdb': 0.25050999592003265, 'train@tur.pdtb.tdb_f1@tur.pdtb.tdb': 0.018154019992650712, 'train@tur.pdtb.tdb_precision@tur.pdtb.tdb': 0.027178362312908218, 'train@tur.pdtb.tdb_recall@tur.pdtb.tdb': 0.043662284676004026, 'train@tur.pdtb.tdb_loss@tur.pdtb.tdb': 2.417450428009033, 'train@tur.pdtb.tdb_runtime': 30.0972, 'train@tur.pdtb.tdb_samples_per_second': 81.436, 'train@tur.pdtb.tdb_steps_per_second': 2.558, 'epoch': 2.0}
{'loss': 2.5281, 'learning_rate': 1.6666666666666667e-05, 'epoch': 2.0}
{'eval_loss': 2.3369669914245605, 'eval_accuracy@tur.pdtb.tdb': 0.266025641025641, 'eval_f1@tur.pdtb.tdb': 0.019102416570771, 'eval_precision@tur.pdtb.tdb': 0.012092074592074592, 'eval_recall@tur.pdtb.tdb': 0.045454545454545456, 'eval_loss@tur.pdtb.tdb': 2.3369665145874023, 'eval_runtime': 4.2234, 'eval_samples_per_second': 73.875, 'eval_steps_per_second': 2.368, 'epoch': 2.0}
{'train@tur.pdtb.tdb_loss': 2.3651373386383057, 'train@tur.pdtb.tdb_accuracy@tur.pdtb.tdb': 0.2537739698082415, 'train@tur.pdtb.tdb_f1@tur.pdtb.tdb': 0.022240251428516898, 'train@tur.pdtb.tdb_precision@tur.pdtb.tdb': 0.02543068088597211, 'train@tur.pdtb.tdb_recall@tur.pdtb.tdb': 0.04539426078175035, 'train@tur.pdtb.tdb_loss@tur.pdtb.tdb': 2.3651373386383057, 'train@tur.pdtb.tdb_runtime': 30.0825, 'train@tur.pdtb.tdb_samples_per_second': 81.476, 'train@tur.pdtb.tdb_steps_per_second': 2.56, 'epoch': 3.0}
{'loss': 2.4187, 'learning_rate': 1.5000000000000002e-05, 'epoch': 3.0}
{'eval_loss': 2.299665927886963, 'eval_accuracy@tur.pdtb.tdb': 0.2692307692307692, 'eval_f1@tur.pdtb.tdb': 0.02109356928059218, 'eval_precision@tur.pdtb.tdb': 0.03489736070381232, 'eval_recall@tur.pdtb.tdb': 0.046442687747035576, 'eval_loss@tur.pdtb.tdb': 2.299665927886963, 'eval_runtime': 4.1862, 'eval_samples_per_second': 74.531, 'eval_steps_per_second': 2.389, 'epoch': 3.0}
{'train@tur.pdtb.tdb_loss': 2.309230327606201, 'train@tur.pdtb.tdb_accuracy@tur.pdtb.tdb': 0.2778457772337821, 'train@tur.pdtb.tdb_f1@tur.pdtb.tdb': 0.03902990420006506, 'train@tur.pdtb.tdb_precision@tur.pdtb.tdb': 0.06372710891586032, 'train@tur.pdtb.tdb_recall@tur.pdtb.tdb': 0.05593693883885006, 'train@tur.pdtb.tdb_loss@tur.pdtb.tdb': 2.309230089187622, 'train@tur.pdtb.tdb_runtime': 30.0978, 'train@tur.pdtb.tdb_samples_per_second': 81.435, 'train@tur.pdtb.tdb_steps_per_second': 2.558, 'epoch': 4.0}
{'loss': 2.3674, 'learning_rate': 1.3333333333333333e-05, 'epoch': 4.0}
{'eval_loss': 2.265451192855835, 'eval_accuracy@tur.pdtb.tdb': 0.27564102564102566, 'eval_f1@tur.pdtb.tdb': 0.029526338586094422, 'eval_precision@tur.pdtb.tdb': 0.06555349412492269, 'eval_recall@tur.pdtb.tdb': 0.05027196284167905, 'eval_loss@tur.pdtb.tdb': 2.265451431274414, 'eval_runtime': 4.2147, 'eval_samples_per_second': 74.027, 'eval_steps_per_second': 2.373, 'epoch': 4.0}
{'train@tur.pdtb.tdb_loss': 2.267656087875366, 'train@tur.pdtb.tdb_accuracy@tur.pdtb.tdb': 0.3002855977152183, 'train@tur.pdtb.tdb_f1@tur.pdtb.tdb': 0.056741967596250636, 'train@tur.pdtb.tdb_precision@tur.pdtb.tdb': 0.05958704029207717, 'train@tur.pdtb.tdb_recall@tur.pdtb.tdb': 0.06987991513443147, 'train@tur.pdtb.tdb_loss@tur.pdtb.tdb': 2.2676563262939453, 'train@tur.pdtb.tdb_runtime': 30.101, 'train@tur.pdtb.tdb_samples_per_second': 81.426, 'train@tur.pdtb.tdb_steps_per_second': 2.558, 'epoch': 5.0}
{'loss': 2.3267, 'learning_rate': 1.1666666666666668e-05, 'epoch': 5.0}
{'eval_loss': 2.2309508323669434, 'eval_accuracy@tur.pdtb.tdb': 0.28846153846153844, 'eval_f1@tur.pdtb.tdb': 0.04511757602606809, 'eval_precision@tur.pdtb.tdb': 0.0604587481910902, 'eval_recall@tur.pdtb.tdb': 0.05886317493487862, 'eval_loss@tur.pdtb.tdb': 2.230950355529785, 'eval_runtime': 4.198, 'eval_samples_per_second': 74.322, 'eval_steps_per_second': 2.382, 'epoch': 5.0}
{'train@tur.pdtb.tdb_loss': 2.2270302772521973, 'train@tur.pdtb.tdb_accuracy@tur.pdtb.tdb': 0.31905344757241944, 'train@tur.pdtb.tdb_f1@tur.pdtb.tdb': 0.06567874231831049, 'train@tur.pdtb.tdb_precision@tur.pdtb.tdb': 0.10655820319540421, 'train@tur.pdtb.tdb_recall@tur.pdtb.tdb': 0.07874759516090875, 'train@tur.pdtb.tdb_loss@tur.pdtb.tdb': 2.2270302772521973, 'train@tur.pdtb.tdb_runtime': 30.1173, 'train@tur.pdtb.tdb_samples_per_second': 81.382, 'train@tur.pdtb.tdb_steps_per_second': 2.557, 'epoch': 6.0}
{'loss': 2.2919, 'learning_rate': 1e-05, 'epoch': 6.0}
{'eval_loss': 2.2040750980377197, 'eval_accuracy@tur.pdtb.tdb': 0.3076923076923077, 'eval_f1@tur.pdtb.tdb': 0.05238624636457454, 'eval_precision@tur.pdtb.tdb': 0.068951452980459, 'eval_recall@tur.pdtb.tdb': 0.06609531203944663, 'eval_loss@tur.pdtb.tdb': 2.2040748596191406, 'eval_runtime': 4.2692, 'eval_samples_per_second': 73.081, 'eval_steps_per_second': 2.342, 'epoch': 6.0}
{'train@tur.pdtb.tdb_loss': 2.1941487789154053, 'train@tur.pdtb.tdb_accuracy@tur.pdtb.tdb': 0.33700530395756834, 'train@tur.pdtb.tdb_f1@tur.pdtb.tdb': 0.09430794124856975, 'train@tur.pdtb.tdb_precision@tur.pdtb.tdb': 0.14321672841822147, 'train@tur.pdtb.tdb_recall@tur.pdtb.tdb': 0.10448862809311754, 'train@tur.pdtb.tdb_loss@tur.pdtb.tdb': 2.1941487789154053, 'train@tur.pdtb.tdb_runtime': 30.1088, 'train@tur.pdtb.tdb_samples_per_second': 81.405, 'train@tur.pdtb.tdb_steps_per_second': 2.557, 'epoch': 7.0}
{'loss': 2.2609, 'learning_rate': 8.333333333333334e-06, 'epoch': 7.0}
{'eval_loss': 2.1802308559417725, 'eval_accuracy@tur.pdtb.tdb': 0.32051282051282054, 'eval_f1@tur.pdtb.tdb': 0.09226467902729692, 'eval_precision@tur.pdtb.tdb': 0.0976247237610874, 'eval_recall@tur.pdtb.tdb': 0.10596022779796237, 'eval_loss@tur.pdtb.tdb': 2.1802308559417725, 'eval_runtime': 4.1992, 'eval_samples_per_second': 74.301, 'eval_steps_per_second': 2.381, 'epoch': 7.0}
{'train@tur.pdtb.tdb_loss': 2.1755270957946777, 'train@tur.pdtb.tdb_accuracy@tur.pdtb.tdb': 0.3402692778457772, 'train@tur.pdtb.tdb_f1@tur.pdtb.tdb': 0.09683712358848338, 'train@tur.pdtb.tdb_precision@tur.pdtb.tdb': 0.12495883964914482, 'train@tur.pdtb.tdb_recall@tur.pdtb.tdb': 0.10848991785430064, 'train@tur.pdtb.tdb_loss@tur.pdtb.tdb': 2.1755268573760986, 'train@tur.pdtb.tdb_runtime': 30.1018, 'train@tur.pdtb.tdb_samples_per_second': 81.424, 'train@tur.pdtb.tdb_steps_per_second': 2.558, 'epoch': 8.0}
{'loss': 2.2362, 'learning_rate': 6.666666666666667e-06, 'epoch': 8.0}
{'eval_loss': 2.1657702922821045, 'eval_accuracy@tur.pdtb.tdb': 0.3173076923076923, 'eval_f1@tur.pdtb.tdb': 0.09104188421596339, 'eval_precision@tur.pdtb.tdb': 0.09372696971815914, 'eval_recall@tur.pdtb.tdb': 0.10594482090183566, 'eval_loss@tur.pdtb.tdb': 2.1657705307006836, 'eval_runtime': 4.2189, 'eval_samples_per_second': 73.953, 'eval_steps_per_second': 2.37, 'epoch': 8.0}
{'train@tur.pdtb.tdb_loss': 2.1500985622406006, 'train@tur.pdtb.tdb_accuracy@tur.pdtb.tdb': 0.34720522235822115, 'train@tur.pdtb.tdb_f1@tur.pdtb.tdb': 0.09773594293240985, 'train@tur.pdtb.tdb_precision@tur.pdtb.tdb': 0.11254750371522314, 'train@tur.pdtb.tdb_recall@tur.pdtb.tdb': 0.11287862582146453, 'train@tur.pdtb.tdb_loss@tur.pdtb.tdb': 2.1500983238220215, 'train@tur.pdtb.tdb_runtime': 30.0887, 'train@tur.pdtb.tdb_samples_per_second': 81.459, 'train@tur.pdtb.tdb_steps_per_second': 2.559, 'epoch': 9.0}
{'loss': 2.2079, 'learning_rate': 5e-06, 'epoch': 9.0}
{'eval_loss': 2.146237850189209, 'eval_accuracy@tur.pdtb.tdb': 0.32371794871794873, 'eval_f1@tur.pdtb.tdb': 0.0907800692341167, 'eval_precision@tur.pdtb.tdb': 0.08980928300751408, 'eval_recall@tur.pdtb.tdb': 0.11353598537371995, 'eval_loss@tur.pdtb.tdb': 2.146238088607788, 'eval_runtime': 4.2613, 'eval_samples_per_second': 73.216, 'eval_steps_per_second': 2.347, 'epoch': 9.0}
{'train@tur.pdtb.tdb_loss': 2.1370937824249268, 'train@tur.pdtb.tdb_accuracy@tur.pdtb.tdb': 0.34312525499796004, 'train@tur.pdtb.tdb_f1@tur.pdtb.tdb': 0.09739995027532845, 'train@tur.pdtb.tdb_precision@tur.pdtb.tdb': 0.10758137972114482, 'train@tur.pdtb.tdb_recall@tur.pdtb.tdb': 0.11319163106394785, 'train@tur.pdtb.tdb_loss@tur.pdtb.tdb': 2.1370937824249268, 'train@tur.pdtb.tdb_runtime': 30.9733, 'train@tur.pdtb.tdb_samples_per_second': 79.133, 'train@tur.pdtb.tdb_steps_per_second': 2.486, 'epoch': 10.0}
{'loss': 2.1916, 'learning_rate': 3.3333333333333333e-06, 'epoch': 10.0}
{'eval_loss': 2.1355934143066406, 'eval_accuracy@tur.pdtb.tdb': 0.3301282051282051, 'eval_f1@tur.pdtb.tdb': 0.09390858638486921, 'eval_precision@tur.pdtb.tdb': 0.09302638928189588, 'eval_recall@tur.pdtb.tdb': 0.11578675392883633, 'eval_loss@tur.pdtb.tdb': 2.1355936527252197, 'eval_runtime': 4.2412, 'eval_samples_per_second': 73.565, 'eval_steps_per_second': 2.358, 'epoch': 10.0}
{'train@tur.pdtb.tdb_loss': 2.129951238632202, 'train@tur.pdtb.tdb_accuracy@tur.pdtb.tdb': 0.34353325173398613, 'train@tur.pdtb.tdb_f1@tur.pdtb.tdb': 0.09714511398788621, 'train@tur.pdtb.tdb_precision@tur.pdtb.tdb': 0.1098373295668496, 'train@tur.pdtb.tdb_recall@tur.pdtb.tdb': 0.11302772522583948, 'train@tur.pdtb.tdb_loss@tur.pdtb.tdb': 2.129951000213623, 'train@tur.pdtb.tdb_runtime': 30.1228, 'train@tur.pdtb.tdb_samples_per_second': 81.367, 'train@tur.pdtb.tdb_steps_per_second': 2.556, 'epoch': 11.0}
{'loss': 2.1863, 'learning_rate': 1.6666666666666667e-06, 'epoch': 11.0}
{'eval_loss': 2.1308038234710693, 'eval_accuracy@tur.pdtb.tdb': 0.32371794871794873, 'eval_f1@tur.pdtb.tdb': 0.0914470217984595, 'eval_precision@tur.pdtb.tdb': 0.09025437440769248, 'eval_recall@tur.pdtb.tdb': 0.11381046934385611, 'eval_loss@tur.pdtb.tdb': 2.1308035850524902, 'eval_runtime': 4.202, 'eval_samples_per_second': 74.25, 'eval_steps_per_second': 2.38, 'epoch': 11.0}
{'train@tur.pdtb.tdb_loss': 2.1268184185028076, 'train@tur.pdtb.tdb_accuracy@tur.pdtb.tdb': 0.3443492452060384, 'train@tur.pdtb.tdb_f1@tur.pdtb.tdb': 0.09727553604941183, 'train@tur.pdtb.tdb_precision@tur.pdtb.tdb': 0.10764109006767425, 'train@tur.pdtb.tdb_recall@tur.pdtb.tdb': 0.11348082912968104, 'train@tur.pdtb.tdb_loss@tur.pdtb.tdb': 2.1268184185028076, 'train@tur.pdtb.tdb_runtime': 30.0532, 'train@tur.pdtb.tdb_samples_per_second': 81.555, 'train@tur.pdtb.tdb_steps_per_second': 2.562, 'epoch': 12.0}
{'loss': 2.178, 'learning_rate': 0.0, 'epoch': 12.0}
{'eval_loss': 2.128204822540283, 'eval_accuracy@tur.pdtb.tdb': 0.3301282051282051, 'eval_f1@tur.pdtb.tdb': 0.09390910364845174, 'eval_precision@tur.pdtb.tdb': 0.09278560857508227, 'eval_recall@tur.pdtb.tdb': 0.11613551003206815, 'eval_loss@tur.pdtb.tdb': 2.128204584121704, 'eval_runtime': 4.2113, 'eval_samples_per_second': 74.086, 'eval_steps_per_second': 2.375, 'epoch': 12.0}
{'train_runtime': 1165.6025, 'train_samples_per_second': 25.233, 'train_steps_per_second': 0.793, 'train_loss': 2.3632200662191813, 'epoch': 12.0}
***** train_logger metrics *****
  epoch                    =       12.0
  train_loss               =      2.799
  train_runtime            = 0:03:42.13
  train_samples_per_second =     23.715
  train_steps_per_second   =      0.756
-------------------------------------------------------------------
Lang1:  zho.rst.sctb    Lang2:  tur.pdtb.tdb
Saving run to:  runs/full_shot/FullShot=v4_zho.rst.sctb_tur.pdtb.tdb_lrfunc=Adafactor_lr=2e-05_plm=bert-base-multilingual-cased
Running with params: BERT_MODEL=bert-base-multilingual-cased lr=2e-05
read 439 examples
read 94 examples
read 159 examples
read 2451 examples
read 312 examples
read 422 examples
Total prediction labels:  49
BertAdapterModel(
  (shared_parameters): ModuleDict()
  (bert): BertModel(
    (shared_parameters): ModuleDict()
    (invertible_adapters): ModuleDict(
      (zh): NICECouplingBlock(
        (F): Sequential(
          (0): Linear(in_features=384, out_features=192, bias=True)
          (1): Activation_Function_Class(
            (f): ReLU()
          )
          (2): Linear(in_features=192, out_features=384, bias=True)
        )
        (G): Sequential(
          (0): Linear(in_features=384, out_features=192, bias=True)
          (1): Activation_Function_Class(
            (f): ReLU()
          )
          (2): Linear(in_features=192, out_features=384, bias=True)
        )
      )
      (tr): NICECouplingBlock(
        (F): Sequential(
          (0): Linear(in_features=384, out_features=192, bias=True)
          (1): Activation_Function_Class(
            (f): NewGELUActivation()
          )
          (2): Linear(in_features=192, out_features=384, bias=True)
        )
        (G): Sequential(
          (0): Linear(in_features=384, out_features=192, bias=True)
          (1): Activation_Function_Class(
            (f): NewGELUActivation()
          )
          (2): Linear(in_features=192, out_features=384, bias=True)
        )
      )
    )
    (embeddings): BertEmbeddings(
      (word_embeddings): Embedding(119547, 768, padding_idx=0)
      (position_embeddings): Embedding(512, 768)
      (token_type_embeddings): Embedding(2, 768)
      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
      (dropout): Dropout(p=0.1, inplace=False)
    )
    (encoder): BertEncoder(
      (layer): ModuleList(
        (0): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (zh): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (tr): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (1): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (zh): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (tr): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (2): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (zh): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (tr): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (3): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (zh): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (tr): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (4): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (zh): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (tr): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (5): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (zh): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (tr): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (6): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (zh): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (tr): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (7): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (zh): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (tr): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (8): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (zh): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (tr): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (9): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (zh): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (tr): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (10): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (zh): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (tr): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
        (11): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (key): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (value): Linear(
                in_features=768, out_features=768, bias=True
                (loras): ModuleDict()
              )
              (dropout): Dropout(p=0.1, inplace=False)
              (prefix_tuning): PrefixTuningShim(
                (prefix_gates): ModuleDict()
                (pool): PrefixTuningPool(
                  (prefix_tunings): ModuleDict()
                )
              )
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
              (adapters): ModuleDict()
              (adapter_fusion_layer): ModuleDict()
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(
              in_features=768, out_features=3072, bias=True
              (loras): ModuleDict()
            )
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(
              in_features=3072, out_features=768, bias=True
              (loras): ModuleDict()
            )
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (adapters): ModuleDict(
              (zh): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (tr): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): NewGELUActivation()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=384, bias=True)
                  (1): Activation_Function_Class(
                    (f): NewGELUActivation()
                  )
                )
                (adapter_up): Linear(in_features=384, out_features=768, bias=True)
              )
              (disrpt): Adapter(
                (non_linearity): Activation_Function_Class(
                  (f): ReLU()
                )
                (adapter_down): Sequential(
                  (0): Linear(in_features=768, out_features=48, bias=True)
                  (1): Activation_Function_Class(
                    (f): ReLU()
                  )
                )
                (adapter_up): Linear(in_features=48, out_features=768, bias=True)
              )
            )
            (adapter_fusion_layer): ModuleDict()
          )
        )
      )
    )
    (pooler): BertPooler(
      (dense): Linear(in_features=768, out_features=768, bias=True)
      (activation): Tanh()
    )
    (prefix_tuning): PrefixTuningPool(
      (prefix_tunings): ModuleDict()
    )
  )
  (heads): ModuleDict(
    (tr): BertStyleMaskedLMHead(
      (0): Linear(in_features=768, out_features=768, bias=True)
      (1): Activation_Function_Class(
        (f): GELUActivation()
      )
      (2): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
      (3): Linear(in_features=768, out_features=119547, bias=True)
    )
    (disrpt): ClassificationHead(
      (0): Dropout(p=0.1, inplace=False)
      (1): Linear(in_features=768, out_features=768, bias=True)
      (2): Activation_Function_Class(
        (f): Tanh()
      )
      (3): Dropout(p=0.1, inplace=False)
      (4): Linear(in_features=768, out_features=49, bias=True)
    )
  )
)
{'train@zho.rst.sctb_loss': 3.721395254135132, 'train@zho.rst.sctb_accuracy@zho.rst.sctb': 0.3348519362186788, 'train@zho.rst.sctb_f1@zho.rst.sctb': 0.021929824561403508, 'train@zho.rst.sctb_precision@zho.rst.sctb': 0.03208082240340305, 'train@zho.rst.sctb_recall@zho.rst.sctb': 0.03964948207646579, 'train@zho.rst.sctb_loss@zho.rst.sctb': 3.7213950157165527, 'train@zho.rst.sctb_runtime': 5.5988, 'train@zho.rst.sctb_samples_per_second': 78.409, 'train@zho.rst.sctb_steps_per_second': 2.501, 'epoch': 1.0}
{'loss': 3.854, 'learning_rate': 1.8333333333333333e-05, 'epoch': 1.0}
{'eval_loss': 3.7257845401763916, 'eval_accuracy@zho.rst.sctb': 0.3404255319148936, 'eval_f1@zho.rst.sctb': 0.026733500417710943, 'eval_precision@zho.rst.sctb': 0.01791713325867861, 'eval_recall@zho.rst.sctb': 0.05263157894736842, 'eval_loss@zho.rst.sctb': 3.725785255432129, 'eval_runtime': 1.533, 'eval_samples_per_second': 61.317, 'eval_steps_per_second': 1.957, 'epoch': 1.0}
{'train@zho.rst.sctb_loss': 3.5184574127197266, 'train@zho.rst.sctb_accuracy@zho.rst.sctb': 0.3325740318906606, 'train@zho.rst.sctb_f1@zho.rst.sctb': 0.019197896120973047, 'train@zho.rst.sctb_precision@zho.rst.sctb': 0.012791308918871562, 'train@zho.rst.sctb_recall@zho.rst.sctb': 0.038461538461538464, 'train@zho.rst.sctb_loss@zho.rst.sctb': 3.5184574127197266, 'train@zho.rst.sctb_runtime': 5.6134, 'train@zho.rst.sctb_samples_per_second': 78.206, 'train@zho.rst.sctb_steps_per_second': 2.494, 'epoch': 2.0}
{'loss': 3.6317, 'learning_rate': 1.6666666666666667e-05, 'epoch': 2.0}
{'eval_loss': 3.5339677333831787, 'eval_accuracy@zho.rst.sctb': 0.3404255319148936, 'eval_f1@zho.rst.sctb': 0.026733500417710943, 'eval_precision@zho.rst.sctb': 0.01791713325867861, 'eval_recall@zho.rst.sctb': 0.05263157894736842, 'eval_loss@zho.rst.sctb': 3.5339672565460205, 'eval_runtime': 1.5437, 'eval_samples_per_second': 60.891, 'eval_steps_per_second': 1.943, 'epoch': 2.0}
{'train@zho.rst.sctb_loss': 3.3201568126678467, 'train@zho.rst.sctb_accuracy@zho.rst.sctb': 0.3325740318906606, 'train@zho.rst.sctb_f1@zho.rst.sctb': 0.019197896120973047, 'train@zho.rst.sctb_precision@zho.rst.sctb': 0.012791308918871562, 'train@zho.rst.sctb_recall@zho.rst.sctb': 0.038461538461538464, 'train@zho.rst.sctb_loss@zho.rst.sctb': 3.3201563358306885, 'train@zho.rst.sctb_runtime': 5.7205, 'train@zho.rst.sctb_samples_per_second': 76.741, 'train@zho.rst.sctb_steps_per_second': 2.447, 'epoch': 3.0}
{'loss': 3.4518, 'learning_rate': 1.5000000000000002e-05, 'epoch': 3.0}
{'eval_loss': 3.3454978466033936, 'eval_accuracy@zho.rst.sctb': 0.3404255319148936, 'eval_f1@zho.rst.sctb': 0.026733500417710943, 'eval_precision@zho.rst.sctb': 0.01791713325867861, 'eval_recall@zho.rst.sctb': 0.05263157894736842, 'eval_loss@zho.rst.sctb': 3.345498561859131, 'eval_runtime': 1.5283, 'eval_samples_per_second': 61.505, 'eval_steps_per_second': 1.963, 'epoch': 3.0}
{'train@zho.rst.sctb_loss': 3.136991262435913, 'train@zho.rst.sctb_accuracy@zho.rst.sctb': 0.3325740318906606, 'train@zho.rst.sctb_f1@zho.rst.sctb': 0.019197896120973047, 'train@zho.rst.sctb_precision@zho.rst.sctb': 0.012791308918871562, 'train@zho.rst.sctb_recall@zho.rst.sctb': 0.038461538461538464, 'train@zho.rst.sctb_loss@zho.rst.sctb': 3.136991500854492, 'train@zho.rst.sctb_runtime': 5.6441, 'train@zho.rst.sctb_samples_per_second': 77.78, 'train@zho.rst.sctb_steps_per_second': 2.48, 'epoch': 4.0}
{'loss': 3.2605, 'learning_rate': 1.3333333333333333e-05, 'epoch': 4.0}
{'eval_loss': 3.172372341156006, 'eval_accuracy@zho.rst.sctb': 0.3404255319148936, 'eval_f1@zho.rst.sctb': 0.026733500417710943, 'eval_precision@zho.rst.sctb': 0.01791713325867861, 'eval_recall@zho.rst.sctb': 0.05263157894736842, 'eval_loss@zho.rst.sctb': 3.172372341156006, 'eval_runtime': 1.5417, 'eval_samples_per_second': 60.973, 'eval_steps_per_second': 1.946, 'epoch': 4.0}
{'train@zho.rst.sctb_loss': 2.9727659225463867, 'train@zho.rst.sctb_accuracy@zho.rst.sctb': 0.3325740318906606, 'train@zho.rst.sctb_f1@zho.rst.sctb': 0.019197896120973047, 'train@zho.rst.sctb_precision@zho.rst.sctb': 0.012791308918871562, 'train@zho.rst.sctb_recall@zho.rst.sctb': 0.038461538461538464, 'train@zho.rst.sctb_loss@zho.rst.sctb': 2.972766160964966, 'train@zho.rst.sctb_runtime': 5.6511, 'train@zho.rst.sctb_samples_per_second': 77.684, 'train@zho.rst.sctb_steps_per_second': 2.477, 'epoch': 5.0}
{'loss': 3.0962, 'learning_rate': 1.1666666666666668e-05, 'epoch': 5.0}
{'eval_loss': 3.0180957317352295, 'eval_accuracy@zho.rst.sctb': 0.3404255319148936, 'eval_f1@zho.rst.sctb': 0.026733500417710943, 'eval_precision@zho.rst.sctb': 0.01791713325867861, 'eval_recall@zho.rst.sctb': 0.05263157894736842, 'eval_loss@zho.rst.sctb': 3.018096685409546, 'eval_runtime': 1.551, 'eval_samples_per_second': 60.604, 'eval_steps_per_second': 1.934, 'epoch': 5.0}
{'train@zho.rst.sctb_loss': 2.828158140182495, 'train@zho.rst.sctb_accuracy@zho.rst.sctb': 0.3325740318906606, 'train@zho.rst.sctb_f1@zho.rst.sctb': 0.019197896120973047, 'train@zho.rst.sctb_precision@zho.rst.sctb': 0.012791308918871562, 'train@zho.rst.sctb_recall@zho.rst.sctb': 0.038461538461538464, 'train@zho.rst.sctb_loss@zho.rst.sctb': 2.828158140182495, 'train@zho.rst.sctb_runtime': 5.6505, 'train@zho.rst.sctb_samples_per_second': 77.692, 'train@zho.rst.sctb_steps_per_second': 2.478, 'epoch': 6.0}
{'loss': 2.9418, 'learning_rate': 1e-05, 'epoch': 6.0}
{'eval_loss': 2.8847603797912598, 'eval_accuracy@zho.rst.sctb': 0.3404255319148936, 'eval_f1@zho.rst.sctb': 0.026733500417710943, 'eval_precision@zho.rst.sctb': 0.01791713325867861, 'eval_recall@zho.rst.sctb': 0.05263157894736842, 'eval_loss@zho.rst.sctb': 2.8847596645355225, 'eval_runtime': 1.5681, 'eval_samples_per_second': 59.946, 'eval_steps_per_second': 1.913, 'epoch': 6.0}
{'train@zho.rst.sctb_loss': 2.7084860801696777, 'train@zho.rst.sctb_accuracy@zho.rst.sctb': 0.3325740318906606, 'train@zho.rst.sctb_f1@zho.rst.sctb': 0.019263755112811715, 'train@zho.rst.sctb_precision@zho.rst.sctb': 0.012849850378454496, 'train@zho.rst.sctb_recall@zho.rst.sctb': 0.038461538461538464, 'train@zho.rst.sctb_loss@zho.rst.sctb': 2.7084858417510986, 'train@zho.rst.sctb_runtime': 5.6538, 'train@zho.rst.sctb_samples_per_second': 77.647, 'train@zho.rst.sctb_steps_per_second': 2.476, 'epoch': 7.0}
{'loss': 2.8179, 'learning_rate': 8.333333333333334e-06, 'epoch': 7.0}
{'eval_loss': 2.776366949081421, 'eval_accuracy@zho.rst.sctb': 0.3404255319148936, 'eval_f1@zho.rst.sctb': 0.026733500417710943, 'eval_precision@zho.rst.sctb': 0.01791713325867861, 'eval_recall@zho.rst.sctb': 0.05263157894736842, 'eval_loss@zho.rst.sctb': 2.7763664722442627, 'eval_runtime': 1.5528, 'eval_samples_per_second': 60.536, 'eval_steps_per_second': 1.932, 'epoch': 7.0}
{'train@zho.rst.sctb_loss': 2.619616746902466, 'train@zho.rst.sctb_accuracy@zho.rst.sctb': 0.3325740318906606, 'train@zho.rst.sctb_f1@zho.rst.sctb': 0.019263755112811715, 'train@zho.rst.sctb_precision@zho.rst.sctb': 0.012849850378454496, 'train@zho.rst.sctb_recall@zho.rst.sctb': 0.038461538461538464, 'train@zho.rst.sctb_loss@zho.rst.sctb': 2.619616746902466, 'train@zho.rst.sctb_runtime': 5.6608, 'train@zho.rst.sctb_samples_per_second': 77.551, 'train@zho.rst.sctb_steps_per_second': 2.473, 'epoch': 8.0}
{'loss': 2.704, 'learning_rate': 6.666666666666667e-06, 'epoch': 8.0}
{'eval_loss': 2.6973154544830322, 'eval_accuracy@zho.rst.sctb': 0.3404255319148936, 'eval_f1@zho.rst.sctb': 0.026733500417710943, 'eval_precision@zho.rst.sctb': 0.01791713325867861, 'eval_recall@zho.rst.sctb': 0.05263157894736842, 'eval_loss@zho.rst.sctb': 2.6973156929016113, 'eval_runtime': 1.5587, 'eval_samples_per_second': 60.308, 'eval_steps_per_second': 1.925, 'epoch': 8.0}
{'train@zho.rst.sctb_loss': 2.555924415588379, 'train@zho.rst.sctb_accuracy@zho.rst.sctb': 0.33712984054669703, 'train@zho.rst.sctb_f1@zho.rst.sctb': 0.020884069076840164, 'train@zho.rst.sctb_precision@zho.rst.sctb': 0.0321396993810787, 'train@zho.rst.sctb_recall@zho.rst.sctb': 0.039271255060728746, 'train@zho.rst.sctb_loss@zho.rst.sctb': 2.555924415588379, 'train@zho.rst.sctb_runtime': 5.6565, 'train@zho.rst.sctb_samples_per_second': 77.61, 'train@zho.rst.sctb_steps_per_second': 2.475, 'epoch': 9.0}
{'loss': 2.6305, 'learning_rate': 5e-06, 'epoch': 9.0}
{'eval_loss': 2.6408743858337402, 'eval_accuracy@zho.rst.sctb': 0.3404255319148936, 'eval_f1@zho.rst.sctb': 0.026733500417710943, 'eval_precision@zho.rst.sctb': 0.01791713325867861, 'eval_recall@zho.rst.sctb': 0.05263157894736842, 'eval_loss@zho.rst.sctb': 2.6408743858337402, 'eval_runtime': 1.5659, 'eval_samples_per_second': 60.031, 'eval_steps_per_second': 1.916, 'epoch': 9.0}
{'train@zho.rst.sctb_loss': 2.5143284797668457, 'train@zho.rst.sctb_accuracy@zho.rst.sctb': 0.35079726651480636, 'train@zho.rst.sctb_f1@zho.rst.sctb': 0.025392578436056697, 'train@zho.rst.sctb_precision@zho.rst.sctb': 0.043858705397166936, 'train@zho.rst.sctb_recall@zho.rst.sctb': 0.041700404858299595, 'train@zho.rst.sctb_loss@zho.rst.sctb': 2.514328956604004, 'train@zho.rst.sctb_runtime': 5.66, 'train@zho.rst.sctb_samples_per_second': 77.562, 'train@zho.rst.sctb_steps_per_second': 2.473, 'epoch': 10.0}
{'loss': 2.5761, 'learning_rate': 3.3333333333333333e-06, 'epoch': 10.0}
{'eval_loss': 2.6045985221862793, 'eval_accuracy@zho.rst.sctb': 0.32978723404255317, 'eval_f1@zho.rst.sctb': 0.026105263157894736, 'eval_precision@zho.rst.sctb': 0.017543859649122806, 'eval_recall@zho.rst.sctb': 0.05098684210526316, 'eval_loss@zho.rst.sctb': 2.604598045349121, 'eval_runtime': 1.5566, 'eval_samples_per_second': 60.386, 'eval_steps_per_second': 1.927, 'epoch': 10.0}
{'train@zho.rst.sctb_loss': 2.4923648834228516, 'train@zho.rst.sctb_accuracy@zho.rst.sctb': 0.3553530751708428, 'train@zho.rst.sctb_f1@zho.rst.sctb': 0.02678901891115034, 'train@zho.rst.sctb_precision@zho.rst.sctb': 0.045202065693868976, 'train@zho.rst.sctb_recall@zho.rst.sctb': 0.04251012145748988, 'train@zho.rst.sctb_loss@zho.rst.sctb': 2.4923648834228516, 'train@zho.rst.sctb_runtime': 5.6608, 'train@zho.rst.sctb_samples_per_second': 77.551, 'train@zho.rst.sctb_steps_per_second': 2.473, 'epoch': 11.0}
{'loss': 2.5467, 'learning_rate': 1.6666666666666667e-06, 'epoch': 11.0}
{'eval_loss': 2.5856194496154785, 'eval_accuracy@zho.rst.sctb': 0.32978723404255317, 'eval_f1@zho.rst.sctb': 0.03093709884467266, 'eval_precision@zho.rst.sctb': 0.03489492963177174, 'eval_recall@zho.rst.sctb': 0.05243808049535604, 'eval_loss@zho.rst.sctb': 2.5856199264526367, 'eval_runtime': 1.5583, 'eval_samples_per_second': 60.32, 'eval_steps_per_second': 1.925, 'epoch': 11.0}
{'train@zho.rst.sctb_loss': 2.4851298332214355, 'train@zho.rst.sctb_accuracy@zho.rst.sctb': 0.357630979498861, 'train@zho.rst.sctb_f1@zho.rst.sctb': 0.027395411605937923, 'train@zho.rst.sctb_precision@zho.rst.sctb': 0.041448959845186255, 'train@zho.rst.sctb_recall@zho.rst.sctb': 0.04291497975708502, 'train@zho.rst.sctb_loss@zho.rst.sctb': 2.4851303100585938, 'train@zho.rst.sctb_runtime': 5.6638, 'train@zho.rst.sctb_samples_per_second': 77.51, 'train@zho.rst.sctb_steps_per_second': 2.472, 'epoch': 12.0}
{'loss': 2.5275, 'learning_rate': 0.0, 'epoch': 12.0}
{'eval_loss': 2.579353094100952, 'eval_accuracy@zho.rst.sctb': 0.3191489361702128, 'eval_f1@zho.rst.sctb': 0.030034101647561526, 'eval_precision@zho.rst.sctb': 0.030116959064327483, 'eval_recall@zho.rst.sctb': 0.05079334365325078, 'eval_loss@zho.rst.sctb': 2.5793535709381104, 'eval_runtime': 1.5561, 'eval_samples_per_second': 60.406, 'eval_steps_per_second': 1.928, 'epoch': 12.0}
{'train_runtime': 218.591, 'train_samples_per_second': 24.1, 'train_steps_per_second': 0.769, 'train_loss': 3.0032310485839844, 'epoch': 12.0}
***** train_log metrics *****
  epoch                    =       12.0
  train_loss               =     3.0032
  train_runtime            = 0:03:38.59
  train_samples_per_second =       24.1
  train_steps_per_second   =      0.769
{'train@tur.pdtb.tdb_loss': 2.6512675285339355, 'train@tur.pdtb.tdb_accuracy@tur.pdtb.tdb': 0.25091799265605874, 'train@tur.pdtb.tdb_f1@tur.pdtb.tdb': 0.018886258876292514, 'train@tur.pdtb.tdb_precision@tur.pdtb.tdb': 0.023935399638724448, 'train@tur.pdtb.tdb_recall@tur.pdtb.tdb': 0.043917119982230406, 'train@tur.pdtb.tdb_loss@tur.pdtb.tdb': 2.6512675285339355, 'train@tur.pdtb.tdb_runtime': 30.121, 'train@tur.pdtb.tdb_samples_per_second': 81.372, 'train@tur.pdtb.tdb_steps_per_second': 2.556, 'epoch': 1.0}
{'loss': 3.186, 'learning_rate': 1.8333333333333333e-05, 'epoch': 1.0}
{'eval_loss': 2.5661208629608154, 'eval_accuracy@tur.pdtb.tdb': 0.2692307692307692, 'eval_f1@tur.pdtb.tdb': 0.021085135838348926, 'eval_precision@tur.pdtb.tdb': 0.05758550131540485, 'eval_recall@tur.pdtb.tdb': 0.046442687747035576, 'eval_loss@tur.pdtb.tdb': 2.5661203861236572, 'eval_runtime': 4.2128, 'eval_samples_per_second': 74.06, 'eval_steps_per_second': 2.374, 'epoch': 1.0}
{'train@tur.pdtb.tdb_loss': 2.4066731929779053, 'train@tur.pdtb.tdb_accuracy@tur.pdtb.tdb': 0.25091799265605874, 'train@tur.pdtb.tdb_f1@tur.pdtb.tdb': 0.018185871282581086, 'train@tur.pdtb.tdb_precision@tur.pdtb.tdb': 0.025389025389025387, 'train@tur.pdtb.tdb_recall@tur.pdtb.tdb': 0.043733096175791596, 'train@tur.pdtb.tdb_loss@tur.pdtb.tdb': 2.4066734313964844, 'train@tur.pdtb.tdb_runtime': 30.1054, 'train@tur.pdtb.tdb_samples_per_second': 81.414, 'train@tur.pdtb.tdb_steps_per_second': 2.558, 'epoch': 2.0}
{'loss': 2.5253, 'learning_rate': 1.6666666666666667e-05, 'epoch': 2.0}
{'eval_loss': 2.3175909519195557, 'eval_accuracy@tur.pdtb.tdb': 0.266025641025641, 'eval_f1@tur.pdtb.tdb': 0.019102416570771, 'eval_precision@tur.pdtb.tdb': 0.012092074592074592, 'eval_recall@tur.pdtb.tdb': 0.045454545454545456, 'eval_loss@tur.pdtb.tdb': 2.3175909519195557, 'eval_runtime': 4.2312, 'eval_samples_per_second': 73.739, 'eval_steps_per_second': 2.363, 'epoch': 2.0}
{'train@tur.pdtb.tdb_loss': 2.3390791416168213, 'train@tur.pdtb.tdb_accuracy@tur.pdtb.tdb': 0.25703794369645044, 'train@tur.pdtb.tdb_f1@tur.pdtb.tdb': 0.02655476466102185, 'train@tur.pdtb.tdb_precision@tur.pdtb.tdb': 0.07663512644388609, 'train@tur.pdtb.tdb_recall@tur.pdtb.tdb': 0.047349811330245836, 'train@tur.pdtb.tdb_loss@tur.pdtb.tdb': 2.3390791416168213, 'train@tur.pdtb.tdb_runtime': 30.1229, 'train@tur.pdtb.tdb_samples_per_second': 81.367, 'train@tur.pdtb.tdb_steps_per_second': 2.556, 'epoch': 3.0}
{'loss': 2.4061, 'learning_rate': 1.5000000000000002e-05, 'epoch': 3.0}
{'eval_loss': 2.2756760120391846, 'eval_accuracy@tur.pdtb.tdb': 0.2724358974358974, 'eval_f1@tur.pdtb.tdb': 0.025204933394430272, 'eval_precision@tur.pdtb.tdb': 0.0576351752822341, 'eval_recall@tur.pdtb.tdb': 0.04814581117619357, 'eval_loss@tur.pdtb.tdb': 2.2756755352020264, 'eval_runtime': 4.2782, 'eval_samples_per_second': 72.928, 'eval_steps_per_second': 2.337, 'epoch': 3.0}
{'train@tur.pdtb.tdb_loss': 2.2761332988739014, 'train@tur.pdtb.tdb_accuracy@tur.pdtb.tdb': 0.29457364341085274, 'train@tur.pdtb.tdb_f1@tur.pdtb.tdb': 0.048816654958897907, 'train@tur.pdtb.tdb_precision@tur.pdtb.tdb': 0.06771064343463966, 'train@tur.pdtb.tdb_recall@tur.pdtb.tdb': 0.06590511750988517, 'train@tur.pdtb.tdb_loss@tur.pdtb.tdb': 2.2761332988739014, 'train@tur.pdtb.tdb_runtime': 30.1461, 'train@tur.pdtb.tdb_samples_per_second': 81.304, 'train@tur.pdtb.tdb_steps_per_second': 2.554, 'epoch': 4.0}
{'loss': 2.3409, 'learning_rate': 1.3333333333333333e-05, 'epoch': 4.0}
{'eval_loss': 2.2415177822113037, 'eval_accuracy@tur.pdtb.tdb': 0.27564102564102566, 'eval_f1@tur.pdtb.tdb': 0.033300214559229886, 'eval_precision@tur.pdtb.tdb': 0.06005620723362659, 'eval_recall@tur.pdtb.tdb': 0.05263972265096968, 'eval_loss@tur.pdtb.tdb': 2.2415173053741455, 'eval_runtime': 4.2123, 'eval_samples_per_second': 74.069, 'eval_steps_per_second': 2.374, 'epoch': 4.0}
{'train@tur.pdtb.tdb_loss': 2.2296903133392334, 'train@tur.pdtb.tdb_accuracy@tur.pdtb.tdb': 0.32966136270909835, 'train@tur.pdtb.tdb_f1@tur.pdtb.tdb': 0.09021890369629991, 'train@tur.pdtb.tdb_precision@tur.pdtb.tdb': 0.10508630859427144, 'train@tur.pdtb.tdb_recall@tur.pdtb.tdb': 0.09763900549537825, 'train@tur.pdtb.tdb_loss@tur.pdtb.tdb': 2.2296903133392334, 'train@tur.pdtb.tdb_runtime': 30.1313, 'train@tur.pdtb.tdb_samples_per_second': 81.344, 'train@tur.pdtb.tdb_steps_per_second': 2.555, 'epoch': 5.0}
{'loss': 2.2947, 'learning_rate': 1.1666666666666668e-05, 'epoch': 5.0}
{'eval_loss': 2.2051444053649902, 'eval_accuracy@tur.pdtb.tdb': 0.3173076923076923, 'eval_f1@tur.pdtb.tdb': 0.08913594145792288, 'eval_precision@tur.pdtb.tdb': 0.1162599504845494, 'eval_recall@tur.pdtb.tdb': 0.09535339684709543, 'eval_loss@tur.pdtb.tdb': 2.2051444053649902, 'eval_runtime': 4.2133, 'eval_samples_per_second': 74.052, 'eval_steps_per_second': 2.373, 'epoch': 5.0}
{'train@tur.pdtb.tdb_loss': 2.1873414516448975, 'train@tur.pdtb.tdb_accuracy@tur.pdtb.tdb': 0.34353325173398613, 'train@tur.pdtb.tdb_f1@tur.pdtb.tdb': 0.09625663476362893, 'train@tur.pdtb.tdb_precision@tur.pdtb.tdb': 0.09803674248907682, 'train@tur.pdtb.tdb_recall@tur.pdtb.tdb': 0.11049124117400608, 'train@tur.pdtb.tdb_loss@tur.pdtb.tdb': 2.1873412132263184, 'train@tur.pdtb.tdb_runtime': 30.1446, 'train@tur.pdtb.tdb_samples_per_second': 81.308, 'train@tur.pdtb.tdb_steps_per_second': 2.554, 'epoch': 6.0}
{'loss': 2.2533, 'learning_rate': 1e-05, 'epoch': 6.0}
{'eval_loss': 2.179443836212158, 'eval_accuracy@tur.pdtb.tdb': 0.3141025641025641, 'eval_f1@tur.pdtb.tdb': 0.08611567048374144, 'eval_precision@tur.pdtb.tdb': 0.09249692562500446, 'eval_recall@tur.pdtb.tdb': 0.10891877982362684, 'eval_loss@tur.pdtb.tdb': 2.1794440746307373, 'eval_runtime': 4.2308, 'eval_samples_per_second': 73.745, 'eval_steps_per_second': 2.364, 'epoch': 6.0}
{'train@tur.pdtb.tdb_loss': 2.1544809341430664, 'train@tur.pdtb.tdb_accuracy@tur.pdtb.tdb': 0.34761321909424725, 'train@tur.pdtb.tdb_f1@tur.pdtb.tdb': 0.09761970244153964, 'train@tur.pdtb.tdb_precision@tur.pdtb.tdb': 0.1067331987524878, 'train@tur.pdtb.tdb_recall@tur.pdtb.tdb': 0.11531985769253354, 'train@tur.pdtb.tdb_loss@tur.pdtb.tdb': 2.1544811725616455, 'train@tur.pdtb.tdb_runtime': 30.14, 'train@tur.pdtb.tdb_samples_per_second': 81.321, 'train@tur.pdtb.tdb_steps_per_second': 2.555, 'epoch': 7.0}
{'loss': 2.2124, 'learning_rate': 8.333333333333334e-06, 'epoch': 7.0}
{'eval_loss': 2.1598634719848633, 'eval_accuracy@tur.pdtb.tdb': 0.3173076923076923, 'eval_f1@tur.pdtb.tdb': 0.08326919000434735, 'eval_precision@tur.pdtb.tdb': 0.07858693840234134, 'eval_recall@tur.pdtb.tdb': 0.1110449314891123, 'eval_loss@tur.pdtb.tdb': 2.1598634719848633, 'eval_runtime': 4.2134, 'eval_samples_per_second': 74.05, 'eval_steps_per_second': 2.373, 'epoch': 7.0}
{'train@tur.pdtb.tdb_loss': 2.1342554092407227, 'train@tur.pdtb.tdb_accuracy@tur.pdtb.tdb': 0.35454916360669114, 'train@tur.pdtb.tdb_f1@tur.pdtb.tdb': 0.10229802743387184, 'train@tur.pdtb.tdb_precision@tur.pdtb.tdb': 0.11114449672713296, 'train@tur.pdtb.tdb_recall@tur.pdtb.tdb': 0.11976628840426834, 'train@tur.pdtb.tdb_loss@tur.pdtb.tdb': 2.1342556476593018, 'train@tur.pdtb.tdb_runtime': 30.1254, 'train@tur.pdtb.tdb_samples_per_second': 81.36, 'train@tur.pdtb.tdb_steps_per_second': 2.556, 'epoch': 8.0}
{'loss': 2.1924, 'learning_rate': 6.666666666666667e-06, 'epoch': 8.0}
{'eval_loss': 2.1412293910980225, 'eval_accuracy@tur.pdtb.tdb': 0.32051282051282054, 'eval_f1@tur.pdtb.tdb': 0.08661192165550062, 'eval_precision@tur.pdtb.tdb': 0.08104752280888644, 'eval_recall@tur.pdtb.tdb': 0.1125653120114341, 'eval_loss@tur.pdtb.tdb': 2.1412293910980225, 'eval_runtime': 4.202, 'eval_samples_per_second': 74.251, 'eval_steps_per_second': 2.38, 'epoch': 8.0}
{'train@tur.pdtb.tdb_loss': 2.11069393157959, 'train@tur.pdtb.tdb_accuracy@tur.pdtb.tdb': 0.3557731538147695, 'train@tur.pdtb.tdb_f1@tur.pdtb.tdb': 0.10115454284419279, 'train@tur.pdtb.tdb_precision@tur.pdtb.tdb': 0.11418690572750148, 'train@tur.pdtb.tdb_recall@tur.pdtb.tdb': 0.12026536820641265, 'train@tur.pdtb.tdb_loss@tur.pdtb.tdb': 2.11069393157959, 'train@tur.pdtb.tdb_runtime': 30.0556, 'train@tur.pdtb.tdb_samples_per_second': 81.549, 'train@tur.pdtb.tdb_steps_per_second': 2.562, 'epoch': 9.0}
{'loss': 2.1647, 'learning_rate': 5e-06, 'epoch': 9.0}
{'eval_loss': 2.1259679794311523, 'eval_accuracy@tur.pdtb.tdb': 0.3301282051282051, 'eval_f1@tur.pdtb.tdb': 0.08819433777655702, 'eval_precision@tur.pdtb.tdb': 0.08441460055096418, 'eval_recall@tur.pdtb.tdb': 0.11604376896876821, 'eval_loss@tur.pdtb.tdb': 2.1259684562683105, 'eval_runtime': 4.2264, 'eval_samples_per_second': 73.822, 'eval_steps_per_second': 2.366, 'epoch': 9.0}
{'train@tur.pdtb.tdb_loss': 2.0979819297790527, 'train@tur.pdtb.tdb_accuracy@tur.pdtb.tdb': 0.35903712770297835, 'train@tur.pdtb.tdb_f1@tur.pdtb.tdb': 0.10243025513572793, 'train@tur.pdtb.tdb_precision@tur.pdtb.tdb': 0.11226013187278534, 'train@tur.pdtb.tdb_recall@tur.pdtb.tdb': 0.12152607487135825, 'train@tur.pdtb.tdb_loss@tur.pdtb.tdb': 2.0979816913604736, 'train@tur.pdtb.tdb_runtime': 30.0574, 'train@tur.pdtb.tdb_samples_per_second': 81.544, 'train@tur.pdtb.tdb_steps_per_second': 2.562, 'epoch': 10.0}
{'loss': 2.1481, 'learning_rate': 3.3333333333333333e-06, 'epoch': 10.0}
{'eval_loss': 2.115567922592163, 'eval_accuracy@tur.pdtb.tdb': 0.32051282051282054, 'eval_f1@tur.pdtb.tdb': 0.08584664858316882, 'eval_precision@tur.pdtb.tdb': 0.08101010101010099, 'eval_recall@tur.pdtb.tdb': 0.11247357094813416, 'eval_loss@tur.pdtb.tdb': 2.115567684173584, 'eval_runtime': 4.2245, 'eval_samples_per_second': 73.854, 'eval_steps_per_second': 2.367, 'epoch': 10.0}
{'train@tur.pdtb.tdb_loss': 2.0913009643554688, 'train@tur.pdtb.tdb_accuracy@tur.pdtb.tdb': 0.35658914728682173, 'train@tur.pdtb.tdb_f1@tur.pdtb.tdb': 0.10179347037784861, 'train@tur.pdtb.tdb_precision@tur.pdtb.tdb': 0.1160997863584337, 'train@tur.pdtb.tdb_recall@tur.pdtb.tdb': 0.12059799321976473, 'train@tur.pdtb.tdb_loss@tur.pdtb.tdb': 2.0913007259368896, 'train@tur.pdtb.tdb_runtime': 30.0946, 'train@tur.pdtb.tdb_samples_per_second': 81.443, 'train@tur.pdtb.tdb_steps_per_second': 2.559, 'epoch': 11.0}
{'loss': 2.1461, 'learning_rate': 1.6666666666666667e-06, 'epoch': 11.0}
{'eval_loss': 2.11118221282959, 'eval_accuracy@tur.pdtb.tdb': 0.3173076923076923, 'eval_f1@tur.pdtb.tdb': 0.08467152119099502, 'eval_precision@tur.pdtb.tdb': 0.07946892008345954, 'eval_recall@tur.pdtb.tdb': 0.11148542865564405, 'eval_loss@tur.pdtb.tdb': 2.1111819744110107, 'eval_runtime': 4.2109, 'eval_samples_per_second': 74.093, 'eval_steps_per_second': 2.375, 'epoch': 11.0}
{'train@tur.pdtb.tdb_loss': 2.0886073112487793, 'train@tur.pdtb.tdb_accuracy@tur.pdtb.tdb': 0.35862913096695226, 'train@tur.pdtb.tdb_f1@tur.pdtb.tdb': 0.10267601589040011, 'train@tur.pdtb.tdb_precision@tur.pdtb.tdb': 0.11264288064300422, 'train@tur.pdtb.tdb_recall@tur.pdtb.tdb': 0.12138006662780221, 'train@tur.pdtb.tdb_loss@tur.pdtb.tdb': 2.0886075496673584, 'train@tur.pdtb.tdb_runtime': 30.1047, 'train@tur.pdtb.tdb_samples_per_second': 81.416, 'train@tur.pdtb.tdb_steps_per_second': 2.558, 'epoch': 12.0}
{'loss': 2.136, 'learning_rate': 0.0, 'epoch': 12.0}
{'eval_loss': 2.1089870929718018, 'eval_accuracy@tur.pdtb.tdb': 0.3173076923076923, 'eval_f1@tur.pdtb.tdb': 0.08452699413532712, 'eval_precision@tur.pdtb.tdb': 0.07907361628698828, 'eval_recall@tur.pdtb.tdb': 0.11148542865564405, 'eval_loss@tur.pdtb.tdb': 2.1089868545532227, 'eval_runtime': 4.2287, 'eval_samples_per_second': 73.782, 'eval_steps_per_second': 2.365, 'epoch': 12.0}
{'train_runtime': 1164.991, 'train_samples_per_second': 25.247, 'train_steps_per_second': 0.793, 'train_loss': 2.333840110085227, 'epoch': 12.0}
***** train_logger metrics *****
  epoch                    =       12.0
  train_loss               =     3.0032
  train_runtime            = 0:03:38.59
  train_samples_per_second =       24.1
  train_steps_per_second   =      0.769
